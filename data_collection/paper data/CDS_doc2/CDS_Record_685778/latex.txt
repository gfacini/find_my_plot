ATLAS Internal Note

DAQ-NO-058

Version 1.1

19 November 1996

**The Data Acquisition System**

**of the ATLAS Combined Testbeam**

**April 1996**

M. Caprini, M. Cobal, P.Y. Duval, I. Efthymiopoulos, P. Fassnacht, R. Jones, P. Keener, V. Kozlov, A. Levasuu, B. Lund-Jensen, C. Maidantchik, L. Mapelli, M. Michelotto, G. Mornacchi, M. Niculescu, A. Patel, J.A. Perlas, G. Polesello, D. Prigent, Z. Qian, A. Rios, J. Rochez, A. Romaniouk, R. Spiwoks, T. Wildish

## 1 Introduction

The aim of this document is to describe the data acquisition system of the ATLAS combined testbeam in April 1996. An overview of the system and its components will be given as well as a report of its performance.

## 2 Testbeam Setup

A combined test of the TRT [1], LArg [2] and TileCal [3] prototypes of the ATLAS detector was done in April 1996 for 20 days. The detectors were exposed in the H8 area of the SPS to a variety of beams of different particles (electrons, pions, muons) from 10 up to 300 GeV/c.

In figure 1 a schematic view of the setup is shown. In order to identify the particle trajectories in front of the TRT detector a set of 3 silicon strip detectors was installed, read out together with the TRT channels. Both, the silicon and the TRT detectors, amount to about 20% of a radiation length.

The LArg electromagnetic calorimeter prototype was moved by 1.8 m towards the back of the RD3 cryostat in order to minimize the dead material in front of the TileCal calorimeter. As a consequence the cells were no longer projective. To reduce the liquid Argon gap in front of the electromagnetic calorimeter an excluder made of Rohacell was installed. The total amount of dead material was slightly larger than 1 X\({}_{0}\). A presampler module fixed in front of the electromagnetic calorimeter and centred on the beam impact point was used to recover the energy lost by early showers.

The LArg electromagnetic calorimeter and the TileCal hadron calorimeter prototypes were put together in a setup as close as possible to the final ATLAS configuration, with a distance between the active parts of the two calorimeters of about 55 cm (0.27 \(\lambda\), almost a factor 2 larger than the ATLAS value). Due to the mechanical constrains of the prototypes the calorimeters were put in a fixed position, with the beam entering at an angle of 11.3\({}^{\circ}\) at \(\eta\) = 0.44, with the total active thickness of the two calorimeters being about 10.3 \(\lambda\).

To define the beam direction 3 x-y wire chambers were used, one put in front of the TRT detector upstream from the magnet, and the other two between the TRT and LArg calorimeter for better positioning even when the magnet was on. The physics trigger was given by "finger" scintillators put in coincidence thus defining a beam spot of about 3\(\times\)3cm\({}^{2}\).

### Transition Radiation Tracker

The TRT sector prototype represents a slice of the ATLAS end-cap TRT at \(\eta\) = 1.0. It consists of 5 sectors containing 256 straws each. The sectors are equipped with LHC-type electronics operating at a frequency of 40 MHz. The front-end parts of the electronics have analog chips (amplifier-shaper-discriminator), digital chips and readout controller chips with drift time measurement function, pipelines and derandomizer. The information from two sectors (512 straws) is going to one Local Logic board and from there via one HIPPI interfaces to a VME crate. There are in addition 3 Silicon strip detectors in front of the TRT which have 600 strips of 50 \(\mu\)m in each projection (x and y).

### Liquid Argon Calorimeter

The LArg electromagnetic calorimeter is a sampling calorimeter with steel plated lead absorbers folded to an accordion shape, separated by 3.8 mm gaps filled with liquid argon. It is segmented in depth in three compartments with radiation lengths of 9, 9 and 7 X\({}_{0}\) at \(\eta\)=0, respectively. The cells measure 0.018\(\times\)0.02 in \(\eta\)\(\times\)\(\phi\) (0.036\(\times\)0.02 for the last compartment).

Figure 1: Schematic Overview of the Testbeam Setup

A square area of two modules (in \(\phi\)) and two sectors (in \(\eta\)) was equipped with three different front-end electronics (warm 0T, cold Si and cold GaAs). Some of these channels were equipped with bi-gain preamplifiers and shapers. The readout scheme is the standard track-and-hold and ADC one. Calibration and pedestal data have been taken regularly every 3 to 4 hours for all the channels.

### Tile Calorimeter

The TileCal barrel hadron calorimeter is a sampling calorimeter with steel as the absorber and scintillating tiles as the active material. The tiles are oriented in planes perpendicular to the colliding beams and staggered in depth. The iron to scintillator ratio is about 4.67:1. Each scintillating tile is read out by two wavelength shifting fibres along the radial direction. All fibres from the one side of each cell are grouped together in bundles and read out by one photomultiplier. Each of the five prototype modules put in the test beam covers 2\(\pi\)/ 64 in azimuth and extends 1 m long in z-direction. In the radial directions the modules are 1.8 m long (about 9 \(\lambda\)), divided in four segments corresponding to 1.5, 2, 2.5 and 3 \(\lambda\) at \(\eta=0\). The five modules were stacked together giving a total front face coverage of about 100\(\times\)100 cm\({}^{2}\).

### Physics Program

During the three-weeks period, a variety of beams starting from 300 down to 10 GeV/c was used. Particles of all available types (electrons, pions, muons) were taken at each energy point. The main goals of the run were:

* test the combined performance of the two calorimeters, using mainly pions;
* test the combined performance of the TRT detector and electromagnetic calorimeter with and without magnetic field, using mainly electrons and photons.

The three detector prototypes performed successfully for the full period of combined data taking. From the online results only without applying any correction, it seems that good results can be achieved. A detailed offline analysis work is in progress and the first results will come out by the end of the year. Although this run is a repetition of the one done in September 1994 its success is rather important for the understanding of the combined performance of the ATLAS detectors, giving the possibility to create and test important tools (like the combined energy reconstruction) to be used later in the ATLAS detector. Another combined run is planned to be repeated later, during the construction and testing of the real ATLAS detector modules.

## 3 Data Acquisition System

The data acquisition system is based on the RD13 data flow protocol [4] and adapted to the specific needs of the testbeam [5]. A sketch of the system is shown in figure 2. The readout of the detectors is based on CAMAC and HIPPI. The data of each detector are collected in a readout crate controlled by a local DAQ processor. HP workstations are used for monitoring of the data in the readout crate. The event building is performed by a processor in a central VME crate collecting the data via the VIC bus. The recording processor writes the data first to a hard disk and then via ethernet to a central service for recording on tape. All modules are connected to ethernet and controlled by SUN workstations.

### Trigger and Busy Logic

The common trigger is based on the coincidence of four scintillators (S1 to S4). It also includes a random trigger for the calorimeter pedestal measurements. In a calibration run each detector generates its own calibration trigger. The "OR" of all trigger signals is sent to all detectors plus a level to distinguish between physics and random trigger.

Triggers are following the burst structure of the beam at H8. This consists of the SPS cycle of 14 s with an extraction of beam for 2.4 s. Within this time physics triggers can be

Figure 2: **Overview of the April 1996 Testbeam Data Acquisition System**

expected. The random triggers arrive also in the periods 1 s before and 1 s after the burst. This is called an "extended burst". In calibration runs only the time outside the physics burst is used.

The busy logic is based on the ideas presented in [6] and is implemented using the CPLEAR busy logic modules [7]. The central busy logic is connected to the detectors through a CORBO module [8] in each readout crate and to the event building processor through another CORBO module in the event building crate. Signals exchanged are the Start-of-Burst (SoB), the trigger and the End-of-Burst (EoB). Each of these signals has a corresponding busy signal. The event building process will receive a signal after the last detector has released its individual busy signal. Only after this one has released its busy signal a new trigger can be accepted (for details see appendix A).

### Readout

The readout is based on CAMAC [9] and HIPPI [10] (for TRT). The data are collected in a VME crate by the local DAQ processor and written into its local memory. This processor is a RAID [11] running EP/LX, a real-time UNIX operating system. On this processor runs the local DAQ process [12] which has a common skeleton and detector specific parts for the readout. The readout part is parametrized and the description stored in the detector parameter and run control data base. Once the data are written to the local memory the busy signal is released. The event handling is based on a common event format and an event buffer management [13].

The number of channels to be read out by each detector and the resulting event size are summarised in table 1. The TRT also reads out the 3 Silicon detectors while the LArg detector readout also includes the scintillators and the beam chambers.

\begin{table}
\begin{tabular}{|l|l|c|c|} \hline \multicolumn{2}{|c|}{Subdetector} & \multicolumn{1}{c|}{\# Channels} & \multicolumn{1}{c|}{Event Size} \\ \hline \hline TRT & Prototype Modules & 1280 & \\ \cline{2-4}  & Silicon Detectors & 3600 & \\ \cline{2-4}  & total & & 15.2 \\ \hline LArg & Prototype Modules & 2496 & \\ \cline{2-4}  & Scintillators \& Beam Chambers & 60 & \\ \cline{2-4}  & total & & 5.8 \\ \hline TileCal & Prototype Modules & 250 & 2.2 \\ \hline total & & 6686 & 23.2 \\ \hline \end{tabular}
\end{table}
Table 1: Number of Channels and Event Size

### Event Building

The event building is done through the VIC bus which links all the readout VME crates with the central VME crate using a VIC interface [14]. A RAID processor runs the RD13 event building process [15] which implements the event building functionality in software. At each event this process increments an event counter and releases the busy signal which enables taking the next event. At the end of a burst it reads the event fragments from the detectors via VIC bus using chained DMA and writes full events into its local buffer.

### Recording

The recording processor, which is an HP VME board, picks up the full events from the event building processor by reading them via the VME backplane, and writes them to a hard disk. The central data recording system (CDR) [16] is a service provided by CN/PDP. It is a set of shell scripts that read the data from the hard disk and transfers them to the central computing services from where they are written to tape.

### Calibration

Calibration tasks are directly in the main data flow where they process 100% of the events. This is a new feature compared to previous runs where the calibration tasks only sampled a fraction of the events. These tasks are detector specific and run on the event building processor. They get the full events and pass them on to the recording. Special software configurations exist for every calibration task to be started at the start of run. The tasks produce histograms and special calibration data files.

### Run Control

The RD13 run control [4] is used for the control of the distributed data acquisition system. It is based on a commercially available tool for message passing (ISIS). The configuration data are stored in three data bases for the hardware configuration, the software configuration and the detector and run parameters. A commercially available DBMS was used for the databases (QUID). Several editors are available for browsing and modifying the data-bases. The run control system also reads the non-standard block [17] from the SPS control system at every Start-of-Run (SoR) and End-of-Run (EoR). The information is forwarded to the recording process which includes it in the data written to disk.

The data acquisition system is started on a SUN workstation by a simple shell script which in turn starts the DAQ main window [18] and other background tasks needed. The DAQ main window is a Motif based user interface which allows the user to change the states of the data acquisition system and to run all the control facilities including the status display, the event dump and the monitoring tasks developed by the detector groups. A user's guide for the people on shift describes all the necessary steps and parameters to run and control the data acquisition system [19].

Different software databases exist to run the detector specific calibration runs. In order to change from one run type to another the DAQ system has to be shut down, the databases to be changed and the DAQ system to be restarted. There are also different sets of data-bases to run each detector independently of the others in a "stand-alone" mode. These sets are also used for running the detectors in the periods following the combined run.

### Monitoring

Monitoring tasks [20] are connected to the main data flow through the shared memory from where they receive a fraction of the events. They are detector specific tasks which either run on the event building processor or on the detector HP workstation. The latter are connected to the main data flow by VIC bus. The monitoring tasks are started through the DAQ main window. They perform some online statistics collection and fill histograms or display the data graphically (e.g. event dump). The histograms can be presented with PAW or, as in case of the LArg detector, with a histogram server exists manages and stores all histograms generated.

### Online Volume Bookkeeping

The online volume bookkeeping system (OVBK) [21] is running as a daemon on one of the SUN workstations. It collects information on SoR and EoR time of a run, beam type and energy, trigger mask and number of events for each run from the run control system and CDR system. It keeps a database of all runs written to file and tape. The user can add comments to the runs. An editor can be used to browse and modify the database and to generate files for presentation. The information is available on the WWW [22].

## 4 Performance

The testbeam data acquisition system was running quite stable with trigger rates of more than 200 triggers per burst. The rates are is limited by the readout which is based on CAMAC equipment. The event building process run at a rate of about 1.3 MByte/s at maximum. In total a volume of 165 GByte of data was collected. This volume contains 1273 data files out of which about one third (423) contain data from physics runs corresponding to about 4\(\cdot 10^{6}\) physics events. The average file size is 137 MByte with a maximum of 770 MByte and a minimum of 21 MByte. The data spans 18 primary tapes and an equal number of backup tapes. Calculated from the time of testbeam period one run took on average about 27 minutes (all runs included: physics, calibration and aborted runs).

The **data flow protocol** proved a successful means for the integration of the different sub-detectors. The modular structure of the code was flexible enough to incorporate each detector specific needs for the readout. It was scalable in the sense that it was running in a multi-detector configuration with the possibility to extend the number of detectors in the near future. The hardware integration was done by an event builder based on the VIC bus and merging events in software.

The **databases** play an important role in the DAQ system. In order to change between the different run types (i.e. physics or calibration runs), the software databases had to be changed. In addition to this change some modifications had to be made in the run control parameter database. This could be facilitated by some automated mechanism, or if the different databases could be merged into a single database. The database themselves were only changed by experts using the available editors. Merging and versioning of the different databases would certainly have been of great help in managing the development of databases at the beginning of the run period.

The **run control system** provided a user-friendly interface to the DAQ system. The scripts to start, check and stop the data acquisition system and all its processes were easy to be used, nevertheless they do not make use of the configuration databases but rather used a hard-coded configuration. The DAQ main window proved to be a very useful tool. The error and log messages were helpful but not always very precise. Some more instructive messages should be developed. The messages should furthermore go to only one place where the user has to look at them.

The **online volume bookkeeping system** proved to be a useful tool. It ran for the last third of the testbeam period without requiring any intervention. The information of all the runs in that period was successfully collected.

One of the biggest improvements compared to earlier testbeam runs was the **central data recording** which performed very satisfactory. All data were recorded centrally and the users did not have to care about that. The average rate of transfer to the computer centre was about 500 kByte/s which should be improved by using an FDDI connection in the future.

## 5 Conclusion

The data acquisition system of the combined ATLAS testbeam was running satisfactory. With trigger rates of more than 200 triggers per burst a total of about 4\(\cdot 10^{6}\) physics events could be collected. The data acquisition system was running stable for the whole period and was felt to be quite user-friendly. The same system in different configurations was used also in the following periods of single detector running throughout 1996 in H8. Also the ATLAS Muon detector used the same system and the SCT detector started converting to it.

The experience of the testbeam period was very useful. Suggestions for improvements will be taken into account for the future running in 1997 when all ATLAS subdetectors in H8 and H6 want to use the data acquisition system presented. The experience gained will also be used in the design of the DAQ system for the ATLAS DAQ prototype.

## Acknowledgements

This note is dedicated to the memory of Giorgio Fumagalli. Giorgio was a primary participant in many of the DAQ developments and its deployment on the testbeam.
We gratefully acknowledge the excellent support of B. Panzer-Steindel (CN/PDP) for the central data recording. We are also indebted to G. Ambrosini for her substantial contribution to many aspects of the DAQ development and to M. Skiadelli for her substantial work on the use of databases. We gratefully acknowledge the professional qualities of H. Rotival (DCS Company), our system manager. We are also indebted to D. Klein (ECP/SA) for the precious work in the organisation and support of many aspects, in particular the documentation and the management of the tape pool.

## References

* [1] T. Akesson et al., Particle Identification Performance of a Straw Transition Radiation Tracker Prototype, CERN/PPE 95-111, to be published in NIM.
* [2] D.M. Gingrich et al., NIM A364 (1995) pp. 290-306.
* [3] TileCal Collaboration, Construction and Performance of an Iron-Scintillator Hadron Calorimeter with Longitudinal Tile Configuration, NIM A349 (1994) 384.
* [4] RD13 Collaboration, Status Report of a Scalable Data Taking System at a Testbeam for LHC, CERN/LHCC 95-47, 1995.
* A Proposal for a Flexible and Adaptable System, RD13/TN124, March 1994.
* [6] C.P. Bee, G. Mornacchi, G. Polesello, The Trigger Logic for the Combined ATLAS Testbeam, RD13/TN125, July 1994.
* [7] C. Jacobs, CP-LEAR Busy Logic with VME Slave Modules, CERN/ECP-EDE 92-1.
* [8] Creative Electronics Systems S.A., RCB 8047 CORBO VME Readout Control Board, 1992.
* [9] Creative Electronics Systems S.A., CBD 8210 CAMAC Branch Driver, 1992.
* [10] Creative Electronics Systems S.A., HIPPI 8262/D VME to HIPPI Destination Interface, 1992.
* [11] Creative Electronics Systems S.A., RAID VME RISC Processor Board, 1992.
* [12] G. Mornacchi, G. Polesello, M. Niculescu, The DAQ System for the Testbeam of ATLAS Subdetectors, RD13/TN169, February 1996.
* [13] G. Ambrosini, Event Format User Interface, RD13/TN110, February 1994; A. Miotto, OS Independent Event Buffer Support in the ATLAS Testbeam DAQ, RD13/TN126, July 1994.
* [14] Creative Electronics Systems S.A., VIC 8251/FC VIC to VME Interface with Mirrored Memory, 1992.
* [15] S. Buono, R. Jones, G. Mornacchi, Readout Module Specification for the June 1994 Testbeam, RD13/TN105, April 1994.
* [16] B. Panzer, M. Niculescu, Central Data Recording, RD13/TN160, November 1995.
* [17] P.Y. Duval, Communication for Reading the SPS Information.