# ATLAS Internal Note DAQ-NO-36 16 December 1994

Tests of Components for an Asynchronous Level-2 Trigger for ATLAS

P.E.L.Clarke, R.Cranfield, G.J.Crone

University College London, Gower Street, London, WC1E 6BT, UK.

B.J.Green, J.A.Strong

Royal Holloway, University of London, Egham, Surrey, TW20 0EX, UK.

R.E.Hughes-Jones, S.Kolya, R.Marshall, D.Mercer

University of Manchester, Manchester, M13 9PL, UK.

K.Korcyl

Institute of Nuclear Physics, Cracow, Poland.

F.Harris, S.Hunt

Oxford University

R.Hatley, J.Leake, R.P.Middleton, F.J.Wickens

Rutherford Appleton Laboratory, Chilton, Didcot, Oxon, OX11 0QX, UK.

A.Guglielmi

Digital Joint Project - CERN, \({}^{\copyright}\)/o CERN, Bldg 513 1-026, CH-1211 Geneve 23, Switzerland.

## 1 Introduction

In this paper we describe work done as part of the programme studying possible architectures and technologies for the ATLAS Level-2 trigger system. The main elements described in this paper are (a) the construction of a demonstrator system that contains prototype hardware elements and (b) discrete event simulation of the demonstrator and sub-systems. The system has been tested with a detector prototype in beam line tests and ongoing laboratory tests will extend these tests to full LHC data rates under more controlled conditions.

A 3-level trigger system has been proposed for ATLAS [1] to reduce the raw data rate of some \(10^{15}\) bytes/s to a readout rate of \(10^{8}\) bytes/s which can be sustained by recording media. The first level, using custom pipelined hardware processors is expected to reduce the rate by a factor of between \(10^{3}\) and \(10^{4}\). The Level-2 system should produce a further reduction of 100, and be able to sustain a decision frequency of up to 100 kHz, with an average decision latency for each trigger of the order of 1 ms. These parameters allow the use of dedicated hardware or programmable devices, depending on the architecture used for transporting data and the guidance received from Level-1.

Level-1 decisions are based on coarse grain data from the calorimeter and muon sub-system, where the major contribution is from jets which satisfy the calorimeter trigger requirements. Events accepted by Level-1 will closely resemble good triggers, and can be rejected only by performing more detailed analyses not possible at Level-1. Level-2 will therefore use full calorimeter data (fine granularity and depth information) and combine this with information from tracking and particle identification detectors. Preliminary investigations [2] show that a rate reduction factor of about 100 is possible in the electromagnetic cluster trigger using fine grain calorimeter and silicon tracker data. Further reductions in rate can be achieved at Level-2 based on event topology.

Figure 1 gives a schematic overview of the functions of the Level-2 system. The first stage of Level-2 processing, known as feature extraction, reduces raw data from a subdetector (track hits or pulse heights) to a few words characterising a track or cluster (e.g. momentum, position). Level-2 then matches these features from different sub-detectors and combines them to form a complete event. Feature extraction is a LOCAL operation on data derived from a small region of one subdetector and adjacent areas. However, feature matching and formation of an overall event trigger are GLOBAL operations requiringconnection of all local feature sources to a global processing farm. For optimal performance it is therefore proposed that the Level-2 system is split into a LOCAL part where the features from different subdetectors are processed in parallel, and a GLOBAL part where the features are combined for the trigger decision.

Analysis at Level-2 of all data from the detector at full granularity and at a rate of up to 100 kHz is likely to be prohibitively expensive. However, there are considerable cost benefits, owing to reduced data transfer and processing needs, if Level-1 is used to indicate Regions of Interest (RoI). In this way, only data in a cone surrounding a possible track or cluster need be processed. For this approach to be effective, Level-2 will also require pointers to all additional energy deposits in the calorimeter or high momentum tracks in muon chambers. As Level-1 generates information at several thresholds for compound triggers, production of additional RoI pointers for Level-2 does not imply significant extra hardware inside Level-1. This multi-threshold information is likely to be particularly important for Level-2 topology triggers.

For the LOCAL part of Level-2 two major models have been suggested, asynchronous and systolic. In each, a Level-1 trigger initiates the transfer of data from the front end of each subdetector into Level-2 buffers, where the data are stored until after the Level-2 decision. In the asynchronous option, the data for feature extraction are transferred from the buffers to one of a large number of general purpose processors, each allocated to a fixed region of the detector. For the systolic option, the data are intercepted before the Level-2 buffers, and each RoI is assembled and routed to one of a few dedicated hardware feature extractors. This paper is concerned with the asynchronous scheme. This scheme has a number of advantages over a scheme using dedicated hardware. For example, analysis algorithms are easily changed, processors can be upgraded and the system can be expanded and there is the possibility of building a unified system using similar devices and structures for different detectors. In addition, it allows greater flexibility to accommodate a broad range of trigger conditions, such as the low luminosity running, where there may be only limited or no RoI guidance.

## 2 Description of the Architecture and Technologies Used

### Overview

The architecture studied is based upon the philosophy that, since data within an RoI is localised inside the detector, a simple, locally connected network will be sufficient to deliver data from the Level-2 buffers to the feature extractors. This principle is illustrated in figure 2 in an example configuration for calorimeter data. The Link Units provide the means to concentrate data from adjacent regions and different layers of the detector to a point where it can sensibly be processed. If the required data crosses a boundary, then it is passed to an adjacent Link Unit. This system allows a flexible topology to suit different detectors, and in particular more than one level of concentration can be implemented. It also allows any number of feature extractors to be placed at any layer of the network, as appropriate for different configurations, or to handle bottlenecks or hot spots of activity.

The complete architecture (figure 3) would operate in the following way. All event data from accepted Level-1 triggers is stored in the associated T2/DAQ Buffer Memory module (T2B). This has a dual-port memory which can accept data at the full LHC rate, and a buffer manager which allocates memory for events and de-allocates it when either Level-2 rejects the event or Level-3 has read the event from the buffer. Data to be used for the Level-2 decision, as indicated by RoI pointers, are sent by the T2Bs into the

Figure 1: Schematic Representation of a Level-2 System

Figure 2: A Calorimeter Example of Asynchronous Local Processing

network of Link Units for transmission to the feature extractor (FEX) responsible for that part of the detector.

Each Link Unit transmits the data to the local feature extractor or through an output link to an adjacent feature extractor if the RoI spans a boundary. The feature extractors pass results into dual-port memories accessible by global processors over a communication network. Note that all communication with the global processors, both input and output, is via the same network.

The final component of this system is the Level-2 Supervisor, which handles the communication with the Level-1 and Level-3 trigger systems, the allocation of global processors and distribution of RoI pointers.

### Selection of Technologies

Given the rate of development of the technologies involved, it is far too early to select the technologies to be used in the final system. However, by selecting from the most promising existing technologies it is hoped to prove the philosophy and to show how closely they can approach the performance required for LHC. By the time that the final system is to be built many of these components will have been superseded, but given the choices being made many are likely to be from the same families and so it should be possible to extrapolate, from the measurements made on prototypes, using manufacturers data values.

#### 2.2.1 Texas Instruments TMS320C40 Digital Signal Processor

The Texas Instruments TMS320C40 digital signal processor (C40) [3] is a leading floating point processor for image processing applications. It has a simple (RISC-like) instruction set, parallel operations and six 20 MByte/s communication links (each supported by a separate DMA channel). A simple algorithm for electron identification from calorimetric data has been tested on a C40 with results as shown in figure 4A. An electron signal is clearly identified based on energy isolation criteria. Figure 4B shows the processing time as a function of the granularity of the input data. The probable RoI size leads us to expect an execution time of 50 microseconds. This establishes the C40 as a strong candidate for feature extraction. The six C40 communication links permit a high level of connectivity making them ideal for data concentration and routing functions.

#### 2.2.2 Alpha Processor

The Alpha processor [4] is the first of a new generation of RISC processors from DEC. The Alpha architecture incorporates features which are important for this project, amongst which are full 64-bit addressing and explicit instructions for multi-processor operation. It is possible to obtain Alpha products as single chips, single board computers or complete packaged systems, thus enabling a variety of levels of integration in an HEP experiment.

Figure 4: Performance of TMS320C40 in Calorimeter Feature Extraction

Figure 3: The Architecture under Study

A number of measurements have already been performed to check the viability of our approach. A simple global algorithm [5] has been benchmarked on a standard Alpha workstation running at 150 MHz. The algorithm calculates the invariant mass of all RoI combinations, and tests the event against criteria for 6 specific physics channels. Figure 5 shows, on a logarithmic scale, the overall timing distribution for the complete algorithm. An average time per event of 25ms is observed, with a small tail to around 100ms. The code uses look-up tables and has been optimised by the C compiler. More detailed measurements indicate that one invariant mass calculation takes approximately 2 ms, whilst each physics channel test takes a little less than 1 ms. The input data is derived from ATLAS simulation work within the EAST collaboration [5]. A full scale Level-2 system would be expected to cope with significantly more complex calculations than just invariant mass and to handle perhaps as many as 100 physics channels.

#### 2.2.3 Scalable Coherent Interface

Scalable Coherent Interface (SCI) [6] provides a very high performance interconnect between processors, memory, etc., through a network of point-to-point links combining the advantages of backplane buses and traditional networking. For example, since all SCI links can transfer data concurrently, there is no arbitration bottleneck. SCI nodes are usually organised into ring structures, with each SCI transaction split into request and response sub-actions. For multi-processor systems, SCI enables the use of a single 64 bit SCI virtual address space shared globally between all SCI nodes, with 16 of the address bits used for node addressing thus permitting extremely large networks to be constructed. To limit transaction latencies to a manageable level it is necessary to configure the network into modest rings (e.g. upto 10 nodes) interconnected by SCI bridges or switches.

#### 2.2.4 Real-Time UNIX Systems

It is proposed that wherever possible UNIX compatible real-time systems should be used throughout the ATLAS trigger and DAQ systems. For the Global processors we have investigated the possibilities of several such operating systems (Chorus[7], VxWorks[8], LynxOS[9]) which might meet the demanding performance requirements for the Level-2 task. At the time of these tests, however, only VxWorks was available on the global processor used.

### Overview

The ATLAS Level-2 Working Group has defined a test-bed environment, shown in Figure 6, within which candidate technologies and architectures should be evaluated. The work described in this paper conforms to this environment and is based on building blocks developed within the DRDC programme.

### Principal Modules

#### 2.4.1 Level-2 Buffer (T2B)

The prototype T2B is shown schematically in figure 7. The buffer is divided into blocks, each normally large enough for 1 event. A list of the start location of free blocks is maintained by a buffer manager in a Free Page FIFO. On receipt of a new event, the next free block address is loaded into the address register and into the Used Page FIFO, and event data is copied into the buffer. If an event is longer than a block, then a second block is used. The buffer manager performs the following actions: (i) maintains a list of event locations; (ii) transfers the RoI data to its output link, when it receives an RoI pointer; (iii) transfers the data to an output buffer for transmission to Level-3, when it receives an event request; (iv) adds relevant block(s) to the Free Page

Figure 5: Timing Distributions of Global Level-2 Test Algorithms

Figure 6: ATLAS Level-2 Test-Bed

FIFO after reception of a Level-2 reject, or after the transfer of accepted data to Level-3. The first version of the T2B [10], designed to operate at up to 100 kHz, is based on an existing commercial C40 board (DBV44 [11]), with the buffer memory on a sister board. The memory has a direct input for data and is connected to the C40 global bus for output and control.

#### 2.4.2 C40 Network

A link unit has been implemented using a C40, with two communication ports for input and two for output. All inbound data is written to a specific area of memory via DMA. The CPU uses a tag in the data (in the final system this would be based on the \(\eta\) and \(\varphi\) of the RoI) to identify in a lookup table which output port to use. The data are then queued for DMA to this output link.

#### 2.4.3 Feature Extractors and Interface to SCI

Feature extraction has already been demonstrated to perform well on C40 processors (see section 2.2.1), and their use greatly simplifies local system interconnections. Integration of local and global processing is achieved by passing the feature data to another C40 which has an interface to SCI.

#### 2.4.4 Global Processors and Alpha - SCI Interface

The initial tests of the system used a standard Alpha workstation for the single global processor in that system. For the future tests, however, it is planned to use single board computers (SBC), produced by Digital Equipment Corporation. They contain a 166 MHz Alpha processor, with Ethernet and SCSI support through a PCI bus [12] capable of 125 MByte/s. These SBCs will be connected to SCI through an interface attached to the PCI bus. It will connect the SCI node chip to the PCI bus with fast FIFOs and dual-port memory. Suitable address translation will map the SCI address space to that of the SBCs.

#### 2.4.5 SCI Network

All SCI interfaces were custom built, using SCI NodeChips(TM)1 [13] and high performance fifos to decouple the processor bus from the NodeChip. Figure 8 shows a block diagram of one of the interfaces used. To ensure deadlock free operation of both request and response sub-actions, four fifos were used. The SCI standard defines specific packet formats for each transaction type.

A processor wishing to initiate a transaction constructs a packet in the Request Output fifo and then initiates packet transmission through the NodeChip. Responses from remote nodes return to the originating node and pass through the Response Input fifo to be handled by the processor. A node also responds to an external request received through the Request Input fifo and returns any data through the Response Output fifo.

One of the SCI interfaces used a NodeChip mounted on a Dolphin mezzanine card [14], interconnected via a custom wire-wrap card (designed within RD24) to a CES RIO module [15] with an R3000 processor. Work to add a TurboChannel port to this interface, allowing a connection to a standard Alpha workstation, is progressing.

The other SCI interfaces used a custom SCI daughter card implemented on a 6U Eurocard which was combined with additional logic to interface either to VME (as in figure 2) or to the C40 global bus. The SCI to VME interface, thus formed, was used both with an embedded VME controller and through a memory mapped interface into a DEC Alpha system. This same daughter card will also be used in the PCI-SCI interface for the Alpha SBCs.

## 3 The Demonstrator System

### Detailed Description of the Demonstrator

The demonstrator system used in the test beam is illustrated in figure 9. All of the C40 processors for the T2 buffers, local network and processors were implemented using standard commercial units, namely DBV42 and DBV44 modules [11].

Figure 7: Schematic View of the Level-2 Buffer (T2B)

Data was derived from a HIPPI spy unit which formed part of a router system [16] placed in the data readout path of the RD-6 Transition Radiation Detector (TRD) [17]. Two channels were equipped to feed data into two Level-2 buffers each under the supervision of their C40 based buffer manager.

Data was fed through a single link unit to enable routing of data to the appropriate FEX. After feature extraction (a null algorithm in initial tests) the data was concentrated in the global gateway (another C40) where it was sent on to a SCI node for final processing by a global processor. Events were recorded at the global gateway by an OS9 system and at the target SCI node consisting of a DEC Alpha processor running the VxWorks real-time kernel.

The whole system was self-triggered by the passage of an event on the RD-6 HIPPI lines, when the router

Figure 8: Schematic of an SCI Interface

Figure 9: Schematic of the Configuration used in the Beam Test

system detected the start of an event. In parallel to the data being sent to the Level-2 buffers, a message was sent to the RoI/T unit, which simulated the function of a Level-2 supervisor. This unit generated an event number and routing instructions for the event data to both the feature extractors and the global processor(s). Another function of the RoI/T was to generate fake events at the start and end of each burst, these were used to reset pointers in the global gateway and thus ensure that no events had been left in the system.

After all features from an event had been received by the global gateway, an accept/reject signal was sent back to the Level-2 buffers to indicate that the data should be kept or overwritten.

The C40 sub-system was controlled from a PC through a daisy-chained JTAG interface. Nodes in the SCI sub-system were each self-configuring. The target Alpha processor with VxWorks was hosted from another Alpha running OSF-1.

### The Beam Tests

The complete system, consisting of two Level-2 buffers, one link unit, two FEXs, global gateway and a four node SCI ring was first successfully operated in the ATLAS test beam line at CERN during September/October 1994. Parasitic operation ensured minimal disruption to other parts of the ATLAS tests and enabled the work programme to remain independent of other activities. Invaluable experience was gained in integrating with real detectors providing real data.

In the initial phase of the beam tests just the local part of the system was used. Since the prototype Level-2 buffers do not yet include a fast read-out port for the DAQ system, an option was included to pass the complete events through the link units and feature extractors to the global gateway. The data concentrated in the global gateway being recorded by the OS9 local DAQ system. In this phase some 50K events were sucessfully recorded to disk.

In a second phase the SCI network and global processor were added. The C40 node sent the data from each event to the Alpha node, with responses being returned. The other two SCI nodes were for diagnostic purposes only. In this configuration event data was recorded both from the global gateway by the OS9 system and by the Alpha VxWorks node to the OSF/1 Alpha. It is believed that this is the first time that 'live' detector data has been passed round an SCI ring at a beam line.

### Laboratory Tests

Prior to the beam tests some testing of the system was done using as a data source the SLATE system [18], a VME module which can be loaded with data from a host computer and then triggered to send that data to the hardware under test at rates up to full LHC data rates.

A programme of further laboratory testing as follows, has now commenced:

* Fully testing the T2B, including the storage of data until the final decision is received from the Global Level-2.
* Exercising dataflow through Link Units and with neighbours, using small networks of C40s. In this way, we will test the local routing under conditions very similar to those which would be encountered in a full system.
* Fully testing the feature extractors, with realistic data, timings and algorithms.
* Testing the SCI ring with several nodes simultaneously sending/receiving data at high rates.
* Reconfiguring the complete system in the laboratory to demonstrate the inter-working of local and global sub-systems at high data rates.

### Further Beam Tests

During 1995 it is planned to use a larger Level-2 system for tests with two/three detectors as shown in Figure 10. The details of this system are still evolving but it is planned to extend the earlier work with some of the building blocks used re-configured, increases in the number of modules and adding other alternative technologies in parallel paths, plus interfaces to external links. The system includes a fuller implementation of the Level-2 supervisor to provide links to Level-1 and to handle the multiple event streams. In this way we hope to merge our work with other groups active in Level-2 triggering, to obtain some comparison of alternative technologies and to allow the possibility of a real global decision.

The aim is to have parallel local systems, one of which would be based on the system used in our 1994 beam tests. The Level-2 buffers, however, would be replaced with a new version that uses an embedded C40 on a custom board, complete with a direct VME read-out of the buffer memory for use by the DAQ system. A prototype of this new design has already been tested. The commercial C40 boards currently used for this function would then be available for use elsewhere in the expanded system. The other local systems would use alternative Level-2 buffers and possibly Link Units, but all of the Feature extractors would be C40 based.

A larger global system would be used, with several Alpha processors and the possibility of alternative processors should they be available with SCI interfaces.

[MISSING_PAGE_FAIL:8]

For the beam-tests, to simplify any necessary modifications, the buffer manager code in the C40 controlling the Level-2 buffers was written in C. Laboratory tests have shown that this C-code is not quite fast enough to handle multi-page events at the full LHC rate, even with optimisation. Using hand coded assembler, however, tests have been run which comfortably exceeded the performance required.

Figure 12 shows a logic analyser trace of a move-64 byte transaction between two SCI nodes (64 bytes of user data with a 16 byte header). The time taken for the transmitting node to send the move request on to the SCI ring and receive the response from the remote node was 2.1\(\upmu\)s. The time from when the data is placed on to the SCI to when the data is clocked into the receiver's fifo is 1.48\(\upmu\)s. The SCI ringlet was occupied for 850ns in transmitting the request.

In the laboratory tests a 2-node SCI ring has been run, with each node generating move-64 byte transactions to itself, giving approximately 25% occupancy of the ring and demonstrating correct operation of the packet collision logic.

## 5 Preliminary Conclusions

The work presented in this paper is merely one stage in the programme to select and develop the architecture and technologies to be used for the final ATLAS Level-2 system. We believe that this programme has to cover the selection of promising technologies, the development of small scale systems using these technologies, so that various architectures using them can be optimised and their true capabilities evaluated. In this way we can gain the necessary experience not only to use each technology to the full, but also to make the unique mixes that this demanding application requires. We will also prove the essential connectivity through the whole system and by working together with detector developers in test beams gain a mutual appreciation of the problems to be faced for the complete ATLAS detector system, so hopefully optimising such issues as the segmentation of the sub-detectors and the interfaces between frontends and the trigger and DAQ systems.

A full scale Level-2 system can only be economically optimised and proven by extrapolation from studies of small scale prototypes. However, the final ATLAS system will involve networks orders of magnitude larger and of greater complexity. Hence, it is critically important to verify the scalability of design concepts from prototype to full scale system, and this is best achieved by simulating both using discrete event simulation techniques. Comparison between simulated and actual behaviour of the prototypes will establish confidence in a simulation of the full scale system. Simulation also provides the means to evaluate variations in the design and permits assessment of system performance under different operating conditions (including error situations) without the necessity of building real life test-beds.

Our experiences in this work reinforce our belief in the importance of this complete view of the programme. The preliminary results indicate that the technologies chosen for the first tests will provide viable test-beds, whether they survive competition with other technologies in further tests remains to be seen.

Figure 12: Timing of a move-64 transaction between 2 SCI nodes.

References

[1] ATLAS Letter of Intent for a General-Purpose pp Experiment at the Large Hadron Collider at CERN CERN/LHCC/92-4; 1 October 1992.

[2] Update on Second Level Electron Triggering in the ATLAS A Detector R.J.Hawkings, A.R.Weidberg, ATLAS INDET-NO-013

[3] Texas Instruments TMS320C40 User Guide.

[4] Alpha Architecture Reference Manual Edited by Richard L.Sites, Digital Equipment Corporation, ISBN 1-55558-098-X.

[5] Modelling of L2 Global Decision Structures EAST 93-03, revision 1.

[6] "SCI, Scalable Coherent Interface", IEEE standard 1596-1992.

[7] Overview of the CHORUS Distributed Operating Systems CS/TR-90-25, Chorus Systemes, 6 Avenue Gustave Eiffel, 78182 St Quentin-en-Yvelines, France.

[8] VxWorks - Wind River Systems, Inc.

[9] LynxOS, Lynx Real-Time Systems Inc, 16780 Lark Avenue, Los Gatos, California, USA

[10] A Second Level Data Buffer with LHC Performance; B.J.Green et al; 6th Pisa meeting on Advanced Detectors

[11] DBV42 & DBV44 Technical Reference Manuals, Loughborough Sound Images Ltd, Loughborough, UK.

[12] Peripheral Component Interconnect (PCI) Specification PCI Special Interest Group, c/o Intel Corporation, 5200 NE Elam Young Pkwy, HF3-15A, Hillsboro, OR 97124, USA.

[13] L64601 SCI NodeChip Technical Manual, LSI Logic Corporation.

[14] CMOS NodeChip mezzanine card, Dolphin, Oslo, Norway.

[15] RIO 8260, Creative Electronic Systems, Geneva, Switzerland.

[16] CERN/EAST note 92-09.

[17] RD-6 Collaboration, CERN/DRDC/P8.

[18] SLATE - Second Level Architecture Test Environment EAST 91-02, 91-13, 92-20 (SLATE program User Manual, version 1.2).