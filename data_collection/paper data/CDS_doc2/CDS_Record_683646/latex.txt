# ATLAS Internal Note

DAQ-NO-098

22 May 1998

Event Building in the ATLAS DAQ Prototype

G. Ambrosini\({}^{\rm a}\), H.-P. Beck\({}^{\rm a}\), D. Francis\({}^{\rm b}\), M. Joos\({}^{\rm b}\), G. Lehmann\({}^{\rm a}\), A. Mailov\({}^{\rm c}\),

L. Mapelli\({}^{\rm b,d}\), G. Mornacchi\({}^{\rm b}\), M. Niculescu\({}^{\rm b,e}\), K. Nurdan\({}^{\rm c}\), J. Petersen\({}^{\rm b}\), D. Prigent\({}^{\rm b}\),

M. Romano\({}^{\rm f}\), J. Rochez\({}^{\rm b}\), R. Spiwoks\({}^{\rm b}\), L. Tremblet\({}^{\rm b}\), G. Unel\({}^{\rm c}\), E. van der Bij\({}^{\rm b}\)

a. Laboratory for High Energy Physics, University of Bern, Switzerland

b. CERN, Geneva, Switzerland

c. Bogazici University, Istanbul, Turkey

d. Spokesperson

e. Institute of Atomic Physics, Bucharest, Romania

f. Politecnico di Milano, Italy

###### Abstract

The event building systems of future experiments at the LHC will require a large total data throughput and a high degree of connectivity. Several high-speed interconnect standards, like ATM, Fibre Channel and the emerging Gigabit Ethernet offer both. In the framework of a prototype data acquisition system, the ATLAS experiment wants to implement an event building system using commercial equipment compliant to one or more of these technologies.

In order to obtain knowledge about the technologies mentioned and the availability of equipment for these, evaluation studies have been carried out using ATM, Fibre Channel and 100 Mobits Ethernet equipment. Tests with Fibre Channel to PCI interfaces in point-to-point configuration, on an arbitrated loop and using a Fibre Channel switch have been performed and can be compared to the corresponding measurements obtained for ATM equipment. Since no Gigabit Ethernet equipment was available the evaluation of this technology is still pending.

## I Introduction

The data acquisition (DAQ) system of the ATLAS experiment [1] at the LHC envisages to contain an event building (EB) system with a total bandwidth of 1 to 10 GByte/s. This system shall be able to assemble event fragments from 100 to 1000 sources at an input frequency of 1 to 10 kHz. Several high-speed standards, like the Asynchronous Transfer Mode (ATM), the Fibre Channel (FC) standard or the emerging Gigabit Ethernet offer the bandwidth and connectivity required.

The final design for the ATLAS DAQ system is not scheduled to take place before 1999 and further investigations of detector requirements, of hardware and software technologies as well as of integration issues are still needed. The ATLAS DAQ group is approaching these pre-design studies using a structured prototype. The DAQ and Event Filter (EF) prototype [2] will be based on the current understanding of the ATLAS DAQ architecture [1]. This prototype will support studies of the full functionality on a small scale without providing necessarily the final performance.

The DAQ/EF prototype contains a fully-functional small-scale EB prototype. However, before it will be implemented, evaluations concerning the interesting technologies have been carried out. The goals of these evaluations were to learn about the technologies and to investigate what kind of equipment and software is available. Another goal was to understand how the technology and the equipment could be used to build the EB system of the DAQ/EF prototype.

This paper describes the high-level design of the EB prototype as well as the technology evaluations carried out and reports on the results obtained.

## II High-Level Design

of the Event Building Prototype

A high-level design of the event building prototype has been accomplished [3]. Functional components and their interactions have been identified.

Figure 1 shows the functional de-composition of the EB system. The _Source (Src)_ and _Destination (Dst)_ components are data flow tasks: the _Src_ receives event fragments from the front-end DAQ and sends them to the _Dst_. The _Dst_ receives the event fragments, assembles them to full events which it sends to the farm DAQ. The _Interconnecting Network_ provides the technology specific data transfer. It has a _Network Manager_ to control and monitor the function of the network. The _Data Flow Manager (DFM)_ embodies the EB protocol. The _Local DAQ (LDAQ)_ component controls all the other components in the EB system and interfaces to the back-end DAQ.

The event building protocol is shown in figure 2. When a _Src_ receives an event fragment it uses the _GetID_ mechanism between itself and the _DFM_ to obtain the identifier of the _Dst_ it

Figure 1: Components of the Event Building Prototype

[MISSING_PAGE_FAIL:2]

bandwidth is used to 55% (Systran) or 64% (Interphase).

The angle in the curve for class 1 and class 2 transfers can be explained by the parallelism when sending multi-frame sequences, while single-frame sequences have to wait for ACKs (class 1) or for R_RDYs and ACKs (class 2): they are sent in a sequential way. In fact, reducing the frame size can increase the data rate [11]. For the Interphase interfaces, class 1 transfers have a slightly better performance than class 2 transfers.

The tests have also been repeated in different combinations of the FC interfaces. It turns out that the interfaces from the two different manufacturers interoperate without any problems.

Next, tests with two System interfaces on Pentium 166 MHz PCs running Windows NT 4.0 have been performed. The cards are connected to the PCI slots via PMC/PCI adapters from Technobox and use a light-weight transport protocol (FXLP) provided by System. DMA measurements of data transfers between the PC's memory and the memory of the FC/PMC card have been carried out. the _overhead_ is about 50 \(\upmu\)s, the _speed_ value from the FC/PMC interface to the PC's memory is about 85 MByte/s, and about 100 MByte/s in the other direction. Figure 5 shows the data rates vs. the size of packets sent via the point-to-point connection between the two PCs. The _overhead_ is about 75 \(\upmu\)s, the _speed_ value is about 40 MByte/s.

### The FC Arbitrated Loop

In an FC Arbitrated Loop (FC-AL standard) several nodes share the same medium. A typical Arbitrated Loop configuration with four nodes is shown in figure 6.

The Systran and Interphase interfaces have been connected on an Arbitrated Loop and transfers in class 2 and class 3 have been tested successfully between any pair of them [13]. The limitation of the classes comes from the Tachyon chip which does not support class 1 on an Arbitrated Loop.

The degradation of the data rate for single transfers due to the additional overhead from the arbitration phase compared to point-to-point transfers is of the order of a few percent for most packet sizes, except for packet sizes of a few percent of the frame size where the degradation is of the order of 10%. This has been tested with up to two concurrent but mutually exclusive transfers.

On an Arbitrated Loop several transfers can overlap. In class 3, however, frames can be lost when several transfers go to the same receiving node. Tachyon can only handle one incoming multi-frame sequence and will drop all incoming class 3 frames not belonging to the actual sequence. Using class 2, Tachyon has the same limitation and will also drop incoming class 2 frames which do not belong to the actual multi-frame sequence, but in addition it will send back an "_N Port Busy_" (P_BSY) frame. The sending node can then retry to send this sequence. This can be handled by Tachyon automatically up to 16 times for the first frame of a sequence. If this is not enough, then the user level of the software has to take an action.

Overtapping of class 2 transfers has been tested in an EB-like application where two nodes on the Arbitrated Loop act as EB sources and two as EB destinations. The data rate vs. packet size is shown in figure 7. The aggregate data rate does not scale with the number of destinations since the maximum bandwidth of this configuration is limited to 100 MByte/s and the nodes spend some time for the arbitration. However, an increase of about 50% between the 1x1 EB-like system to the 2x2 EB-like system can be observed and a total _speed_ value of 80 MByte/s has been fitted in the latter case, corresponding to 80% of the maximum bandwidth. This value is nevertheless not reached for packet sizes of 64 kByte due to an increased _overhead_ value of about 40 \(\upmu\)s and will only be reached at packet sizes of about a few hundred kByte.

Figure 5: Data Rate for Systran FC Interfaces on a PC

Figure 6: Arbitrated Loop with Four Nodes

### _The FC Switch_

The FC interfaces have been tested successfully with the _SilkWorm_ FC switch from Brocade Communication Systems, Inc. [14],[15]. This switch supports class 2 and class 3 transfers and allows full connectivity between any of its ports. A typical configuration with four nodes connected is shown in figure 8.

Comparisons of point-to-point transfers and single transfers going through the switch have shown a delay of the sequences through the switch to be of about \(\sim\)2 \(\upmu\)s, confirming the manufacturer's specification. No limit on the bandwidth was observed. In fact, it turns out that the pairs of host and FC/PCI interface are the bottleneck.

Comparisons of single transfers using the switch and concurrent but mutually exclusive transfers using the switch have shown no observable degradation in the transfer parameters. Up to three concurrent transfers have been shown to have scaling behaviour: the data rates of the transfers add up. The switch has been loaded with about 200,000 frames/s (for packets of 4 Byte) and with about 180 MByte/s (for packets of 64 kByte). This is only a small portion of the load the switch can accept according to the manufacturer which states that the switch is capable of accepting 8 million frames/s and 3.2 GByte/s aggregate bandwidth.

EB-like transfers have been tested with up to six FC interfaces. The data rates are shown in figure 9, for 1x1, 2x2 and 3x3 EB-like systems, all using class 2 transfers.

EB using the switch shows very clearly scaling behaviour of the data rate with the number of destinations. In particular, the data rate for packet sizes of about 10 kByte is 35 MByte/s for each destination. This corresponds to about two to three times the performance required by the ATLAS experiment.

The _SilkWorm_ switch further allows multicast and broadcast. Multicast is defined for class 3 transfers and requires registration of the nodes in multicast groups (FC-PH2 standard). The registration is handled by an alias server which is an integral part of the _SilkWorm_ switch. Broadcast is also defined for class 3 but does not require registration and uses an implicit address identifier instead. The protocol to register multicast groups with the alias server of the switch has been implemented and multicast and broadcast of transfers have been tested successfully.

Another interesting feature of the SilkWorm switch is that it supports cascading of switches. This feature, which has not been tested yet, allows to build bigger networks as required by the ATLAS experiment.

Soon the SilkWorm switch will also allow to attach Arbitrated Loops directly to the switch. This will allow to build EB systems in which Arbitrated Loops are used to group several EB sources sharing the bandwidth of the Arbitrated Loop before they are connected to a network of switches. The number of switch ports can thus be reduced and each switch port will be used much more efficiently. This will reduce the cost of the system considerably. A typical configuration using Arbitrated Loops and cascaded switches is shown in figure 10.

## IV Evaluation of ATM Technology

for the Event Building Prototype

The ATM technology as one candidate for constructing EB systems has already been investigated thoroughly in the CERN DRDC RD31 project [16]. Some studies have been repeated using the 155 Mbit/s standard (140 Mbit/s payload bandwidth) to integrate this technology in the ATLAS DAQ prototype.

Two different sets of measurements have been carried out in order to estimate the overhead for connection oriented (TCP/ IP) and connectionless (AAL5) communication protocols. The setup for these tests was based on two single board computers (RIO2) running LynxOS 2.4 connected to two ATM/PMC interfaces from CES [9] and on two Pentium PCs running Windows NT 4.0 with ATM/PCI interfaces from Interphase [6]. The CES interfaces have been used both with the native AAL5 communication protocol and with TCP/IP, while the Interphase interfaces have been tested using TCP/IP only since the driver to access directly the ATM adaptation layer is not yet available.

Figure 8: Switch with Four Nodes Connected

Figure 10: Network using Arbitrated Loops and Cascaded Switches

Figure 9: Event Building using an FC SwitchFigure 11 shows the different data rates obtained: the TCP/IP protocol (using option _TCP_NODELAY_) induces a substantially higher _overhead_ (\(\sim\)150 \(\upmu\)s compared to \(\sim\)20 \(\upmu\)s with the AAL5 protocol). The _speed_ as measured with the CES interfaces using AAL5 is 17.5 MByte/s corresponding to 100% of the bandwidth available to the user. Due to the limited size of their receive buffer queue the CES interfaces reach only a data rate of 6 MByte/s when the TCP/IP protocol is used. The same limitation occurs at much higher packet sizes in the interfaces from Interphase allowing for a throughput of about 15 Mbytes/s corresponding to \(\sim\)85% of the bandwidth available to the user.

The high overhead of the TCP/IP protocol which dramatically degrades the transfer performance of packets with sizes interesting for EB purposes (1 to 10 kBytes) leads to the necessity of applying connectionless communication protocols as for example AAL5.

An ATM switch from the Fore Systems, Inc. [17] will be used to carry out the same tests as for the Fibre Channel technology. Some tests previously performed by the RD31 project can be found in [16].

## V Evaluation of Ethernet Technology

for the Event Building Prototype

Gigabit Ethernet technology promises to be a cheap alternative to the technologies mentioned before. It will offer high bandwidth and a high degree of connectivity at a lower price. This technology is in the tradition of Ethernet technology and it can be hoped that most of the higher-level protocol and the user applications can be "inherited" without any modification.

No equipment for Gigabit Ethernet technology has been available so far. However, in order to learn about Ethernet technology for EB systems, Ethernet equipment running at 100 Mbit/s has been tested. It can be hoped that this technology can be regarded as one step in the migration to Gigabit Ethernet.

100 Mbit/s Ethernet/PCI interfaces from Intel [18] have been tested on PCs running Windows NT. Using a TCP/IP driver (without the option _TCP_NODELAY_) the data rates shown in figure 12 could be measured. The _overhead_ value is about 30 \(\upmu\)s and the _speed_ value is about 11 MByte/s which corresponds to about 93% of the physical bandwidth available to the user. For packets of a size bigger than 30 kByte, however, the performance drops to about 9 MByte/s, probably due to an internal memory limitation of the PC.

Tests with a switch from 3COM [19] have been performed. The switch, which uses a store-and-forward technique, introduces an additional delay of 10 \(\upmu\)s for packets of minimum size. This delay increases with the size of the packets by about 73 \(\upmu\)s per kByte.

## VI Conclusion

Before constructing the EB system of the ATLAS DAQ prototype several technologies for the high-speed interconnect have been tested. It has been found that ATM and FC do provide the necessary components. Both technologies have mature products which can be used for EB systems or other components in the DAQ prototype. Differences in performance and cost will have to be understood on the background of the requirements of the ATLAS DAQ prototype. It is still too early to conclude on 100 Mbit/s Ethernet technology, and Gigabit Ethernet technology will be investigated when equipment becomes available.

The tests have shown that the role interfaces play in the construction of an EB system are very important. The scaling of cascaded switches and the construction of large networks has been shown in simulations [16],[20], but needs further confirmation from testing. One of the goals to be achieved in the ATLAS DAQ prototype is certainly to obtain a better understanding of the scalability issue.

Another important issue, going beyond the rather technology oriented studies presented in this paper, is the integration aspect of an EB system and of the ATLAS DAQ prototype. The interfaces, drivers, libraries and switches have to be assembled in a way that they allow to investigate the functional architecture of the proposed ATLAS DAQ system.

Figure 11: Data Rates for ATM Interfaces

Figure 12: Data Rate of 100 Mbit/s Ethernet Interfaces

## VII Acknowledgements

We would like to thank the people from Systran and from Brocade for their technical support and for loan of equipment used in the evaluation.

## VIII References

* [1] ATLAS Collaboration, Technical Proposal for a General-purpose pp Experiment at the Large Hadron Collider at CERN, CERN/LHCC/94-43, see [http://atlasti.nfo.cern.ch:80/Atlas/Welcome.html](http://atlasti.nfo.cern.ch:80/Atlas/Welcome.html).
* [2] G. Ambrosini et al., The ATLAS DAQ and Event Filter Prototype "-1" Project, presented at Computing in High-energy Physics 1997, Berlin, Germany, [http://atd-doc.cern.ch/Atlas/Conferences/CHEP/ID388/ID388.ps](http://atd-doc.cern.ch/Atlas/Conferences/CHEP/ID388/ID388.ps).
* [3] G. Ambrosini et al., A logical Model for Event Building in DAQ-1, ATLAS DAQ Prototype Note 042, [http://atd-doc.cern.ch/Atlas/postscript/Note042.ps](http://atd-doc.cern.ch/Atlas/postscript/Note042.ps).
* [4] For the different documents within the family of the Fibre Channel standard see the Fibre Channel Association, [http://www.amdahl.com/ext/CARP/FCA/FCA.html](http://www.amdahl.com/ext/CARP/FCA/FCA.html).
* [5] Systran Corp., [http://www.systran.com](http://www.systran.com).
* [6] Interphase Corp., [http://www.iphase.com](http://www.iphase.com).
* [7] Technobox Corp., [http://www.technobox.com](http://www.technobox.com).
* [8] Tachyon, HP-FC-5000, Hewlett-Packard Comp., [http://tachyon.rose.hp.com](http://tachyon.rose.hp.com).
* [9] Creative Electronics Systems S.A., [http://www.ces.ch/CES_info](http://www.ces.ch/CES_info).
* [10]IBM Corp., [http://www.chips.ibm.com/products/embed-ded/chips/ibm82660.html](http://www.chips.ibm.com/products/embed-ded/chips/ibm82660.html).
* [11]R. Spiwoks et al., Evaluation of FC/PMC Cards in the Environment of the ATLAS DAQ Prototype, ATLAS DAQ Prototype Note 021, [http://atddoc.cern.ch/Atlas/postscript/Note021.ps](http://atddoc.cern.ch/Atlas/postscript/Note021.ps).
* [12]R. Spiwoks, Performance Tests with FC/PCI Cards in the Environment of the ATLAS DAQ Prototype, ATLAS DAQ Prototype Note 034, [http://atddoc.cern.ch/Atlas/postscript/Note034.ps](http://atddoc.cern.ch/Atlas/postscript/Note034.ps).
* [13]R. Spiwoks, Experience with FC Arbitrated Loop in the Environment of the ATLAS DAQ Prototype, ATLAS DAQ Prototype Note 036, [http://atddoc.cern.ch/Atlas/postscript/Note036.ps](http://atddoc.cern.ch/Atlas/postscript/Note036.ps).
* [14]Brocade SilkWorm Switch, Brocade Communications Systems, Inc., see [http://www.brocade.com](http://www.brocade.com).
* [15]R. Spiwoks, E. v.d. Bij, Evaluation of a FC Switch in the Environment of the ATLAS DAQ Prototype, ATLAS DAQ Prototype Note 053, [http://atddoc.cern.ch/Atlas/postscript/Note053.ps](http://atddoc.cern.ch/Atlas/postscript/Note053.ps).
* [16]C. Bizeau et al., RD31 Status Report 1997, NEBULAS: High-performance Data-driven Event Building Architectures based on Asynchronous Self-routing Packet-switched Networks, CERN/LHCC 97-05; and the references therein.
* [17]For Systems, Inc., http:www.fore.com.
* [18]Intel Corp., [http://www.intel.com](http://www.intel.com).
* [19]3COM Corp., [http://www.3com.com](http://www.3com.com).
* [20]R. Spiwoks, Evaluation and Simulation of Event Building Techniques for a Detector at the LHC, PhD Thesis, University of Dortmund, Germany, 1995; CERN/THESIS/96-002; [http://atddoc.cern.ch/~spiwoks/papers/diss/diss.ps](http://atddoc.cern.ch/~spiwoks/papers/diss/diss.ps).