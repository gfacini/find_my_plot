# Computer modeling the ATLAS Trigger/DAQ system performance

Robert Cranfield1, Piotr Golonka3, Anna Kaczmarska3, Krzysztof Korcyl2,

Jos Vermeulen3 and Sarah Wheeler4

1University College, London, England; Email: rc@hep.ucl.ac.uk

2Institute of Nuclear Physics, Krakow, Poland;

Email: Piotr.Golonka@cern.ch, Anna.Kaczmarska@ifj.edu.pl, Krzysztof.Korcyl@ifj.edu.pl

3NIKHEF, Amsterdam, The Nederlands; Email: i73@nikhef.nl

4University of Alberta Edmonton Alberta, Canada; Email: Sarah.Wheeler@cern.ch

on behalf of the ATLAS TDAQ DataFlow group [1]

###### Abstract

In this paper simulation ("computer modeling") of the Trigger/DAQ system of the ATLAS experiment at the LHC accelerator is discussed. The system will consist of a few thousand end-nodes, which are interconnected by a large Local Area Network. The nodes will run various applications under the Linux OS. The purpose of computer modeling is to verify the rate handling capability of the system designed and to find potential problem areas. The models of the system components are kept as simple as possible but are sufficiently detailed to reproduce behavioral aspects relevant to the issues studied. Values of the model parameters have been determined using small dedicated setups. This calibration phase has been followed by a validation process. More complex setups have been wired-up and relevant measurement results were obtained. These setups were also modeled and the results were compared to the measurement results. Discrepancies were leading to modification and extension of the set of parameters. After gaining confidence in this way in the system component models a model of the full size ATLAS system was run. Predictions for the latency, throughput and queue development in various places have been obtained. The queue development is extremely important, as packet loss may cause severe performance degradation. We also tested various ideas on traffic shaping aimed at limiting probability of creating congestions in the network and possible packet loss.

## I Introduction

The Trigger/DAQ system for the ATLAS detector at the CERN LHC will be based on a large local area network (LAN). More than 1600 Readout Buffers (ROBs) transmit detector data on request via the network to tens to hundreds of Level 2 Processing Units (L2PU) and of the order of 100 Event Filter Subfram Interfaces (SFIs). The SFIs build complete events and pass them on request to one of several thousands of event filter (EF) farm processors. Commodity Ethernet has been chosen for the network because of its cheapness, widespread usage and availability over a range of transmission speeds and physical media. The L2PUs and SFIs are commodity PCs running the "DataCollection software", and in the case of the L2PUs, the Level 2 (LVL2) trigger software, under the Linux OS. An example of the network architecture for the Trigger/DAQ system, which we used in our modeling studies, is presented in Fig. 1.

On the basis of information provided by the Level 1 Trigger part of the detector data from events accepted by the Level 1 Trigger and stored in the ROBs are fetched from the buffers by the L2PUs. These L2PUs will run sequences of trigger algorithms and will handle a maximum event rate of 75 kHz. For the events accepted by the trigger algorithms all event data will be fetched from the buffers by SFIs to form complete events (event building) with a rate of not more than a few kHz. The complete events are sent on request to the Event Filter farm, where off-line algorithms will reduce the trigger rate further (by approximately an order of magnitude). The expected total rate of data flowing from the ROBs to the L2PUs and EFs is at maximum about 5 GB/s.

## II Modeling

Two discrete-event modeling tools are used: **at2sim**[2] based on the Ptolemy framework [3] and **simdaq** - a dedicated C++ program [4, 5].

The model of the Trigger/DAQ system implemented in both tools is an object oriented model, in which most objects represent hardware (e.g. switches, computer links, processing nodes), software (e.g. low level network communications in

Fig. 1: A possible implementation of the network architecture for the ATLAS Trigger/DAQ system; SV: LVL2 Supervisor, DFM: DataFlow Manager, pROS: pseudo-ROS (buffers results generated by LVL2 trigger)Linux OS, Data Collection applications) or data items (e.g. Ethernet packets).

The type of simulation used for the computer models in known as "discrete event simulation". Basically, the simulation program maintains a time-ordered list of "events" i.e. points in time at which the simulated system changes state in a way implied by the type of "event" occurring. Only at the time of occurrence of an event is the modeled system allowed to change its state; in most cases only a small part of the state of the simulated system needs to be updated. The state change can result in the generation of new events for a later time; these events are then entered at the correct position in the event list. The simulation program executes a loop in which the earliest event is fetched from the event list and then handled.

Because of the size of the network it is not possible to build and test a full size prototype prior to constructing the final system. Apart from checking averages to be equal to "paper model" results there is therefore no way to check the computer model results other than by comparison of results from both tools. "Paper model" results are results from straightforward calculations of average message frequencies, bandwidth requirements and CPU capacity requirements using first level trigger rates and details of the trigger processing and mapping of the detector onto the ROBs.

At the time of submission of this paper the models implemented with both tools do not yet allow comparison of the results obtained for the full system, but the results do complement each other. The focus of the at2sim program is currently a proper simulation of the event building, while in simdaq details more relevant to the LVL2 trigger are taken into account (in particular proper handling of the step-wise execution of the trigger algorithms and associated requesting of event data from the ROBs).

### _Parameterization of system components_

The models of the system components are kept as simple as possible but are sufficiently detailed to reproduce behavioral aspects relevant to the issues studied. Each model has measurable parameters.

We have developed a parameterized model describing the behavior of the members of a class of typical Ethernet switches. This type of switch is used in ATLAS trigger/DAQ test setups and could be used in the final system. The switches operate in the store-and-forward mode and have a modular architecture: modules contain interconnected groups of ports and provide intra-module transfers while the inter-module communication proceeds via a backplane. The model has ten parameters such as the amount of input and output buffering, transfer limits when moving packets to and from the backplane for inter-module transfers and transfer speed for inter and intra-module transfers. These parameters have been identified to determine transfer latency and bandwidth limitations in case of congestion. The switch model supports flow-control and provides statistics on the usage of flow-control. It also offers very detailed statistics with respect to queue development in the output ports of the switch. This has proven to be very useful for quantifying the effect of various traffic shaping schemes.

The models of all components other than switches are built around a parameterized model of a multi-tasking operating system with interrupt-driven network communication. The behavior of the Linux operating system running multi-threaded on a single-processor machine has been successfully modeled (Fig. 2).

The details of recent improvements of the Linux networking subsystem (NAPI, Interrupt Coalescence, Flow Control) are also taken into account ([6]). The CPU time consumption due to communication, with multiple-level processing of the incoming messages, can be reliably estimated (hardware and software/protocol stack interrupts, specific overheads of the high-level data-formatting routines are properly modeled).

The ATLAS trigger/DAQ system applications are built around a common software framework - _Data Collection Software_, executed on multiprocessor PC computers running the Linux operating system. The applications have one or multiple execution threads. Their high-level message-passing subsystem, responsible for proper message formatting, packetization and buffer management, may either be steered sequentially or executed as a dedicated _Input Thread_. Both models of message-passing approaches are properly modeled. The specific functionality of the data-collection and of the higher-level trigger applications is concentrated in models of "Tasks" being run by the model of the Operating System. The tasks model the activities and state changes due to incoming messages or computations.

In the L2PU (see Fig. 3) the Input Thread is responsible for receiving messages from the network. The messages are passed to the Message Dispatcher. Various handlers may subscribe to the Dispatcher and they will be notified when the requested type of message arrives. The LVL1 Results handler passes the LVL1 Result message to the LVL1 Result Queue. When the Worker Thread finishes processing an event and becomes free, it checks the status of the LVL1 Result Queue and fetches the event from the head of the queue.

The parameters used for parameterization of the operating system and applications are described in [7].

Fig. 2: Model of sharing CPU resource between network interrupts and requests from processing tasks

### _Model calibration_

All parameterized models of system components are calibrated using results from dedicated, small setups.

To calibrate the switches we used packet generators based on FPGA devices or on programmable NICs (which support user modification of the firmware). We developed procedures to measure the values of all ten parameters of the switch model.

The values of the parameters of the low-level communication models have been determined with simplified setups where maximum achievable message rates were measured. We have observed that the data packets in a simplistic streaming scenario start to be lost at a certain rate, due to saturation of the available CPU capacity. The inverse of this rate provides an estimate of the average CPU time needed to process a single incoming data packet. The model correctly predicts this quantity, taking interrupt coalescence and multiple stages of processing of the message into account (see Fig.4).

The Trigger/DAQ application parameter values were obtained by inserting time stamps into the application's code. The log files with time stamps were later analyzed to find the time intervals between the starts of different but successive actions of the application. Wherever possible, a small setup was used, consisting of only two machines connected back to back with the calibrated application running on one machine and a tester application, sending messages in a predefined order on the other. The times calculated from the time stamps were used to predict the maximum rate the application can sustain and were cross-checked with results of maximum rate measurements. Various parameters were studied with respect to a possible impact on the performance of the application in question. For example, the plot on the right side of Fig. 5 shows that the DFM model is also sensitive to the number of LVL2 accepts inside the group of events received from the LVL2 Supervisor.

### _Model validation_

The biggest challenge for modeling is to predict the scalability of the final system. Correct modeling of testbeds with various sizes increases our confidence in the models used.

The plot in Fig. 6 shows good agreement between model predictions and measurement results obtained with test setup aimed at testing scalability of the Event Builder part of the system. The maximum event building rate scales linearly with the number of SFIs. The intercept of the line fitted for buffers receiving data from a single Read-Out Link (ROL) (1600 network access points) is smaller than that of the line for buffers aggregating the data from 8 ROLs (200 access points), since a smaller number of request messages is required for the latter case. This results in a smaller fraction of the CPU capacity spent on generation of requests and increases the total number of events that can be processed per second.

## III Full size model results

Results for the full size system were produced. Various ideas on traffic shaping aimed at improvement of performance and at avoidance of performance degradation due to packet losses were evaluated.

Fig. 4: Comparison between parameterized model predictions and measurement results: average CPU time needed to receive a single message in the network streaming test.

Fig. 5: Comparison between parameterized model predictions and measurements results for the DFM Application: CPU consumption vs DFM rate and DFM rate vs number of accepts in the DFM message.

Fig. 3: Example of multi-threaded application: Level2 Processing Unit is composed of the Input Thread and a few Worker Threads which execute analysis algorithms.

### _Tests of various traffic shaping ideas_

Models of the full Trigger/DAQ system provide the possibility to study the effect of various ideas for traffic shaping on the development of queues in the system. Limiting the sizes of these queues in the switches is essential to avoid packet loss in the network. This is important, as the data from lost packets have to be re-transmitted, which can result in a large amount of re-transfers, which, in turn may cause the whole system to crash. Information on the length of the queues in the end nodes makes it possible to estimate buffer sizes needed. The queue sizes can be limited by controlling the points in time at which packets generated by the end nodes are injected into the network, taking into account the buffering capabilities on the path to the destination.

In the event building subsystem, the traffic shaping can be achieved by limiting the number of outstanding requests for data sent to the detector buffers. Each SFI can generate only a limited number of requests for detector data (credits). The SFIs are allowed to generate new requests only after they have received replies on previously generated requests (i.e. when they regained a credit). By limiting the number of outstanding requests the SFIs can control the build-up of the queue with replies in the central switch. In Fig. 7 the maximum queue length in the central EB switch is shown as a function of the number of credits for different scenarios for reading out the detector data. Collecting data from buffers which aggregate detector data from more than one readout link (per readout link always one packet per event is sent) results in a proportional increase in the number of packets waiting in the switch for port availability. The event building rate is identical for all data points with more than five SFI credits - the rate is limited by the time needed to transfer fragment from all buffers (\(\sim\)2MB) over the Gigabit connection between the EB central switch and an SFI.

### _The impact of the L2PU assignment strategy_

Results for the second-level trigger decision times for a model of the full system, running at the LHC design luminosity and for a first-level trigger rate of 75 kHz, are presented in Fig 8. The distribution with the long tail arises from round-robin assignment of events to the L2PUs. The other distribution is obtained when the supervisor at the time of arrival of each LVL1 accept assigns the event to the L2PU with the lowest number of second-level trigger decisions to be returned to the supervisor, i.e. the L2PU handling the smallest number of events at the time of assignment. This assignment scheme is referred to as "least queued" assignment. The long tail arising from round-robin assignment can also be suppressed by only allowing less than a certain maximum number of events being handled by each L2PU at the same time. In that case LVL1 accepts need to be stored by the supervisor if no L2PU is available for assignment and the event can only be assigned once an L2PU is available (signaled by the reception of a second-level trigger decision).

The model is based on a realistic trigger menu; the mapping of the detector and relevant details of the various steps of the second-level trigger algorithms have been taken into account. Groups of 12 ROBs are assumed to be connected via Gigabit Ethernet links (one per group; data from the same event is sent as a single message, also if requested from several ROBs, this message may consist of more than one packet) to the LVL2 central switch. Groups of 5 second-level trigger processors (dual-CPU 4 GHz PCs) are connected via small Gigabit Ethernet switches to the same central switch. Each second-level trigger processor can run at maximum 4 threads, with an event assigned to each thread. The algorithm execution times and acceptance factors of the various steps have been obtained by extrapolating results from algorithm benchmarks. The values

Fig. 6: The Event Building rate as a function of the number of SFIs for two readout scenarios: for buffers receiving data from a single ROL and for buffers aggregating data from 8 ROLs.

Fig. 7: Central EB switch queue build-up for various scenarios for reading out the detector dataof other parameters of the model are estimates for 4 GHz PCs. The Ethernet switches are assumed to be non-blocking. The peaks in the distribution arise from the different steps in the trigger algorithms (for each step a fixed computation time is assumed, though in reality these computation times may vary from event to event). The average utilization of the L2PUs in the model is 83%, at maximum more than 200 packets may be queued for availability of an output port of the central Ethernet LVL2 switch for ports connecting to the small switches. This result shows that the use of the small switches potentially could give rise to problematic build-up of queues. Limiting the number of outstanding requests per L2PU makes it possible to limit the queue sizes. For a maximum of 6 outstanding requests essentially the same latency distributions as in Fig 8. are found, but now at maximum somewhat more than 40 packets are queued. This is more than the number of 30 one would naively expect as the data sources may send multi-packet messages and as also requests from the LVL2 supervisor to the L2PUs are transferred via the same ports.

## IV Conclusion

The behavior of the calibrated component models is in good agreement with the behavior of the real components in small test setups. The first experimental results for event building in a larger test set-up also show an encouraging agreement with the model predictions. However, further validation is required. Models for the full-scale system already allow determination of possible problem areas and investigation of techniques for preventing build-up of queues in switches and processors. In the models build-up of queues may result in long second-level trigger decision times or event-building times; message loss due to queue overflow may in reality also occur. Models have been run for the full system, for event building based on the calibrated component models and for the LVL2 trigger so far based on "paper model" assumptions ("paper model" assumptions are the assumptions made in the calculations of average message frequencies, bandwidth requirements and CPU capacity requirements using first level trigger rates and details of the trigger processing and mapping of the detector onto the ROBs). It has been shown that a credit-based pull scenario for event building and for collection of input data for the second-level trigger are essential for limiting queue lengths and most likely can be applied without compromising the throughput. Long second-level trigger decision times can be prevented, also in case of a high average processor utilization of the L2PUs, if the number of events being handled by each L2PU is minimized. This can be achieved by minimizing (with the help of least queued assignment) and by limiting the number of events being handled by each L2PU at the same time.

## References

* [1][http://atlas.web.cern.ch/Atlas/GROUPS/DAQTRIG](http://atlas.web.cern.ch/Atlas/GROUPS/DAQTRIG) /DataFlow/DFlowAuthors.pdf
* [2] at2sim sources are browsable at: [http://atd4q-sur.cern.ch/cgi-bin/viewcs.cgi/DAQ/Modeling/at2sim/](http://atd4q-sur.cern.ch/cgi-bin/viewcs.cgi/DAQ/Modeling/at2sim/)
* [3] Ptolemy project; [http://ptolemy.eecs.berkeley.edu](http://ptolemy.eecs.berkeley.edu)
* [4] simdag sources and documentation: [http://www.nikher.nl/pub/experiments/atlas/daq/modelling.html](http://www.nikher.nl/pub/experiments/atlas/daq/modelling.html)
* [5] J.C. Vermeulen et al, IEEE Trans.Nucl.Sci.45:1989-1993,1998
* [6] P.Golonka, "Linux network performance study for the ATLAS Data Flow System", ATLAS DataFlow note ATL-DO-TR0001
* [7] P.Golonka, K.Korcyl, "Calibration of the ATLAS Data Collection component models."; ATLAS Data Collection note 57

Fig. 8: The least queued strategy improves the LVL2 latency distribution with respect to the distribution obtained for round robin assignment