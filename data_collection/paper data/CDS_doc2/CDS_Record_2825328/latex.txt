# Constituent-Based Top-Quark Tagging with the ATLAS Detector

The ATLAS Collaboration

###### Abstract

This note presents the performance of constituent-based jet taggers on large radius boosted top quark jets reconstructed from optimized jet input objects in simulated collisions at \(\sqrt{s}=13\) TeV. Several taggers which consider all the information contained in the kinematic information of the jet constituents are tested, and compared to a tagger which relies on high-level summary quantities similar to the taggers used by ATLAS in Runs 1 and 2. Several constituent based taggers are found to out-perform the high level quantity based tagger, with the best achieving a factor of two increase in background rejection across the kinematic range. To enable further development and study, the data set described in this note is made publicly available.

ATLAS

ATLAS

## 1 Introduction

Collisions at the Large Hadron Collider (LHC) produce heavy particles such as the top quark which exist very briefly before decaying to lighter particles. When these lighter particles are quarks and gluons, their strong interactions produce a shower of collimated hadrons known as a _jet_. The top quark decays almost exclusively to a \(W\) boson and a bottom quark (\(b\)) [1]. In the case where the \(W\) subsequently decays to two quarks (\(qq^{\prime}\)), three quarks are the result, as shown in Figure 1. In the case that the originating top quark's momentum transverse to the beam axis is large compared to its mass, i.e. has a large Lorentz boost, the hadronic particles resulting from the three quarks can overlap, and are reconstructed as a single large-\(R\) jet.

Events with boosted top quarks are of great interest for both precision measurements of Standard Model (SM) processes [2; 3; 4; 5] and searches for new physics beyond the Standard Model (BSM) [6; 7; 8; 9; 10; 11]. For this reason, identifying large-\(R\) jets which result from boosted top decays is a key task in data analysis. This task, known as _boosted top tagging_, is vital to suppress the dominant background: jets originating from light quarks and gluons (Quantum Chromodynamic or \(QCD\) jets) which occur at a much higher rate than boosted top jets in LHC collisions.

The constituent particles of large-\(R\) jets originating from top quarks have distinct kinematic properties due to the top quark's prompt decay to three quarks before hadronization. Boosted top tagging methods use these properties to distinguish top quark decays from the QCD background [12; 13; 14]. A robust literature exists [15; 16; 17; 18] of _jet substructure_ quantities that summarize the information contained in the constituent's kinematic properties. These _high level quantities_ can be starkly different between jets due to top quarks and those due to light quarks and gluons. The information from a small set of high-level summary quantities can be combined using a neural network or other machine learning (ML) method to classify jets as due to top quark decays or QCD. This technique is used by ATLAS to identify boosted top quark decays [19; 20; 21].

Recently, ML models have demonstrated a powerful capacity for classification even in cases where the feature space is high-dimensional [22; 23; 24; 25]. Applied to the task of boosted top tagging, such models can be trained using the kinematic properties of the jet's constituents rather than high level summary quantities. This gives the models access to a superset of the information contained in any set of high-level quantities. A recent survey of top tagging algorithms [26] in the context of a simplified detector simulation [27] and jet reconstruction demonstrated the power of these constituent-based taggers over those which use high-level quantities.

An open question is whether the performance of these taggers in simplified simulations translates to a more realistic context, where the detector response is simulated with a high-fidelity Geant4-based [28] detector simulation and the jets cover a wide range in transverse momentum (\(p_{\mathrm{T}}\)). Recent works by the ATLAS and CMS collaborations have shown promising performance for constituent-based taggers on Geant4 simulation samples with a reasonably large \(p_{\mathrm{T}}\) range [19; 29], but the performance of many other constituent-based taggers has never been documented. This includes the taggers that achieved the highest performance on data sets produced with simplified detector simulation.

This note provides an assessment of the performance of five of these constituent-based taggers in such realistic simulation samples. The simulation samples contain jets which cover an entire order of magnitude in \(p_{\mathrm{T}}\) (0.35-4 TeV). In tandem with this note, the ATLAS collaboration will also release the simulated data set used in this study to encourage further development of top tagging algorithms.

This rest of this note is organized as follows. Section 2 describes the generation of the simulation samples usedin this study. Section 3 describes the jet selection process, and the contents of the public data set. Section 4 describes the pre-processing applied to the simulated data set for this specific study, and the six taggers trained are presented in Section 5. The performance of the taggers is presented in Section 6, and the conclusions from the study are discussed in Section 7.

## 2 Simulation Samples

Samples of simulated collisions generated using Monte Carlo methods are used throughout this study. All simulated samples are produced using the ATLAS detector simulation [30] based on Geant4[28]. Events are generated at leading-order (LO) with Pythia8[31] using the NNPDF2.3LO [32] set of parton distribution functions and the A14[33] set of tuned parameters. The effects of simultaneous proton-proton collisions ("pile-up") is simulated by overlaying inelastic interactions on top of the underlying hard1 scattering process. The applied pile-up conditions are from the 2017 data taking period [34]. Boosted top quarks are obtained in simulated events containing the decay of a heavy \(Z^{\prime}\) boson (\(Z^{\prime}\to t\bar{t}\) ), with \(m_{Z^{\prime}}=2\) TeV. The cross section of this process is reweighted to produce an approximately flat jet \(p_{\mathrm{T}}\) distribution to efficiently populate the full kinematic region. Background QCD jets are obtained in simulated events containing pairs of light quarks or gluons.

Footnote 1: In HEP nomenclature, “hard” and “soft” refer to high energy and low energy effects respectively.

Unified Flow Objects (UFOs) [35] are jet input objects optimized for reconstructing large-\(R\) jets by making use of different ATLAS sub-systems in different kinematic ranges. At low \(p_{\mathrm{T}}\), the inner tracking detector provides exceptional spatial and momentum resolution for charged constituents, so low \(p_{\mathrm{T}}\) constituents are reconstructed from tracks using the Particle Flow algorithm [36]. At high transverse momentum, the tracking detector loses momentum resolution but retains high spatial resolution, and so high \(p_{\mathrm{T}}\) constituents are reconstructed using energy measurements from the hadronic calorimeter [37] and spatial measurements from the tracking detector. This scheme provides accurate reconstruction of constituent particles across a wide

Figure 1: Feynman diagram of the decay of a top quark (\(t\)) into a \(W\) boson and a bottom quark (\(b\)). The \(W\) boson subquently decays into a pair of quarks (\(qq^{\prime}\)).

kinematic range.

Jets are reconstructed using the anti-\(k_{t}\) algorithm [38] with a radius parameter of 1.0, as implemented in the FastJet package [39]. This algorithm produces a list of UFOs taken to be the constituents of the jet. The Constituent Subtraction [40; 41] and Soft-Killer [42] algorithms are then applied to mitigate contamination from any radiation that comes from pile-up collisions rather than the quarks or gluons which initiated the jet. Further, the Soft-Drop algorithm [43] is applied to remove soft and wide-angle radiation, which can result from either pile-up or the remnants of the colliding protons ("underlying event").

## 3 Jet Reconstruction and Selection

Both the leading and sub-leading jets in each event are included. Jets are required to have transverse momentum of at least 350 GeV, pseudo-rapidity of \(|\eta|<2.0^{2}\), a jet mass of at least 40 GeV, and at least three constituents. This last requirement is included to ensure the preprocessing scheme in Section 4 is well defined. Jets in the signal sample must satisfy additional requirements which ensure the jet is due to the decay of a top quark, and the decay products of the top quark are fully contained within the jet. These requirements are placed on a _truth jet_ reconstructed from simulated hadronic particles before detector response is modeled, and are summarized in Table 1. They require that the truth jet be spatially oriented with the observed jet, have a ghost-associated bottom hadron [44], and satisfy a \(p_{\mathrm{T}}\)-dependent requirement on the \(k_{t}\) splitting scale \(\sqrt{d_{23}}\)[45].

### Simulated Data Set Contents

The simulated data set used in this study is publicly available through the CERN open data portal. Detailed documentation and training scripts are also available. The data set is split into two orthogonal sets of jets,

\begin{table}
\begin{tabular}{l|l} \hline \hline Jet Requirements & Top quark jet requirements \\ \hline Jet \(|\eta|<2.0\) & \(dR(\mathrm{jet,truth\ jet})<0.75\) \\ Jet \(p_{\mathrm{T,truth}}>350\) GeV & \(dR(\mathrm{truth\ jet,top\ parton})<0.75\) \\ Number of constituents \(\geq 3\) & Ungroomed truth jet mass \(>140\) GeV \\ Jet mass \(>40\) GeV & Number ghost associated \(b\)-hadrons \(\geq 1\) \\  & Truth jet \(\sqrt{d_{23}}>\exp\bigl{(}3.3-6.98\times 10^{-4}\times\mathrm{truth\ jet\ }p_{\mathrm{T}}\bigr{)}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: A summary of the requirements applied on all of the jets in the simulation samples to produce the training and testing sets. The additional top quark jet requirements constitute the truth labeling strategy, and are only applied to jets in the sample of simulated top quarks.

named _train_ and _test_. These consist of roughly 42 million and 2.5 million jets, respectively. In both sets, half of the jets are signal (\(Z^{\prime}\to t\bar{t}\)) and half are background (QCD). For each jet the data set contains contains the four-vector of the jet constituents, the four-vector of the jet itself, the 15 high-level quantities listed in Appendix B, a training weight (see Section 3.2), and a label to designate the jet as signal or background. See Table 1 for a summary of the top truth labeling strategy.

The four-vectors of the constituents contain of four quantities: the \(p_{\mathrm{T}}\), energy, pseudo-rapidity (\(\eta\)), and azimuthal angle (\(\phi\)). The \(p_{\mathrm{T}}\) and energy are given in units of Mega-electron-volts (MeV). The angular coordinates \(\eta\) and \(\phi\) give the location of the particle in the ATLAS coordinate system. Note that there is a discontinuity in the \(\phi\) coordinate as \(\phi=\pm\pi\) refer to the same azimuthal angle. Histograms of these four quantities are shown in Figure 2.

All data is stored in the widely-used HDF5 file format [46], which only supports rectangular arrays, requiring jets to have a uniform number of constituents. Jets are either zero-padded or truncated such that every entry has 200 constituents. As jet constituents are sorted by decreasing \(p_{\mathrm{T}}\), truncation eliminates the softest constituents in the jet.

Figure 2: Distributions of the of the jet constituent \(p_{\mathrm{T}}\), energy, \(\eta\), and \(\phi\). Note the jet radiation is isotropic in the azimuthal angle \(\phi\) and peaked at low pseudo-rapidity. The larger average number of constituents in top quark jets is also visible.

### Jet \(\pt\) and Training Weights

Processes that produce pairs of QCD jets feature a rapidly falling \(\pt\) distribution. Simulation of these events is done in intervals of \(\pt\) to allow for efficient generation of high-\(\pt\) events. The unweighted spectrum therefore contains unphysical features (see left panel of Fig. 3). A physical spectrum can be produced using re-weighting, though the weights of high \(\pt\) events can be many of orders of magnitude lower than those of low \(\pt\) events, which would pose a challenge for tagger training. Note that a physical \(\pt\) spectrum is not required for tagger training, so long as the tagger learns to classify jets across the whole \(\pt\) range. Here, weights are derived only to ensure that the signal and background \(\pt\) spectra match. This prevents the tagger from associating signal jets with a particular \(\pt\), and assigning high scores to background jets which happen to have this \(\pt\). Distributions of the jet \(\pt\) with training weights applied are shown in the right panel of Figure 3.

## 4 Simulated Data Pre-Processing

The training and performance of ML models can often be improved by applying _pre-processing_ to the data to eliminate irrelevant features and capitalize on well-known symmetries. For example, the exact location of a jet in \(\eta\) and \(\phi\) has no bearing on whether it is a top quark or QCD jet. Tagger performance can often be improved by shifting each jet so it is centered at the origin, reflecting the translational invariance of the problem. In this study, a pre-processing modeled after those used in Refs. [47] and [26] is applied to the jet constituents. This pre-processing is not applied to the simulated data contained in the public data set, though it can be reproduced using the scripts that accompany the release.

The pre-processing applied to the \(\eta\) and \(\phi\) coordinates of the jet constituents shifts the jet so that the highest-\(\pt\) constituent is at the origin, and rotates it such that the second highest-\(\pt\) constituent is on the negative \(\phi\) axis. If the third highest-\(\pt\) constituent is in the negative \(\eta\) half plane, the jet is reflected about the \(\phi\) axis. Constituent \(\pt\) and energy values are pre-processed by taking their logarithms, placing their values on an \(\mathcal{O}(1)\) scale.

Figure 3: Distributions of jet \(\pt\) for signal and background jets, before (left) and after (right) re-weighting the background events to match the signal distribution.

In addition to pre-processing the constituent four vectors, three other constituent-level quantities are calculated and used in training the constituent-based taggers. The first is the distance of the constituent from the jet axis, calculated as

\[R=\sqrt{\eta^{2}+\phi^{2}}, \tag{1}\]

where \(\eta\) and \(\phi\) are taken after pre-processing. The second (third) is calculated by normalizing the constituent \(p_{\mathrm{T}}\) (energy) by the total \(p_{\mathrm{T}}\) (energy) in the jet, and then taking the logarithm of this fraction. Including these three quantities produces a significant performance boost in the DNN, PFN, and ParticleNet taggers (see Section 5), relative to training with only the pre-processed four-vectors. Explicitly calculating quantities that are known to be useful and providing them as input is a common method of boosting an ML model's performance. Distributions of the pre-processed constituent level quantities used are shown in Figure 4. In addition to the constituent-level pre-processing, a simple pre-processing is also applied to the 15 high level quantities. Their distributions are shifted such that they have a mean of zero, and scaled such that they have unit standard deviation.

## 5 Top-Quark Taggers

Six top quark taggers were trained in this study, five constituent-based taggers and one high-level-quantity-based tagger which serves as a baseline. For details of the hyper-parameter tuning process and values of the hyper-parameters used for each model, see Appendix A. An important hyper-parameter for the constituent-based taggers is the maximum number of constituents considered, which impacts training time and memory consumption. In this study, a maximum of 80 constituents are considered by all constituent-based taggers. The majority of jets in the data set have fewer than 80 constituents, as shown in Figure 5. All taggers are trained to minimize the cross entropy loss with the Adam optimizer [48]. Training was continued until the loss measured over a held-out validation set failed to decrease for 20 epochs.

Figure 4: Distributions of the seven constituent-level quantities used as inputs to the top-quark taggers. See text for details.

### High Level Quantity Baseline

The high level quantity densely connected neural network tagger (hlDNN) is trained on the 15 high level quantities described in Appendix B. This tagger is modeled after the current ATLAS recommendation for tagging boosted top quarks [21], and serves as the baseline against the constituent based taggers are compared. The network is a standard multi-layer perceptron [49] (MLP).

### Densely Connected Neural Network

The simplest constituent-based tagger is a standard MLP operating directly on a vector of the constituent information, sorted by decreasing constituent \(p_{\text{T}}\). Given that each node in the network's layers are connected to every node in the previous layer, MLPs are often termed densely connected neural networks (DNN). Apart from the pre-processing of the constituent inputs, the DNN has no inductive bias, or specialization to the top tagging task. In particular it has no mechanism for making sense of the variable length nature of the list of jet constituents, save the ordering of the jet constituents by decreasing \(p_{\text{T}}\). Jets with less than 80 constituents are zero padded, and jets with more than 80 constituents are truncated, removing the lowest \(p_{\text{T}}\) constituents. The DNN uses all 7 of the pre-processed constituent level quantities described in Section 4 as inputs.

### Energy Flow Network

The Energy Flow Network [50] (EFN) is a model specifically engineered for jet tagging. It uses the DeepSets structure [51], which ensures permutation invariance with respect to the network inputs. It also naturally

Figure 5: Distributions of the number of constituents in a jet. For this study, all constituent-based taggers are trained while considering a maximum of 80 constituents.

handles the variable length of the list of jet constituents. The mathematical structure of the EFN limits it to consider information which is linear in constituent \(p_{\mathrm{T}}\), ensuring infrared and collinear (IRC) safety [52]. Since the constituent energy is not generally linear in constituent \(p_{\mathrm{T}}\) as UFO jet inputs are massive, this precludes it from being used as a network input. The EFN only uses the constituent's angular coordinates and the logarithm of the constituent \(p_{\mathrm{T}}\) as inputs.

### Particle Flow Network

The particle flow network [50] (PFN) has a very similar structure to the EFN. Like the EFN it naturally deals with the variable length of the list of jet constituents and enforces permutation invariance. However it relaxes the IRC safety requirement, permitting the constituent energy and any other constituent level quantity, to be used as inputs. The PFN is provided all 7 of the pre-processed constituent level quantities described in Section 4 as inputs.

### ResNet 50

ResNet 50 [53] is a large-scale convolutional neural network (CNN) designed for image classification tasks. CNNs operate on images, represented as a two dimensional array whose values give pixel intensity. Images are initially transformed via a stack of two-dimensional convolutional layers. A typical challenge for networks with many such layers is that gradients calculated through backpropagation become very small. ResNet 50 ameliorates this by allowing connections which skip convolutional layers and prevent gradients from vanishing.

Noting the similarity between energy deposits in the ATLAS calorimeter and standard two-dimensional images [22, 23], the jets are converted into "jet images" by binning each constituent's \(\eta\) and \(\phi\) coordinates into 64 bins, equally spaced in the range \([-2,2]\). The image is then a 64x64 square array where the pixel values are the sum of the raw \(p_{\mathrm{T}}\) of the constituents within the pixel, normalized such that the sum of the pixel intensity over the image is one. Pixel intensities are then rescaled by \(\log(1+100x)\) to make visible the lower \(p_{\mathrm{T}}\) patterns in the jet substructure.

Typically the images produced for single jets are very sparse, with only a few pixels containing non-zero values. Example signal and background jet images are shown in Figure 6, along with images averaged over the signal and background sets to show typical patterns in substructure. Note that constructing a jet image is a natural way of handling the variable length of the list of jet constituents and providing permutation invariance, but requires granulating the coordinates \(\eta\) and \(\phi\). Further, in this study the jet images do not explicitly encode constituent energy information.

To train ResNet 50 on jet images, several modifications from the standard architecture are required. First, the jet images have a single value associated with each pixel compared to the three values used for color images. This requires modifications to the network's input layer. The standard ResNet 50 architecture, which is defined for identification between many classes of images, is very large and is simplified to approach this binary classification task. This study follows [47] and divides the number of filters in the convolutional blocks of the network by 4, reducing the number of parameters in ResNet 50 by approximately an order of magnitude. A dropout layer with a drop probability of 0.5 is also added after the global pooling layer to reduce possible over-training.

### ParticleNet

ParticleNet [47] is a graph neural network (GNN) which represents jets as a graph, composed of nodes and edges. Each constituent in a jet is associated with a node, where are all of the input quantities are taken as features of the node. In this study, these are the 7 constituent level quantities defined in Section 4. Each node is connected by an edge to its \(k\) nearest neighbors in the \(\eta\)-\(\phi\) plane, where \(k\) is a network hyper-parameter. ParticleNet applies a specialized form of the EdgeConv operation [54] to this graph. This operation is similar to the two dimensional convolution used in CNNs, but defined on graphs instead of images.

Like the EFN and PFN, ParticleNet naturally handles the variable lengths of jets and enforces permutation invariance. However the EdgeConv operation acts on the feature vectors of pairs of constituents that are spatially close to each other, rather than each constituent separately. This allows ParticleNet to exploit the local relations between constituents, giving it access to more fine-grained information. ParticleNet also bears similarity to ResNet 50, in that it includes skip connections between EdgeConv blocks to prevent gradients from vanishing.

Figure 6: Example jet images (top) and averaged jet images (bottom), shown for background (left) and signal (right) jets. The concentration of radiation at the center and along the negative \(\Delta\phi\) axis, visible in the averaged images, is a result of the angular pre-processing scheme.

## 6 Tagger Performance

Performance metrics for the six taggers evaluated on the testing set are shown in Table 2, along with the number of trainable parameters and the inference time. ROC curves for all of the taggers are shown in Figure 7. In all metrics, ParticleNet achieves the best performance, followed by the PFN, the DNN, and the high-level-quantity-based tagger. Several constituent-based taggers outperform the high-level-quantity-based tagger, likely taking advantage of the additional information contained in the jet constituents.

The EFN and ResNet50 fail to outperform the high-level-quantity-based tagger, despite access to the constituent information. The structure of the EFN ensures IRC safety, but at the expense of not fully exploiting the available information. The weaker relative performance of ResNet50 is more suprising, given its strong performance in samples generated with parametric simulation [26]. This highlights the importance of developing taggers using realistic data sets and with advanced jet reconstruction. Both the EFN and ResNet 50 receive only the constituent \(\eta\), \(\phi\), and \(p_{\mathrm{T}}\) information, while the other constituent based taggers receive the full set of constituent level quantities in Figure 4. While the normalized constituent \(p_{\mathrm{T}}\) and constituent distance from the jet axis can be computed from the constituent \(\eta\), \(\phi\), and \(p_{\mathrm{T}}\), the constituent energy can not given the jet constituents are massive. This suggests that the constituent energy values contain essential information that can be utilized by the DNN, PFN, and ParticleNet but not the EFN and ResNet 50.

The background rejection as a function of the jet \(p_{\mathrm{T}}\) is shown in Figure 8 for the best-performing taggers and the hlDNN. Unlike the high-level-quantity-based-tagger, the constituent-based taggers' performance are best in the mid \(p_{\mathrm{T}}\) range of about 1-2 TeV. The high \(p_{\mathrm{T}}\) decrease in background rejection is suspected to result from the higher collimation of jets at high \(p_{\mathrm{T}}\), since the tracking detector and calorimeters have a limited ability to distinguish particles that are close together.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline \hline Model & AUC & ACC & \(\varepsilon_{bkg}^{-1}\) @ \(\varepsilon_{sig}=0.5\) & \(\varepsilon_{bkg}^{-1}\) @ \(\varepsilon_{sig}=0.8\) & \# Params & Inference Time \\ \hline ResNet 50 & 0.885 & 0.803 & 21.4 & 5.13 & 1,486,209 & 9 ms \\ EFN & 0.901 & 0.819 & 26.6 & 6.12 & 1,670,451 & 4 ms \\ hlDNN & 0.938 & 0.863 & 51.5 & 10.5 & 93,151 & 3 ms \\ DNN & 0.942 & 0.868 & 67.7 & 12.0 & 876,641 & 3 ms \\ PFN & 0.954 & 0.882 & 108.0 & 15.9 & 689,801 & 4 ms \\ ParticleNet & 0.961 & 0.894 & 153.7 & 20.4 & 764,887 & 38 ms \\ \hline \hline \end{tabular}
\end{table}
Table 2: The performance of each top quark tagger is measured with several metrics evaluated on the testing set. AUC is the area under the recieving-operator-characteristic curve, ACC is the accuracy, and \(\varepsilon_{bkg}^{-1}\) is the inverse background efficiency (or background rejection) evaluated at working points which yield a given signal efficiency across the entire testing set. For all metrics higher is better, and the table is sorted by increasing AUC. For all taggers, the statistical uncertainty on the metrics, evaluated through repeated training runs, is much smaller than the relative performance gaps between taggers. Also shown are the number of trainable parameters and inference time for each tagger. Inference time is measured using a Nvidia Tesla V100 GPU.

Figure 8: Background rejection, or inverse background efficiency (\(\varepsilon_{bkg}^{-1}\)), of several top quark taggers as a function of the jet \(p_{\rm\,T}\) for the \(\epsilon_{sig}=0.5\) (left) and \(\epsilon_{sig}=0.8\) (right) working points.

Figure 7: The inverse of efficiency in background QCD jets (\(\varepsilon_{bkg}^{-1}\)) versus the efficiency in top quark jets (\(\varepsilon_{sig}\)) for the top quark taggers studied. Three of the constituent-based taggers (ParticleNet, PFN, DNN) surpass the performance of the high-level-quantity-based tagger (hlDNN).

### Model Dependence

To estimate the dependence of tagger performance on variations in the parton shower and hadronization models, the tagger performance can be evaluated on simulated data samples produced with alternative descriptions of these processes. While a small set of alternatives do not necessarily span the space of possibilities, it does give an estimate of the sensitivity of tagger performance to uncertainties in QCD modeling.

In this study, the model dependence of the tagger performance is measured using samples of simulated events containing the Standard Model production of a \(t\bar{t}\) pair, where one top quark decays into hadrons and the other decays leptonically. The jet selection described in Section 3 ensures only the hadronic decay is considered. Three samples of simulated events are generated:

1. The PowhegBox v2 generator [55, 56] is used to produce \(t\bar{t}\) events with matrix elements evaluated at next-to-leading order (NLO) in QCD, using the NNPDF3.0NLO [57] PDF set and setting the \(h_{\text{damp}}\) parameter to 1.5 times the mass of the top quark. The partons are then showered using Pythia 8[31].
2. The same events as above are re-used, but the partons are showered using Herwig7[58, 59].
3. Events are generated with MadGraph5_aMC@NLO [60] and partons are showered with Pythia8.

No background jets are included in these data sets. To remove events in which a high \(p_{\text{T}}\) gluon recoils against the \(t\bar{t}\) system, each jet is additionally required to have an associated top quark, and the truth top quarks are required to have \(\Delta R>2.0\).

After training on the \(Z^{\prime}\to t\bar{t}\) events described above, taggers are evaluated on the three alternative samples. The signal efficiency in each sample is measured using the threshold which results in a signal efficiency of 0.5 or 0.8 in the \(Z^{\prime}\to t\bar{t}\) testing sample. The dependence of the signal efficiency on jet \(p_{\text{T}}\) is shown for the hlDNN and EFN in Figure 9, and for the PFN and ParticleNet in Figure 10.

The high-level-quantity-based tagger shows a decrease in signal efficiency of 15-20% in the highest \(p_{\text{T}}\) bins for the \(\epsilon_{sig}=0.5\) working point. The EFN shows a slightly smaller decrease in signal efficiency. In contrast, the PFN and ParticleNet show a larger decrease in signal efficiency of 20-40% for the \(\epsilon_{sig}=0.5\) working point. This suggests that the performance of the PFN and ParticleNet is more dependent on QCD modeling than the high-level-quantity-based tagger. However, since the training and alternative data samples are taken from simulations of different physical processes, other effects such as differences in the initial and final state radiation between the \(Z^{\prime}\to t\bar{t}\) and SM \(t\bar{t}\) processes could also lead to a decrease in signal efficiency. For the PFN and ParticleNet, the signal efficiency evaluated over an alternative data sample decreases with \(p_{\text{T}}\) regardless of the parton shower and hadronization model used. This suggests the trend results from the differences between the simulated processes, and not the differences in the QCD modeling. The smaller decrease in signal efficiency of the EFN is likely due to the IRc safety requirement, implying that the differences between the training sample and the three alternative data samples are primarily due to IRC unsafe information.

Figure 9: Comparison of the signal efficiency (\(\varepsilon_{\rm sig}\)) of top quark taggers in several samples of simulated top quarks, as a measure of model dependence. Shown is the efficiency using the threshold which results in an average signal efficiency of 0.50 (left) or 0.80 (right) in the \(Z^{\prime}\to t\bar{t}\) testing sample. The top row shows the efficiency for the high-level-quantity-based tagger (hIDNN), and the bottom row shows the efficiency for the EFN.

Figure 10: The signal efficiency of top quark taggers for the alternative samples, shown for the PFN (top row) and ParticleNet (bottom row).

## 7 Conclusion

The performance of constituent-based jet taggers for boosted top quarks in realistic simulated collisions is presented. Three of the constituent-based taggers trained in this study (DNN, PFN, ParticleNet) show stronger performance than a tagger which uses high-level quantities. Of these, ParticleNet shows the strongest performance, followed by the PFN and the DNN. These taggers also exhibit a larger decrease in performance in the alternative samples compared to the high-level-quantity-based tagger. The background rejection of these constituent-based taggers is greatest for jets with \(p_{\mathrm{T}}\) between 1 and 2 TeV, in constrast to the baseline tagger whose background rejection falls with increasing jet \(p_{\mathrm{T}}\).

Other constituent-based taggers studied, the EFN and ResNet50, did not surpass the performance of the baseline established by the high-level-quantity-based tagger. This is in contrast to behavior seen in simulated samples generated with the parametric simulation Delphes, underscoring the importance of the development of taggers in a realistic context. The simulated data set used in this study is made available for such future development.

## Appendix A Hyper Parameter Tuning and Lists

The Ray Tune software package [61] was used to automate and parallelize the many model trainings needed to tune the hyper-parameters of each tagger. Ray Tune interfaces with several "search algorithms" which suggest new hyper-parameter settings to be tested. In this study, the Hyperopt tree of parzen estimators algorithm [62, 63] was used, due to its ability to handle conditional search spaces where one hyper-parameter might be conditioned on the setting of another. For all taggers the algorithm minimized the validation loss evaluated over a validation set, which was taken to be one fifth of the training set selected at random for each training run.

Four of the taggers trained in this study (hIDNN, DNN, EFN, and PFN) were tuned using the combination of Ray Tune and Hyperopt. The other two taggers (ResNet 50, ParticleNet) are large models which require long training times, making running many trials infeasible. For this reason the Asynchronous Hyperband Scheduler [64] (ASHA) was used to aggressively terminate poorly performing trials before they reached the maximum number of training epochs. This greatly accelerated the hyper-parameter tuning process, allowing 48 and 24 training runs to be conducted for ResNet 50 and ParticleNet, respectively. Given the smaller number of trials, the tuning of ResNet 50 and ParticleNet should not be considered extensive. For ResNet 50, no hyper-parameter settings were found which improved on the performance of the settings used in [47], so these settings are taken to be optimal.

Results of the hyper-parameter tuning process are shown in the Table below. The hyper-parameters listed in a italics were not optimized using the procedure described above, but are not expected to significantly influence the tagger performance.

\begin{tabular}{l|l} \hline \hline Model & Hyper-parameters \\ \hline \multirow{6}{*}{hlDNN} & Hidden Layers: 5 \\  & Nodes per Layer: 180 \\  & _Activation Functions: ReLU_ \\  & _Kernel Initialization: glorot uniform_ \\  & Learning Rate: \(4\times 10^{-5}\) \\  & Batch Size: 250 \\  & Batch Normalization: not used \\ \hline \multirow{6}{*}{DNN} & Hidden Layers: 5 \\  & Nodes per Layer: 400 \\  & _Activation Functions: ReLU_ \\  & _Kernel Initialization: glorot uniform_ \\  & L1 Regularization: \(2\times 10^{-4}\), applied to all layers \\  & Learning Rate: \(1.2\times 10^{-5}\) \\  & Batch Size: 250 \\  & Batch Normalization: applied before activation function for all \\  & layers except output layer \\ \hline \multirow{6}{*}{EFN} & \(\Phi\) Hidden Layers: 5 \\  & \(\Phi\) Nodes per Layer: 350 \\  & Latent Dropout: 0.084 \\  & \(F\) Hidden Layers: 5 \\  & \(F\) Nodes per Layer: 300 \\  & \(F\) Dropout: 0.036 \\  & _Activation Functions: ReLU_ \\  & _Kernel Initialization: glorot normal_ \\  & Learning Rate: \(6.3\times 10^{-5}\) \\  & Batch Size: 350 \\ \hline \multirow{6}{*}{PFN} & \(\Phi\) Hidden Layers: 5 \\  & \(\Phi\) Nodes per Layer: 250 \\  & Latent Dropout: 0.072 \\  & \(F\) Hidden Layers: 5 \\  & \(F\) Nodes per Layer: 500 \\  & \(F\) Dropout: 0.022 \\  & _Activation Functions: ReLU_ \\  & _Kernel Initialization: glorot normal_ \\  & Learning Rate: \(7.9\times 10^{-5}\) \\  & Batch Size: 250 \\ \hline \hline \end{tabular}

## Appendix B High Level Quantities

The high level quantities included in the public simulated data set and used to train the baseline high level quantity tagger are listed in Table 3, and their distributions are shown in Figure 11.

\begin{table}
\begin{tabular}{c|l|l} \hline \hline Model & \multicolumn{2}{c}{Hyper-parameters} \\ \hline  & Bottom Layer: 7x7 2D convolution with strides (2, 2) and zero padding & \\  & Number of Stages: 4 & \\  & Blocks per Stage: (3, 4, 6, 3) & \\  & _Block Type: bottleneck_ & \\  & Block Output Filters: (64, 128, 256, 512) & \\  & _Activation Functions: ReLU_ & \\  & _Kernel Initialization: he uniform_ & \\  & Batch Normalization Momentum: 0.1 & \\  & _Global Pooling: average_ & \\  & Initial Learning Rate: \(1\times 10^{-2}\) & \\  & _Scheduler: decrease learning rate by factor of 0.1 every 10 epochs_ & Batch Size: 256 \\ \hline  & \(\Phi\) Number of Stages: 3 & \\  & Blocks per Stage: (3, 3, 3) & \\  & Block Output Features: (64, 224, 384) & \\  & k Nearest Neighbors: 18 & \\  & Top Layer Nodes: 125 & \\  & _Activation Functions: ReLU_ & \\  & _Kernel Initialization: glorot normal_ & \\  & Batch Normalization Momentum: 0.7 & \\  & Global Pooling: max & \\  & Learning Rate: \(4.2\times 10^{-4}\) & \\  & Batch Size: 250 & \\ \hline \hline \end{tabular}
\end{table}
Table 3: A listing of the 15 quantities used to train the baseline high level quantity taggerFigure 11: Histograms of the 15 high level quantities used as inputs to the high level quantity baseline tagger.

## References

* [1] P. Zyla et al., _Review of Particle Physics_, PTEP **2020** (2020) 083C01, and 2021 update (cit. on p. 2).
* [2]ATLAS Collaboration, _Measurement of the charge asymmetry in highly boosted top-quark pair production in \(\sqrt{s}\) = 8 TeV \(pp\) collision data collected by the ATLAS experiment_, Phys. Lett. B **756** (2016) 52, arXiv: 1512.06092 [hep-ex] (cit. on p. 2).
* [3]ATLAS Collaboration, _Measurement of the energy asymmetry in \(t\bar{t}j\) production at 13 TeV with the ATLAS experiment and interpretation in the SMEFT framework_, (2021), arXiv: 2110.05453 [hep-ex] (cit. on p. 2).
* [4]ATLAS Collaboration, _Measurement of jet-substructure observables in top quark, \(W\) boson and light jet production in proton-proton collisions at \(\sqrt{s}=13\) TeV with the ATLAS detector_, JHEP **08** (2019) 033, arXiv: 1903.02942 [hep-ex] (cit. on p. 2).
* [5]CMS Collaboration, _Measurement of differential \(\bar{\mathrm{t}}\) production cross sections using top quarks at large transverse momenta in \(pp\) collisions at \(\sqrt{s}\) = 13 TeV_, Phys. Rev. D **103** (2021) 052008, arXiv: 2008.07860 [hep-ex] (cit. on p. 2).
* [6]CMS Collaboration, _Search for electroweak production of a vector-like quark decaying to a top quark and a Higgs boson using boosted topologies in fully hadronic final states_, JHEP **04** (2017) 136, arXiv: 1612.05336 [hep-ex] (cit. on p. 2).
* [7]ATLAS Collaboration, _Search for single production of a vector-like \(T\) quark decaying into a Higgs boson and top quark with fully hadronic final states using the ATLAS detector_, Phys. Rev. D **105** (2022) 092012, arXiv: 2201.07045 [hep-ex] (cit. on p. 2).
* [8]ATLAS Collaboration, _Search for \(t\bar{t}\) resonances in fully hadronic final states in \(pp\) collisions at \(\sqrt{s}\) = 13 TeV with the ATLAS detector_, JHEP **10** (2020) 061, arXiv: 2005.05138 [hep-ex] (cit. on p. 2).
* [9]CMS Collaboration, _Search for a W' boson decaying to a vector-like quark and a top or bottom quark in the all-jets final state at \(\sqrt{s}\) = 13 TeV_, (2022), arXiv: 2202.12988 [hep-ex] (cit. on p. 2).
* [10]CMS Collaboration, _Search for single production of a vector-like \(T\) quark decaying to a top quark and a Z boson in the final state with jets and missing transverse momentum at \(\sqrt{s}\) = 13 TeV_, JHEP **05** (2022) 093, arXiv: 2201.02227 [hep-ex] (cit. on p. 2).
* [11]CMS Collaboration, _Search for a heavy resonance decaying to a top quark and a W boson at \(\sqrt{s}\) = 13 TeV in the fully hadronic final state_, JHEP **12** (2021) 106, arXiv: 2104.12853 [hep-ex] (cit. on p. 2).
* [12]T. Plehn, G. P. Salam and M. Spannowsky, _Fat Jets for a Light Higgs_, Phys. Rev. Lett. **104** (2010) 111801, arXiv: 0910.5472 [hep-ph] (cit. on p. 2).
* [13]T. Plehn, M. Spannowsky, M. Takeuchi and D. Zerwas, _Stop Reconstruction with Tagged Tops_, JHEP **10** (2010) 078, arXiv: 1006.2833 [hep-ph] (cit. on p. 2).
* [14]D. E. Kaplan, K. Rehermann, M. D. Schwartz and B. Tweedie, _Top Tagging: A Method for Identifying Boosted Hadronically Decaying Top Quarks_, Phys. Rev. Lett. **101** (2008) 142001, arXiv: 0806.0848 [hep-ph] (cit. on p. 2).
* [15]J. Thaler and L.-T. Wang, _Strategies to Identify Boosted Tops_, JHEP **07** (2008) 092, arXiv: 0806.0023 [hep-ph] (cit. on pp. 2, 20).