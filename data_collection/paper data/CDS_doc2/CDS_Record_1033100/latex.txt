# The Detector Control System of the ATLAS SemiConductor Tracker

during Macro-Assembly and Integration

A. Abdesselam\({}^{a}\), A. Barr\({}^{a}\), S. Basiladze\({}^{b}\), R.L. Bates\({}^{f}\), P. Bell\({}^{c,d}\), N. Bingefors\({}^{e}\), J. Bohm\({}^{p}\), R. Brenner\({}^{e}\), M. Chamizo Llatas\({}^{g}\), A. Clark\({}^{s}\), G. Codispoti\({}^{h}\), A.P. Collijn\({}^{o}\), S. D'Auria\({}^{f}\), O. Dorholt\({}^{i}\), F. Doherty\({}^{c}\), P. Ferrari\({}^{d}\), D. Ferrere\({}^{g}\), E. Gornickl\({}^{j}\), S. Koperny\({}^{k}\), R. Lefevre\({}^{g}\), L-E. Lindquist\({}^{c}\), P. Malecki\({}^{j}\), B. Mikulec\({}^{g}\), B. Mohn\({}^{l}\), J. Pater\({}^{c}\), H. Pernegger\({}^{d}\), P. Phillips\({}^{m}\), A. Robichaud-Veronneau\({}^{g}\), D. Robinson\({}^{r}\), S. Roe\({}^{d}\), H. Sandaker\({}^{d,i}\), A. Sfyrla\({}^{g}\), E. Stanecka\({}^{j}\), J. Stastny\({}^{p}\), G. Viehhauser\({}^{a}\), J. Vossebeeld\({}^{n}\), P. Wells\({}^{d}\)

\({}^{a}\) Oxford University, Oxford, United Kingdom

\({}^{b}\) Nuclear Physics Institute of the Moscow State University, Moscow, Russia

\({}^{c}\) University of Manchester, Manchester, United Kingdom

\({}^{d}\) European Organization for Nuclear Research (CERN), Geneva, Switzerland

\({}^{e}\) Uppsala University, Uppsala, Sweden

\({}^{f}\) University of Glasgow, Glasgow, United Kingdom

\({}^{g}\) University of Geneva,Geneva, Switzerland

\({}^{h}\) University of Bologna, Bologna, Italy

\({}^{i}\) University of Oslo, Oslo, Norway

\({}^{j}\) Institute of Nuclear Physics (IFJ PAN), Polish Academy of Sciences, Cracow, Poland

\({}^{k}\) Faculty of Physics and Applied Computer Science, AGH University of Science and Technology,Cracow, Poland

\({}^{l}\) University of Bergen, Bergen, Norway

\({}^{m}\) Rutherford Appleton Laboratory (RAL), Didcot Oxon, United Kingdom

\({}^{n}\) University of Liverpool, Liverpool, United Kingdom

\({}^{o}\) The National Institute for Nuclear Physics and High Energy Physics (NIKHEF), Amsterdam, The Netherlands

\({}^{p}\) Institute of Physics of the Academy of Sciences of the Czech Republic, Prague, Czech Republic

\({}^{r}\) University of Cambridge, Cambridge, United Kingdom

###### Abstract

The ATLAS SemiConductor Tracker (SCT) is one of the largest existing semiconductor detectors. It is situated between the Pixel detector and the Transition Radiation Tracker at one of the four interaction points of the Large Hadron Collider (LHC). During 2006-2007 the detector was lowered into the ATLAS cavern and installed in its final position.

For the assembly, integration and commissioning phase, a complete Detector Control System (DCS) was developed to ensure the safe operation of the tracker. This included control of the individual powering of the silicon modules, a bi-phase cooling system and various types of sensors monitoring the SCT environment and the surrounding test enclosure. The DCS software architecture, performance and operational experience will be presented in the view of a validation of the DCS for the final SCT installation and operation phase.

Keywords:LHC, ATLAS, SCT, DCS.

## 1 Introduction

The ATLAS detector is a multi-purpose detector currently being constructed at one of the four interaction points along the 27 km circumference of the LHC accelerator. The detector will study p-p collisions with 14 TeV centre-of-mass energy at a design luminosity of 10 \({}^{34}\) cm\({}^{-2}\)s\({}^{-1}\) and with a 25 ns bunch crossing separation time. Three layers of trackers, calorimeters and muon chambers are centered around the interaction point. The central tracker, the Inner Detector (ID), is situated inside a 2 T central superconducting solenoid magnet (see Fig. 1). It consists of the Pixel detector (Pixel) closest to the beam, surrounded by the SemiConductor Tracker (SCT) detector and the outer Transition Radiation Tracker (TRT). During the envisaged 10 years of operation the total particle fluence is expected to be approximately 2 x 10 \({}^{14}\) cm\({}^{-2}\) 1-MeV neutron equivalent at the innermost layer of the SCT [1], resulting in a continuous degradation of the detectors.

he SCT will provide at least four space points per track and will have a pseudorapidity coverage up to \(\eta=2.5\). To achieve this, the detector consists of one central barrel, where 2112 barrel modules have been mounted to cover the full outer surfaces of four cylinders, and one end-cap in each forward region, where the 1976 end-cap modules have been mounted on 18 disks.

The barrel detector modules [2] are all identical and made of four single-sided p-on-n microstrip sensors with 80 \(\mu\)m pitch bonded together and glued back-to-back at 40 mrad stereo angle to provide two-dimensional track reconstruction. Each side has 768 strips of 12 cm total active length which are read out by 12 ABCD3TA [3] radiation hard ASICs, mounted on a copper/kapton hybrid located at the module centre. The end-cap modules [4] have four different radial geometries depending on their position relative to the collision centre and their read-out hybrid is mounted at the sensor edge. A binary read-out architecture has been chosen, being the most cost-effective implementation meeting the performance requirements.

Parallel to the detector construction a detector control system (DCS) has been developed. The DCS must ensure the safe and reliable operation and control of the SCT. The large number of read-out channels, and the required relatively rapid response time of the system are two of the main challenges faced by the SCT DCS system. Preliminary DCS results have been presented in [5, 6, 7]. Currently, the SCT DCS is in its final development stage, after being extensively tested and validated during detector assembly, integration and commissioning. A summary of these results, focusing on the validation of the safe operation and control of the SCT, is presented in this paper.

Figure 1: The ATLAS Inner Detector. The SCT can be seen in the middle consisting of 4 cylindrical barrels and 18 disks, 9 at each end. The thermal enclosures protect the SCT and Pixel environmental conditions.

## 2 SCT DCS Overview

The main goal of the SCT DCS is to control and monitor the supply of powering, cooling and other necessary control signals as well as to protect the detector from any failure or error conditions. If any situation occurs where the detector is in danger, the DCS system should provide a rapid response, either by software actions (if possible) or by hardware interlocks. The SCT DCS also needs to provide a safe communication between the subsystems, with the ATLAS DCS as well as with the SCT data acquisition system (DAQ). Both transitions between operation states and errors need to be propagated to the ATLAS DCS and to the SCT DAQ in an unambigous way.

Several power lines are needed to operate a single SCT module: the bias voltage for sensor operation and several low voltages for the read-out electronics and the conversion of optical signals into electrical signals. These voltages are all controlled by the DCS system. The DCS system also provides slow commands for basic configuration of the SCT detector modules.

Due to the large heat dissipation and to the increasing damage due to the radiation during the SCT operational lifetime, the modules need to be cooled and to be in a dry nitrogen atmosphere. The cooling is provided by a bi-phase C\({}_{3}\)F\({}_{8}\) cooling system [8] running at -25 \({}^{\circ}\)C which cools the detector modules to the operating temperature of -7 \({}^{\circ}\)C. The SCT detectors are surrounded by a thermal enclosure, which permits cold operation.

The radiation damage of the SCT sensors results in increased sensor leakage current, type inversion, increasing depletion voltage and signal degradation due to charge trapping. It is expected that for the worst affected detector modules the bias voltage needs to be raised from the nominal value of 150 V up to a maximal value of 480 V. The initial power consumption of the front-end ASICs is expected after irradiation to increase from 5.5 W nominal to 7.5 W maximum [2]. The SCT DCS monitors the sensor leakage and electronics currents.

The ATLAS DCS is based on a custom solution developed for the LHC experiments. The sub-detector hardware is read out by a standard Controller Area Network1 (CAN) fieldbus and a custom developed OPC CANopen server [9]. A custom Embedded Local Monitor Board (ELMB) [10] has

Figure 2: Overview of the global ATLAS DCS structure. Both the SCT and the ID DCS are managed by separate Subdetector Control stations (SCS) which each controls several independent PVSS projects (see text).

been developed, to provide an interface between the subdetector equipment and the read-out system where this is needed. A Supervisory Control and Data Acquisition system [11], PVSS II2, controls the data communication, alarm handling and data display. The core of the software is provided by the subdetectors individually, whereas the CERN Joint Controls Project (JCOP) [12] has developed several software tools to provide interfaces to the most common hardware. JCOP also provides the top layer solution for the overall control of ATLAS based on a Finite State Machine (FSM) [13] written in SMI++3 which interacts with the subdetector DCS via state transitions and error messages.

Footnote 2: [http://www.pvss.com](http://www.pvss.com)

Footnote 3: State Management Interface, http://http://[http://smi.web.cern.ch/smi/](http://smi.web.cern.ch/smi/)

The main aim of the common ATLAS DCS is to make it as homogeneous as possible, with rapid communication, without compromising the functionality of the subdetector DCS. The overall architecture of the ATLAS DCS, including the SCT DCS and the ID DCS, for the part which is relevant to the SCT detector, is shown in Fig. 2.

## 3 Hardware and Software Architecture

The core of the SCT DCS is divided into two subsystems, each controlling and monitoring a part of the detector system: the Power Supply system controlled by the _MoPS_ software project and the Environmental monitoring controlled by the _Envr_ software project, as described below. In addition, several DCS systems common to the ID subsystems have been developed by the SCT group and

Figure 3: The SCT DCS configuration (subset) used during surface tests. The Power Supply, Cooling, Environmental and Test Enclosure projects are shown, as well as their connection to the hardware.

are essential to the operation of this detector, notably the Cooling System and the Test Enclosure (monitoring an early prototype of the Thermal Enclosure). Fig. 3 shows a subset of the hardware controlled by the four SCT and ID DCS systems. All systems were linked together during the integration phase as distributed PVSS projects to allow DCS to DCS communication.

### Power Supply DCS

The largest part of the SCT DCS system is the Power Supply (PS) system [14]. Each detector module is powered and controlled by two independent floating power supplies. The high voltage supplies provide the 150 - 480 V nominal bias voltage necessary to deplete the sensor. The low voltage power supplies provide digital and analogue voltages to the read-out ABCD3TA ASIC [3] and the ASICs [15] used for optical links. The PS system also provides the bias voltage for the p-i-n diodes and the control voltage which sets the VCSEL currents [15]. In addition to the power, each module is provided with two signal lines to reset the module and select which of the two clock inputs to be used, four sense wires that remotely probe the V\({}_{dd}\) and V\({}_{cc}\) voltages. Finally, two current sources on each Low Voltage (LV) power channel supplying the Negative Thermal Coefficient (NTC) (R25 = 10 k\(\Omega\)\(\pm\) 5%) thermistors mounted on the SCT detector module, two for the barrel modules (one on each side) and one for the end-cap modules, which allow the hybrid temperatures to be read. Several parameters to set limits on the currents and to tune and control the PS cards are programmable.

\begin{table}
\begin{tabular}{|l c c c c c|} \hline
**Param.** & **Typical** & **Max.** & **Trip/Limit** & **FW trip** & **CC trip** \\ \hline V\({}_{bias}\) & 150 V & 480 V & 480 V & set volts & 10 V1  \\ V\({}_{cc}\) & 3.5 \(\pm\) 0.2 V & 5.1 V & 5.1 V & - & - \\ V\({}_{dd}\) & 4.0 \(\pm\) 0.2 V & 5.1 V & 5.1 V & - & - \\ V\({}_{ccPS}\) & 5.1 V2  & 10 V & 10.2 V & - & - \\ V\({}_{ddPS}\) & 5.0 V2  & 10 V & 10.2 V & - & - \\ V\({}_{VCSL}\) & 4.0 V & 6.6 V & 9.6 V3  & - & - \\ V\({}_{PIN}\) & 6.0 V & 10 V & 13.0V3  & - & - \\ I\({}_{bias}\) & 0.3 \(\mu\)A & 5 mA & 5 mA & 5 \(\mu\)A & 3 \(\mu\)A \\ I\({}_{cc}\) & 900 mA & 1300 mA & 1500 mA & - & - \\ I\({}_{dd}\) & 580 mA & 1300 mA & 1500 mA & - & - \\ I\({}_{VCSL}\) & 2 mA & 10 mA & 10 mA4  & - & - \\ I\({}_{PIN}\) & 0.2 mA & 2 mA & 2 mA4 & - & - \\ T1\({}_{module}\) & 0 \({}^{\circ}\)C & -29 \({}^{\circ}\)C - 56 \({}^{\circ}\)C & - & \(>\)38 \({}^{\circ}\)C & - \\ T2\({}_{module}\) & 0 \({}^{\circ}\)C & -29 \({}^{\circ}\)C - 56 \({}^{\circ}\)C & - & \(>\)38 \({}^{\circ}\)C & - \\ \hline \end{tabular}
\end{table}
Table 1: The 14 PS parameters per detector module, their typical and maximum values, hardware trips and limits, programmable firmware trips and the Crate Controller trips.

To ensure safe operation of each detector module, several safety mechanisms are implemented in the PS system. Each High Voltage (HV) channel has an absolute (hardware) over-current and over-voltage protection that automatically trips the voltage if any of these parameters exceed the limits. The HV current trip limit is programmable and is usually set to a value lower than the hardware protection. The HV firmware constantly monitors the voltage on the channel output and switches off the channel if the voltage exceeds the last set value.

Each LV channel contains a hardware protection for the analog and digital voltages and currents at the output of the LV module. If any of the trip conditions occurs and lasts for a time longer than a predefined trip reaction time, the hardware logic interrupts the micro-controller to trip the LV power module. The over-voltage trip reaction time is fixed to 10 \(\mu\)s while the over-current trip reaction time is a programmable parameter which can be set within a range of 2 ms up to 510 ms in steps of 2 ms. The default value is 2 ms. The VCSEL control voltage, the PIN-bias and the corresponding currents are limited by the hardware to the values given in Table 1. The LV channel also checks for a limit on the detector module hybrid temperature and makes a trip if the temperature is too high. The maximum allowed temperature of the hybrid is 38 \({}^{\circ}\)C. This upper limit is hard-coded in the LV channel firmware, but at the same time it is a programmable parameter that can be set to a lower value. The temperature trip is issued only by the firmware, the hardwired protection against over heating the module is provided by the interlock system described in Section 3.4. An overview of all read-out and set parameters associated with one module is found in Table 1.

The PS system has a modular structure. Each PS crate contains 12 LV cards with four output channels each and six HV cards with eight output channels each. The crates are equipped with a Crate Controller (CC) board built around the ELMB card. The CC is an interface between the PS boards and the higher levels of the control system. It handles multiplexed commands from the supervisory DCS application, translates those into sequences of single commands and propagates them to the PS boards using a custom communication protocol. This protocol uses an 8-bit parallel address/data bus and several control signal lines, physically located on the backplane of the crate. The communication with the DCS computer is made using a CAN bus network, according to the CANopen [16] communication protocol. The CC provides safety mechanisms for bias current and module temperature complementary to the hardware and firmware trips. There is a programmable temperature trip, which turns off both LV and HV channels, and a software HV over-current trip. Both are set to lower action levels than the trips implemented in channel firmware and are intended to gently ramp down a channel.

The PS crate is also connected to the hardware interlock system via a SCT Interlock Card (SIC), which distributes interlock signals through the backplane to the LV and HV cards.

The _Monitor Power Supply_ (MoPS) project provides a graphical user interface for monitoring the online parameters and for sending commands to the hardware. An effort has been put into making a project architecture with navigation possibilities that permit access grouped according to both the hardware structure (e.g. crate, channel) and the detector structure (e.g. barrels, disks, cooling circuits).

As part of the overall ATLAS strategy, the DAQ controls the operation of the detectors. This is natural for the SCT since many parameters in the MoPS project are important for the data quality. This is implemented via a Distributed Information Management (DIM) system [17] which is enabled using the PVSS-DIM toolkit. It provides the DAQ-DCS Communication (DDC) [18]enabling the DAQ to monitor PVSS parameters and to send commands to the MoPS DCS. It is therefore possible to control and monitor the PS hardware either directly using the DCS or via the DAQ interface.

Operating the PS system is a non trivial task due to the large number of PS channels and the corresponding monitoring and control parameters. In total, one PS crate needs to handle approximately 2500 different variables. Of those, around 1000 are purely read-out values for the modules that are updated every 15 seconds to ensure fast detection of any alarm state in the system. The rest of the parameters are mainly used to configure and control the PS cards in the three possible states: OFF, STANDBY and ON (see Section 4.1).

Due to the size and complexity of the system it is challenging to have a high read-out frequency, required for detector safety, and at the same time to keep the mean CAN bus occupancy below 65% [19]. Sending the configuration parameters (14 CAN frames for one PS channel) significantly increases the bus occupancy and slows down the data handling in the OPC server and the MoPS project (Section 4.2). Hence, to comply with the system read-out and setup time as requested by ATLAS DCS, and at the same time to respect the CAN bus occupancy bounds, three sets of configuration parameters with associated alarm limits for each PS channel are stored in the non-volatile EEPROM memory on the CC board. The EEPROM memory also stores the mapping between PS channels and DAQ groups. Using the stored mapping, the custom command format allows for the configuration and passing of commands to individual DAQ groups. Each group action requires only one frame sent over the CAN bus, significantly reducing the CAN bus occupancy.

The configurations are written to the EEPROM by the MoPS project, typically before running the detector, using the standard CANopen SDO data transfer and handshaking mechanism between the MoPS and the CC. The EEPROM can be overwritten every time new detector settings are defined, and has an endurance of at least 100 000 write/erase cycles.

The PS system needed for the complete SCT detector comprises 88 PS crates connected to 16 CAN bus branches (8 to each of the two underground counting rooms). A pair of CAN buses is read out by one computer. This granularity ensures a satisfactory performance and a reasonable number of computers on the network. During integration of the SCT detector and tests with cosmic rays, up to 12 crates were read out by one computer. This corresponds in total to approximately 27500 parameters, 1/8 of the final system.

### Environmental DCS

Environmental monitoring is required to supervise the running conditions for the SCT. Four types of quantities monitor the detector: the temperature of the carbon fiber structure (mechanical temperatures), the temperature of the air inside the detector volume, the temperature of the cooling pipes (sensors located at the exhaust of the pipe) also used for the interlock (Section 3.4), and the relative humidity. The number of sensors of each type is shown in Table 2.

The environmental conditions were monitored by two different systems during reception and combined tests. The _Environmental DCS_ project (Envr) handled the temperature and humidity sensors located inside the detector volume while the _Test Enclosure_ project took care of extra sensors placed inside a temporary thermal enclosure (the test box) that provided thermal isolation from outside. The test enclosure was used initially since the final thermal enclosure was not ready t for assembly. After installation of the thermal enclosure, the sensors from the test enclosure project were used to read out the dew point at the exhaust air line of the detector.

The Envr project displays the monitored values for the different sensors and calculates alarms and warnings that are propagated to the MoPS project if the values are outside a safe range (Section 4.4). The separate test enclosure project has the same functionalities for the sensors inside the test enclosure, as well as calculating the dew point using the temperature and humidity readings.

### ID Cooling DCS

The ID cooling system [8] cools the SCT and Pixel detectors to the required operational temperature for irradiated detectors of \(-7\)\({}^{\circ}\)C.

The total final system must remove up to 85 kW of heat from the Inner Detector and have a stability better than \(\pm 2\)\({}^{\circ}\)C in order to avoid thermal shocks and cycles. To achieve this the detectors will be cooled using an evaporative fluorocarbon cooling system with C \({}_{3}\)F \({}_{8}\) running in thin wall CuNi (SCT) [2] or Al (Pixel) [23] cooling tubes through the detectors with good thermal contact to each module. For initial testing, warm runs of \(+15\)\({}^{\circ}\)C have been made. For warm runs, the hybrid temperature on the SCT modules is 27 \({}^{\circ}\)C. By comparison, the hybrid will be kept at 0 \({}^{\circ}\)C during cold runs.

The cooling process is controlled by an independent Programmable Logic Controller (PLC) which is read out by a PVSS project via ethernet and a Schneider OPC server. The monitored

\begin{table}
\begin{tabular}{|l c c c c c|} \hline
**Part** & \begin{tabular}{c} **Cooling** \\ **(Interlock)** \\ \end{tabular} & \begin{tabular}{c} **Cooling** \\ **(Monitoring)** \\ \end{tabular} & 
\begin{tabular}{c} **Mechanical** \\ **(Monitoring)** \\ \end{tabular} & **Air** & **Humidity** \\ \hline Barrel 3 & 36 & 0 & 9 & 32 & 3 \\ Barrel 4 & 44 & 0 & 9 & 32 & 4 \\ Barrel 5 & 52 & 0 & 11 & 32 & 4 \\ Barrel 6 & 60 & 0 & 14 & 32 & 4 \\ External & 0 & 0 & 0 & 8 & 0 \\ \hline Total Barrel & 192 & 0 & 43 & 136 & 15 \\ \hline Disk 1 & 16 & 4 & 4 & 9 & 1 \\ Disk 2 & 24 & 6 & 4 & 9 & 1 \\ Disk 3 & 24 & 6 & 4(2) & 9 & 1 \\ Disk 4 & 24 & 6 & 4 & 9 & 1 \\ Disk 5 & 24 & 0 & 4 & 9 & 1 \\ Disk 6 & 24 & 6 & 4 & 3 & 1 \\ Disk 7 & 16 & 0 & 4 & 3 & 1 \\ Disk 8 & 16 & 0 & 4 & 3 & 1 \\ Disk 9 & 8 & 2(0) & 4 & 6(7) & 0 \\ Cylinder & 0 & 29(27) & 0 & 0 & 10 \\ \hline Total EC A & 176 & 59 & 36 & 60 & 18 \\ \hline Total SCT & 544 & 114 & 113 & 257 & 51 \\ \hline \end{tabular}
\end{table}
Table 2: The number of environmental sensors and their physical distribution. Numbers in parentheses are for end-cap (EC) C. The total number for the SCT takes into account that there are two end-caps.

parameters from different parts of the cooling plant are listed in Table 3.

### Interlock system

The main purpose of the interlock system is to protect the silicon detector modules from overheating if the cooling is stopped or fails. To achieve a reliable and predictable system, the interlock is built using the two temperature sensors located at the end of a half cooling loop for 24 silicon detector modules in the barrel or for either 10 or 13 modules in the end-caps depending on the radial position of the modules. If one of the temperature sensors fails the interlock can still operate safely.

The system is fully implemented in hardware without microprocessors or the need for software. The main components of the interlock system are the IBOX [21] which converts the analogue signal from the temperature sensor to a binary signal, the IMatrix [22] that associates PS channels to cooling loops and the SIC that interfaces the interlock system to the PS system. One channel from the interlock system, using two temperature sensors, turns off 24 barrel modules (for the end-caps, this number can be 12 or 16 depending on the module mapping). The interlock logic in the IMatrix is written in VHDL and programmed into LC5768VG Complex Programmable Logic Device (CPLD)4. If the interlock is triggered by high temperature on the cooling loop then the associated PS channels are switched off in approximately 1 second.

Footnote 4: Lattice Semiconductor Corporation, [http://www.latticesemi.com](http://www.latticesemi.com)

The system also offers protection against laser light coming from the optical read-out system. This is ensured by interlocking the VCSEL power the on-detector opto package for optical read-out, whenever the back door to the DAQ rack is open. Another important function of the interlock system is to integrate general safety measures from the ATLAS Detector Safety System (DSS) [20]. During macro-assembly, the cooling interlock and a general safety interlock (panic button) were implemented.

\begin{table}
\begin{tabular}{|l c c c|} \hline
**Plant Parameters** & **Type** & **Qty** & **Typical** \\ \hline Global PLC status & R & 1 & - \\ Condensation Buffer Tank Temp. & R/W & 1 & 31 \({}^{\circ}\)C \\ Condensation Buffer Tank Pressure & R/W & 1 & 14 bar \\ Liquid in storage Tank Weight & R & 1 & \(>\)250 kg \\ Mixed and Chilled Water Temp. & R & 1 & \(\sim\)15 \({}^{\circ}\)C \\ Input Liquid Mass Flow & R & 1 & 8-9 g/s \\ Output Liquid Mass Flow & R & 1 & 8-9 g/s \\ Distribution Rack Temperature & R/W & Nx1 & 20 \({}^{\circ}\)C \\ Distribution Rack Pressure & R/W & Nx1 & 11-13 bar \\ \hline \end{tabular}
\end{table}
Table 3: The number of monitored parameters from different parts of the cooling plant. Four distribution racks were used during the tests presented here (N=4). Typical values are taken during stable warm running. R/W stands for Read/Write.

## 4 DCS control

### Power Supply State transitions

The safe operation of the SCT detector requires several intermediate states when the detector power is ramped up or down. The intermediate states together with the programmable over-current trip reaction time prevent channels (modules) from tripping due to a too high ramping current. It is also needed when the LHC refills and it is desirable to turn the bias voltage down (but not off) to prevent detector damage in case of beam losses.

Each module of the SCT detector has three different pre-defined operational states for each HV and LV. They are OFF, STANDBY and ON and each state is defined by an operational value for each parameter and the associated alarm thresholds. Typical ON operational values are given in Table 1 and STANDBY5 is derived from these settings with reduced high and low voltage levels (Vbias = 50 V, Vdd = 3.0 V and Vcc = 2.5 V). The operational values and firmware programmable alarm limits are uploaded to the EEPROM (Section 4.2) for rapid state transitions, but it is possible to adjust each parameter manually to tune module performance or to understand problems.

Footnote 5: Definition of this state may change slightly for the final system

When the detector is being prepared for a run it is first cooled to a temperature well below the operational temperature when powered, under close supervision from the environmental project. Once a stable condition is obtained, the detector is ready for powering and for DAQ operation. The DAQ and DCS systems are dependent on each other as can be seen in Table 4.

An important principle during running is that the status of working modules should not be modified. Thus an implementation is in place to be able to apply requested state transitions only on

\begin{table}
\begin{tabular}{|c|c|c|} \hline
**Step** & **DCS states** & **DAQ states** \\ \hline \(\bigtriangledown\) & Project started & INITIAL \\  & Modules masked off & \\ \hline
0 & MASKED OFF & Start software \\ \hline \(\bigtriangledown\) & \(\downarrow\) & BOOTED \\ \hline \(\bigtriangledown\) & \(\downarrow\) & Configure SW \\ \hline \(\bigtriangledown\) & Mask on & CONFIGURED \\ \hline
1 & OFF & \(\downarrow\) \\ \hline \(\bigtriangledown\) & Ramp up power to recover tripped channels & \\ \hline
2 & STANDBY & \(\downarrow\) \\ \hline \(\bigtriangledown\) & Ramp up power & \(\downarrow\) \\  & recover tripped channels & \\ \hline
3 & ON & Configuration and probing of modules \\ \hline \(\bigtriangledown\) & \(\downarrow\) & RUNNING \\ \hline \end{tabular}
\end{table}
Table 4: The DCS states in relation to the DAQ states. When the detector arrives at the last DAQ state it is ready for data collection.

channels in a given state. This allows for the easy recovery of groups of modules without interfering with working modules.

When a temperature interlock is released both the HV and LV channels of the corresponding modules return to their OFF state and user action is needed in order to bring the modules back into an operational state. For a VCSEL interlock the module returns to the same state as before the interlock.

### DCS configurations

The configuration of the DCS system consists of the set values for each run state with the corresponding alert configuration as well as the mapping of the parameters to the hardware location and the detector position.

The operational values have four alert levels giving upper and lower thresholds for warnings and alarms of each of the run states: warning low, warning high, alarm low and alarm high. Each parameter (Tables 1 and 2) has its own set of alert thresholds. For the Envr project, these thresholds are input by the user in the software for each type of sensor. For the PS project, the operational values are sent to the CC for fast state transitions, while the majority of alert thresholds are handled by the MoPS project. An extra level of safety has been implemented in the CC for some of the more critical "_alarm high_" alert levels (the hybrid temperature, the bias voltage and the HV current limits) so these levels are also stored in the CC for each module (Section 4.4). For this reason, one never expects to observe any "_alarm high_" condition on the module temperature or HV.

The operational values and alerts for the PS DCS are stored in an Oracle database, using a schema created with the JCOP framework functions [12]. The parameters are grouped per crate and per state and form a configuration unit or "_recipe_" that is are stored with an associated tag and a version number. This method allows different configurations to be stored for different conditions (e.g. for stable beam, cosmics). Online changes to the configuration parameters can be made via the DCS interface and uploaded to the Oracle database.

To load a new configuration into the SCT DCS, three basic steps are needed: first the configuration is loaded from the Oracle database to the memory of the projects, then the relevant values are sent to the CC which stores these new values in the EEPROM. A quick comparison test is performed on the loaded parameters, to avoid reconfiguring if no change has been made.

The most extensive testing of the configuration storage scheme was made during the end-cap tests where the configurations for all crates where written to the Oracle database from ascii files. Although only one configuration scheme per crate per state was used the overall functionality of the loading and storage method was demonstrated.

Geographical mapping of the hardware is done by adding PVSS _aliases_ to the _datapoints_. In the MoPS project, the datapoint contains the crate-channel information and the alias associates a specific channel to a module position on the detector.

### Conditions Archiving

The DCS conditions data are stored in an Oracle database using the PVSS Oracle Relational DataBase manager. The parameters for archiving can be set globally or individually for a single channel using panels within the projects.

A reduction of the data volume is necessary, to avoid filling the disk space with values due to noise fluctuations. Consequently, "deadbands" are specified for all relevant datapoints and values are written to the database only when new values are measured outside the deadband. The deadbands are all specified in absolute values and can be found in Table 5.

The data from all archived datapoints are copied from Oracle to the COOL conditions database, available to the ATLAS offline data analysis framework (Athena) [24]. This is done using the CondDB program [25] running on a separate computer. The DCS data for a specific module (currents, voltages, etc) are associated with its unique offline identifier in the database. A reduction of data going from Oracle to COOL is not needed, but time stamps are rounded to the nearest second. This is about the time resolution which is available from the DCS, although a better synchronization of the distributed systems, and a precise measurement of all the delays which are involved may allow for a better precision if required.

Although for the initial barrel tests the DCS data were stored in the internal PVSS database and then copied to COOL, by the time of the combined tests (SCT + TRT together) all data was copied online to the COOL database successfully.

### Systems Safety Actions

The different DCS systems presented above have the common purpose of protecting the detector in case of an incident. To serve this purpose, the projects make use of their own alert thresholds (Section 4.2) but also communicate this information across projects.

Using the alert thresholds defined in the configurations, the DCS projects compare the online values of the different parameters and associate an alert condition (_OK, Warning, Alarm_) to each of them. The alert is then either acted upon inside the project or transmitted to another project to take action. In particular, when an alert is triggered in either the environmental, the cooling or the test enclosure DCS, the alert information is passed on to the MoPS project. Depending on the cause and severity of the alert, action will be taken by either the PS hardware, as explained in Section 3.1, or by the MoPS project. There are three different levels of action in the PS system. The first to act

\begin{table}
\begin{tabular}{|l c|l c|} \hline
**Param.** & **Deadband** & **Param.** & **Deadband** \\ \hline V\({}_{bias}\) & 0.8 V & V\({}_{ddRET}\) & 70 mV \\ V\({}_{cc}\) & 30 mV & I\({}_{VCSL}\) & 50 mA \\ V\({}_{ccPS}\) & 70 mV & I\({}_{PIN}\) & 20 \(\mu\)A \\ V\({}_{dd}\) & 30 mV & T1\({}_{module}\) & 0.2\({}^{\circ}\)C \\ V\({}_{ddPS}\) & 70 mV & T2\({}_{module}\) & 0.2\({}^{\circ}\)C \\ V\({}_{VCSL}\) & 70 mV & T\({}_{mech}\) & 0.2\({}^{\circ}\)C \\ V\({}_{PIN}\) & 70 mV & T\({}_{air}\) & 0.2\({}^{\circ}\)C \\ I\({}_{bias}\) & 40 nA & T\({}_{cool}\) & 0.2\({}^{\circ}\)C \\ I\({}_{cc}\) & 15 mA & T\({}_{mcool}\) & 0.2\({}^{\circ}\)C \\ V\({}_{ccRET}\) & 70 mV & T\({}_{devepoint}\) & 0.2\({}^{\circ}\)C \\ I\({}_{dd}\) & 15 mA & Humidity & 0.2\% \\ \hline \end{tabular}
\end{table}
Table 5: Parameters and their corresponding archive deadband, as used for the last part of the end-cap testing.

is the MoPS project. If the DCS fails to act the CC may intervene on some critical parameters else the PS hardware may trip. In addition, the interlock will turn the module power off if the cooling pipes are overheating and the DSS in some cases may cut the power to the racks if the situation in the SCT or in ATLAS has become dangerous.

For the Envr project, an alarm high value on any temperature (mechanical, air or cooling) initiates the MoPS project to perform a controlled emergency shutdown of all powered modules. The cooling pipe temperatures are compared to the dew point provided by the test enclosure project, resulting in the same action as for the temperatures themselves. The same mechanism also prevents the module power from being ramped on if such an event occur. For the cooling project, a global software interlock prevents the MoPS from turning on the modules if no cooling is present. All projects are connected to each other (Section 3) and thus a loss of communication causes a software interlock in the MoPS project similar to the cooling software interlock.

Table 6 describes the alerts and the automatic safety actions. The alerts have their assigned priority within the projects, the highest priority corresponds to the most dangerous conditions for the detector. For some parameters in the MoPS project, the CC provides an extra protection, checks them against the safety limits and performs an automatic power ramp down if necessary. Typical high alerts occur at over-temperature of the hybrid or when the bias current exceeds the safe operation limit. All other alarm states are currently used purely for information and the user must take an action if considered necessary.

\begin{table}
\begin{tabular}{|c|c|} \hline
**Case** & **DCS Action** \\ \hline \hline T\({}_{module}>\) T\({}_{alarm}\) & Switch off LV and HV \\ \hline I\({}_{bias}>\) I\({}_{alarm}\) & Switch off HV \\ \hline Off state: \(V_{bias}>\) 10 V & Switch off HV \\ \hline Off state: LV output on & Switch off LV \\ \hline CC to HV/LV & Send emergency \\ communication lost & message to MoPS \\ \hline CC to MoPS & Reset Communication \\ communication lost & \\ \hline \hline T\({}_{cool}<\) T\({}_{devpoint}\) + 10 \({}^{\circ}\)C & Pop up messages \\ \hline T\({}_{cool}<\) T\({}_{devpoint}\) + 5 \({}^{\circ}\)C & Switch off LV/HV \\ \hline T\({}_{cool}>\) 22 \({}^{\circ}\)C & Switch off LV/HV \\ \hline T\({}_{air}>\) 30 \({}^{\circ}\)C & Switch off LV/HV \\ \hline T\({}_{mech}>\) 30 \({}^{\circ}\)C & Switch off LV/HV \\ \hline \hline Cooling plant failure & Switch off LV/HV \\ \hline \hline Communication loss & Pop up messages, operator to \\ between projects & reestablish communication. \\  & Interlock to ensure safety. \\ \hline \end{tabular}
\end{table}
Table 6: The DCS alerts and their associated safety action.

## 5 Results and Performance

As shown in the previous chapters the SCT DCS was extensively tested during the macro-assembly and integration of the SCT (Table 7). Not only has it been essential as a tool to examine the detector modules but also important for the operation and safe running of the detector as a whole. A considerable scale-up of the system has been made in comparison with previous test systems. A large amount of data has been recorded to verify the performance of the SCT detector as well as to validate the SCT DCS before starting the detector operation underground. This section presents the results of the SCT DCS software validation, performance and scale up.

A first part of the validation is to show that the DCS can handle the operation and control of the detector over time, for a large number of parameters, during stable running. A selection of the collected data is presented, for the qualification of the system, for the monitoring and control of temperature and humidity, and for the powering of the detector. Results for a single module are shown for clarity in the case all modules show the same behavior.

### Temperature

The thermal stability of the detector is essential for its stable operation. For this reason, the supervision and evaluation of all the temperature parameters are essential. Figure 4 shows the typical temperature development of a full cooling cycle (running warm with C \({}_{3}\)F \({}_{8}\)) as measured by the environmental cooling pipe temperatures (Envr project) during the reception tests at CERN following the arrival of the innermost barrel.

At cooling start-up (warm running) the temperatures drop from room temperature to \(\sim\)9 \({}^{\circ}\)C before stabilising at \(\sim\)12 \({}^{\circ}\)C. The temperatures rise to \(\sim\)14 \({}^{\circ}\)C when the module power is turned on and the hybrids start heating. At shutdown the inverse behaviour is seen. At around 17h00 a dip for all sensors is registered corresponding to a change in back-pressure settings of the evaporative cooling system on all 16 pipes connected to this barrel. The individual sensors show an acceptable level of stability throughout the run. The spread of the temperatures remains low (\(\sim\)2.5 \({}^{\circ}\)C).

A closer examination of the temperature time-dependence reveals a regular pattern where the temperature oscillates with an amplitude of \(\sim\) 0.25 \({}^{\circ}\)C, with period of \(\sim\)5 min. These oscillations are also found when monitoring the hybrid temperatures measured by the MoPS project as shown in Fig. 5. They originate from changes in the back-pressure of the cooling due to its own regulation.

\begin{table}
\begin{tabular}{|c|c|c|} \hline
**Time** & **Test** & **Location** \\ \hline
2004 & Single barrel test & Oxford \\
2005 & EC C assembly test & Liverpool \\
2005 & EC A assembly test & NIKHEF \\
2005 & Barrel reception tests & CERN \\ Spring 2006 & ID barrel combined tests & CERN \\ Summer 2006 & EC reception tests & CERN \\ Fall 2006 & ID EC C combined tests & CERN \\ \hline \end{tabular}
\end{table}
Table 7: The different test periods of the SCT during macro-assembly and integration.

The behaviour of the back-pressure regulator is shown in the same figure. The module only sees a back-pressure range of about half of the actual variation due to latency.

The temperature profile for the different types of temperature sensors in the SCT barrel can be seen in Fig. 6 for one cooling cycle. The coldest temperatures are measured on the cooling pipes. The mechanical temperatures are \(\sim 2\ ^{\circ}\)C higher and has a slower time constant. The air temperature and the module temperature follow the cooling pipe temperature with an offset of approximately \(6\ ^{\circ}\)C and \(13\ ^{\circ}\)C respectively. The effect of the DAQ and PS operation, which shows as the dips in the central region, are naturally deeper for the module temperature than the cooling pipe temperature.

The temperature uniformity of the detector modules [26] across the innermost barrel can be seen in Fig. 7, where "LMT #" is the azimuthal position and "Module position", its position along the \(z\)-axis. Each square corresponds to a single SCT module, where the mean of the two thermistor values on each module has been calculated. Except for one cooling loop (made out of 4 rows) showing a higher temperature, the modules all have a good overall uniformity of about \(2\ ^{\circ}\)C. Three modules (shown in white) were missing in the run when the data was collected, due to temporary

Figure 4: Evaporative cooling cycle as measured by the environmental cooling pipe temperature sensors on the innermost barrel during reception test at CERN. Each sensor (shown by a different color) monitors the cooling temperature of 24 modules.

Figure 5: Hybrid temperature fluctuations (black line) and the corresponding back pressure readings (blue dashed line) from the cooling plant during ID Barrel Combined Tests

power problems.

The distributions of hybrid temperatures during cosmic ray runs are shown in Fig. 8. The uniformity is comparable with previous results. The absolute temperature depends on the cooling temperature settings. The end-cap tests were done at a lower temperature than the barrel test.

### Humidity

The control of environmental humidity is important to avoid condensation and electronic breakdown of both the sensors and electronics. During barrel testing the humidity was monitored both in the test enclosure by non-radiation hard IST6 humidity sensors and by radiation hard Xeritron humidity sensors mounted on the SCT end flanges. The non-radiation hard sensors showed that the test enclosure dried out in a few hours while the Xeritron sensor showed a much slower trend.

Footnote 6: [http://www.ist-ag.com/](http://www.ist-ag.com/)

Fig. 9 shows the humidity trend recorded by the Xeritron humidity sensors when the test enclosure was flushed with dry air. The slow response for these sensors at low humidity is known

Figure 6: A view of all temperatures for a cooling cycle on the SCT: cooling pipe (black line - 4), air (red line - 2), mechanical (green line - 3) and module hybrid temperatures (blue line - 1) are shown for the data collected during Barrel reception tests.

Figure 7: Module hybrid temperature distribution for the innermost barrel during reception tests at CERN. The temperature shown is the mean of the two temperature sensors on a module.

and described in [27], and is amplified if there is poor air circulation around the sensor. The response on increasing humidity, as can be seen from the same figure, is very good.

The curves in Fig. 9 also show two dips, which correspond to the changes in temperatures of the SCT induced by turning the power on or off or by clocking the modules. This sensor temperature dependence can easily be corrected by using a function based on the individual resistances and total resistance of the Xeritron sensor.

During the end-cap C testing the dew point calculation was made using non-radiation-hard Honeywell sensors mounted on the cylinder. Figure 10 shows the dew point calculation for one of these sensors over a five day period. The value for the dew point is low (around \(-39\)\({}^{\circ}\)C) and stable, well below the monitored cooling temperature in the same region (around 12 \({}^{\circ}\)C) indicating that there was no risk of condensation. The SCT specifications for humidity are also fulfilled since the difference is more than the required 5 \({}^{\circ}\)C.

Figure 8: Module hybrid temperature distributions during ID Combined Tests (cosmics run). The left-hand plot shows the distribution for the 4 barrels and the right-hand plot, for the End-Cap C.

Figure 9: Relative humidity readings from the two Xeritron sensors (red and black lines) inside the detector volume during reception testing of the innermost barrel over 46 hours.

### Power

Stable and reliable powering of the detector is equally important and all LV and HV parameters need to be monitored and controlled to ensure a minimum number of noise hits in the detector.

The behaviour of the bias voltage at start-up is shown in Fig. 11, where the current is stable at 300 nA except during ramp up when the current peaks at about \(\sim 3\)\(\mu\)A due to charging currents.

Figure 12 shows the current for one module during one hour of cosmics ray data collection, during which \(\sim 7000\) cosmic ray events were recorded. The fluctuations around the mean value of 258 nA are approximately in agreement with the read-out resolution which, summing the contributions from ADC and the noise of electronic elements, accounts to about 100 nA. A detailed study of the data recorded shows only 37 entries of the 660 readings from the same module yield a \(\Delta I_{bias}>100\) nA. This shows that the bias leakage current remains essentially stable throughout data taking, subject only to small changes due to variations in module temperature, which can in turn be influenced by DAQ activity.

The weak dependence on DAQ actions is shown in Fig. 13 where both module bias and pin

Figure 11: When the high voltage (red dashed-dotted line) is ramped from 0 V to 150 V the bias leakage current (black full line) increases significantly due to charging currents. The current is otherwise stable during ID barrel combined tests.

Figure 10: Dew point calculation (lower black line) and monitored cooling temperature (upper red line) during five days of data taking for end-cap C cosmics tests.

current are plotted together. When the module is not clocked and configured (I \({}_{PIN}\sim 40\)\(\mu\)A) the module temperature falls by 1-2 \({}^{\circ}\)C and there is a tendency for the bias current to decrease slightly during that time.

The stability of the LV parameters, which ensures the operation of the chips and the data readout, is extremely good and the DCS control and read-out performs satisfactorily. Figure 14 shows the behaviour of the digital and analogue voltage at start-up for a single module but this behavior

Figure 12: The bias leakage current during one hour of cosmic ray data collection for one barrel module. The width of the distribution is in agreement with the read-out resolution.

Figure 13: Pin and bias currents for 30 min of barrel module testing during ID barrel tests. Between 11:38 and 11:46 the module was not clocked and configured (Pin I \(\sim 40\)\(\mu\)A) and during that time the bold blue line which is the mean of the preceding 10 Ibias read-outs shows a tendency to decrease slightly.

is typical for all modules. One can clearly see that the module goes through the three PS states, from off to standby and then to on. In addition, the increase in the analogue current (I\({}_{cc}\)) due to the module configuration can be seen going from \(\sim\)150 mA to \(\sim\)1000 mA. The current only reaches the latter nominal value of \(\sim\)1000 mA after the DAQ sends a serial bitstream to the modules. In stable operation, the variations of the currents are of the order of the resolution of the PS read-out. This is also true for the other LV parameters such as V\({}_{VCSL}\), V\({}_{PIN}\) and their corresponding current values.

Using the entire DCS one can easily observe the effect of the changes in PS states from the various parameters. As an example Fig. 15 shows the temperature development when the module moves through the states at start-up, going from OFF (1), where the temperature is dominated by the coolant temperature, to STANDBY (2) and then to ON (3) as described in Section 4.1. At (4) the modules have reached the stable operation temperatures and physics runs can be started.

### Interlock and trip events in the detector

As described in Section 4.4, several hardware, firmware and software measures are taken in the case of over-temperature. In particular, the interlock system, as a fast hardware solution, acts independently whenever a failure is not identified by the other systems.

Figure 16 shows a series of overheating incidents where the module temperatures rose quickly until the safety interlock acted to cut the power off. The failure happened when a capillary feeding a cooling half loop on the outermost barrel was blocked by debris present in the cooling system as a result of an earlier cooling pump failure. In the absence of the heat load produced by the modules, the loop appeared to be behaving normally. When the modules were powered, there was insufficient fluid flow to cool them. This event and other related incidents shows that the interlock system acts reliably and fulfils the safety requirements.

The hardware and firmware trips were also thoroughly tested during commissioning. Figure 17 shows an example of how the CC reacted correctly when receiving a fake module temperature readout. The reading of 259 \({}^{\circ}\)C provoked the CC to ramp down both the high and low voltage of this card and set it to the OFF state. The temperature threshold for the CC is listed in Table 1. This

Figure 14: The behaviour of the LV analogue current (solid line), digital current (dashed line), digital voltage (dotted line) and analogue voltage (dashed-dotted line) at start-up during ID barrel combined tests.

event proves that the CC firmware protection was working according to specification. The cause for fake module temperature read-out has been fixed in new firmware versions.

### DCS Performance

Running 1/8 of the detector together with all the services during the commissioning and the cosmic tests was a great opportunity to study the performance of different aspects of the DCS, namely the hardware, software and database issues.analogue

First, the CAN bus performance was evaluated as this is a determinant factor to ensure the communication with the hardware. For safe operation away from the maximum value for the ELMB (see Section 3.1) the occupancy should be less than 60%.

Figure 16: Module temperature (black dots) and cooling pipe temperature (blue and green dots) showing the typical behavior at an interlock incident during ID barrel combined tests.

Figure 15: Module switch on and preparation for data taking involves several steps during which the temperature of the module gradually rises. Data taken during ID barrel combined tests.

For the PS project this load is reached by 4 crates sending data simultaneously. When there are more than 4 CC nodes, the high rate of data transmission causes a "traffic jam" on the CAN bus. To prevent this situation, an inhibit time parameter was implemented in the CC. This is a counter with a 100 ms resolution, which inhibits the next read-out transmission for a period of time. Optimal settings of the inhibit times for nodes connected to one CAN bus were sought and found to be 0 ms delay for crates 0-3, 1000 ms delay for crates 4-8, 2000 ms delay for crates 9-11. Taking into account the time needed for sending data from one crate (\(\sim\)800 ms) and inhibit times set in the system, it was found that for a minimum read-out time of 4 - 5 seconds a maximum 11 CC nodes could be connected to the same CAN bus. Due to constraints within the PVSS application the allowed read-out time was approximately 10 seconds.

The PS hardware performance can be discussed in terms of the switching on/off speed and the trip rate. The ramp down speed of one crate is defined by the ramping speed chosen by the user and by the time it takes the CC to send the commands to all the channels. It takes 6 seconds for the 48 channels to start ramping. Since the crates are all independent, this value does not vary with the number of crates on the bus. For this reason the total time to shut down all crates on one bus is not much more than 6 seconds since the number of messages will be equal to the number of crates, which gives a maximum of 11 messages. Cosmic ray runs allowed for studies of low level hardware trips, which typically occurred after long periods of running the PS system. Occasional LV and HV trips not related to the detector conditions were solved in the PS firmware. The intercommunication between the different DCS projects is defined by the ethernet speed. Since the SCT DCS will be on a dedicated local network there should be no reason for this to be slow or to fail.

On the software side, an issue for the system performance was the large CPU consumption of the DCS computer. The CPU usage raised rapidly when collecting the data after a SYNC message. This effect was mainly related to the PVSS internal archiving and also due to the alert archiving. The problem of the internal archiving was later solved using the external Oracle archive for the end-cap tests while the alerts were simply not used (see below). In Fig. 18 the periodic spikes in CPU usage related to SYNC are shown for the OPC server (bottom) and for PVSS and the OPC server combined (top).

Figure 17: LV trip event during ID barrel combined tests. The black (solid) and red (dashed) lines show the two module thermistors, the green (dashed-dotted) line is the bias voltage and the blue line (dotted) is the digital voltage (V\({}_{dd}\)).

The use of alerts within the MoPS project was not possible during the macro-assembly phase. This was due to an internal mechanism of the software which attempted to save all alerts to an archive file. Since the number of alerts active at the same time in the MoPS project was large, the archive file size would eventually increase enough to make the project crash. Later improvements resolved this problem.

The first attempt to make use of the databases during DCS operation was made during combined testing. For the configuration database, performance could be evaluated for a complete DCS system. When it was used to configure the crates, the time required to save the actual settings (no alerts) for a system of 11 crates (6700 data points) into a configuration was measured to be 43 s, the time for storing to the Oracle database was 170 s, the time to retrieve a system configuration from the database was 11 s, and the time required to apply the configuration to the PS was about 4 s. These times are greatly improved with the use of the final computers for the experiment and of the latest releases of the JCOP framework.

The combined tests provided an excellent opportunity to develop and test more efficient data handling strategies. The volume of DCS data was also evaluated. As explained in Section 4.3, no deadband was used for the barrel tests. This resulted in a large amount of data stored in the COOL conditions database (18 GB for 3 months of data taking). For the end-cap tests, the deadbands were enabled and the improvement is clearly visible: 2 GB of data stored for 1.5 month of data taking. The number of stored parameters changed between the tests; the number of modules under test was reduced by half for the end-cap tests while the environmental parameters were doubled. The data removed in the smoothing process are due to noise fluctuations in the readings from the hardware and are therefore not useful. The usage of both databases for the DCS is therefore scalable to the full SCT detector in ATLAS, given that the smoothing procedure is applied for conditions.

## 6 Conclusion and Outlook

The first large scale test of the DCS for the SCT was made during the macro-assembly and commissioning phase prior to installation into the ATLAS detector. The highest priority for the system

Figure 18: CPU consumption of the MoPS project PC during barrel cosmics run. The plot shows regular peaks at 100% with a period corresponding to the MoPS reading rate (15 seconds) for both PVSS application and OPC server (top) and for the OPC server in stand alone mode (bottom).

was to secure safe operation of the SCT but the assembly and commissioning phase was also an ideal test bed to study the architecture and performance of the DCS.

The tests show that the architecture of the system meets the requirements in speed, configurability and scalability. The scale up to a full population of one bus has been achieved and the SCT DCS passed both the operational and the safety requirements. The setting and read-out of all parameters as well as their display and storage have been successful throughout the testing and commissioning phase. Especially the concept of loading settings and alarm levels from the database to the memory of the Crate Controller of the large power supply system before operation through state transitions proved to operate well. The response time to complete a state transition was only a few seconds and, because of the distributed architecture, insensitive to upscaling of the system. The DCS system was partitioned to give an update rate of 10 seconds for the future full SCT system with around 100 000 monitoring parameters.

It was also demonstrated that the DCS system provided a satisfactory protection for the detector. All safety measures both on the PVSS project level as well as in the trip limits set in the Crate Controller and the hardware were tested and proved to be working. The functionality of the independent hardware interlock were proven on several occasions protecting the detector modules from critical overheating.

All in all, these tests showed that the SCT DCS was fully operational, stable and deemed sufficiently advanced for the installation and operation of the SCT detector in the pit, including a final scale-up to the full system. Some remaining issues are related to the integration of the SCT DCS into the ATLAS DCS. Preparation was made towards the implementation of the Finite State Machine needed for the ATLAS integration, however the FSM for the SCT will first be available for the detector commissioning in the pit. With this final step the designed DCS system will provide a fast and reliable system for a safe operation of the SCT in ATLAS.

## Acknowledgements

The authors would like to thank the different groups who provided the necessary hardware equipment to make this study possible, the central ATLAS DCS and the JCOP group for their support as well as the SCT collaboration for their effort during the tests.

## References

* [1] S. Baranov _et al._, _Estimation of Radiation Background, Impact on Detectors, Activation and Shielding Optimization in ATLAS_, ATL-GEN-2005-001.
* [2] The ATLAS SemiConductor Tracker Collaboration, _The Barrel Modules of the ATLAS SemiConductor Tracker_, Nucl. Instrum. Meth. A, **568** (642-671), 2006.
* [3] F. Campabadal _et al._, _Design and performance of the ABCD3TA ASIC for readout of silicon strip detectors in the ATLAS semiconductor tracker_, Nucl. Instrum. Meth. A, **552** (292-328), 2005.
* [4] The ATLAS SemiConductor Tracker Collaboration, _ATLAS SCT End-Cap Module Production_, Nucl. Instrum. Meth. A, **575** (353-389), 2007.
* [5] M. Chamizo Llatas _et al._, _The control and monitoring system for the ATLAS semi-conductor tracker_, Nucl. Instrum. Meth. A, **552** (163), 2005.