**ATLAS Internal Note**

**INDET-NO-045**

**5 May 1994**

**Comparison of algorithms for**

**associating an MSGC signal to a beam**

**crossover in ATLAS**

_Steve Snow_

Department of Physics

Manchester University

Manchester M13 9PL

England

Several possible algorithms for processing the signal from an microstrip gas chamber (MSGC) have been tested on simulated data. Any reasonable algorithm will give a high efficiency for finding the signal if the occupancy of the chamber is low. However, when running at full luminosity the ATLAS MSGCs can expect an occupancy of one or a few percent, high enough to have an impact on their efficiency. We have chosen a practical measure of efficiency which takes account of the basic ( zero luminosity ) efficiency, the increase of occupancy caused by wrongly associating a hit to more than one BCO, and the time for which a strip is dead following a hit. We have used this measure to test the digital Fastplex and variations on the analogue RD20 algorithm. Using experience gained from this work we are able to suggest a fairly simple analogue algorithm which extracts just about all of the useful information from an MSGC signal. With this algorithm for comparison we can say that neither RD20 nor Fastplex are optimal, but they are only four to seven percent away from the optimum of 91% overall efficiency at high luminosity. These comparisons depend strongly on the noise level that is finally achieved in the readout chips, we have assumed 2000 electrons ENC for all algorithms.

**1. Introduction**

The statistics of primary ionistaion are such that for efficient detection of minimum ionising particles (MIPs) an MSGC must have a drift space of order \(3mm\). The time taken for electrons to drift across this space in a typical gas is about \(60ns\). The first stage in processing this signal will have to be a pre-amplifier and in order to keep the power consumption of this pre-amplifier down it is desirable that it should not have a very fast rise time. On the other hand, the interval between bunch cross overs ( BCOs ) at LHC will be \(25ns\). One can immediately see that it will not be easy to associate an MSGC signal with one and only one BCO.

The ATLAS design uses a small number of precision tracking layers; typically a track will go through four MSGC layers each with \(uv\phi\) strips. With so few hits on each track one cannot afford to lose any of them. A hit is obviously lost if the algorithm fails to tag it as coming from the right BCO, but it is also lost or degraded by the presence of a random hit in one of the neighbour or next-neighbour strips. A hit can also be lost if there is some inefficiency caused by hits from previous or.following BCOs in the same strip. Figure 1 illustrates the dead area in space and time which surrounds a genuine hit. We use this dead area times the occupancy, \(K\), to calculate an overall efficiency;

\[E=e(1-K(5(\epsilon+R)+D))\]

When running at full luminosity, the occupancy of MSGC channels due to one particular BCO is expected to be between \(2\%\) near the beam and \(0.5\%\) far from the beam, for this analysis we have taken \(1.2\%\) to be a typical value.

\(E\) is the quantity that we must try to maximise. In the following work we have generally tried to choose an algorithm which will keep \(D\) small and then tuned it to maximise \(E_{R}=e(1-5K(\epsilon+R))\) and then checked the final value of \(D\) and calculated \(E\).

**2. The Monte Carlo**

A very simple Monte Carlo has been used for this work. Our aim has been to find the best algorithm, rather than to find out the exact efficiency that this algorithm will have when run on real data. Thus we only deal with a single strip of the MSGC and we assume that all of the ionisation from a MIP stays in the same drift cell; none diffuses to or from other cells. This means that our signals will be somewhat larger than in real life, but on the other hand we have chosen to simulate a very conservative gain value.

A MIP is assumed to pass through the chamber at an angle of 30 degrees to the perpendicular but not to cross into other drift cells. This is typical of a high \(P_{t}\) track in the forward region of ATLAS. The gas mixture used for these studies is \(DME(60\%)/CO_{2}(40\%)\), it gives \(5.1\) primary ionisations per \(mm\) each with an average of \(1.9\) ion pairs, though this average conceals a very long tail towards higher numbers of secondaries. The electrons are drifted to the strips at a speed of \(65\mu mns^{-1}\) and the gain is taken to be \(1000\) with fluctuations parameterised by the empirical formula, \({\rm Probability}(g)=\sqrt{g/\overline{g}}e^{-3g/2\overline{g}}\) [1]. Time is treated in discrete intervals of \(2.5ns\) and uncorrelated gaussian noise is added to each time bin. The resulting signal is then passed through the pre-amp shaper.

In the case of RD20 simulation the shaper has an impulse response of \(\frac{1}{\tau}e^{\frac{-t}{\tau}}\). A delta function input gives a maximum response at time \(\tau\) later. The magnitude of the noise at input is adjusted so that the r.m.s. of the output noise is equal to the peak output that would be produced by an impulse of 2000 electrons at the input.

In the case of the Fastplex simulation the shaper described in [2] is used which has a response of \(e^{\frac{-t}{\tau_{f}}}-e^{\frac{-t}{\tau_{r}}}\). We used \(\tau_{r}=6.83ns\) which gives a \(10\%-90\%\) rise time of \(15ns\) and we neglected the fall because it is so slow \(\sim 1\mu s\). The input noise is adjusted so that the difference between the signal at time \(t\) and \(t+50ns\) has an r.m.s. equal to the step size produced by a signal of 2000 electrons at the input.

In either case the signal is finally sampled at intervals of \(25ns\), the BCO period. The method of sampling has some affect on the performance. A sampling which consists of taking the average value of the signal over a \(25ns\) interval gives a significantly worse performance than one which takes a snapshot lasting only a few \(ns\) and is insensitive to the signal for the rest of the time. Both Fastplex and RD20 are assumed to use the snapshot type of sampling. The RD20 readout saves the samples in a switched capacitor analogue pipeline, whereas Fastplex processes the samples immediately and saves either a 1 or a 0 for each \(25ns\) bin.

## 3 Where is the Information?

Before starting to test realistic algorithms we took a look at the timing information that would be availible in the signal if we had no noise and a very fast amplifier.

The optimum method to extract the timing information is to take the half-way point between the first and the last ionisation clusters to arrive. If there were \(N_{p}\) primary clusters then the time resolution obtained by this method would be \(1/\sqrt{2(N_{p}+1)(N_{p}+2)}\) times the total drift time [3]. This is shown by the dashed curve in Figure 2. In reality the number of primary ionisations has Poisson fluctuations and the Monte Carlo was used to evaluate this algorithm, with the results plotted as squares vs. \(\overline{N_{p}}\).

A slightly more realistic assumption is that we measure the time of arrival of the first primary ionistion and ignore the last one. This throws away half of the information relative to the optimum estimator, so it is a factor of \(\sqrt{2}\) worse at high \(N_{p}\). The results of this simuation is shown as circles in Figure 2.

If the signal is processed with a slower shaper it seems likely that it will become more sensitive to the average arrival time of the charge than to the time of the first cluster. If each primary ionisation event produced just one electron then the resolution of this average time would be \(1/\sqrt{12N_{p}}\) times the drift time, shown as the solid curve in Figure 2. In fact the situation is much worse because fluctuations towards high numbers of secondary electrons ( delta rays ) happen with a few percent probability. The triangles in figure 2 show the resolution of the mean charge arrival time for two gasses with different secondary ionisation statistics.

Plotting the r.m.s. time resolution of each estimator described above is not ideal. We really want to know about the tails of the timing distribution; how many events are so far out that they tag the wrong BCO. This is shown in Figure 3 where the assumption has been made that the drift time of the chamber is exactly 2 BCO intervals. It can be seen that when \(\overline{N_{p}}\) has a typical value of 10, we could in principle tag 99% of BOOs correctly by looking for the first cluster to arrive but only 95% if we measured the mean time of charge arrival.

**4. Deconvolution RD20 style**

The RD20 project is to build a readout for silicon microstrip detectors at LHC. This type of detector produces a signal which is almost a delta function in time. In this case the shape of the output signal is the same every time and it is possible to deconvolute it and restore the delta function by sampling the output at three times spaced by intervals \(\Delta t\) and forming the weighted sum; \(S_{d}(n)=W_{1}B(n+1)+W_{0}B(n)+W_{-1}B(n-1)\). The weights are related to the sampling interval and the shaping time by; \(W_{1}=e^{\frac{\Delta t}{\tau}}\), \(W_{0}=-2\) and \(W_{-1}=e^{\frac{-\Delta t}{\tau}}\). If the signal occured in bin zero then \(S_{d}\) will have a large positive value, otherwise it will be zero apart from noise. This preocedure has been shown to work well in detailed simulations of a silicon microstrip detector with \(\tau=45ns\) and \(\Delta t=15ns\) [4]. Figure 4a shows this deconvolution applied to a signal which is a series of delta functions. Provided that the sampling of the smoothed signal is in phase with the delta functions and the right weights are used then the deconvolution is exact. Figure 4b shows an average MSGC signal which is a current lasting for two BCOs. The deconvolution of this signal is far from being exact but we expect to improve it by adjusting the weights and phase.

In the RD20 chip the shaping time is expected to be adjustable between \(45ns\) and \(75ns\). The argument of Section 3 suggests that with MSGCs we may benefit by using a faster rise time, therfore we have compared the performance at 45 and \(75ns\). Since the MSGC signal is spread over at least two BCOs it is likely that different weights will be needed from those required to deconvolute a delta function.

**5. \(45ns\) Shaping Time**

Four thousand events were generated using the Monte Carlo described in Section 2 with \(\tau\) set to \(45ns\) which gives a \(10\%-90\%\) signal rise time of \(26ns\). Each event has a MIP passing through the detector at time zero and added to this is noise from \(t=-500\) to \(+500ns\). The smoothed signal, in \(25ns\) bins is saved on disk. A typical pulse is shown in Figure 5a. We have tried using up to four weights, applied to the signals of four adjacent time bins to produce a deconvoluted signal;

\[S_{d}(n)=B(n)W_{0}+B(n+1)W_{1}+B(n+2)W_{2}+B(n+3)W_{3}\]

which is then tested against a threshold cut, \(T_{e}\). The value of \(T_{e}\) is always chosen to be at \(4\times\) the noise of \(S_{d}\). This noise is determined by applying the deconvolution to the four bins immediately preceding the real signal and using the r.m.s. of the result over the 4000 events. We are aiming to find weights which will make \(S_{d}\) large in one bin and small elsewhere. We have started with some simple examples.

a) A simple threshold; \(W_{2}=1\), other weights zero. The probability that \(S_{d}\) will be above the cut with these weights is plotted against the time bin, n, in the clear histogram of Figure 5b. This algorithm is efficient at detecting the presence of a pulse but says little about when it started. If we refined the algorithm by just selecting the first bin above threshold then it would be better at tagging the correct BCO but it would be dead for about five BCOs while the signal fell back below threshold.

b) An upwards step or first derivative; \(W_{2}=1,W_{1}=-1\), others zero. Because the sum of the weights is zero this is insensitive to a base line shift. The probability that this \(S_{d}\) will be above the cut is shown by the diagonally shaded histogram of Figure 5b. It is an improvement over the simple threshold but it is still likely to tag two or three adjacent bins.

c) A bin which is below the average of its two neighbours, or second derivative; \(W_{3}=1,W_{2}=-2,W_{2}=1,W_{0}=0\). This is shown as the cross hatched histogram in Figure 5b. It is quite selective at tagging the start of the pulse, but with only 94% efficiency, and it also has some probability of tagging the end of the tail.

The pedagogical examples of Figure 5 indicate that weights like b) or c) may be effective. We should demand that the sum of the weights be zero so that we do not tag a large number of BCOs when there is a big pulse. The overall normalisation of the weights is irrelevant because of the way that we adjust the cut position each time to be at \(4\times\) noise. This leaves just one relevant parameter for three weights which we choose to be \(X\);

\[W_{0}=0\qquad W_{1}=X\qquad W_{2}=-(1+X)\qquad W_{3}=1\]

If we want to use the fourth weight this can be done with one more parameter Y;

\[W_{0}=-Y\qquad W_{1}=Y/3+X\qquad W_{2}=Y/3-(1+X)\qquad W_{3}=Y/3+1\]

**5.1 Optimisation of Weights and Phase**

There is one more variable that we have not considered yet; the phase of the \(25ns\) clock relative to the arrival of MIPs in the detector. We have always assumed that the MIP goes through the detector at \(t=0\). In fact, in ATLAS we will be able to tune this phase and it might be better to use a different phase and different weights to compensate. Therefore we have generated data sets with the phases offset in steps of one tenth of a cycle from the usual one.

The optimisation consists in simply stepping through all combinations of phase, \(X\) in the range \(-1<X<2\) and \(Y\) in the range \(-2<Y<2\) in steps of size 0.1 in all variables and finding the combination which gives the largest value of \(E_{R}\). When we choose to use just three weights the results of optimisation vs. \(X\) for each phase value are given in Table 1. The overall optimum is with phase = 0.5, X=0.7. If we use all four weights the results of optimisation vs. X and Y for each phase value are given in Table 2. The overall optimum is with phase = 0.4, X=1.1 and Y=0.3.

In order to investigate the effect of pile-up several Monte Carlo data samples were generated, each with a second particle passing through the MSGC at a fixed number of BCOs after the first one. The phase in these samples was set at 0.4. The efficiency of tagging the two hits, using the optimised weights from the tables above, is shown in Figure 6 as a function of the hit separation. The pile-up parameter, \(D\), is extracted from this plot by measuring the area of the dip. With three weights \(D=1.10\) and with four weights \(D=0.84\).

**6. \(75ns\) Shaping Time**

Exactly the same procedure was used to optimise the deconvolution weights for a shaping time of \(75ns\), which gives a \(10\%-90\%\) rise time of \(43ns\). When using just three weights the results of optimisation vs. X for each phase value are given in Table 3. The overall optimum is with phase=0.4 and X=1. If we use all four weights the results of optimisation vs. X and Y for each phase value are given in Table 4. The overall optimum is with phase = 0.4, X=1.9, Y=0.5. The effect of pile-up on the optimised deconvolution algorithms with \(75\)ns shaping time is shown in Figure 6. The dead times are D=1.28 with three weights and D=0.9 with four weights.

**6.1 Simulation of Zero-Crossing Discriminator**

Neil Jackson has looked at the possibility of using more complex methods of extracting the BCO tag than simply weighting a number of consecutive bins and applying a threshold to the result. His most successful method was to simulate a zero-crossing discriminator with \(\tau=75ns\). The signal is given by;

\[\text{Signal}(n)=\text{MAX}(B(n+5)-B(n),(B(n+5)-B(n+11))/0.7)\]

and the tag is given by;

\[\text{Tag}(n)=(B(n+3)-0.65B(n+4)-0.35B(n)>0)\text{and}(B(n+2)-0.65B(n+3)-0.35B (n)<0)\]

The idea behind this algorithm is to read out the strip if Signal(n) above threshold and to read out an extra bit corresponding to the value of Tag(n). A large proportion of good signals will have the tag bit set and one would use these to find tracks offline. Having found a track one could fill in most of the missing hits by looking at signals without tags. At this level occupancy is no longer a problem.

However this philosophy is different from the other algorithms that we are considering, so that a direct comparison can not be made. Instead, we have simplified it by only using signals which are above threshold _and_ have the tag bit set. Table 5 shows the result of this algorithm as a function of phase. The best performance is with phase = 0.3 but it is only slightly worse with phase = 0.4 and pile-up data already existed with this phase so we used it to measure the dead time of the zero-crossing algorithm to be 2.97 BCOs ( Figure 7 ).

**7. Fastplex-B Algorithm**

The Fastplex-B scheme [3] uses an integrator with a \(10\%-90\%\) rise time of \(15ns\) and a very slow fall time, it makes a threshold cut on the difference between signals sampled at times t and t-50ns. The discriminator is applied every \(25\)ns and the output is a string of \(1\)s and \(0\)s which are saved in a digital pipeline until the level \(1\) trigger arrives. A development of this idea, described by Jurriaan Schmitz at the Cosenor's MSGC meeting is to accept certain patterns of 1s and 0s as a good hit and veto on others.

We have implemented this algorithm in our simulation, which required some modifications. We found that for this algorithm to work well the whole of the rise in the signal should occur within \(50ns\). Thus we had to increase the drift velocity to \(70\mu mns^{-1}\) and reduce the gas gap to \(2.8mm\) and adjust the sampling phase, as in ref. [2].

As a consistency test we wished to reproduce as closely as possible the probabilities of each of the digital patterns given by Jurriaan's simulation. For this test we changed to his simulated values of gain (2000), noise (\(2400e^{-}\)) and crossing angle (zero degrees). With these parameters and a threshold cut of \(6\times\) noise we were able to reproduce Jurriaan's results closely enough.for us to be confident that we are simulating the same thing as he did.

For a fairer comparison with the RD20-style algorithms we changed the parameters back to gain=1000, noise=2000e\({}^{-}\) and angle=30 degrees but we kept the drift distance at \(2.8mm\) and velocity at \(70\mu mns^{-1}\). Bin n is tagged if it or either of its neighbours has a 1 and if there is no 1 in either of bins \(n-2\) or \(n+2\). This algorithm is good at tagging the correct BCO; \(\epsilon=98.2\%,R=37\%\). Figure 7 shows how this algorithm is affected by pile-up. There is almost total loss of efficiency if there is another hit within \(\pm 3\) BCOs because of the veto on signals in bins \(n+2\) or \(n-2\). The pile-up parameter is \(D=5.6\) BCOs.

**8. An Optimum Algorithm?**

The double correlated sampling of the Fastplex-B algorithm is a powerful technique because it gets the whole of the charge which is deposited in the gas by one track into one bin. However, much of this information is thrown away by the digital section which follows it. A better readout can be imagined in which the signal is saved in an analogue pipeline and some cuts are applied to it to extract the maximum information when a trigger arrives. The cuts that we have tried are, where C(n) are the contents of the pipeline, to read out C(n) if:

\[C(n)>4\times\mbox{noise}\quad\mbox{and}\quad C(n)>C(n-1)-2\times\mbox{noise} \quad\mbox{and}\quad C(n)>C(n+1)-2\times\mbox{noise}\]

The first cut will be true if there is a significant signal in the previous \(50ns\). The fact that the whole of the MSGC charge appears in this comparison means that the signal/noise is maximised. The second two cuts will be true if neither of the adjacent overlapping \(50ns\) intervals collected more charge than the one we are looking at. In the case of a typical, good MSGC signal lasting for \(50ns\) only the correct BCO will pass all three cuts. In the case of a primary ionisation fluctuation giving charge in only one of the \(25ns\) intervals, this algorithm will tag both of the possible BCOs as having a hit, thus maintaining efficiency at the cost of a slight increase of occupancy.

The efficiency of this algorithm was tested on the same data sample as the Fastplex-B and gave as expected very good results; \(\epsilon=98.9\%\) and \(R=9.8\%\). The effect of pile-up is shown in Figure 8 where the dead time is \(1.45\) BCOs.

Having found this algorithm which works so well we wished to determine how much like a perfect integrator the pre-amp shaper needs to be in order to have such good behavior.

Consequently we generated several more data sets with different rise and fall times. Our original Fasplex-B data had infinite fall time so we tried changing it to \(600ns,300ns\) and \(150ns\). This has little effect on \(\epsilon\) or \(R\) but it does effect the dead time ( Figure 8 ) because the later signal of the two is sitting on a falling tail which may push it below threshold. We chose a fall time of \(600ns\) as acceptable and varied the rise time to \(22ns,30ns\) and \(50ns\) ( Figure 9 ). The rise time can be surprisingly slow before the performance begins to deteriorate, rise times \(\leq 22ns\) are best.

**9. Summary and Conclusion**

\begin{tabular}{l l l l l l} Algorithm & Right Tag & Wrong Tag & Dead & Overall & Correlation \\ Efficiency & Probability & Time & Efficiency & \\ \(\epsilon(\%)\) & \(R(\%)\) & \(D(BCOs)\) & \(E(\%)\) & \\ RD20,\(\tau=45ns\),3 wts. & 96.8 & 63 \(\cdot\) & 1.10 & 86.2 & 0.79 \\ RD20,\(\tau=45ns\),4 wts. & 96.9 & 59 & 0.84 & 86.9 & 0.78 \\ RD20,\(\tau=75ns\),3 wts. & 98.5 & 79 & 1.28 & 86.5 & 0.80 \\ RD20,\(\tau=75ns\),4 wts. & 97.9 & 51 & 0.90 & 88.1 & 0.77 \\ Zero Cross,\(\tau=75ns\) & 94.7 & 6 & 2.97 & 85.6 & 0.97 \\ Fastplex-B & 98.2 & 37 & 5.60 & 83.4 & 0 \\ Opti? rise=15,fall=inf & 98.9 & 10 & 1.45 & 90.7 & 0.97 \\ Opti? rise=15,fall=600 & 99.0 & 9 & 1.84 & 90.4 & 0.97 \\ Opti? rise=15,fall=300 & 99.1 & 8 & 2.10 & 90.3 & 0.97 \\ Opti? rise=15,fall=150 & 98.4 & 10 & 2.29 & 89.3 & 0.96 \\ Opti? rise=22,fall=600 & 98.9 & 9 & 1.87 & 90.3 & 0.97 \\ Opti? rise=30,fall=600 & 98.4 & 10 & 2.01 & 89.6 & 0.96 \\ Opti? rise=50,fall=600 & 97.9 & 13 & 2.30 & 88.7 & 0.95 \\ \end{tabular}

The table above summarises previous information and in addition gives the correlation between the signal read out by the electronics and the number of electron-ion pairs produced by the track in the drift gap. This correlation should be large if analogue readout is to be used to gain an improvement in position resolution. It can be seen that either RD20 or Fastplex-B can associate a signal with the correct BCO with efficiencies of \(>98\%\) and with the probability of tagging another BCO wrongly of \(45\pm 10\%\). In these respects their performance does not look so very different, but the RD20 algorithm is much less affected by pile-up, being dead for only 0.9 BCOs after a hit compared with Fastplex-B which is dead for 5.6 BCOs. The algorithm labelled "Optimum?" gives significantly better results than the others in all respects except dead time, in which it is slightly inferior to RD20. This algorithm uses the double correlated sampling ( DCS ) technique followed by an analogue pipeline. DCS works best with an ideal integrator and we have show that in this case "ideal" means rise time \(\leq 25ns\) and fall time \(\geq 500ns\) (\(\tau_{r}\leq 11ns,\tau_{f}\geq 230ns\)).

The infinity of possible algorithms have now been reduced to a few candidates which can be tested with a more detailed simulation, which should include many microstrips, diffusion and \(\delta\)-rays, and a realistic occupancy of tracks at each angle. But we believe that the Overall Efficiency column of Table 7 will be a good guide to the results of the detailed simulation and the "Optimum?" algorithm will be the best one for ATLAS.

**Acknowledgements**

This work has benefitted from the interesting discussions I have had with with Mike Esten, Neil Jackson, Rob Edgecock, Jurriaan Schmitz and with all my colleagues in Manchester.

**References**

[1] Curran et al, Phil. Mag. 40 (1949) 929.

[2] Jurriaan Schmitz. "Simulation of frontend preamplifiers for the MSGC" NIKHEF-H/93-10.

[3] This problem happens to be given as an example in Roger Barlow's "Statistics". John Wiley & Sons (1989) Chichester.

[4] Trond S. Hogh. Thesis for the University of Trondheim (1992). "Event Time and Pulse Height Estimation in Silicon Detectors Using Deconvolution Techniques."

**Figure 1. The heavy line surrounds the area around a track hit in space (strip number) and time (BCO intervals) which is effectively dead for other tracks. The track goes through the square marked \(\times\) and produces a tag of that BCO with efficiency \(\epsilon\). It may also cause tags to be set in the previous or following time bins, with total probability \(R\). In addition it may leave the strip which it hit dead for \(D\) BCOs after it has passed through. We assume that two hits become confused with each other and useless for tracking it they occur within \(\pm 2\) strips of each other.**

[MISSING_PAGE_EMPTY:11]

Figure 4: Examples of deconvolution with RD20 weights. The upper line, offset by 3, shows the input signal. The dashed line, offset by 2, shows the shaped signal with triangles at the sampling points. The lower set of triangles is the deconvoluted signal. a) Deconvolution of delta functions is exact. b) Deconvolution of other signals is approximate.

Figure 5: **a) A typical pulse with RD20 shaper, \(\tau=45ns\). b) Probability of tagging each BCO time bin with various decovolution weights described in the text.**