# Investigating the impact of 4D Tracking in ATLAS Beyond Run 4

The ATLAS Collaboration

This document presents the first investigation of the usage of precision timing information in the ATLAS tracker beyond the HL-LHC Run 4. The Inner Detector of the ATLAS Experiment will be upgraded to a full-silicon Inner Tracker (ITk) to cope with the extreme conditions of the High-Luminosity phase of the Large Hadron Collider, currently foreseen to start with Run 4 towards 2029. ATLAS will also be installing a High-Granularity Timing Detector (HGTD) in the forward pseudorapidity region. The HGTD will help mitigate the effects of pile-up in the forward region by distinguishing between collisions occurring close in space but well-separated in time. Due to the high radiation dose in proximity of the interaction point, the two innermost pixel layers of the ITk are designed to be replaced after 2000 fb\({}^{-1}\). This represents a unique opportunity to bring in technological innovation and expand the physics potential of HL-LHC by including fast-timing through 4-dimensional (4D) tracking in the ATLAS barrel region. While HGTD will provide unique handles to improve the reconstruction of physics objects in the forward region, its capability is limited by its reduced \(\eta\) acceptance. There are also compelling physics reasons to consider fast-timing in the central region. In particular, barrel timing information can significantly improve the identification of b-jets, enhancing the prospects to observe Di-Higgs. This note documents the main physics impacts that a 4D tracking upgrade beyond Run 4 could have in ATLAS. The studies are based on full simulated Monte Carlo samples, but use a simplified, and idealistic, model for track-time resolution. The goal is to assess early the physics merits of timing information in the central pseudorapidity region, before a dedicated long-term simulation effort is potentially launched as a second step.

## 1 Introduction

Pico-second timing information is a ground-breaking new feature to mitigate the impact of pile-up and enhance the reconstruction of events at high luminosity hadron colliders. Timing, being uncorrelated with spatial information, allows to disentangle pile-up interactions that are very close in space but well-separated in time. This is relevant for the High-Luminosity phase of the LHC (HL-LHC), in which pile-up scenarios of about 200 simultaneous interactions are expected [1] and it becomes even more important for future hadron collider concepts, such as the Future Circular Collider (FCC-hh), where the pile-up is foreseen to be almost one order of magnitude larger compared to HL-LHC. Both the ATLAS [2] and CMS [3] experiments will incorporate dedicated precision timing detector layers for the HL-LHC upgrade. The next generation of silicon detectors for charged-particle tracking in such environments is expected to feature 4-dimensional (4D) trackers capable of measuring simultaneously spatial and temporal coordinates [4].

The ATLAS Inner Detector for the HL-LHC Run 4 will be upgraded to a full-silicon Inner Tracker (ITk) which has excellent spatial resolution but does not have fast-timing capabilities. A High-Granularity Timing Detector (HGTD) [5, 6] will be installed in the forward pseudo-rapidity1, \(\eta\), region, providing track timing information at the 30-50 ps level. While HGTD will help to mitigate the effects of pile-up and will lead to gains in specific physics benchmarks such as Vector Boson Fusion (VBF) processes, its coverage is restricted to the forward region, limiting its overall impact to the ATLAS physics programme. In contrast to CMS, whose MIP Timing Detector [7] will provide hermetic timing coverage up to \(|\eta|\leq 3.0\), HGTD only covers the region \(2.4\leq|\eta|\leq 4.0\). This limited \(\eta\) acceptance has a non-trivial effect even in the reconstruction of forward jets, typical of VBF signatures. The reason is that the times of forward tracks within jets need to be compared with the time of the hard-scatter primary vertex (\(t_{HS}\)) that serves as a reference time for placing timing selection cuts. The identification of \(t_{HS}\) becomes challenging when only forward tracks are available. This means that the ability of HGTD to tag pile-up forward jets depends on how many forward tracks originate in the hard-scatter process. A recent study which can be found in Ref. [6], shows that with an ideal knowledge of the \(t_{HS}\), the forward pile-up jet rejection of HGTD can improve by more than a factor of 2.

Footnote 1: ATLAS uses a right-handed coordinate system with its origin at the nominal interaction point (IP) in the centre of the detector and the \(z\)-axis along the beam pipe. The \(x\)-axis points from the IP to the centre of the LHC ring, and the \(y\)-axis points upward. Cylindrical coordinates \((r,\phi)\) are used in the transverse plane, \(\phi\) being the azimuthal angle around the \(z\)-axis. The pseudo-rapidity is defined in terms of the polar angle \(\theta\) as \(\eta=-\ln\tan(\theta/2)\).

The two innermost pixel layers of the ITk are anticipated to be replaced after a certain number of years of data taking beyond Run 4 due to the high radiation dose received, offering the opportunity to extend fast-timing information to the ATLAS central pseudo-rapidity region, for hermetic timing coverage.

There are three key opportunities that hermetic 4D tracking could enable:

* improved vertex \(t_{HS}\) for forward pile-up jet suppression, complementing and extending the HGTD performance, and for the identification of long-lived particle (LLP) decays with large \(c\tau\);
* improved reconstruction of physics objects in the central region, particularly b-tagging, long-lived particle decays with small \(c\tau\), and missing transverse energy;
* improved track and vertex reconstruction (resolution, purity, efficiency, and CPU consumption).

This note aims at quantifying the impact of hermetic 4D tracking coverage in ATLAS for the first time, with emphasis on vertex reconstruction, flavour tagging, and neutral long-lived particles. Low-levelinvestigations on how to improve track reconstruction, resolution, purity, and efficiency, and to reduce CPU consumption by filtering out-of-time hits, are planned for subsequent longer-term studies.

## 2 The ATLAS Inner Tracker (ITk)

The ITk is an all-silicon tracking detector made of two subsystems: a Pixel Detector [8] surrounded by a Strip Detector [9]. The Strip Detector has four strip double-module layers in the barrel region and six end-cap disks, covering the pseudo-rapidity range up to \(|\eta|\) = 2.7. The Pixel Detector consists of five flat barrel layers and multiple inclined or vertical ring-shaped end-cap disks, extending the coverage up to \(|\eta|\) = 4.0. During the last years, the layout of the ITk has undergone several updates compared to the one reported in [8]. The details of the latest version can be found in Ref. [10].

The replaceable layers of the Pixel Detector are the innermost ones, located at 34.0 mm and 99.0 mm in the barrel and 33.2 mm, 58.7 mm and 80.0 mm in the end-caps. In terms of radiation tolerance, the innermost part the ITk is designed to collect up to 2000 fb\({}^{-1}\) of data before being replaced. This corresponds (with a safety factor of 1.5) to a radiation dose of about 10 MGy [8], one order of magnitude more than the current ATLAS Inner Detector. In turn, this requirement implies that a key feature for any new 4D tracking technology used for replacing the pixel modules will be radiation hardness [11].

## 3 Simulated samples

The ATLAS Monte Carlo full simulation [12] is based on the Geant4 toolkit [13]. The generation of the simulated event samples includes the effect of multiple \(pp\) interactions per bunch crossing, as well as the effect on the detector response due to interactions from bunch crossings before or after the one containing the hard interaction.

A \(t\bar{t}\) sample at \(\sqrt{s}\) = 14 TeV was simulated to evaluate the performance of vertex reconstruction and b-tagging at an average pile-up \(<\mu>\) of 200. This sample was generated using Powheg Box[14, 15, 16] interfaced to the Pythia[17] parton shower model with the A14 set of tuned parameters. The EvtGen 1.2.0 program [18] was used for the simulation of the b- and c-hadron decays.

For the LLP study, as described in Ref. [19], Monte Carlo samples with a Higgs boson produced in association with a \(Z\) boson were generated at leading order (LO) using MadGraph5_aMC@NLO 2.6.2[20], with a Higgs boson mass of 125 GeV, and normalised to the cross sections reported in Ref. [21]. No pile-up was added to the events. The generated events were interfaced to Pythia 8[17] to simulate subsequent decays of the Higgs boson and super-symmetric particles in the signal model and also the parton showering. The A14 set of tuned parameters [22] and the NNPDF2.3lo PDF set [23] were used in the event generation. As explained in detail in Ref. [19], the Higgs boson decays into a pair of electrically neutral next-to-lightest super-symmetric particles (NLSP) which are long-lived. Each of the NLSP then decays into a photon and a lightest super-symmetric particle (LSP). The \(Z\) boson decays into a charged-lepton pair (electrons or muons). Final state and kinematics of the Higgs boson decay depend on the masses of the NLSP and LSP, in addition to the lifetime of the NLSP. Several cases can be considered depending on the Supersymmetry (SUSY) [24, 25, 26, 27, 28, 29, 30] scenario under investigation. In our study, we focus on NLSP masses of \(m_{\rm NLSP}=60\) GeV and consider different scenarios for its lifetime, as described in Section 7.3.

## 4 The pile-up challenge

The increasing level of pile-up through the subsequent runs of the LHC, leading to an average pile-up of 200 during HL-LHC, constitutes one of the biggest computational and performance challenges of LHC physics. From a computational standpoint, the large number of simultaneous collisions makes the information to process for each event much larger and complex. For example, the space-point (or hit) combinatorics to reconstruct the trajectory of charged-particles increases enormously. To set the scale, for each event, \(\mathcal{O}(10^{4})\) charged-particles will be produced during HL-LHC within the detector acceptance, leading to \(\mathcal{O}(10^{5})\) hits. With the combinatorics scaling roughly as the square of the number of charged-particle tracks, any improvements in the association of hits to the correct track and, in turn, of the tracks to the correct vertex or jet can have a large and broad impact in the LHC programme. From a performance point of view, pile-up contaminates reconstructed physics objects, such as vertices or jets arising from the hard-scatter interaction, reducing the sensitivity of all physics analyses. While the computational aspects are left for future studies, this note discusses the impact of timing information on ATLAS performance and physics, focusing on the availability of track timing in the central pseudorapidity region. Figure 1 shows the longitudinal view of a simulated \(t\bar{t}\) event with \(<\mu>\) of 200 in the ATLAS Inner Detector. Figure 1 shows the tracks associated to vertices when no track-timing information is available. Based purely on spatial information, the association of tracks to vertices becomes challenging in two main situations. The first situation is when the typical separation between pile-up vertices becomes comparable or smaller than the longitudinal impact parameter resolution. This is the case for tracks beyond \(|\eta|>2.4\) and this fact motivated the addition of the HGTD to the ATLAS Experiment. The second situation is the identification of displaced tracks from heavy flavour decays in the central pseudo-rapidity region. In this case, while the longitudinal impact parameter resolution of prompt tracks, \(z_{0}\), is significantly smaller than in the forward region, tracks from B-hadron decays can have large impact parameters that are comparable or larger than the separation between pile-up vertices. Beyond these two main cases, even the association of tracks to primary vertices can be challenging in the presence of close-by pile-up vertices. When track-timing information is available, as depicted in Figure 1 with the usage of different colours to displays tracks belonging to different interactions, the association of tracks to vertices becomes more unambiguous. The concept can be better visualised in Figure 1, which provides a zoomed-in view in the spatial coordinate. In Figure 1 track-timing information is explicitly used to apply a compatibility time cut between the tracks and the hard-scatter vertex \(|\,t_{trk}\)-\(t_{HS}\)\(|\)\(<30\) ps in order to clean up the environment, which shows very promising potential in the improvement of the track association to the hard-scatter vertex.

## 5 Track-time assignment procedure

The very first step in the evaluation of the impact of 4D tracking on higher level objects is the assignment of time to tracks. As a preliminary procedure, the track-time is assigned from truth level information. Starting from a reconstructed track, the time of the corresponding truth-level information primary vertex is retrieved and the time of that vertex, which is relative to the simulated bunch crossing, is assigned to the track after applying a Gaussian smearing. Several widths are investigated for the Gaussian smearing, to emulate the detector response in terms of time resolution, namely 30 ps, 60 ps and 90 ps. For the small fraction of tracks, of the order of a few percents, which do not have truth information due to simulation and reconstruction effects, a random time is assigned based on the beam-spot time profile.

Figure 1: Longitudinal view of a simulated \(t\bar{t}\) event with \(<\mu>=200\) with (1(a)) no track-timing compared to (1(b),1(c),1(d)) with track-timing information. The truth hard-scatter is indicated with a green star, while the truth pile-up vertices are displayed as solid circles. The reconstructed tracks displayed associated with the truth vertices are displayed.

## 6 Impact on physics object reconstruction

The addition of precision timing information in the central tracker can have a broad impact on the ability of the ATLAS detector to reduce pile-up contamination and improve the reconstruction of physics objects. This section focuses only on two key areas as examples of the potential for improvements: the precise determination of the \(t_{HS}\) for all events and the identification of \(b\)-jets.

The knowledge of the \(t_{HS}\) is a crucial element for the full exploitation of HGTD in the forward region. The limited acceptance of the HGTD makes this determination challenging in events with relatively low forward jet activity. One example are VBF events containing only one jet within the HGTD acceptance. In this case, HGTD can assign a time to the jet, but still needs a reference hard-scatter vertex time from the more central jet in order to be able to either reject or accept this jet. While improvements on HGTD-based algorithms to reconstruct \(t_{HS}\) are expected, the availability of time information for every track in the event will significantly boost the HGTD potential by providing precise \(t_{HS}\) in all final states, including those containing a single forward jet.

The reconstruction of \(t_{HS}\), in conjunction with the time-stamp of every track, can also provide means to reduce pile-up contamination in the reconstruction of the hard-scatter vertex itself. One known limitation of the current vertex reconstruction algorithms under HL-LHC conditions is vertex merging due to primary vertices occurring very close in space. This effect leads to a region of between 0.5 mm and 1 mm where it is not possible to find reliably nearby vertices. Recent proposals for improved vertex reconstruction using machine learning, such as [31] promise to improve this challenge, but are still far from solving it. Track and vertex time information is a very clear way to address this challenge.

Another application of \(t_{HS}\) is to improve the usage of calorimeter time in physics analyses, which is limited by the knowledge of the hard-scatter vertex time. This improvement can be exploited to improve the sensitivity of searches for long-lived particles decaying to displaced photons or jets.

Section 6.1 discusses how a 4D tracker upgrade can provide \(t_{HS}\) for the above applications.

The availability of time information for tracks in the barrel region will have a major impact in the identification of jet flavours, i.e. flavour tagging. The long lifetime of B- and D-hadrons requires the selection of tracks with large impact parameters for the identification of heavy-flavour-quark-initiated jets. The need for large selection windows around the longitudinal impact parameters implies more pile-up contamination that can lead to fake secondary vertices. Therefore, while the HGTD physics case was made on the basis of impact parameter detector resolution being comparable or larger than the expected average distance between primary vertices, the case for barrel track-time relies on the physics of B- and D-hadrons decays leading to similarly large impact parameters relative to the vertex separation.

Section 6.2 describes the impact of 4D tracking on the ability to identify \(b\)-jets.

While this note focuses on these two important applications with broad physics impact, the potential of a 4D tracker extends well beyond. For example, pixel hit time information can provide new ways to improve track seeding and reconstruction. Even if only one tracker layer is instrumented with pico-second timing, it might be possible to reject track seeds that are incompatible to each other in time and drastically reduce CPU time. One particularly interesting direction this could enable is the possibility to lower the minimum track transverse momentum threshold of 0.9 GeV in the central region. These investigations are left to a future document.

### Determination of \(t_{HS}\)

Primary vertex reconstruction in ATLAS is performed using the Adaptive Multi Vertex Finder (AMVF) [32], based solely on spatial information. The AMVF algorithm assigns tracks to multiple vertices, incorporating a weight, \(w_{trk}\), which represents the likelihood of a track belonging to a specific vertex. For this study, we consider only reconstructed vertices which are not purely made up of pile-up tracks, and this is determined by using truth information. The \(t_{HS}\) is computed as the weighted average of the track times:

\[t_{HS}^{reco}=\frac{\sum_{trk}w_{trk}t_{trk}}{\sum_{trk}w_{trk}} \tag{1}\]

In the following we consider three cases in the calculation of \(t_{HS}\):

* _All Tracks_, i.e. all the tracks contributing to the vertex;
* _HS only_, i.e. only the tracks that have been produced from the HS vertex;
* _Clustering_, i.e. implementing a time-based clustering algorithm.

The _HS only_ case makes use of truth matching and is meant to quantify the ideal performance assuming a perfect track-to-vertex association. The clustering algorithm utilised in this task is the Density-based spatial clustering of applications with noise (DBSCAN) [33]. DBSCAN is specifically designed to identify clustered points amidst noise, making it highly suitable for the determination of the vertex time. In this context, it serves as a simplified version of 4D vertexing, where a 1D temporal clustering is applied in addition to the spatially reconstructed vertex obtained from AMVF.

The calculation of \(t_{HS}\) is repeated for each of the track-time resolution scenarios (30 ps, 60 ps, 90 ps).

Figure 2 shows an example of a distribution of track-time for a reconstructed vertex in the three cases described above, when the track-time resolution is emulated to be 30 ps. In the _All tracks_ case, one can notice many mis-associated tracks with incorrect times compared to the _HS only_ case. These tracks will affect the time resolution of the vertex if they are included in the computation of the average time. The clustering procedure effectively removes those spurious contributions, allowing to determine the \(t_{HS}\) with a resolution close to that of the _HS only_ case.

After determining the \(t_{HS}\), it is possible to estimate its resolution by extracting the RMS of the distribution \(\Delta t=t_{HS}^{truth}-t_{HS}^{reco}\), i.e. the difference between the true \(t_{HS}\) and the reconstructed one. This is a rough estimation because of the non Gaussian tails where pile-up contributes the most.

Figure 3 shows the improvement of the \(t_{HS}\) resolution achieved by using the time-clustering algorithm. For a track-time resolution of 30 ps, the \(t_{HS}\) resolution improves from 28 ps, when all tracks are used, to about 7.2 ps using DBSCAN. This is very close to the 5.6 ps obtained when considering only the tracks from the hard-scatter. It demonstrates that, while there is still space for improvement which might arise from the usage of a simultaneous treatment of the space and time coordinates in the vertex finding and fitting procedures, a very good resolution is already achieved.

To better understand the pile-up impact on the determination of the \(t_{HS}\) resolution, we study it as a function of the fraction of pile-up tracks in the vertex ("vertex PU fraction"), as shown in Figure 4.

The vertex PU fraction has a smoothly falling distribution and it is binned to obtain almost constant statistics per bin. In each bin of vertex PU fraction, the difference between the truth and the reconstructed \(t_{HS}\) follows a Gaussian distribution. Therefore, in this case, a more accurate determination of the resolutionis achieved by using the width of a Gaussian function used to fit the difference between the truth and the reconstructed \(t_{HS}\) in each bin of the vertex PU fraction.

While the majority of the vertices have low pile-up contamination arising from the good performance of the tracking system, a non-negligible number of vertices has sizeable pile-up contamination. The ability of removing the pile-up tracks depends strongly on the track-time resolution. At 30 ps, shown in Figure 4(a), the clustering algorithm leads to almost the ideal resolution across the entire vertex PU fraction spectrum. When increasing the track-time resolution, the improvement in the \(t_{HS}\) resolution decreases as shown in Figure 4(b) for the 60 ps case and 4(c) for the 90 ps case.

Figure 5 shows a comparison of the \(t_{HS}\) resolution as a function of the vertex PU fraction obtained with the clustering algorithm for the three track-time resolution cases.

Figure 2: Distribution of the track-time in the 30 ps track-time resolution scenario for a single event, for the tracks that belong to a reconstructed vertex. Three cases are shown: _All Tracks_, i.e. all the tracks contributing to the vertex, _HS only_, i.e. only the tracks that have been produced from the HS vertex and _Clustering_, i.e. tracks that belong to a re-clustered vertex.

### Flavour tagging

Identifying b-quark-initiated jets, \(b\)-tagging, plays an especially important role at the LHC because heavy quarks provide crucial insights into various phenomena, such as the search for new particles, the measurement of fundamental parameters of the Standard Model and the characterisation of the Higgs boson, whose most abundant decay mode is into pairs of b-quarks. The state-of-the-art flavour tagging algorithm in ATLAS is based on a graph neural network (GN1) built upon tracks associated to jets [34]. In this note we demonstrate that, beside the already high performances that this architecture can reach with the new ITk detector, timing information allows to improve the performances considerably. GN1 uses a one stage Graph Neural Network [34] that takes as input a jet and their associated tracks. The output of the algorithm is a set of scores for each jet, which represents the probability for the the jet to be tagged as a \(b-\), \(c-\) or light-quark-initiated jet. We focus this study on light-jets (\(l\)-jets) vs \(b\)-jets discrimination, but the conclusions are expected to extend also to \(c\)-jets.

One of the key features utilised in \(b\)-tagging algorithms is the long lifetime of \(B\)-hadrons, \(\tau_{B}\sim 1.5~{}ps\). This results in the \(b\)-jet having the peculiar signature of a displaced secondary vertex with respect to the beam-line. The tracks arising from this displaced vertex have large signed longitudinal (\(z_{0}\)) and transverse (\(d_{0}\)) impact parameters, projected onto the jet axis and defined with respect to the primary vertex, compared to tracks originating from light-quark-initiated jets. The observables used to capture such features are the impact parameter significances, i.e. the ratio between the impact parameter and their estimated error \(\sigma\):

* \(S(d_{0})=d_{0}/\sigma_{d_{0}}\) for the transverse impact parameter significance;
* \(S(z_{0})=z_{0}sin\theta/\sigma_{z_{0}sin\theta}\) for the longitudinal impact parameter significance.

In the high pile-up environment foreseen during HL-LHC, flavour tagging becomes more challenging because pile-up tracks mis-associated to hard-scatter jets worsen the discrimination power provided by

Figure 3: Distribution of the \(t_{HS}\) resolution. The 30 ps track-time resolution scenario is used. The resolution obtained when using all tracks is compared with that obtained with an _a posteriori_ clustering technique as well as with that obtained by using only HS tracks.

Figure 4: Resolution of the \(t_{HS}\) in three track-time resolution scenarios (30 ps, 60 ps and 90 ps respectively for Figure 4(a), Figure 4(b) and Figure 4(c)) shown as a function of the vertex PU fraction. The resolution obtained when using all tracks is compared with that obtained with an _a posteriori_ clustering technique as well as with that obtained by using only HS tracks. The distribution of the vertex PU fraction is also overlaid.

the impact parameter significance of tracks. Figure 6 shows the track impact parameter significance with and without PU for b-jet and light-jets. As expected, only the longitudinal impact parameter significance is affected by pile-up. In particular Figure 6 shows that the large positive tails in the \(S(z_{0})\) distribution for light-jets is mostly dominated with pile-up track contributions. Reducing such PU contamination can enhance the differentiation among flavours.

To evaluate the impact of track timing on GN1, we define a temporal track-level variable that will be used as an additional input to GN1. This new variable is called track-time significance \(S(t_{trk})\) and it is defined as follows:

\[S(t_{trk})=\frac{|t_{trk}-t_{HS}|}{\sigma_{t}} \tag{2}\]

In this equation, \(\sigma_{t}\) is the track-time resolution defined in Section 5.

Figure 7 shows that the distribution of the track-time significance comprises of two main components: the tracks originating from the hard-scatter, indicated as HS and that populate the core of the distribution, and the tracks originating from pile-up, indicated as PU, which have a higher significance.

From now on, we refer to the enhanced architecture of GN1 which uses \(S(t_{trk})\) as GNT. The other input variables are identical to those defined in [35].

The statistics of the \(t\bar{t}\) sample used in this analysis is divided into three parts: a training (\(28\times 10^{5}\) jets), a validation (\(7\times 10^{5}\) jets) and a test (\(26\times 10^{5}\) jets) set. The total statistics of the sample used in this study is equivalent to that used in previous ITk publications [35], but it is limited compared to the results presented in the Run 3 studies [34], therefore changes in efficiency and rejection can be expected in the future if larger data-sets will be used.

Figure 5: Comparison of the \(t_{HS}\) resolution obtained with an _a posteriori_ temporal 1D clustering technique in three track-time resolution scenarios 30 ps, 60 ps and 90 ps shown as a function of the vertex PU fraction

The outputs of the networks, i.e. the three scores (\(p_{b}\), \(p_{c}\), \(p_{l}\)) introduced earlier and representing the probabilities for the jet to be tagged with a certain flavour, can be combined to create a global discriminant \(D_{b}\) indicating the probability that a jet is a b-jet defined as follows:

\[D_{b}=\log\frac{p_{b}}{f_{c}\cdot p_{c}+(1-f_{c})\cdot p_{l}} \tag{3}\]

In this formula, \(f_{c}\) is a parameter related to the fraction of \(c\)-jets that can be tuned. For GN1, it is set to \(f_{c}=0.05\). For GNT, this parameter can be optimised as well. Since we are not focusing on studying the

Figure 6: Track signed 6(a) transverse \(d_{0}\) and 6(b) longitudinal \(z_{0}sin\theta\) impact parameter significance with and without PU for b-jet and light-jets.

Figure 7: Distribution of the track-time significance for tracks that have been truth-matched to the hard-scatter (HS) or to a pile-up (PU) vertex.

performance of \(c\)-jets, we chose \(f_{c}\) (additional information can be found in Appendix A) to have an almost constant \(c\)-jets rejection at a given \(b\)-jet efficiency working point, \(\epsilon_{b}=77\%\). A working point is defined by the b-tagging efficiency obtained as the fraction of jets in the \(D_{b}\) distribution above a certain cut on the b-jet curve. For each b-tagging efficiency, the corresponding rate of light-jets that are mis-tagged as b-jets can be calculated as the integral of the light-jet curve above the same cut.

Figure 8 shows the \(D_{b}\) discriminant for light- and b-jets in a comparison between GN1 and GNT with no smearing on track-time (GNT ideal). It can be observe that, in general, \(D_{b}\) takes large values for b-jets and smaller values for light-jets, as expected from its definition, but it is apparent that the discrimination between the two flavours improves when introducing timing information.

A more quantitative assessment of the performance is obtained by studying the light-jet rejection as a function of the b-tagging efficiency. The light-jet rejection is defined as inverse of the mis-identification efficiency of light-jets as b-jets. Figure 9 shows a comparison between three setups of the b-tagging algorithm: with track-time and without smearing (GNT ideal), with track-time information in the 30 ps track-time resolution scenario (GNT 30ps) and without timing (GN1). A large improvement arises from the usage of \(S(t_{trk})\). For example, for a 77% b-tagging working point, the light-jet rejection improves by a factor of about 3.5 in the ideal case scenario and by a factor of more than 2 in the 30 ps track-time resolution scenario.

This improvement is analysed through several figures of merit. For example, Figure 10, shows the \(l\)-jet mistag rate for a fixed \(\epsilon_{b}=70\%\) b-tagging working point, as a function of the jet PU fraction defined similarly as in Sect. 6.1, but this time by considering the fraction of PU tracks in a jet rather than in a vertex. Different setups of the GNN algorithm are shown: with perfect timing (GNT ideal), with timing information in the 30 ps track-time resolution scenario (GNT 30ps) and without timing (GN1). We can observe that the \(l\)-jet mistag rate as a function of the jet PU fraction gets flattened in GNT, with improvements of almost one order of magnitude for the GNT ideal case and about a factor of 4 for the GNT 30ps case in the highest jet PU fraction bins.

Figure 8: Distribution of the b-tagging score \(D_{b}\) for light- and b-jets in a comparison between GN1 and GNT ideal.

One of the auxiliary tasks of the GNN algorithm is the prediction of the origin of each track within the jet. The track is labelled with one of the exclusive categories defined in Ref. [34], after analysing the particle interaction that led to its formation. These categories include tracks from pile-up, indicated as "PU tracks", as well as tracks from the decay of a B-hadron, indicated as "FromB tracks", and tracks from the decay of a C-hadron which itself is from the decay of a B-hadron, indicated as "FromBC tracks". By examining the differences in the labels that tracks are assigned when using GN1 or GNT, as shown in Figure 11, we can gain insights into the source of the observed effects. Figure 11(a) illustrates that, as expected, track timing information helps mostly in discriminating PU tracks from non-PU tracks. The rejection of non-PU tracks could be improved up to a factor of 10 in the ideal case with no smearing emulating the time resolution, and up to a factor of 4 in the case of 30 ps track-time smearing. The classification task in Figure 11(b) shows also that while the main improvement arises from a much more accurate classification PU tracks, the classification of tracks from heavy flavour hadrons (FromB or FromBC) is largely unchanged when using track-timing information, as expected.

To further understand the requirement on a future 4D tracker, we study the impact of several track-time resolution scenarios on b-tagging efficiency and \(l\)-jet rejection. The results are reported in Figure 12. Even in the pessimistic 90 ps track-time resolution case, for a 77% b-tagging working point, the light-jet rejection improves by a factor of about 1.5. The impact of different vertex resolutions for a given track-time resolution is included in Appendix A.

Two additional effects which are expected to impact the performance of 4D tracking have also been emulated: the impact of missing or mis-associated hits-on-track. In order to study missing pixel hits on tracks, we assume that the timing information in the ITk will come from the second innermost pixel layer.

Figure 9: Background rejection as a function of the b-tagging efficiency for different setups of the GNN algorithm: with perfect timing (GNT ideal), 30 ps track-time resolution (GNT 30ps) and no timing (GN1 std).

Figure 11: Performance of the track classification task in GN1 and GNT. 11(a): rejection of non-PU tracks as a function of the efficiency of identifying PU tracks. 11(b) rejection of non-PU, non-FromB or non-FromBC tracks as a function of the efficiency of categorising correctly the tracks within the corresponding category.

Figure 10: The \(l\)-jet mistag rate as a function of the jet PU fraction for a fixed \(\epsilon_{b}=70\%\) b-tagging working point. Different setups of the GNN algorithm are shown: perfect timing (GNT ideal), 30 ps track-time resolution (GNT 30ps), and no timing (GN1). The distribution of the jet PU fraction is also overlaid.

This is because this is the layer further away from the interaction point, with less occupancy and less radiation than the innermost layer. If the track analysed has no hits in the second innermost layer, \(t_{trk}\) is extracted to emulate the HS profile but its error \(\sigma_{t}\) is assumed to be 180 ps, i.e. as large as the temporal spread of the beam-spot. This corresponds to saying that no clear discrimination between HS and PU tracks can be made on the basis of timing information. The impact is shown in Figure 13. For example, for a 77% b-tagging efficiency working point, a small degradation of about 5% is present in the light-jet rejection when comparing the results with the default performance of GNT at 30 ps. Assessing the impact of mis-associated hits-on-track requires a more complex procedure. While studying these effects in detail is beyond the scope of this proof-of-concept and requires access to full 4D tracking detector simulation, we conducted a simple emulation to understand at which scale the performance might be influenced. We utilise the so-called Truth Match Probability value, or \(P_{match}\). The formula for \(P_{match}\) is given below:

\[P_{match}=\frac{2N_{common}^{pix}+N_{common}^{strip}}{2N_{track}^{pix}+N_{ track}^{trip}} \tag{4}\]

where the denominator contains the number of pixel and strip detector hits attributed to the track, while the numerator contains the number of pixel and strip hits common to both the track and the truth particle. Each track is assigned a \(P_{match}\) between 0 and 1. A score of 1 indicates that each hit composing a track was assigned correctly. While \(P_{match}\) is not perfect for our study because it includes information from the strip detectors which are not replaceable in HL-LHC, it still provides a proxy for the number of mis-associated hits and their correlation with other track parameters. By using \(P_{match}\) in our studies we assume that

Figure 12: Background rejection as a function of the b-tagging efficiency for different track-time resolution scenarios: 30 (GNT 30ps), 60 (GNT 60ps) and 90 (GNT 90ps), in a comparison with the default algorithm without track timing (GN1).

mis-associated hits are mis-associated in both time and space. The excellent performance of the ITk is confirmed by the large amount of tracks that have a \(P_{match}\) score of 1 or very close to 1. In fact, only about 1% of the tracks have a \(P_{match}<0.8\). We utilise this cut value and assign a "wrong" (i.e. taken randomly from a PU vertex) time stamp to the tracks which have \(P_{match}<0.8\). The effect of this emulated mis-tagged hits is presented in Figure 13. The degradation in performance is comparable with that of the missing hits-on-track case.

Figure 13: Background rejection as a function of the b-tagging efficiency for different setups of the GNT algorithm in the 30 ps track-time resolution scenario: with no assumption on mis-associated or missing hits (GNT 30 ps), emulating tracks which have received a timestamp from mis-associated hits (GNT 30 ps mistag hits), and emulating tracks with missing hits in the second ITk pixel layer (GNT 30 ps missing hits).

## 7 Impact on physics analyses

The impact of 4D tracking on object reconstruction will largely benefit several areas of the HL-LHC Physics programme. In the following, we provide three examples related to some of the key research fields in the ATLAS experiment: searches for Higgs boson pair (HH) production, searches for invisible decays of the Higgs produced through Vector Boson Fusion (VBF) production, and searches for long-lived particles (LLP).

### Higgs boson pairs

Probing the Higgs boson self-coupling \(\lambda\), i.e. the way the Higgs boson couples to itself, is a flagship analysis for HL-LHC. Such a measurement is the key missing piece that would allow the shape of the Higgs potential to be better understood and thus shed light on the dynamics responsible for the electroweak symmetry breaking mechanism. Measuring \(\lambda\) is, therefore, also an excellent probe for scenarios of physics Beyond the Standard Model (BSM): should any of them exist, they may manifest as a deviation from the SM predicted value. A direct probe of the Higgs self-coupling is possible through studying Higgs boson pairs (HH).

Several searches for Higgs pair production have been performed by the ATLAS [36, 37, 38] and CMS Collaboration [39, 40, 41, 42] with the existing data-sets. The current results obtained by combining the three leading final states \(HH\to b\bar{b}b\bar{b}\), \(HH\to b\bar{b}\tau\tau\) and \(HH\to\gamma\gamma b\bar{b}\)[43, 44] show that to observe a HH signal with the Run 2 data, it would have to be about 3 times as large as what the SM predicts. These results have been extrapolated to the HL-LHC data-set by both ATLAS and CMS [44, 45, 46, 47, 48] and show that an evidence for SM-like HH production is feasible by combining the same three final states mentioned above. Additional improvements may arise from including other final states.

Nevertheless, more data or better algorithms are needed to increase the precision with which we will extract \(\lambda\) and draw more quantitative conclusions about the Higgs potential. As HH is a very rare process, most of the analyses look for a signal in a final state in which at least one of the two Higgs bosons decays into its most abundant mode, i.e. in a pair of b-quarks. Therefore improving b-tagging is one of the most powerful ways to improve HH searches.

The ATLAS \(HH\to b\bar{b}b\bar{b}\) Run 4 extrapolation study [48] considered the impact of potential improvements in b-tagging efficiency in the analysis sensitivity. Figure 14 shows the \(HH\to b\bar{b}b\bar{b}\) discovery significance. In particular, Figure 14(a) shows the discovery significance as a function of b-tagging efficiency values ranging from 65% to 85%, assuming as light-jet rejection rate that of the 77% working point used in the corresponding Run 2 analysis. In the scenario of statistical only errors (No syst. unc.), a 5% increase in b-tagging efficiency (from 77% to 82%) corresponds to an increase in the \(HH\to b\bar{b}b\bar{b}\) discovery significance of about 0.3 \(\sigma\). As shown in Figure 14(b), this corresponds to the sensitivity enhancement of more than 500 fb\({}^{-1}\) of data. Figure 9 and 12 show that this improvement in b-tagging efficiency could be within reach with 4D tracking.

Depending on the point in time at which the innermost pixel layers of the ITk could be replaced with a 4D tracking system, the improvement might decrease (i.e. less time to exploit the new technology), but it must be noted that similar level of improvements will be brought in each one of the HH searches with at least one \(H\to b\bar{b}\) decay, making 4D tracking a very powerful way to enhance the reach of HH searches.

### Search for invisible decays of the Higgs boson produced in VBF events

The analysis of Vector Boson Fusion events constitutes a major component of the HL-LHC programme. The HGTD detector is expected to play a key role in the reconstruction of these signatures which are characterised by the presence of at least one forward jet. In particular, the HGTD detector is able to improve the rejection of forward pile-up jets by about 40%, which enhances the signal over background ratio (S/B) in Higgs to invisible decays [5; 6]. As explained in section 6, the lack of timing information in the central region makes the determination of \(t_{HS}\) with the HGTD difficult in VBF events that contain relative small forward activity, such as in VBF events with only one forward jet.

In order to evaluate the impact that a precise \(t_{HS}\) determination in all events might have on pile-up jet suppression, a study was conducted in Ref. [6] where the reconstructed vertex time was replaced by the truth time from the simulation, emulating ideal (perfect) timing. Figure 15(a) shows the performance of pile-up suppression of forward jets using the ITk alone, HGTD, and HGTD with ideal time. The conclusion is that the precise knowledge of \(t_{HS}\) in all events significantly improves the rejection of pile-up jets. Similar gains in performance are expected with a 4D tracking detector with the characteristics described in this document, although a dedicated study should be performed to verify it.

Improved forward pile-up jet rejection is not the only way a 4D tracker can increase the sensitivity to \(H\to\) invisible decays. Figure 15(b) shows the S/B gain relative to ITk only separately for events containing exactly two forward jets (FF), exactly one central and one forward jet (CF) and events with either FF or CF jets. HGTD is expected to contribute more effectively in the FF case, where a precise determination of \(t_{HS}\) is simpler. A 4D tracker with hermetic timing acceptance will be able to improve S/B in the combined FF plus CF category, significantly enhancing the discovery potential in this final state. As a concrete example, as indicated by Figure 15(a), a factor of two improved pile-up rejection relative to ITk when the hard-scatter jet efficiency is 85% translates into a S/B improvement of approximately 40% 2

Footnote 2: This value is obtained by reading the y-axis for the dotted blue curve corresponding to an x-axis value of \(1-\frac{1}{2}\).

Figure 14: The discovery significance extrapolated from the Run 2 analysis to HL-LHC for the \(HH\to b\bar{b}b\bar{b}\) process in 14(a) evaluated for a range of b-tagging efficiency values from 65% to 85% assuming the light-jet rejection rate as the 77% working point used in Run 2 and in 14(b) evaluated at various integrated luminosity values ranging from 1000 fb\({}^{-1}\) to 3000 fb\({}^{-1}\). Details can be found in Ref. [48].

### Long-lived particles

A 4D tracking detector covering the central pseudo-rapidity region can provide new handles to improve the sensitivity to some LLP scenarios. There are two main cases. One is expanding the reach to LLP decays with small \(c\tau\). LLP decays with short decay lengths can directly benefit from the improved capability to identify displaced vertices by reducing pile-up contamination. The second, less obvious, is the search for displaced/delayed photons (or jets). We focus on the second case in this section.

Searches for delayed photons rely on the excellent timing capabilities of the ATLAS liquid-argon (LAr) electromagnetic calorimeter. The time resolution of high \(p_{T}\) photons has been measured in Run 2 to have a constant term of roughly 190 ps. This constant term arises from the lack of knowledge in the \(t_{HS}\). Since the ATLAS Run 1-3 Inner Detector does not have the ability to determine the time of the hard-scatter vertex with pico-second precision, the full 180 ps spread of the temporal beam-spot is the limiting factor to photon time resolution. A 4D tracking detector will solve this challenge. By providing a reference time of 5-10 ps for all events, the constant term of the photon time resolution can be drastically reduced, enhancing the power of ATLAS to search for displaced/delayed photons (or jets) at much lower \(c\tau\) values than what would be achievable without it. Therefore, this is a case where 4D tracking provides additional information to improve calorimeter time reconstruction.

It should be noted that already during Run 4 HGTD will also be able to provide a precise \(t_{HS}\) in events with significant forward jet activity, such as in VBF topologies. LLP searches involving displaced photons

Figure 15: Impact of track-time information on Vector Boson Fusion topologies. In 15(a), the rejection of pile-up jets as a function of the efficiency for hard-scatter jets in VBF H \(\rightarrow\) invisible events with an average of 200 pile-up collisions per bunch crossing. The solid lines are obtained without using the time information. Black dashed (red fine-dashed) lines are obtained using time information and the reconstructed (true) \(t_{HS}\) of the event. The error bars reflect the statistical uncertainty of the samples. Target working points use 85% of efficiency for hard-scatter jets. Details can be found in Ref. [6]. In 15(b), normalised signal over background gain relative to ITk-only pile-up jet suppression performance, as a function of the additional pile-up jet rejection from HGTD. The solid black (dotted red) line represents the HGTD improvement from the CF (FF) event topologies separately. The dotted blue line shows the total improvement when the combined HGTD+ITk pile-up suppression algorithm is applied to all jets in the event. Details can be found in Ref. [5].

in VBF production will benefit from the use of timing information provided by the HGTD, however a 4D tracking detector will bring this advantage to all production modes.

Previous ATLAS searches for displaced photon signatures arising from the decay of Higgs-produced next-to-lightest super-symmetric particles (NLSPs) have thus far probed NSLP lifetimes of \(\tau=2,10\) ns. [19]. This search considers the production of a Higgs boson in association to a vector boson, which provides a clean trigger signature. In this section, we assess how the improved knowledge of \(t_{HS}\) might impact the sensitivity of this search.

The dominant background source for displaced photon searches is prompt photons, i.e. photons which are produced at the interaction point. Calorimeter photon timing information is used to reduce this source of background. Given the uncertainty on timing from the electromagnetic calorimeter and on \(t_{HS}\), photons produced by NLSPs which decay on or below centimeter scales could still be reconstructed as prompt photons, therefore significantly reducing sensitivity to these exotic events.

In order to estimate the potential gain of a 4D tracker detector on this search, we first simulate the the delay time of the photon measured at the calorimeter as follows:

\[t_{\text{ECal}}^{\text{Measured}}=t_{0}+t_{\text{IP}\rightarrow\text{ECal}}\]

The first term, \(t_{0}\), is truth time of the hard-scatter interaction. We model \(t_{0}\sim\mathcal{N}(0,180\text{ ps})\). The \(t_{\text{IP}\rightarrow\text{ECal}}\) term is the time for the photon to reach the electromagnetic calorimeter, either as a prompt photon which travels directly from the IP (background) or as a SUSY-produced photon which travels via a displaced long-lived vertex from the NLSP decay (signal). This time is smeared by the assumed intrinsic electromagnetic calorimeter's timing resolution (without accounting for the constant term which is already considered in the \(t_{0}\) term) of 100 ps. This specific value is only an estimate based on [19]. Future versions of this study should revise this assumption.

Then, the reconstructed time is obtained as follows:

\[\Delta t^{\text{Reconstructed}}=t_{\text{ECal}}^{\text{Measured}}-t_{\text{IP }\rightarrow\text{ECal}}^{\text{Reconstructed}}-t_{0}^{\text{Reconstructed}}\]

The first term is the reconstructed photon time in the calorimeter. This measurement will have a resolution given by the sum in quadrature of the vertex \(t_{0}\) resolution and the ECAL intrinsic resolution. The \(t_{\text{IP}\rightarrow\text{ECal}}^{\text{Reconstructed}}\) term subtracts off the expected time for a prompt photon to travel from the IP to the electromagnetic calorimeter in order to isolate the effect of timing uncertainty. This term utilizes the measured photon coordinates at the calorimeter and the primary vertex \(z\) position. The final term, \(t_{0}^{\text{Reconstructed}}\), is the \(t_{HS}\) determination. Without a 4D tracker, we assign \(t_{0}^{\text{Reconstructed}}=0\). This reflects the lack of knowledge in \(t_{HS}\). In the case of a 4D tracker that can provide precise \(t_{HS}\) information, we model \(t_{0}^{\text{Reconstructed}}\sim\mathcal{N}(t_{0},30\text{ ps})\). This assumes a 30 ps \(t_{HS}\) resolution which is a conservative underestimation of the expected resolution shown in Section 6.1.

Figure 16 shows the distribution of \(\Delta t\) for NLSPs with mass \(m_{\text{NLSP}}=60\) GeV and lifetime \(\tau=2\) ns. There is greater separation between signal and background for the case of a 4D tracker. In particular, the addition of precise \(t_{HS}\) information significantly reduces the spread of \(\Delta t\) for background (prompt) photons, enabling the exploration of shorter decay lengths.

Figure 17 shows the proportion of correctly classified signal versus incorrectly classified background with and without 4D tracking. In this plot, signal is defined as events whose \(\Delta t\) is in excess of some value, with all other events constituting background. There is a significant separation between these curves which

indicates improved analysis sensitivity. For example, for a factor of 100 background rejection, there is an absolute 20% increase in signal acceptance, corresponding to a 33% relative improvement.

Using this ROC curve, we examined the signal acceptance at a constant rate of incorrectly rejected background across detector regimes and NLSP lifetimes. Taking a rate of incorrectly classified background events of 0.01, figure 18 shows the ratio of signal acceptance with and without a 4D tracking detector for NLSPs with \(\tau\) ranging from 0.5 to 25 ns. As expected, 4D tracking can provide a very significant improvement in signal acceptance for lower lifetimes. Though this advantage diminishes, there is still a noticeable improvement at longer lifetimes as well.

Figure 16: Comparison of \(\Delta t\) for signal and background events in searches for displaced photons arising from NLSPs with mass \(m_{\rm NLSP}=60\) GeV and lifetime \(\tau=2\) ns with and without a 4D tracking detector.

Figure 17: Comparison of ROC curves for accepting displaced photon signals arising from NLSPs with mass \(m_{\rm NLSP}=60\) GeV and lifetime \(\tau=2\) ns with and without a 4D tracker.

This study shows, in a simplified way, how the improved knowledge of \(t_{HS}\) can impact the search for displaced photons. Similar improvements are expected to hold for other related final states involving delayed photons or jets.

## 8 Conclusions and next steps

This note summarises the first studies performed in ATLAS to investigate the physics impact of a potential 4D tracking upgrade of the replaceable innermost Pixel Detector layers beyond Run 4. These studies are based on a simplified Gaussian model for track-time resolution, without making any assumption on specific technologies or implementations, and without a full detector simulation model that includes the impact of different material budget, cooling and data transmission needs, power consumption, etc. The goal was to assess the physics case of an ATLAS 4D tracking upgrade under simplified assumptions, before a potential longer-term conceptual detector design and a corresponding full simulation study are considered.

The results documented in this note, under the stated assumptions, suggest that the physics case of a 4D tracking upgrade can be broad and compelling. We have focused primarily on vertexing and b-tagging and their impact on searches for Higgs pairs and long-lived particles, but have also mentioned other areas where barrel timing can potentially bring additional physics improvements, such as track reconstruction, or pile-up jet suppression in the forward region, maximising the investment on the HGTD.

In this first public result, we show that a very precise determination of \(t_{HS}\) is achievable with a simple track clustering technique. For example, the vertex time resolution obtained is of the order of O(5 ps) when the track-time is known with a resolution of 30 ps. In such a scenario, for a 77% b-tagging working point, the light-jet rejection improves by a factor of more than 2. Such \(b\)-tagging improvements could potentially increase the \(HH\to b\bar{b}b\bar{b}\) discovery significance by up to 0.3\(\sigma\), with similar improvements expected in other HH (and not only) physics searches with b-quarks in the final state. Additionally, the precise knowledge of \(t_{HS}\) will enhance VBF event reconstruction through improved forward pile-up rejection and

Figure 18: Ratio of signal efficiency with and without 4D tracking, corresponding to rate of incorrectly classified background of 0.01 across NLSP lifetimes.

would enable searches for delayed photons or jets at shorter lifetimes compared to those accessible now or at the start of Run 4.

Several variations of the track-time resolutions as well as potential detector effects have been examined in order to set preliminary benchmarks for the particle physics community on the requirements that 4D tracking R&D would need to meet in the years to come.

These results motivate future in-depth studies to incorporate a preliminary layout with more realistic detector assumptions, as well as more sophisticated reconstruction algorithms covering the full range of physics capabilities.

## Appendix A: Flavour tagging characterisation

Several studies were performed to characterise the flavour tagging results obtained when using track-time information.

For example, the \(t_{HS}\) resolution has a slight remaining dependence on the vertex PU fraction after we apply the _a posteriori_ clustering procedure. Figure 19 shows the impact of different resolutions on the \(t_{HS}\) for a given track-time resolution. In particular, in the 30 ps track-time resolution case, the two extreme values of \(t_{HS}\) resolution obtained as a function of the vertex PU fraction are either 6 ps or 11 ps (see Figure 5). The impact on b-tagging performance is negligible.

One of the parameters that can be tuned in the GNN b-tagging algorithm is \(f_{c}\), a tunable parameter to adjust the relative importance of \(p_{c}\), \(p_{I}\) in the discriminant. Figure 20 shows a scan for the determination of \(f_{c}\), where GN1 is shown along with GNT (the ideal case when perfect time resolution is assumed on both tracks and HS vertex) and GNT 30ps (the case with 30ps resolution on track-time and 6ps resolution on \(t_{HS}\)). These resolution values are justified from Figure 3.

Figure 21 shows the \(l\)-jet mistag rate as a function of the jet PU fraction for a fixed \(\epsilon_{b}=70\%\) b-tagging working point in different track-time resolution scenarios, while Figure 22 shows the PU track classification as a result of one of the auxiliary tasks of the GNN b-tagging algorithm. As expected, the \(l\)-jet mistag rate and the confusion between non-PU and PU tracks increases with the worsening of the track-time resolution.

Figure 23 shows the classification of the tracks as predicted by the GN1 and GNT 30ps algorithms. In particular, 23(a) for \(b\)-jets in events in which the b-jets migrate from not being "loose" tagged at \(\epsilon_{b}=85\) %

Figure 19: Light-jet rejection as a function of the b-tagging efficiency for two setups of the GNT algorithm: with a track-time resolution of 30 ps and either 6 ps or 11 ps \(t_{HS}\) resolution, representing the two extreme values of \(t_{HS}\) resolution obtained as a function of the vertex PU fraction shown in Figure 5.

to being "tight" tagged at \(\epsilon_{b}=60\) %; 23(b) for \(l\)-jets that are tight tagged as \(b\)-jet at \(\epsilon_{b}=60\) % without time. The matrices are more diagonal in the GNT 30ps case, indicating a superior accuracy of the track reconstruction.

Figure 21: The \(l\)-jet mistag rate as a function of the jet PU fraction for a fixed \(\epsilon_{b}=70\%\) b-tagging working point. Different setups of the GNN algorithm are shown: without timing (GN1) and with timing information in the 30 ps, 60 ps and 90 ps track-time resolution scenario (GNT 30ps, GNT 60ps, GNT 90ps). The distribution of the jet PU fraction is also overlaid.

Figure 20: Scan for the determination of \(f_{c}\), where GN1 is shown along with GNT ideal and GNT 30ps.

classification in the time-assisted algorithm.

Figure 22: Performance of the PU track classification task in GN1 and GNT. The rejection of non-PU tracks as a function of the efficiency of identifying PU tracks is shown for different setups of the GNN algorithm: without timing (GN1) and with timing information in the 30 ps, 60 ps and 90 ps track resolution scenario (GNT 30ps, GNT 60ps, GNT 90ps).

Figure 23: Track classification in GN1 and GNT 30ps: 23(a) for \(b\)-jets in events in which the b-jets migrate from not being “loose” tagged at \(\epsilon_{b}=85\) % to being “tight” tagged at \(\epsilon_{b}=60\) %; 23(b) for \(l\)-jets that are tight tagged as \(b\)-jet at \(\epsilon_{b}=60\) % without time.

## Appendix B: Pile-up densities

Along with the vertex PU fraction, we study the \(t_{HS}\) resolution as a function of two additional variables: the well-established average spatial pile-up density \(\langle\rho_{z}\rangle\) and the newly introduced average temporal pile-up density \(\langle\rho_{t}\rangle\). In the following formulas \(\sigma_{z}\) and \(\sigma_{t}\) are the spatial and temporal standard deviation of the beam-spot.

The average spatial pile-up density is defined as:

\[\langle\rho\rangle(z_{HS})=\frac{\langle\mu\rangle}{\sqrt{2\pi}\sigma_{z}} \exp\left(-\frac{z_{\text{HS}}^{2}}{2\sigma_{z}^{2}}\right) \tag{5}\]

The average temporal pile-up density is defined as:

\[\langle\rho\rangle(t_{HS})=\frac{\langle\mu\rangle}{\sqrt{2\pi}\sigma_{t}} \exp\left(-\frac{t_{\text{HS}}^{2}}{2\sigma_{t}^{2}}\right) \tag{6}\]

While these variables provide only averaged information and are not sensitive to the local pile-up contamination, they are complementary to the vertex PU fraction defined earlier as they do not depend on the performance of the reconstruction algorithms and, as such, will remain stable should the tracking and vertexing algorithms change.

Figure 24: Resolution of the \(t_{HS}\) in the 30 ps track-time resolution scenario as a function of two different variables: Figure 24(a) as a function of the average spatial pile-up density \(\langle\rho\rangle(z_{HS})\) and Figure 24(b) as a function of the average temporal pile-up \(\langle\rho\rangle(t_{HS})\) density. The resolution obtained when using all tracks is compared with that obtained with an _a posteriori_ clustering technique as well as with that obtained by using only HS tracks. The distribution of spatial and temporal pile-up density is also overlaid in the corresponding figures.

The \(t_{HS}\) resolution as a function of spatial and temporal \(\langle\rho\rangle\) is shown in Figure 24 for the 30 ps track-time resolution scenario. The distribution of spatial and temporal average pile-up density is also overlaid in the corresponding figures. On average, in the HL-LHC scenario there will be about one vertex every 0.7 \(mm\) or every 2.3 \(ps\). In both distributions, there is a flattening of the resolution when the pile-up tracks are removed by means of the clustering algorithm. The difference between the two is in the trend of the resolution achieved when all tracks are used. The resolution is worse at high average spatial pile-up densities due to the large presence of vertices nearby in the \(z\)-coordinate and the increasing amount of mis-associated tracks as a function of \(\langle\rho\rangle(z_{HS})\). Instead, the \(t_{HS}\) resolution appears to be worse at small average temporal pile-up densities. This is because the vertexing algorithm uses only spatial information, making tracks from vertices close in space but very far in time still compatible with the same vertex. The number of mis-associated tracks is roughly flat as a function of \(\langle\rho\rangle(t_{HS})\), as expected since time is unused in the current vertexing algorithms. Tracks that belong to vertices that are far in time populate the low \(\langle\rho\rangle(t_{HS})\) bins. The contamination from such spurious tracks will have a larger impact on the average vertex time compared to what happens at high \(\langle\rho\rangle(t_{HS})\) where the tracks belong to vertices close by in time, therefore not impacting the average vertex time as much as they do in the low \(\langle\rho\rangle(t_{HS})\) regime.

## References

* [1] ATLAS Collaboration, _Expected performance of the ATLAS detector under different High-Luminosity LHC conditions_, ATL-PHYS-PUB-2021-023, 2021, url: [https://cds.cern.ch/record/2765851](https://cds.cern.ch/record/2765851) (cit. on p. 2).
* [2] ATLAS Collaboration, _The ATLAS Experiment at the CERN Large Hadron Collider_, JINST **3** (2008) S08003 (cit. on p. 2).
* [3] CMS Collaboration, _The CMS Experiment at the CERN LHC_, JINST **3** (2008) S08004 (cit. on p. 2).
* [4] D. Berry et al., _4-Dimensional Trackers_, 2022, arXiv: 2203.13900 [physics.ins-det] (cit. on p. 2).
* [5] ATLAS Collaboration, _A High-Granularity Timing Detector for the ATLAS Phase-II Upgrade: Technical Design Report_, ATLAS-TDR-031; CERN-LHCC-2020-007, 2020, url: [https://cds.cern.ch/record/2719855](https://cds.cern.ch/record/2719855) (cit. on pp. 2, 19, 20).
* [6] ATLAS Collaboration, _Tagging and suppression of pile-up jets in the forward region using timing information with the ATLAS detector at \(\sqrt{s}=14\) TeV at HL-LHC_, ATL-HGTD-PUB-2022-001, 2022, url: [https://cds.cern.ch/record/2834329](https://cds.cern.ch/record/2834329) (cit. on pp. 2, 19, 20).
* [7] CMS Collaboration, _A MIP Timing Detector for the CMS Phase-2 Upgrade_, tech. rep., CERN, 2019, url: [https://cds.cern.ch/record/2667167](https://cds.cern.ch/record/2667167) (cit. on p. 2).
* [8] ATLAS Collaboration, _ATLAS Inner Tracker Pixel Detector: Technical Design Report_, ATLAS-TDR-030; CERN-LHCC-2017-021, 2017, url: [https://cds.cern.ch/record/2285585](https://cds.cern.ch/record/2285585) (cit. on p. 3).
* [9] ATLAS Collaboration, _ATLAS Inner Tracker Strip Detector: Technical Design Report_, ATLAS-TDR-025; CERN-LHCC-2017-005, 2017, url: [https://cds.cern.ch/record/2257755](https://cds.cern.ch/record/2257755) (cit. on p. 3).
* [10] ATLAS Collaboration, _Expected tracking and related performance with the updated ATLAS Inner Tracker layout at the High-Luminosity LHC_, ATL-PHYS-PUB-2021-024, 2021, url: [https://cds.cern.ch/record/2776651](https://cds.cern.ch/record/2776651) (cit. on p. 3).
* [11] D. Braga et al., 'Electronics for Fast Timing', _Snowmass 2021_, 2022, arXiv: 2204.00149 [physics.ins-det] (cit. on p. 3).
* [12] ATLAS Collaboration, _The ATLAS Simulation Infrastructure_, Eur. Phys. J. C **70** (2010) 823, arXiv: 1005.4568 [physics.ins-det] (cit. on p. 3).
* a simulation toolkit_, Nucl. Instrum. Meth. A **506** (2003) 250 (cit. on p. 3).
* [14] P. Nason, _A new method for combining NLO QCD with shower Monte Carlo algorithms_, JHEP **11** (2004) 040, arXiv: hep-ph/0409146 (cit. on p. 3).
* [15] S. Frixione, P. Nason and C. Oleari, _Matching NLO QCD computations with parton shower simulations: the POWHEG method_, JHEP **11** (2007) 070, arXiv: 0709.2092 [hep-ph] (cit. on p. 3).
* [16] S. Alioli, P. Nason, C. Oleari and E. Re, _A general framework for implementing NLO calculations in shower Monte Carlo programs: the POWHEG BOX_, JHEP **06** (2010) 043, arXiv: 1002.2581 [hep-ph] (cit. on p. 3).