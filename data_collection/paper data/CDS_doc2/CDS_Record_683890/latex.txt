**Views on the Status and Future of ATLAS Computing**

_Jurgen Knobloch - ATLAS Computing Coordinator Geneva, 7 January 1999 This document is available at [http://nicewww.cern.ch/atlas/CCviews.doc](http://nicewww.cern.ch/atlas/CCviews.doc) or as HTML: at [http://nicewww.cern.ch/atlas/CCviews.html_](http://nicewww.cern.ch/atlas/CCviews.html_)

The basic strategy for ATLAS computing is defined in the ATLAS Computing Technical Proposal (CTP) (CERN/LHCC/96-43) [1]. The main directions have not changed since the agreement within ATLAS on the CTP in December 1996.

A number of tasks planned in the CTP could not yet be fulfilled due to lack of resources. Other directions have required slight adjustments. But the main strategy remains unchanged and the principal goals set for the past two years have been achieved. In the following, I shall summarise the achievements, the adjustments, and the tasks still to be achieved as well as the tasks that had not originally been planned.

#### CTP - achievements and changes

_Chapter 3.4: Technology trends_

More than two years after the original extrapolations, today's technology has well confirmed the predicted trends. It is becoming more and more certain that the aggregate compute power can be achieved at the anticipated price. As an example, CERN pays now less than 20 CHF/CERNunit (or 200 CHF/SPECint95) for simulation farms - this is only a factor 2.5 away from the estimation for 2005. It is much less clear whether the large number of required processors can be managed with the required reliability. Joint studies between the LHC experiments on processor farms have been started recently as an LCB project in the event filter domain [2]. The results of these studies will also be relevant for reconstruction, simulation and analysis farms.

The prices for disk and tape storage follow the anticipated trends. The networking infrastructure has made a major step forward in Europe with the advent of the Trans-European Research Network (TEN-155) project providing 155 Mbps connectivity between the major centres [3]. The transatlantic bandwidth follows a similar though slower evolution, in particular concerning the price/performance ratio.

_Chapter 3.6: Tools for remote communication and collaboration_

Videconferencing has become a routine tool for communication within the software community. The weekly software meetings, tutorials and now also the Domain Interface Group (DIG) meetings are transmitted on a regular basis. Desktop videoconferencing has also proven quite useful for ad-hoc working discussions. The separation between the world of CODEC and MBONE videoconferencing has been overcome with the advent of CODEC systems on PCs where both systems can be running on the same PC serving as a bridge between both systems [4]. It is expected that collaborative tools will play a major role in the near future, in particular in the domains of remote training, training "just in time", and remote help desk.

_Chapters 3.7 & 3.8: System Architecture and Evaluation_

I discuss in a later chapter the question of Regional Centres. On the question of system architecture and the evaluation of various analysis models, a project, MONARC [5] has recently been approved by the LCB. First results from these studies, which have a strong participation from ATLAS, are expected by the end of 1999.

_Chapter 3.10: Milestones on the Computing Model_

**End 1997: Proof of feasibility of the federated OO-database concept**

The proof of feasibility was already in the CTP recognised as an ongoing task. By end 1997 and also now, we have no reason to question the proposed strategy [6]. In the framework of the MONARC project, a number of references of recent work on the topic have been collected [7].

_End 1998 Provide ~1byte working prototype of database_

The milestone has been achieved recently [8]. A full report is expected soon.

_Mid 1998: Test of prototype regional centre model_

_End 1998: Decision on role, size, number and location of regional centres_

Both milestones have been missed. A discussion on the question of Regional Centres follows in a later chapter.

_Chapter 4.2: Existing software_

The current production software of ATLAS is mostly written in FORTRAN 77 with a few C and C++ modules. The optimisation and the understanding of the ATLAS detector required considerable further development since end 1996. Last changes in the ATLAS detector needed to be implemented into the simulation software. Reconstruction algorithms were adapted to these changes and optimised to obtain the best performance from the ATLAS detector. This work is now essentially terminated and will led to the ATLAS physics performance document in April 99. The physics results obtained from these studies will serve as benchmarks for any future developments.

_Chapter 4.3: Object-oriented software_

During 1997 and 1998 major developments of ATLAS OO software have been achieved. Currently, the ATLAS repository contains almost 200,000 lines of C++ code and ATLAS people contributed in a major way to the 600,000 lines of code of GEANT4.

In particular, I would like to mention the following items:

* Framework. The ARVE framework has been "ramped-up" making it fully ASP-compliant. It contains now an interface to GEANT-3 data for the development of object-oriented algorithms. The work on the framework takes place within the "Control Domain". A component model has been developed which will be tried out shortly in the ARVE environment.
* Track-finding software. The pattern-recognition software packages IPATREC and XKALMAN are now mostly coded in C++. They are currently still used in the ATRECON framework. A port to the OO framework ARVE is planned for the first half of 1999. A common clustering package for Silicon detector data has been developed.
* Muon reconstruction software. An OO design and implementation has been developed for the DATCHA test set-up. It is being generalised for the general Muon spectrometer.
* Graphics software. The graphics software has been designed from ground up. It is ready to produce event displays for any data shortly after they become available in ARVE.
* Event. The structure for transient event data has been defined and coded for ID data and for Muon data. A persistent scheme has been implemented and is being tried out in the context of the 1 TB milestone.
* Simulation. The ATLAS simulation program CHAOS using the GEANT4 toolset [9] is being developed. An overall design exists, ARVE will be used as framework. Initial prototype implementations of test-beam set-ups will allow a comparison between GEANT3, GEANT4, and experimental data by the middle of 1999.
* Magnetic field and tracking in the field. An OO design and the corresponding code exists. The design has been successfully reviewed.
* Trigger simulation. OO work has started.

_Chapter 4.4. The ATLAS software process_

The ATLAS software process (ASP) as defined at the time of the CTP has undergone a number of changes. The major ones are:

* Definition of a ramp-up procedure. This allows software that was produced outside the ASP to be brought up to full ASP compliance.
* A procedure to evaluate and accept external software has been defined.
* "Domain Interface Group Chairman" has replaced the role of "Chief Architect".
* For the domains, we have introduced in addition to the "Domain Architect" the role of a "Domain Manager". In certain cases the same person may take both roles.
* The mandate of people in the DIG is now limited to two years, renewable.
* It is now allowed to do simultaneously a review of code and design.
* The documentation has been streamlined and made more readable. [10]

We are just completing a first round of the various reviews and start to learn first lessons, which will lead to a number of improvements of the ASP. Any improvements will aim at reducing the overhead imposed while still maintaining the main reason for the ASP - software quality and long-term maintainability.

_Chapter 4.5 The ATLAS software development environment_

The ASP is supported and supplemented by a number of tools forming the development environment.

* Configuration management. We have developed the Software Release Tool (SRT) [11] from an original BaBar tool. SRT is based on a software repository in cvs. All ATLAS offline software was moved to this repository. At the last LCB meeting, it has been decided to migrate towards a common SRT between the LHC experiments (and Fermilab). This new product will be maintained jointly by CERN and Fermilab. ATLAS specific documentation on how to use the repository and how to browse its contents from the web is available [12]. We have implemented these tools in order to satisfy an urgent and immediate need. It is quite obvious that the question of configuration management is far more involved and we plan to issue a "Configuration Management Plan" before summer 99.
* Verification of coding rules. Already at the time of the CTP, we had defined a number of C\(++\) coding rules in order to avoid the most common mistakes and to improve the software quality. We have now coded most of these rules into an automatic verification tool "CodeCheck" [13]. This work is also being shared with other LHC experiments.

### Chapter 4.6: Strategy for change

Out of the different possibilities to change from a procedural FORTRAN environment to an object-oriented C\(++\) environment, we consider the following:

For reconstruction and simulation, we start from the same framework and from the same generic detector description database.

The simulation program will be developed from scratch based on the GEANT4 tool-set.

For the reconstruction program, initial input data will be provided from GEANT3 data converted to transient objects. Later the same transient object will be derived from GEANT4 generated events. The initial algorithms in the reconstruction program will be the XKALMAN and IPATREC packages. For the Muon software a choice will have to be made between a generalised DATCHA reconstruction program and an initially OO-wrapped version of the MUONBOX package. Those elements that do not comply with the ATLAS software standards will have to be redesigned.

Test-beam simulation and reconstruction software will be ideal to try out these ideas in prototype implementations.

The domains for ATLAS software components have been defined and their interactions and interfaces are discussed in the Domain Interface Group (DIG)[14].

See also the chapters "Project management" and "Overall architecture" below.

### Chapter 4.7: Training

Training is an important element of the strategy for change. We have foreseen in the CTP to establish a training plan. This was not yet done because of other obligations of the training coordinator. This task must now be given highest priority. There are a number of key issues for training on ATLAS software:

Content of the courses:

Basic software engineering

Methods (OMT, UML)

Tools - SDE (StP, Rose, SRT, cvs, configuration management, testing, Objectivity)

Languages (C\(++\),...)

HEP components (GEANT 4, LHC\(++\),...)

ATLAS components (ASP, Packages,...)

The training must be organised in a distributed manner in order to reach the relevant ATLAS

population. This distributed training can be achieved by videoconferences, recording of tutorials, or by spreading the knowledge of people who have followed centrally organised courses.

The next steps are:* Identify an ATLAS training coordinator.
* Establish a training plan in collaboration with the CERN Education Services who have expressed interest to collaborate.
* Implement the training plan

_Chapter 4.8: Planning_

A detailed list of milestones has been established for the next 12 months [15]. A coarser list of milestones exists for the rest of the development cycle. The milestones will be updated on a regular basis and will be followed up by the general ATLAS procedures.

_Chapter 4.9: Costs_

Most of the software cost issues discussed in the CTP have been solved by the decision of the CERN management to cover the software costs for all LHC\(++\) components (Objectivity, Explorer, OpenInventor, OpenGL, STL) from a central CERN budget.

_Regional Centres_

At the time of writing of the CTP, the question of Regional Centres was still left open. In the meantime it has become clear that a fully centralised model must be excluded for a number of reasons. Firstly, funding at CERN does not allow for the full required infrastructure described in the CTP. Secondly, trans-continental networking bandwidth will always be more elusive than the one within countries or regions. Thirdly, the CERN rules require that 2/3 of the computing of experiments be done outside of CERN.

It is still assumed that the first pass reconstruction be done at CERN. Furthermore, one copy of all raw data and of the ESD and AOD will be held at CERN.

Regional centres will take up the task of participating in Monte Carlo productions and serve their community to perform most of the analysis tasks.

A typical regional centre serving e.g. 20% of the ATLAS community will have approximately the following characteristics:

* 3x10\({}^{4}\) SPECint95 (1.2x10\({}^{6}\) MIPS) CPU power for analysis
* 1-2x10\({}^{4}\) SPECint95 (0.4-0.8x10\({}^{6}\) MIPS) CPU power for simulation
* 200 Tbyte (per year of running) of managed storage
* 50-100 Tbyte (per year of running) of disk storage
* 1 Mbps networking bandwidth per simultaneous user
* 100 Mbps effective networking bandwidth to CERN if the transfer of ESD and AOD is done via network (preferred solution)

The regional centre must provide the manpower to install, maintain and run the infrastructure as well as personnel to maintain a local copy of the required ATLAS and external software.

_Critical items_

**Project management**ATLAS Computing has not yet been defined as a formal project and consequently, there is not a project leader, but a coordinator. This fact has led also to rather undefined reporting lines. I had made in 1995 [16] a proposal for a hierarchical computing organisation where computing coordinators of the systems (Inner Detector, Liquid Argon, Tile, Muon, Trigger Simulation) play a central role. With the existing domains of the systems and the domain managers (coordinators), I feel that we are now in a position to take this idea up again in order to formalise and clarify the reporting lines. A computing coordinator of a system must be aware of all software activities within that system and see that these activities serve a common goal. He or she will also be responsible for defining and following the milestones relevant to the system.

We must also put in place a management system for software elements, similar to the one serving the management of work-packages for detector production. Work has started on this item.

### Overall architecture

For the sake of easier discussion with the community, a first version of an overall design of the ATLAS offline software has been presented in the form of data-flow diagrams for the overall design and for the design of the reconstruction program [17]. The design of the simulation program has been presented in the form of a Class-Category Diagram [18]. Proposals of the design of the "Event" (Raw Data, ESD,AOD) have been made as well as a global design of a "Detector Description" [19]. In addition, we will implement a "Component Network" providing a secure connection and communication path between different components [20]. All these elements of the overall design will have to be unified. We plan to present this unified overall design in the status report to be submitted to the LHCC by the end of 1999.

### Commercial components

We rely in our overall strategy to a large extent on commercial software components. This view is shared with other experiments (e.g. CMS, LHCb, BaBar, CHORUS, and to some extent ALICE). A common LCB project, LHC\(++\)[21], has been approved to identify the relevant commercial components, to make them available to the collaborations on the required computing platforms, provide expert support for these components, and to provide the necessary small amount of "glue"-software in order the have the components seamlessly work together. LHC\(++\) also provides the common HEP specific components that make up together with the commercial components the equivalent of the CERNLIB for the OO era. The strategy relies on standard interfaces between the components, allowing the replacement of individual ones as need arises.

### Analysis tools

The question of analysis tools has created quite some emotion within the ATLAS community because there are several options available, each having their own proponents. We have now a clear plan of how to address this question. In this rapidly evolving field, it is far too early to decide now which solution to use in 2005. We will collect requirements for analysis tools for the interim period in the next few months. This will then lead to a proposal which one or several tools to use for this interim period. In the ATLAS graphics domain, it has been shown that several histogram packages (such as JAS, HepExplorer, PAW, and - to some extent - ROOT) can be used rather interchangeably. Further candidates are HippoDraw and Orca. At a recent LCB meeting, we have requested to establish a contingency plan for the Explorer component of LHC\(++\).

The ATLAS offline software project will require some 600-800 person-years of effort until the start-up of LHC. Studies have shown that this effort is available within the collaboration, however in a very distributed way [21]. This distributed software development requires a high degree of coordination and support which has to be provided by a central team. It is furthermore critical to take the right basic design decisions early on in the project. Currently, the central team at CERN is not sufficiently staffed. Individuals had to take up several important roles. In case of their absence, certain activities that are essential for the rest of the collaboration will stop. We need to find backup people for important roles such as the software librarian. A key person who has designed the component model that will be very central to the software system is on a short-term contract due to end this summer. We have to create a more long-term solution for a software engineer and OO-designer who can take up this task.

The systems will have to identify people who can take the roles of domain managers and domain architects as well as developers within the domains.

For the overall support, we have to identify rapidly a training manager.

_References:_

[1] ATLAS Computing Technical Proposal, CERN/LHCC 96-43, 19 December 1996

[http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/TDR/html/TDR-1.html](http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/TDR/html/TDR-1.html)

[2] LCB project on Event Filter Farms - LCB_005 [http://wwwinfo.cern.ch/pdp/pc/EF/](http://wwwinfo.cern.ch/pdp/pc/EF/)

[3] The Next Generation of European Research Networking

[http://www.dante.net/ten-155.html](http://www.dante.net/ten-155.html)

[4] LCB project on Videoconferencing - LCB_002 [http://vrvs.cern.ch/Proj/](http://vrvs.cern.ch/Proj/)

[5] MONARC, Model Of Networked Analysis at Regional Centres, LCB_003

[http://www.bo.infn.it/monarc/](http://www.bo.infn.it/monarc/)

[6] RD45, [http://wwwinfo.cern.ch/asd/rd45/index.html](http://wwwinfo.cern.ch/asd/rd45/index.html)

[7] Links on studies with Objectivity [http://www.bo.infn.it/monarc/references.html](http://www.bo.infn.it/monarc/references.html)

[8] Mail by RD Schaffer on the completion of the 1TB milestone

[http://atlasinfo.cern.ch/Atlas/private/mail/model/9901/msg00000.html](http://atlasinfo.cern.ch/Atlas/private/mail/model/9901/msg00000.html)

(the link requires the usual access - USER:atlas, PASSWORD:insider)

[9] GEANT4 - Object-oriented simulation toolkit

[http://wwwinfo.cern.ch/asd/geant/geant4.html](http://wwwinfo.cern.ch/asd/geant/geant4.html)