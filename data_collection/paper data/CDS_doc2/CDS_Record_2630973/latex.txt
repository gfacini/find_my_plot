Performance of mass-decorrelated jet substructure observables for hadronic two-body decay tagging in ATLAS

The ATLAS Collaboration

###### Abstract

Mass-decorrelated jet substructure observables have the potential to increase the sensitivity of searches for new physics in final states with high-\(p_{\mathrm{T}}\) hadronically decaying resonances, both by minimising the sculpting of background jet mass distributions and enabling more robust background estimation. A broad study of different analytical and multivariate mass-decorrelation techniques is performed in Monte Carlo simulation: designed decorrelated taggers (DDT), fixed-efficiency \(k\)-NN regression, convolved substructure (CSS), adversarially trained neural networks (ANN), and adaptive boosting for uniform efficiency (uBoost) are studied and compared. Performance is evaluated using metrics for background rejection and mass-decorrelation.

ATLAS

ATLAS

ATLAS

## 1 Introduction

Searches for physics beyond the Standard Model in proton-proton (\(pp\)) collisions at the Large Hadron Collider (LHC) [1] often rely on the identification of hadronically decaying resonances, reconstructed as jets in the calorimeters. At the energy frontier, such resonances can be produced with substantial transverse momentum (\(p_{\mathrm{T}}\)), resulting in collimated decay products. In this boosted region of phase space, the end result of a hadronic decay is reconstructed as a single, large-radius jet. With a vast background of non-resonant multijet production, the task of identifying jets originating from the decay of high-\(p_{\mathrm{T}}\) hadronic resonances becomes crucial to searches for new physics and measurements at the energy frontier.

The identification of resonance decays to jets relies on jet substructure, which is generally derived from the correlations between jet constituents. These correlations reflect differences in the radiation patterns for jets produced through resonant and non-resonant decays, and can therefore be used to distinguish the two. A substantial number of jet substructure variables have been proposed based on theoretical considerations, many of which have been used for jet classification in ATLAS [2; 3; 4; 5; 6]. Furthermore, it has been shown that improvements in the identification of e.g. hadronically decaying \(W\) bosons versus light quark- and gluon-initiated multijets can be achieved through a combination of several jet substructure variables using multivariate analysis (MVA) approaches, in particular using boosted decision trees (BDT) and neural networks (NN) [7; 8].

MVA classifiers trained using jet substructure variables to identify hadronically decaying resonances exhibit non-linear correlations with the reconstructed large-\(R\) jet mass, since the classifiers learn that the mass is a powerful feature for discriminating against the non-resonant background. Therefore, fixed-value cuts on such MVA classifier observables tend to distort the shape of the large-\(R\) jet mass distribution for non-resonant jet production, making it resemble the resonance jet mass distribution. This sculpting also depopulates the side-band regions in the large-\(R\) jet mass around the resonance pole mass, complicating the background estimation as such estimations are often performed using a fit to the side-bands. Sculpting can also occur when cutting on single-variable jet substructure taggers, but to a lesser extent since these are generally less dependent on the jet mass than the MVA classifiers. This has limited impact for simple tagging of e.g. hadronically decaying \(W\) bosons, \(Z\) bosons, or top quarks, where the pole mass is known. However, it renders standard MVA jet classifiers less useful to searches for new hadronically-decaying particles which are contained within a jet, where the pole masses of hypothesised resonances are not known _a priori_. Specifically, this is the case for searches for hadronically decaying resonances with masses below \(500\GeV\), where jet trigger thresholds typically impede dijet-type searches [9]. In particular, this issue has been explored in the context of resonance searches in the large-\(R\) jet mass spectrum in the range \(50\GeV\lesssim m\lesssim 300\GeV\)[10; 11; 12; 13], where the low mass of the resonance relative to its \(p_{\mathrm{T}}\) leads to boosted final states.

The aim of this study is to assess various techniques for decorrelating jet substructure classifiers from the reconstructed jet mass: analytical, single-variable taggers as well as MVA-based ones. In particular, designed decorrelated taggers (DDT) [14], fixed-efficiency \(k\)-nearest neighbours (\(k\)-NN) regression [15], and convolved substructure (CSS) [16] provide analytical methods for decorrelating a single jet substructure observable from the jet mass. Similarly, the use of adversarial training of neural networks (ANN) [17] and adaptive boosting for uniform efficiency (uBoost) [18] have been proposed as ways to leverage the classification power of MVA taggers while reducing their correlation with the jet mass through specialised training methods. For concreteness, the identification of jets from the hadronic decay of \(W\) bosons versus multijets is studied, where the \(W\) boson is used as a typical example of a resonance decaying to two quarks.

However, the results should hold for other resonance masses as a direct result of the mass-decorrelation, and because similar mass-decorrelation procedures can be applied for any resonance mass hypothesis.

After a brief description of the ATLAS detector in Section 2, the Monte Carlo (MC) datasets used for the study are described in Section 3. The event selection applied to the simulated jets is described in Section 4. In Section 5, the metrics chosen to quantify tagger classification power and mass-decorrelation are detailed. Section 6 describes the different decorrelation techniques considered in this study. Finally, results for the various techniques are presented in Section 7, evaluated according to multiple relevant metrics.

## 2 The ATLAS detector

The ATLAS detector [19] at the LHC covers nearly the entire solid angle around the collision point. It consists of an inner tracking detector surrounded by a thin superconducting solenoid, electromagnetic and hadronic calorimeters, and a muon spectrometer incorporating three large superconducting toroidal magnets.

The inner-detector system (ID), providing charged particle tracking, is immersed in a 2 T axial magnetic field and consists of the high-granularity silicon pixel detector closest to the interaction point followed by the silicon microstrip tracker (SCT). These silicon detectors, covering the range \(|\eta|<2.5\), are complemented by the transition radiation tracker (TRT), which enables radially extended track reconstruction up to \(|\eta|=2.0\).

For this study, the most important subsystem is the calorimetry, which covers the pseudorapidity range \(|\eta|<4.9\). Within the region \(|\eta|<3.2\), electromagnetic calorimetry is provided by barrel and endcap high-granularity lead/liquid-argon (LAr) electromagnetic calorimeters, with an additional thin LAr presampler covering \(|\eta|<1.8\) to correct for energy loss in material upstream of the calorimeters. Hadronic calorimetry is provided by the steel/scintillating-tile calorimeter, segmented into three barrel structures within \(|\eta|<1.7\), and two copper/LAr hadronic endcap calorimeters. The solid angle coverage is completed with copper/LAr and tungsten/LAr calorimeter modules optimised for electromagnetic and hadronic measurements respectively.

## 3 Samples

The study of different techniques for constructing mass-decorrelated jet substructure taggers is performed using two Monte Carlo (MC) samples. The background process is taken to be multijet production, which simulates the non-resonant production of jets predominantly originating from gluons and light-flavour quarks. This sample is simulated using the Pythia8 (v8.186) [20] generator with the NNPDF2.3LO [21] parton distribution function (PDF) set and the A14 tune [22]. In order to generate a large sample of hadronically decaying, high-\(p_{\mathrm{T}}\)\(W\) bosons, the signal process is taken to be the decay of a high-mass resonance \(W^{\prime}\) to vector bosons, in turn decaying hadronically: \(W^{\prime}\to WZ\to qqqq\). Signal events are generated using Pythia8 with the NNPDF2.3LO PDF set and the A14 tune, for a range of values of the \(W^{\prime}\) mass, ranging from 400 GeV to 5 TeV, to ensure sufficient population of the full region between 200 GeV and 2 TeV in \(W\) jet \(p_{\mathrm{T}}\). The simulated signal sample is re-weighted according to the generator-level jet to ensure a similar distribution to that of the background sample, in order to remove the dependence on the choice of signal model used to generate the sample of \(W\) jets.

The response of the detector to the simulated particles is modelled with a detailed simulation of the ATLAS detector [23] based on Geant4[24]. The MC samples are overlaid with additional simulated \(pp\) collisions (pile-up) generated using Pythia8 with the A2 tune [25] and MSTW2008LO PDF set [26] to roughly match the 2015 and 2016 data pile-up conditions with a mean number of 24 collisions per bunch crossing. The simulated events are processed using the same reconstruction algorithms and analysis chain as is used for real data.

## 4 Event and object selection

From the simulated MC samples, a baseline selection is imposed on all jets to ensure that they are well-reconstructed and representative of the initiating process, either the hadronic decay of a boosted \(W\) boson (signal) or non-resonant multijet production, predominantly initiated by gluons or light-flavour quarks (background).

### Jet reconstruction

The hadronic decay products from the boosted \(W\) boson or the non-resonant QCD radiation from multijet processes manifest as energy depositions in the cells of the ATLAS calorimeters. Noise-suppressed calorimeter cells are topologically grouped to form clusters, which are calibrated using the local cell weighting (LCW) procedure [27]. Jets are reconstructed from these clusters using the anti-\(k_{t}\) algorithm [28] as implemented in FastJet [29] with a distance parameter of \(R=1.0\). To remove the contribution of soft radiation, either from the underlying event or from pile-up, and to improve the jet mass resolution, the reconstructed jets are groomed using the jet trimming algorithm [30] with parameters \(R_{\rm sub}=0.2\) and \(f_{\rm cut}=5\%\). The trimming procedure yields a subset of constituent topological clusters from which substructure moments, including the calorimeter jet mass, are computed. Finally, the jets are calibrated in a two-step procedure that first corrects the jet energy scale and then the jet mass scale [31, 32].

In addition to energy deposits in the calorimeter, the charged jet constituents leave tracks in the inner detector. The track-assisted jet mass \(m_{\rm TA}\)[32] can be computed as \(m_{\rm TA}=\left(p_{\rm T}^{\rm calco}/p_{\rm T}^{\rm track}\right)\times m^{ \rm track}\), where \(p_{\rm T}^{\rm calco}\) is the transverse momentum of the groomed calorimeter jet and \(m^{\rm track}\) and \(p_{\rm T}^{\rm track}\) are the invariant mass and \(p_{\rm T}\), respectively, of the four-momentum sum of tracks associated with the jet. The resolution of the calorimeter jet mass degrades with jet \(p_{\rm T}\), and for \(W\) jets with \(p_{\rm T}\gtrsim 1\) TeV [32] the track-assisted jet mass has better resolution. From the two, a combined jet mass \(m_{\rm comb}\) is defined as the average weighted by the relative resolutions as a function of \(p_{\rm T}\). The combined mass improves the jet mass resolution across a large range of jet \(p_{\rm T}\), and is therefore used as the default jet mass variable throughout this study. For brevity, the symbol \(m\) is used to refer to the combined mass \(m_{\rm comb}\) in the following.

### Event selection

The initial selection retains only events with at least one reconstructed vertex formed from at least two reconstructed tracks in the inner detector.

In order to ensure a correct jet-by-jet labelling, a separate set of jets are reconstructed from the stable simulated particles with a lifetime \(\tau\) such that \(c\tau>10\) mm, excluding muons, neutrinos, and pile-up activity. These truth jets are reconstructed using the anti-\(k_{t}\) algorithm with \(R=1.0\) similar to the calorimeter jets, but without the application of any jet grooming algorithms. Calorimeter jets are paired with truth jets by a matching in \((\eta,\phi)\). To ensure a clean ensemble of reconstructed \(W\) jets, a three-step matching procedure identical to that in Refs. [7; 8] is performed, requiring the \(W\) and both quarks from the hadronic decay process \(q_{1,2}\) to be within \(\Delta R<0.75\) of the truth jet matching to a particular reconstructed jet. In each event, the two highest-\(p_{\mathrm{T}}\) jets are selected, provided they satisfy \(p_{\mathrm{T}}^{\mathrm{truth}}>200\) and \(|\eta^{\mathrm{truth}}|<2\), in order to ensure a realistic kinematic regime and containment within the central detector, respectively.

From the jets passing the truth-level selection, a subset is selected to focus on the kinematic regime relevant to the physics aims mentioned in Section 1; reconstructed jets are required to have a combined mass in the range \(m\in[50,300]\) GeV and a reconstructed transverse momentum in the range \(p_{\mathrm{T}}\in[200,2000]\) GeV. These kinematic bounds, motivated by physics applications, are chosen to simplify and concretise the mass-decorrelation task for some of the methods studied. Finally, all jets are required to have more than two constituent clusters (\(N_{\mathrm{const}}>2\)) in order to ensure that all substructure moments are well-defined. Across \(p_{\mathrm{T}}\), the number of multijets failing this requirement is \(<1\%\). Among \(W\) jets, \(<1\%\) of jets fail this requirement at \(p_{\mathrm{T}}=200\) GeV and approximately \(3\%\) at \(p_{\mathrm{T}}=2\) TeV. This selection defines the inclusive ensemble of jets based on which jet classification performance is evaluated throughout the study.

After the baseline selection, \(10^{6}\) signal and \(10^{6}\) background jets each are retained for training, ensuring balance between the classes. In addition, a separate set of \(10^{6}\) signal and \(10^{7}\) background jets are used for final performance evaluation, in order to ensure sufficient statistics for differential studies.

### Sample weights

For performance evaluation throughout the study, the multijets are weighted by cross-section, resulting in a physical, smoothly falling \(p_{\mathrm{T}}\)-spectrum. As mentioned in Section 3, the \(p_{\mathrm{T}}\) distribution for the \(W\) jets is reweighted to resemble that of the multijets. When training the MVA classifiers, the signal and background jets are separately reweighted to a uniform \(p_{\mathrm{T}}\) distribution, to ensure equal training attention to the entire \(p_{\mathrm{T}}\) range of \([200,2000]\) GeV, while ensuring physical differential distributions of all other variables for each \(p_{\mathrm{T}}\).

Throughout Section 6, the training dataset is used for all results unless explicitly noted otherwise. Conversely, in Section 7, only the evaluation dataset is used, ensuring that all results in the study are reported on data unseen by each classifier.

## 5 Evaluation metrics

As used in previous studies of MVA jet substructure taggers [7; 8], the classification power is a natural evaluation metric. However, in the development and study of mass-decorrelated jet taggers, the evaluation of classification performance must be complemented by a metric for quantifying the degree of mass-decorrelation. As classification and mass-decorrelation are generally opposing objectives, any combined figure of merit will be ambiguous and subject to a use case-dependent weighting of the two tasks. Below, the chosen metrics for classification and mass-decorrelation performance are described.

### Classification

Classification performance is measured by the relative selection efficiency of signal jets \(\varepsilon_{\text{sig}}^{\text{rel}}\) and the associated relative background rejection factor \(1/\varepsilon_{\text{bkg}}^{\text{rel}}\) for cuts on the classification variables. These measures are defined as:

\[\varepsilon_{\text{sig}}^{\text{rel}}=\frac{N_{\text{sig}}^{\text{tagged}}}{N_{ \text{sig}}^{\text{total}}},\qquad\frac{1}{\varepsilon_{\text{bkg}}^{\text{rel} }}=\frac{N_{\text{bkg}}^{\text{total}}}{N_{\text{bkg}}^{\text{tagged}}} \tag{1}\]

where \(N^{\text{total}}\) refers to the total number of events of a given class passing the baseline selection described in Section 4, and \(N^{\text{tagged}}\) refers to the number of events passing both the baseline selection and the tagging cut(s). The (relative) background rejection at \(\varepsilon_{\text{sig}}^{\text{rel}}=50\%\) is used as a summary metric to evaluate the classification performance, where \(50\%\) is chosen as a typical working point for jet tagging studies [7, 8] which is also indicative of the level of selection efficiency used by searches in this kinematic regime [11, 13].

### Mass-decorrelation

The linear correlations between the jet mass, substructure variables, and MVA jet classifier outputs were studied in Ref. [7]. However, a linear correlation coefficient has insufficient capacity to express the highly non-linear correlations observed between the jet mass and existing MVA jet taggers [7, 8]. Instead, a figure of merit which directly quantifies the sculpting of the background jet mass distribution, resulting from a cut on a jet tagger variable, is proposed.

The Kullback-Leibler divergence [33] for discrete probability distributions \(P,Q\) is defined as:

\[\text{KL}(P\parallel Q)=-\sum_{i}P_{i}\log_{n}\left(\frac{Q_{i}}{P_{i}}\right) \tag{2}\]

where \(i\) enumerates the discrete bins of the distributions. The Kullback-Leibler divergence measures the relative entropy of \(P\) with respect to \(Q\), and can therefore be used to quantify the similarity between discrete distributions.

To avoid divergences and to symmetrise the metric with respect to \(P\) and \(Q\), the Jensen-Shannon divergence [34] is used:

\[\text{JSD}(P\parallel Q)=\frac{1}{2}\big{(}\text{KL}(P\parallel M)+\text{KL} (Q\parallel M)\big{)},\qquad\text{with}\quad M=\frac{P+Q}{2} \tag{3}\]

In this study, the Jensen-Shannon divergence is used to measure the difference between the normalised mass distributions of the background jets passing and failing, respectively, a given jet tagger cut:

\[\text{JSD}\equiv\text{JSD}\left(N_{\text{bkg}}^{\text{pass}}(m)\,/\sum_{i}N_ {\text{bkg},i}^{\text{pass}}(m)\,\Big{\|}\,N_{\text{bkg}}^{\text{fail}}(m)\,/ \sum_{i}N_{\text{bkg},i}^{\text{fail}}(m)\right) \tag{4}\]

using bins in \(m\) between \(50\) and \(300\) in increments of \(5\). For brevity, this metric will be referred to simply as JSD for any given signal or background selection efficiency. Using a logarithm base \(n=2\) in Equation (2), JSD will be in the range \([0,1]\) with smaller values indicating a smaller relative entropy between the jet mass distributions and therefore a greater degree of mass-decorrelation; and _vice versa_. For summary performance evaluation, JSD or \(1/\text{JSD}\) at \(\varepsilon_{\text{sig}}^{\text{rel}}=50\%\) will be used to quantify the mass-decorrelation of a given jet classifier.

## 6 Analysis

A variety of approaches to constructing mass-decorrelated jet taggers, from simple linear transforms to complex neural network architectures, are detailed below.

### Designed decorrelated taggers

A simple approach to substructure decorrelation is provided by designed decorrelated taggers (DDT) [14]. The original method relies on the jet scaling variable \(\rho=\log\left(m^{2}/p_{\mathrm{T}}^{2}\right)\), where \(m\) is the mass of the jet, and \(p_{\mathrm{T}}\) is the transverse momentum. It is observed that profiling the substructure variable \(\tau_{21}\) as a function of \(\rho\) exposes a linear relationship, which can be exploited to perform a linear transform, removing the mean bias of \(\tau_{21}\) with respect to \(\rho\).

In practice [10; 13] it is found that the jet scaling variable

\[\rho^{\mathrm{DDT}}=\log\left(\frac{m^{2}}{p_{\mathrm{T}}\times\mu}\right) \tag{5}\]

more robustly removes residual dependence on the jet \(p_{\mathrm{T}}\), and therefore leads to better decorrelation across the jet kinematic phase space. Here, the parameter \(\mu\) balances the dimensions and is usually taken to be \(\mu=1\) GeV.

To perform the decorrelation, the mean value of \(\tau_{21}\) is plotted as a function of \(\rho^{\mathrm{DDT}}\). This is shown in Figure 1. A linear relationship between the two variables is observed, roughly in the range \(\rho^{\mathrm{DDT}}\in[1.5,4.0]\). Towards higher values, the linearity breaks down due to effects arising from the fixed-radius jet clustering algorithm used, while for trimmed jets it breaks down towards low values of \(\rho^{\mathrm{DDT}}\) around the Sudakov peak where soft QCD effects become important.

Figure 1: Mean values of \(\tau_{21}\) and \(\tau_{21}^{\mathrm{DDT}}\) as functions of \(\rho^{\mathrm{DDT}}\) for the multijet background. A linear fit to the \(\tau_{21}\)-profile, used in the definition of \(\tau_{21}^{\mathrm{DDT}}\), is performed on the indicated range.

A linear fit is performed to the \(\tau_{21}\) profile in the range \(\rho^{\rm DDT}\in[1.5,4.0]\). From this fit, the transform \(\tau_{21}\mapsto\tau_{21}^{\rm DDT}\) is defined as

\[\tau_{21}^{\rm DDT}=\tau_{21}-a\times(\rho^{\rm DDT}-1.5) \tag{6}\]

where \(a\) is the measured slope of the fit in Figure 1. Figure 1 shows how the DDT transformation removes the linear correlation of \(\tau_{21}\) with \(\rho^{\rm DDT}\). Consequently, since \(\rho^{\rm DDT}\) effectively encodes information about the kinematics of the jet (through \(m\) and \(p_{\rm T}\)), the DDT transform yields a jet substructure discriminant which is decorrelated from the jet mass. Jets with \(\rho^{\rm DDT}\notin[1.5,4.0]\) are kept and the transform in Equation (6) is applied to these as well.

### Fixed-efficiency regression

Whereas the DDT transform requires the existence of a linear relationship between a substructure variable and kinematic variable(s) in order to remove the mean bias, more general strategies do not have such requirements.

First, a fixed percentile of inclusive background efficiency \(\varepsilon_{\rm bkg}^{\rm rel}\) for the \(D_{2}\) distribution is computed for multijets in bins of \(\rho=\log(m^{2}/p_{\rm T}^{2})\) and \(p_{\rm T}\), resulting in a two-dimensional profile. The efficiency selected in this case is \(\varepsilon_{\rm bkg}^{\rm rel}=16\%\), corresponding roughly to a signal efficiency of \(\varepsilon_{\rm sig}^{\rm rel}=50\%\). Second, in order to determine its functional dependence, a two-dimensional regression fit using the \(k\)-nearest neighbours (\(k\)-NN) algorithm [15] is performed, yielding the distribution \(D_{2}^{(16\%)}(\rho,p_{\rm T})\). Finally, for each jet a new observable \(D_{2}\mapsto D_{2}^{k\text{-NN}}\) is constructed by subtracting the predicted values from \(D_{2}\):

\[D_{2}^{k\text{-NN}}=D_{2}-D_{2}^{(16\%)} \tag{7}\]

The profiles of the fixed-efficiency \(D_{2}\) cut as a function of \(\rho\) and \(p_{\rm T}\) are shown in Figure 2. The fixed-efficiency regression generalises the central concept behind DDT, making the method admissible to a more general class of substructure variables.

\(N_{2}\)[35] was initially studied as a potential base variable, in order to emulate the method used in Ref. [12]. However, Ref. [12] uses jets groomed with the soft drop method [36; 37] in contrast to the trimmed jets described in Section 4, which has significant consequences for the behaviour of the jet substructure moments computed for each jet collection. It was found that, while the distribution of \(N_{2}\) computed using soft drop-groomed jets is robust with respect to both mass and \(p_{\rm T}\), this is not the case for trimmed jets. \(D_{2}\)[38] was found to have a much more stable behaviour than \(N_{2}\) when used with trimmed jets, and thus it is used instead.

### Convolved substructure

The DDT constructed in Sec. 6.1 removes the average dependence of a hadronic two-body decay tagging observable \(x\) on the jet mass. This idea is extended in Ref. [16], by the introduction of convolved substructure (CSS), to remove the dependence on the entire shape by convolving the distribution of the substructure variable of interest with a shape function:

\[\frac{1}{\sigma}\frac{d\sigma}{dx}\mapsto\frac{1}{\sigma}\frac{d\sigma}{dx_{ \rm CSS}}=\frac{1}{\sigma}\frac{d\sigma}{dx}\otimes F_{\rm CSS}(x|\alpha, \Omega_{D}), \tag{8}\]where \(F_{\text{CSS}}(x|\alpha,\Omega_{D})\) is a Gamma distribution

\[F_{\text{CSS}}(x|\alpha,\Omega_{D})=\left(\frac{\alpha}{\Omega_{D}}\right)^{ \alpha}\frac{1}{\Gamma(\alpha)}x^{\alpha-1}e^{-\frac{\alpha x}{\Omega_{D}}}\,. \tag{9}\]

Equation (8) is defined only at the level of distributions; in practice, the convolution indicated by \(\otimes\) is estimated by transforming \(x\mapsto x_{\text{CSS}}=G^{-1}(C(x)|\alpha,\Omega_{D})\), where \(C(x)\) is the cumulative distribution function of \(x\) and \(G\) is the cumulative distribution function corresponding to \(\frac{1}{\sigma}\frac{d\sigma}{dx_{\text{CSS}}}\). The parameters \(\alpha\) and \(\Omega_{D}\) are optimised inclusively and per bin of jet mass, respectively. The mass binning is chosen such that the shape of the \(x\) distribution does not change significantly. For these studies, 25 evenly spaced mass bins were used between 50 and 300 \(\text{GeV}\). A simple two-step optimisation is performed to choose \(\alpha\) and \(\Omega_{D}\): first the optimal values of \(\Omega_{D}\) are evaluated in each mass bin for a fixed \(\alpha\); then, \(\alpha\) is modified and the procedure is repeated. Based on Ref. [16], \(\alpha\) is scanned between 0.5 and 3.0 in increments of 0.5. The optimal value is determined to be 1.5. In order to reduce fluctuations of the best-fit \(\Omega_{D}\) values found for the optimal \(\alpha\), the measurements of \(\Omega_{D}\) as a function of the jet mass for \(\alpha=1.5\) are fitted using the functional form proposed in Ref. [16, Eq. (3.8)].

The transformed \(D_{2}^{\text{CSS}}\) distribution in the highest \(p_{\text{T}}\) bin is shown in Figure 3. In order to mitigate the effect of limited statistics for the training dataset, a kernel-density estimation (KDE) based smoothing with a length scale of 0.15 is applied to all training distributions. For the CSS substructure variable, the smoothing is performed after the convolution. The convolution procedure from Equation (8) evolves the distribution of \(D_{2}^{\text{CSS}}\) at one mass to the distribution at a reference mass. Therefore, using the lowest mass bin as the reference, the \(D_{2}^{\text{CSS}}\) in this bin is approximately unchanged. In contrast, the distribution of \(D_{2}^{\text{CSS}}\) in the high mass bin visibly differs from \(D_{2}\) in the same mass bin. Not only has the mean value of \(D_{2}^{\text{CSS}}\) for the high mass shifted towards the low mass \(D_{2}^{\text{CSS}}\), but the entire distribution is now coherently closer. Using the CSS technique, the \(D_{2}\) distribution is transformed to be similar for all jet masses, thereby directly reducing correlation with the jet mass. Finally, transforming the entire distribution means that in

Figure 2: Profiles of the \(\varepsilon_{\text{bkg}}^{\text{rel}}=16\%\) profile of \(D_{2}\) for multijets, \(D_{2}^{(16\%)}(\rho,p_{\text{T}})\), (a) as measured in the training dataset and (b) as fitted using \(k\)-NN regression. Dashed lines indicate the phase space limits arising from the jet mass selection, cf. Section 4.

addition to decorrelating the average value of \(D_{2}\) from the jet mass, in principle CSS also decorrelates higher-order moments of the \(D_{2}\) distribution, leading to a more complete decorrelation.

### Adversarial neural networks

Neural networks have proved to be powerful MVA classifiers, capable of exploiting correlations between several high-level, physics motivated input variables to improve the classification of \(W\) jets, among other things. However, for the classification of jets produced by the decay of a resonance with a fixed pole-mass, the jet mass is a powerful discriminating feature. Therefore, neural network classifiers trained only on conventional jet substructure variables tend to learn the jet mass as a feature, even without having seen it during training [7]. This is possible due to the typically significant correlations of the jet mass with each of the input jet substructure variables.

#### Introduction

It was proposed in Ref. [39] to use adversarial training to make neural network classifiers independent of certain variables or parametrised systematic uncertainties. In particular, Ref. [40] proposes adversarial training in order to reduce the correlation with the jet mass. The classifier, taking \(N\) substructure variables as input and outputting a jet tagging variable in the range \([0,1]\), can be trained stand-alone to minimise the classification loss \(L_{\rm clf}\), which introduces the correlation with the jet mass. A second neural network called the adversary can be introduced, tasked with inferring e.g. the jet mass from the output of the classifier, minimising its own loss \(L_{\rm adv}\). If the adversary is able to infer the jet mass from the classifier output beyond random guessing, some generally non-linear correlation must exist between the two. The adversary loss \(L_{\rm adv}\) is evaluated only for multijets, since the mass-decorrelation metric in Eq. (4) is computed for the background jet mass distribution.

Figure 3: Distributions of \(D_{2}\) and \(D_{2}^{\rm CSS}\) for multijets in the highest mass bin, as well as the reference distribution in the lowest mass bin. A kernel-density estimation (KDE) based smoothing is applied to all training distributions.

The combined, adversarial neural network is trained by min-max optimisation of the joint objective:

\[\min_{\theta_{\text{clf}}}\max_{\theta_{\text{adv}}}L_{\text{clf}}(\theta_{\text{ clf}})-\lambda L_{\text{adv}}(\theta_{\text{clf}},\theta_{\text{adv}}) \tag{10}\]

balancing the classification task (first term) with the decorrelation task (second term). Here, \(\theta\) denotes the weights for each network. The inner optimisation (\(\max_{\theta_{\text{adv}}}\)) leads the adversary to minimise \(L_{\text{adv}}(\theta_{\text{clf}},\theta_{\text{adv}})\) with respect to the adversary network weights \(\theta_{\text{adv}}\). The outer optimisation (\(\min_{\theta_{\text{clf}}}\)) leads the classifier to minimise the effective loss \(L_{\text{classifier}}(\theta_{\text{clf}},\theta_{\text{adv}})=L_{\text{clf}}( \theta_{\text{clf}})-\lambda L_{\text{adv}}(\theta_{\text{clf}},\theta_{ \text{adv}})\) with respect to the classifier network weights \(\theta_{\text{clf}}\). The adversary loss \(L_{\text{adv}}\) depends on the weights of both networks, since the adversary's task is conditional on the classifier configuration.

The difference in sign between the two terms means that the classifier is trained to maximise \(L_{\text{adv}}\), making it harder for the adversary to infer the jet mass. The trade-off between the two competing objectives is controlled by the parameter \(\lambda\): For \(\lambda\to\infty\), the classifier is allowed only to retain information "orthogonal" to the jet mass; for \(\lambda\to 0\), the standard neural network classifier is recovered. Since it is otherwise unconstrained, \(\lambda\) can be considered an additional hyperparameter of the adversarial neural network, to be optimised according to some use case-specific figure of merit. Appendix A provides additional details on the implementation of the adversarial training, the training procedures for the stand-alone and adversarially trained classifiers, and the optimisation of the classifier hyperparameters. An overview is provided below.

## Features

Feature selection and feature engineering are essential to the performance of neural networks. This study focuses on jet substructure variables as highly engineered, physics motivated input features. The results of the feature selection in Ref. [8] are used in lieu of a full input feature optimisation due to the similarity of the problem and the datasets used. The ten jet substructure variables used in the training of the neural network-based jet classifier are listed in Table 1. In contrast to Ref. [8], the jet mass and \(p_{\text{T}}\) are not used as input features. Using only substructure moments as input features is intended to provide a more direct comparison with the analytical taggers described above, all of which are based on single substructure moment as their base observable. Although the training and testing datasets are reweighted to have identical \(p_{\text{T}}\) distributions for signal and background, including the jet \(p_{\text{T}}\) could allow MVA taggers to better and more directly utilise \(p_{\text{T}}\)-dependent information about the different substructure moments. Finally, although \(p_{\text{T}}\) is not used as an input to the classifier itself, it is used as part of the adversarial training, similar to some of the analytical mass-decorrelation methods.

To validate this selection of inputs features, a comparison in terms of classification was performed between a neural network classifier trained on the features in Table 1 and one trained on an expanded set of 20 substructure variables, including the nominal ten. No significant improvement in performance was found. Similarly, no improvements were found by performing a principal component analysis (PCA) [49] on either set of inputs.

## Configuration

The optimal architecture and learning configuration for both the classifier and adversary (collectively referred to as hyperparameters) are chosen using Bayesian optimisation with Spearmint[50]. The training is performed with the Adam[51] optimiser and the training batch size is fixed at 8192, found to balance high computational throughput and memory requirements. The classifier is optimised according to the classification loss \(L_{\text{clf}}\) for 50 epochs using 3-fold stratified cross-validation. The optimisation identifies a network with three hidden layers, each with 64 nodes with rectified linear unit (ReLU) activation, and a single output node with sigmoid activation as leading to the best classification power.

Mass-decorrelation and robustness with respect to the jet \(p_{\text{T}}\) of the adversarially trained classifier is achieved by having the adversary parametrise mixture density network [52], constructing a posterior probability density function (p.d.f.) in the jet mass \(m\), conditional on the auxiliary input \(\log p_{\text{T}}\), using a Gaussian mixture model (GMM). This weighted jet mass distribution plays the role of the adversary prior.

In contrast with the stand-alone classifier optimisation, the adversary cannot be meaningfully optimised separately, according to the second term in Equation (10). Therefore, to tune the adversarially trained neural network tagger according to expected performance, the optimisation is performed by maximising the metric \(1/\varepsilon_{\text{bkg}}^{\text{rel}}+\lambda/\text{JSD}\) computed at \(\varepsilon_{\text{sig}}^{\text{rel}}=50\%\) for a fixed value of \(\lambda\), chosen to be \(\lambda=10\) since this value is found to yield robust results for mass-decorrelation, as discussed in Section 7.3. Throughout the optimisation, a classifier pre-trained with the chosen stand-alone classifier hyperparameters is used.

The adversarial optimisation is performed for 200 epochs, following 10 epochs of adversary-only pre-training, using 3-fold stratified cross-validation. An adversary with a single hidden layer comprising 64 nodes with ReLU activation, parametrising a GMM posterior p.d.f. with 20 components is found to have sufficient capacity to perform the decorrelation. The combined adversarial neural network architecture is shown in Figure 4.

### Adaptive boosting for uniform efficiency

When used for jet classification, boosted decision tree (BDT) algorithms like AdaBoost (from "adaptive boosting") [53] are also found to yield selection efficiencies which are non-uniform with respect to the jet mass. The uBoost method [18] seeks to mitigate this non-uniformity by updating training weights, i.e. performing the adaptive boosting, based on both classification error and the uniformity of the classification with respect to the jet mass at a fixed selection efficiency. Jets with masses in regions where the selection efficiency is lower than the target efficiency are boosted to have larger training weights; and _vice

\begin{table}
\begin{tabular}{c l l} \hline \hline
**Variable** & \multicolumn{1}{c}{**Type**} & **Reference** \\ \hline \(C_{2}\), \(D_{2}\) & Energy correlation ratios & [38] \\ \(\tau_{21}\) & \(N\)-subjettiness & [41] \\ \(R_{2}^{\text{FW}}\) & Foxâ€“Wolfram moment & [42] \\ \(\mathcal{P}\) & Planar flow & [43] \\ \(a_{3}\) & Angularity & [44] \\ \(A\) & Aplanarity & [45] \\ \(Z_{\text{cut}}\), \(\sqrt{d_{12}}\) & Splitting scales & [46; 47] \\ \(KtDR\) & \(k_{t}\)-subjet \(\Delta R\) & [48] \\ \hline \hline \end{tabular}
\end{table}
Table 1: Substructure variables used for the neural network- and BDT-based MVA jet classifiers. Feature selection based on Ref. [8].

versa_. In this way, using adaptive training weights, uBoost balances classification power and a uniform selection efficiency in the mass observable during training.

The BDT classifiers use the substructure variables listed in Table 1, the same used for the neural network classifiers, as input features. The hyperparameter configuration adopted for AdaBoost is the same as the one used for the BDT classifier in Ref. [7]. For the remaining uBoost hyperparameters, the default values in Ref. [54] are used. Similar to the adversarial neural network classifier, the degree of mass-decorrelation for uBoost is controlled by a hyperparameter \(\alpha\), called the uniforming rate. For \(\alpha\to 0\), the adaptive boosting only takes classification loss into account, and the standard AdaBoost classifier is retrieved. Conversely, for larger \(\alpha\), the uniform efficiency boosting becomes gradually more important. For comparison with other taggers, a value of \(\alpha=0.3\) is chosen, since it leads to roughly the same level of background rejection as the ANN for the chosen default value of \(\lambda\) and for the chosen BDT configuration.

The AdaBoost and uBoost classification objectives during training are shown in Figure 5.

The AdaBoost classification loss is seen to decrease monotonically and reach a plateau for the testing dataset after 500 epochs of training. In contrast, the classification objective for uBoost initially decreases due to improved discriminating power, and then rebounds as the adaptive boosting for uniform efficiency takes effect. Using a fixed duration of 500 epochs for all BDT-based models yields a collection of consistently trained jet classifiers with varying degrees of mass-decorrelation. For these, the level of mass-decorrelation is given by the degree of divergence at the end of the fixed training duration, which in turn is controlled by rate at which the uniformity boosting takes effect, as controlled by \(\alpha\).

## 7 Results

The various mass-decorrelated jet taggers all result in a single discriminant variable which classifies jets as either \(W\) jets or non-resonant multijets while keeping the shape of the background jet mass distribution unchanged. As a representative example, the distributions of the NN and ANN tagger discriminants are shown in Figure 6. The NN tagger powerfully separates the two classes of jets. The same is true for ANN

Figure 4: Adversarial neural network architecture. The classifier network is tasked with predicting jet labels (\(y\)) based on jet substructure variable inputs (\(x\)), outputting a tagger variable (\(z\)). The adversary network is tasked with inferring the value(s) of the variables from which the classifier is to be decorrelated (\(d\); here the jet mass \(m\)), optionally aided by auxiliary features (\(a\); here \(\log p_{\mathrm{T}}\)), by parametrising a posterior p.d.f. as a Gaussian mixture model (GMM). The adversarial training is implemented using a gradient reversal layer, the trade-off between \(L_{\mathrm{clf}}\) and \(L_{\mathrm{adv}}\) controlled by the parameter \(\lambda\).

tagger, although the two distributions become less separated as a result of the mass-decorrelation. This behaviour is particularly prominent for the MVA taggers, as discussed in Section 7.1.

As detailed in Section 5, the performance of each \(W\) jet tagger is evaluated for both classification and mass-decorrelation. The physics tasks for which mass-decorrelated taggers are immediately useful are typically characterised by \(p_{\mathrm{T}}\) given by jet and photon trigger thresholds of around 500 and 200 GeV, respectively. Therefore, in order to investigate the performance of the various taggers in these different kinematic regimes, results are studied in two different \(p_{\mathrm{T}}\) bins: \([200,500]\) GeV and \([500,1000]\) GeV.

Figure 5: Classification objective during training of AdaBoost and uBoost classifiers for training and testing datasets for \(\alpha=0.3\).

Figure 6: Distributions of the jet tagger variables for the standard neural network classifier and the adversarially trained neural network classifier, for multijets and \(W\) jets.

### Classification

The classification performance, measured in terms of receiver operating characteristic (ROC) curves, is shown in Figure 7 for each tagger. This shows a deterioration in the discrimination power of the ANN with respect to the NN tagger.

For \(200\GeV<p_{\mathrm{T}}<500\GeV\) shown in Figure 7(a), among all taggers, the best discrimination power is achieved by the the MVA taggers, NN and AdaBoost, as measured using the background rejection at fixed signal efficiency at \(\et=50\%\). The two standard MVA taggers are seen to yield background rejections which are more than twice those of the standard, single-variable taggers. In this kinematic region, for a particular choice of decorrelation parameters (\(\lambda=10,\alpha=0.3\)), all mass-decorrelated taggers are seen to perform very similarly in terms of classification.

For \(500\GeV<p_{\mathrm{T}}<1000\GeV\) shown in Figure 7(b), the MVA taggers still perform considerably better than the single-variable taggers, by roughly a factor of 10 in terms of \(1/\et=\et=50\%\). However, considerable variation is observed between the mass-decorrelated taggers. Similar to the low-\(p_{\mathrm{T}}\) bin, the mass-decorrelated MVA taggers in Figure 7(b) report greater background rejection compared to the mass-decorrelated single-variable taggers, but only by a factor of roughly 2 for \(\et=50\%\). Among the analytical mass-decorrelation methods, \(k\)-NN is seen to provide the best classification; better than CSS, with both based on the \(D_{2}\) substructure variable.

For all single-variable taggers, the observed background rejection after mass-decorrelation is greater than for the original taggers when both the signal and background are assumed to follow the falling \(p_{\mathrm{T}}\) spectrum taken from multijet MC; an effect which is particularly true in the high-\(p_{\mathrm{T}}\) bin. The standard, single-variable taggers are seen to lose classification power with \(p_{\mathrm{T}}\), which is an expression of relative shifts of the distributions for multijets and \(W\) jets as a function of \(p_{\mathrm{T}}\). Since the DDT and fixed-efficiency \(k\)-NN regression methods both decorrelate the substructure variable not only from mass, but also from \(p_{\mathrm{T}}\) through \(\rho\) and \(\rho^{\mathrm{DDT}}\), the fact that these methods recover lost performance exactly by removing \(p_{\mathrm{T}}\)-dependence

Figure 7: Rejection of multijets (background) as a function of \(W\) jet (signal) selection efficiency, for standard and mass-decorrelated version of analytical and multivariate (MVA) jet taggers in two regions of jet \(p_{\mathrm{T}}\), without the addition of a jet mass-window cut.

is not surprising. In particular, the background rejections of \(\tau^{\text{DDT}}_{21}\) and \(D^{K\text{-NN}}_{2}\), respectively, are close to equal in the two \(p_{\text{T}}\) bins. By contrast, the CSS method does not use \(p_{\text{T}}\) directly in the decorrelation, and therefore experiences the smallest relative increase in classification power.

The same effect is also seen in the low-\(p_{\text{T}}\) bin, although to a much smaller extent. In the case of DDT, an instructive parallel is provided by the observation that the linear transform acts similarly to a Fisher discriminant, extracting additional information from \(\rho^{\text{DDT}}\) which slightly improves classification. Overall, it is important to emphasise that the relative background efficiencies in Figure 7 are computed with respect to jets with invariant masses in the range \(m\in[50,300]\GeV\). Therefore, the shifts of the substructure variable distributions leading to the observed improvement in classification power following mass-decorrelation for single-variable taggers are dominated by jets which are far from the \(W\) boson mass.

In contrast to the mass-decorrelated single-variable taggers, the mass-decorrelated MVA taggers exhibit a considerable decrease in classification performance relative to their standard variants. This, however, is expected since the standard variants are trained to provide equal attention to all \(p_{\text{T}}\). In addition, the greater initial degree of correlation with the jet mass observed for standard MVA taggers compared to standard single-variable taggers means that there is more potential for degradation of classification power. Finally, the deterioration of MVA tagger classification power due to the mass-decorrelation procedures is expected and acceptable to relevant analyses as part of a trade-off between the two.

### Mass-decorrelation

To study the mass-decorrelation of various taggers, the most direct measure is the inspection of the normalised multijet mass distribution before and after the application of the taggers. Such comparisons of jet mass distributions are shown in Figure 8.

Cutting on each of the standard taggers sculpts the background jet mass distribution, thereby introducing artificial structures not present in the original spectrum. Such sculpting may directly impact the sensitivity of physics searches by reducing the ability to constrain systematic uncertainties on e.g. multijet backgrounds. In particular, the standard MVA taggers sculpt the background jet mass distribution to resemble the \(W\) jet mass peak.

Each of the mass-decorrelation procedures serves to mitigate such sculpting. For the mass-decorrelated single-variable taggers, CSS and in particular \(k\)-NN regression are both seen to remove most sculpting effects. The DDT transform removes the sculpting in the lower jet mass region, while some disagreement persists at higher masses as a result of the limitations of the assumption of linearity underlying the method.

The mass-decorrelation of both MVA taggers is considerably improved following the application of their respective training methods for decorrelation. In particular, the ANN tagger yields a largely smooth, un-sculpted distribution of multijet events passing the cut. In contrast, the multijet distribution passing the cut on \(z^{(\alpha=0.3)}_{\text{uBoost}}\) retains some residual sculpting, particularly around \(m\approx 100\GeV\), as a consequence of persisting non-uniformity in selection efficiency.

Similar residual sculpting is not observed for the ANN tagger, which is evident from the local background efficiency as a function of the jet mass, for a range of inclusive background efficiencies, as shown in Figure 9 for the neural network taggers.

Figure 9(a) shows the sculpting of the multijet background around the \(W\) jet mass peak, and the mass-decorrelation effect is evident in Figure 9(b), where the background efficiency profiles are roughly uniform as a function of the jet mass. The deviation from uniformity in Figure 9(b), most evident between jet masses of 150 and 200 GeV, reflects the residual correlation with the jet mass, which may be reduced by more fine-tuned optimisation and longer adversarial training. For the analytical taggers, the local background efficiencies exhibit similar behaviour: the initial, non-uniform background efficiency profiles are transformed to be more constant as a function of the jet mass as a result of the decorrelation procedures.

As the quantitative summary metric for the mass-decorrelation, Figure 10 shows the JSD as a function of the background efficiency at which the cut is performed. As expected, the JSD values for the mass-decorrelated taggers are consistently and considerably lower -- i.e. have lower degrees of correlation with the jet mass -- than the standard ones.

Fixed-efficiency \(k\)-NN regression is seen to lead to the greatest degree of mass-decorrelation, especially in the vicinity of the background efficiency percentile at which the regression is performed (16%). The other methods exhibit more uniform mass-decorrelation across \(\varepsilon_{\text{bkg}}^{\text{rel}}\). Among the MVA taggers, the ANN performs considerably better than uBoost for the chosen values of \(\lambda\) and \(\alpha\), for the chosen metric.

Figure 8: Normalised jet mass distribution for inclusive multijets before cuts, compared to the same distributions after cuts on the studied jet discriminants. Also shown for reference is the jet mass distribution for \(W\) jets before tagging. Cuts are chosen to correspond to a \(W\) jet (signal) selection efficiency of \(\varepsilon_{\text{sig}}^{\text{rel}}=50\%\).

The fact that the JSD is computed from histograms with finite statistics imposes a lower bound for the mass-decorrelation given the chosen testing dataset. This statistical limit on JSD can be estimated using bootstrap sampling [55], which is shown as a function of the background selection efficiency in Figure 10 as a smoothed, dashed line and shaded band, indicating the mean and standard deviation, respectively, of the bootstrap sampling. Additionally, for a given physics task, full mass-decorrelation might not be necessary or optimal. Depending e.g. on the level of systematic uncertainty, some intermediate level of correlation with the jet mass might provide a balance with classification power which is better suited for the interpretation of the jet mass.

Figure 10: Profiles of the Jensen-Shannon divergence (JSD) for cuts corresponding to various multijet (background) selection efficiencies. Standard classifiers are indicated with filled markers. Mass-decorrelated classifiers indicated with open markers. The shaded grey band indicates the statistical limit on JSD from the finite number of simulated jets.

Figure 9: Jet mass-dependent multijet selection efficiencies for various inclusive efficiencies for the standard neural network tagger and the adversarially trained neural network tagger.

the physics task at hand than full mass-decorrelation.

### Combined metric

A combined metric, reflecting both classification performance and mass-decorrelation, is necessary to assess the trade-offs balanced by each of the mass-decorrelation procedures. A more complete picture of the performance is found by plotting the two metrics together. Figure 11 shows the mass-decorrelation (\(1/\text{JSD}\)) versus the background rejection (\(1/\varepsilon_{\text{bkg}}^{\text{rel}}\)) for tagger cuts at \(\varepsilon_{\text{sig}}^{\text{rel}}=50\%\), in two \(p_{\text{T}}\) bins. The \(x\)-axis measures classification power and the \(y\)-axis measures mass-decorrelation, with larger values along each indicating better performance. For any given task, a specific direction in the plane of Figure 11 will correspond to the best trade-off.

For each of the mass-decorrelated MVA taggers, several working points are evaluated, by scanning \(\lambda\) for the ANN tagger and \(\alpha\) for uBoost. For high values of \(\lambda\) (\(\gtrsim 10\)), the ANN method starts to saturate given the chosen network configurations, training procedures, and datasets.

The dashed line and shaded band at high \(1/\text{JSD}\) indicate the statistical limit of the mass-decorrelation, estimated using bootstrap sampling.

Figure 11 shows that for equal levels of mass-decorrelation, the (A)NN tagger generally provides the greatest background rejection. The BDT-based MVA taggers have comparable performance to the NN-based taggers for the standard variants, but the adversarial training mass-decorrelation method is seen to perform better than the uBoost method for the chosen configurations. From Figure 11(b), the effect of

Figure 11: Unified plot of the metrics for classification (background rejection, \(1/\varepsilon_{\text{bkg}}^{\text{rel}}\)) and mass-decorrelation (inverse Jensen-Shannon divergence, \(1/\text{JSD}\)), for cuts corresponding to \(\varepsilon_{\text{sig}}^{\text{rel}}=50\%\), in two \(p_{\text{T}}\) bins. Greater values along each axis indicate better performance. Standard classifiers are indicated with filled markers. Mass-decorrelated classifiers are indicated with open markers, with parameter scans traced out by dashed lines. The shaded grey band indicates the statistical limit on \(1/\text{JSD}\) from the finite number of simulated jets.

the analytical, single-variable mass-decorrelation methods on improving the classification power while similarly decorrelation from the jet mass, as discussed above, is particularly evident.

The fixed-efficiency \(k\)-NN regression method is seen to be the most effective of the analytical decorrelation methods, leading to a tagger variable which is close to fully decorrelated from the jet mass within statistical uncertainties. The saturation of the ANN tagger at high \(\lambda\) means that an upper limit on \(1/\mathrm{JSD}\) exists for \(\lambda\gtrsim 10\) with the chosen configuration. This is a reasonable observation, considering the complexity of the ANN training procedure and the fact that the tagger is evaluated on a dataset 10 times larger than the training dataset. This discrepancy in data sample sizes means that complex decorrelation methods like the ANN are likely decorrelated at, or close to, the statistical limit of the training dataset, but not necessarily at the statistical limit for the much larger evaluation dataset. The simplicity of e.g. the \(k\)-NN method means that it will be more robust to such differences in statistics. For these reasons, raising the upper limit on \(1/\mathrm{JSD}\) for ANN will likely be possible by increasing training statistics, performing a more fine-tuned model architecture optimisation, and similar.

### Robustness

For the chosen metrics, an important consideration is their robustness for different jet kinematics. The background rejection and \(1/\mathrm{JSD}\) as a function of jet \(p_{\mathrm{T}}\) and the number of reconstructed primary vertices, \(N_{\mathrm{PV}}\), is shown in Figure 12. In each \(p_{\mathrm{T}}\) bin the background rejection is computed for a cut corresponding to \(\mathrm{\varepsilon_{sig}^{rel}}=50\%\), thereby isolating the measurement of background rejection from the measurement of the uniformity of the \(\varepsilon_{\mathrm{sig}^{rel}}^{rel}\) as a function of \(p_{\mathrm{T}}\) for a fixed cut value.

The dashed line and shaded band at high \(1/\mathrm{JSD}\) indicate the statistical limit of the mass-decorrelation for the mean background efficiency in each \(p_{\mathrm{T}}\) bin, estimated using bootstrap sampling, added in quadrature to the largest absolute difference between the mean statistical limit for the average \(1/\varepsilon_{\mathrm{bkg}}^{rel}\) and the lowest and highest measured \(1/\varepsilon_{\mathrm{bkg}}^{rel}\) in each \(p_{\mathrm{T}}\) bin.

Across \(p_{\mathrm{T}}\), the standard MVA taggers yield the largest background rejection as well as the greatest correlation with the jet mass. The standard, single-variable taggers all show the best performance for \(p_{\mathrm{T}}\) close to the lower selection threshold of 200 \(\mathrm{GeV}\) and decreasing with \(p_{\mathrm{T}}\), regaining performance again towards \(p_{\mathrm{T}}\sim 2\TeV\). Since the single-variable taggers are highly physics-motivated and constructed from physically weighted distributions, the fact that their performance is optimal in the high-population low-\(p_{\mathrm{T}}\) end of the spectrum is not surprising.

For the analytical taggers, the improvement in classification power arising from the mass-decorrelation procedures is seen to increase with \(p_{\mathrm{T}}\). As no jet mass-window cut is applied in Figure 12, this is due to the mass- and \(p_{\mathrm{T}}\)-dependence of \(\tau_{21}\) and \(D_{2}\), which decreases classification power when integrated over either of these variables. The CSS methods, which removes mass-dependence, leads to the smallest improvement of the three analytical mass-decorrelation techniques; the DDT and fixed-efficiency \(k\)-NN regression methods, which decorrelate from the jet \(p_{\mathrm{T}}\) in addition to the jet mass, yield bigger improvements across \(p_{\mathrm{T}}\) by removing this additional dependence. In particular, \(k\)-NN yields a powerful single-variable tagger which is close to fully decorrelation from the jet mass, within statistical uncertainties, up to \(p_{\mathrm{T}}\approx 1\TeV\).

However, for \(p_{\mathrm{T}}\gtrsim 1\TeV\), \(D_{2}^{k\text{-NN}}\) experiences a significant drop in \(1/\mathrm{JSD}\). This is due to the fact that the method is designed to decorrelate \(D_{2}\) from the jet mass at \(\varepsilon_{\mathrm{bkg}}^{rel}=16\%\), which corresponds to \(\varepsilon_{\mathrm{sig}}^{rel}=50\%\) for the inclusive sample. However, the cuts in Figure 12 are calculated to correspond to \(\varepsilon_{\mathrm{sig}}^{rel}=50\%\) in each \(p_{\rm T}\) bin, and since the background rejection of \(D_{2}^{k\text{-}{\rm NN}}\) increases with \(p_{\rm T}\), the cut at high \(p_{\rm T}\) will correspond \(\varepsilon_{\rm bkg}^{\rm rel}\) considerable smaller than 16%. The fact that mass-decorrelation for \(k\)-NN is tuned to \(\varepsilon_{\rm bkg}^{\rm rel}=16\%\), and since the decrease in \(1/{\rm JSD}\) around this value is found to become more prominent with increasing \(p_{\rm T}\), therefore explains the behaviour seen for \(k\)-NN in Figure 12. If a fixed cut value were used, \(k\)-NN would be able to decorrelate \(D_{2}\) from the jet mass to roughly within statistical uncertainty across \(p_{\rm T}\).

The mass-decorrelated MVA taggers have robust performance across \(p_{\rm T}\), with the ANN outperforming uBoost in mass-decorrelation at low \(p_{\rm T}\) and uBoost performing slightly better at high \(p_{\rm T}\). In the lowest \(p_{\rm T}\) bins, \(z_{\rm ANN}\) is seen to be as mass-decorrelated as \(D_{2}^{k\text{-}{\rm NN}}\). However, for \(p_{\rm T}\gtrsim 400\) GeV the mass-decorrelation for ANN degrades. To some extent, this is an effect of the parametrisation of the adversary network in terms of \(\log p_{\rm T}\). The parametrisation serves to guide the attention of the adversary, and using the logarithm of \(p_{\rm T}\) leads to competitive mass-decorrelation as a function of \(p_{\rm T}\), but with an emphasis on low \(p_{\rm T}\). Studies have indicated that an adversary parametrised by \(p_{\rm T}\) would have slightly more robust performance, but report comparatively worse summary metrics, cf. Figure 11, since the testing dataset is

Figure 12: Plot of the metrics for classification (background rejection, \(1/\varepsilon_{\rm bkg}^{\rm rel}\); _top_) and mass-decorrelation (inverse Jensen-Shannon divergence, \(1/{\rm JSD}\); _bottom_), for cuts corresponding to \(\varepsilon_{\rm sig}^{\rm rel}=50\%\), as a function of the reconstructed jet \(p_{\rm T}\) (_left_) and the number of reconstructed vertices, \(N_{\rm PV}\) (_right_). Standard classifiers are indicated with filled markers. Mass-decorrelated classifiers indicated with open markers. Statistical uncertainties are indicated with shaded boxes, derived using bootstrap sampling. The statistical limit on \(1/{\rm JSD}\), also accounting for variation in \(\varepsilon_{\rm bkg}^{\rm rel}\) for different taggers within the same bin, is shown as a shaded grey band (_bottom_). Only mass-decorrelated taggers are shown for \(N_{\rm PV}\) (_right_).

dominated by jet with \(p_{\mathrm{T}}\) just above the lower selection threshold.

However, this effect persists across \(\varepsilon_{\mathrm{bkg}}^{\mathrm{rel}}\) and \(\lambda\), and is connected to the inability of the adversary to fully reverse the sculpting around the \(W\) jet peak. Therefore, a more likely cause is the difference in the training procedure, compared to uBoost. Where the uBoost classifier starts from a random initialisation and is then trained by adaptive boosting, balancing competing objectives, the ANN tagger is treated as a minimal perturbation by starting from a fully trained standard NN classifier (see Appendix A for details). The ANN tagger has to reverse a large degree of mass-sculpting; the uBoost tagger is trained not to learn it in the first place. This choice works well in some regimes, particularly at moderately low \(p_{\mathrm{T}}\), but appears to work less well at high \(p_{\mathrm{T}}\), where the initial mass sculpting of the standard NN tagger is more dramatic cf. Figure 12. The fact that \(1/\mathrm{JSD}\) for uBoost is roughly constant across \(p_{\mathrm{T}}\) can be seen as an expression of this difference in training. Therefore, performing the adversarial training by starting from a classifier without pretraining may lead to more robust mass-decorrelation as a function of \(p_{\mathrm{T}}\), but comparatively worse performance in some regions. Alternative training procedures and improved architecture design may be able to optimally reconcile \(p_{\mathrm{T}}\)-robustness and summary performance. Additionally, having the adversary decorrelate the classifier variable from the jet mass mass and \(p_{\mathrm{T}}\) simultaneously, similar to what is done by \(k\)-NN and to a lesser extent DDT, may also lead to more robust performance.

Finally, the mass-decorrelated taggers are found to be robust as a function of the number of reconstructed vertices. The background rejection for all mass-decorrelated taggers exhibits a regular, linear relationship with \(N_{\mathrm{PV}}\), with a slight negative slope. The behaviour of \(1/\mathrm{JSD}\) as a function of \(N_{\mathrm{PV}}\) is less regular, due to its dependence on the statistics in each bin. Since the simulated data samples are generated with a bell-shaped distribution of \(N_{\mathrm{PV}}\), centered around 15, the statistical limit on \(1/\mathrm{JSD}\) will be maximal in this region, and decreasing with statistics towards lower and higher values of \(N_{\mathrm{PV}}\). However, the mass-decorrelated taggers all exhibit a regular behaviour across \(N_{\mathrm{PV}}\) in terms of \(N_{\mathrm{PV}}\). Therefore, although the absolute performance of each tagger changes with \(N_{\mathrm{PV}}\), the relative performance of the taggers is largely unaffected by pile-up.

### Mass-window cut

For the general case of mass-decorrelated jet tagging, computing the background rejection with respect to the full sample of multijets passing the baseline selection described in Section 4 is well-motivated. However, for the specific case of simple \(W\) jet tagging, an additional cut on the jet mass of \(m\in[60,100]\,\mathrm{Ge\kern-1.0ptV}\) is typically used to further increase the tagging power. The ROC curves for the various taggers in the two \(p_{\mathrm{T}}\) bins, with the addition of such a jet mass-window cut is shown in Figure 13.

Comparing to Figure 7, this cut leads to increases in background rejection at similar signal efficiencies for all taggers, although the relative performance of the various taggers is generally the same. Furthermore, the effect seen most clearly in Figures 7(b) and 11(b), where the analytical mass-decorrelation methods also lead to improvements in classification power, mostly disappears after imposing the mass-window cut. In this way, the effect of \(W\) jet classification \(per\,\,se\) is separated from the effect of non-uniform selection efficiencies outside the window around the \(W\) boson pole mass.

## 8 Conclusion

This note presents a preliminary study of various techniques for the construction of mass-decorrelated jet taggers for two-body hadronic resonance decays. Jets from the hadronic decay of high-\(p_{\mathrm{T}}\,W\) boson are used to demonstrate the usefulness of such taggers, but the mass-decorrelation techniques should be applicable to hadronic two-body decays with different resonance masses. Designed decorrelated taggers (DDT), convolved substructure (CSS), and fixed-efficiency \(k\)-NN regression are used to decorrelate analytical, single-variable taggers from the jet mass. These are compared with multivariate techniques, where the mass-decorrelation is performed using adversarial training for neural network taggers (ANN) and adaptive boosting for uniform efficiency for BDT taggers (uBoost). Metrics for evaluating classification power and mass-decorrelation are proposed and studied in simulated data samples. Standard multivariate taggers are found to yield superior classification performance compared to single-variable taggers, but exhibit strong non-linear correlations with the jet mass, potentially reducing sensitivity in searches for new physics. Fixed-efficiency \(k\)-NN regression is able to decorrelate single-variable taggers roughly to within statistical uncertainty at \(p_{\mathrm{T}}\lesssim 1000\). The adversarially trained neural network tagger is generally found to have better classification power for similar levels of mass-decorrelation than the remaining mass-decorrelated taggers in the primary kinematic regime of interest. These results provide an encouraging first look at the prospect for using mass-decorrelated substructure observables in physics searches.

## References

* [1] L. Evans and P. Bryant, _LHC Machine_, Journal of Instrumentation **3** (2008) S08001, url: [http://stacks.iop.org/1748-0221/3/i=08/a=S08001](http://stacks.iop.org/1748-0221/3/i=08/a=S08001).

Figure 13: Rejection of multijets (background) as a function of \(W\) jet (signal) selection efficiency, for standard and mass-decorrelated versions of analytical and multivariate (MVA) jet taggers with the addition of a jet mass-window cut \(m\in[60,100]\), in two \(p_{\mathrm{T}}\) bins.

ATLAS Collaboration, _A new method to distinguish hadronically decaying boosted \(Z\) bosons from \(W\) bosons using the ATLAS detector_, Eur. Phys. J. C **76** (2016) 238, arXiv: 1509.04939 [hep-ex].
* [3] ATLAS Collaboration, _Identification of high transverse momentum top quarks in \(pp\) collisions at \(\sqrt{s}=8\) TeV with the ATLAS detector_, JHEP **06** (2016) 093, arXiv: 1603.03127 [hep-ex].
* [4] ATLAS Collaboration, _Identification of Boosted, Hadronically Decaying \(W\) Bosons and Comparisons with ATLAS Data Taken at \(\sqrt{s}=8\) TeV_, Eur. Phys. J. C **76** (2016) 154, arXiv: 1510.05821 [hep-ex].
* [5] ATLAS Collaboration, _Boosted hadronic top identification at ATLAS for early \(13\) TeV data_, ATL-PHYS-PUB-2015-053, 2015, url: [https://cds.cern.ch/record/2116351](https://cds.cern.ch/record/2116351).
* [6] ATLAS Collaboration, _Identification of Boosted, Hadronically-Decaying \(W\) and \(Z\) Bosons in \(\sqrt{s}=13\) TeV Monte Carlo Simulations for ATLAS_, ATL-PHYS-PUB-2015-033, 2015, url: [https://cds.cern.ch/record/2041461](https://cds.cern.ch/record/2041461).
* [7] ATLAS Collaboration, _Identification of Hadronically-Decaying \(W\) Bosons and Top Quarks Using High-Level Features as Input to Boosted Decision Trees and Deep Neural Networks in ATLAS at \(\sqrt{s}=13\) TeV_, ATL-PHYS-PUB-2017-004, 2017, url: [https://cds.cern.ch/record/2259646](https://cds.cern.ch/record/2259646).
* [8] ATLAS Collaboration, _Performance of Top Quark and W Boson Tagging in Run 2 with ATLAS_, ATLAS-CONF-2017-064, 2017, url: [https://cds.cern.ch/record/2281054](https://cds.cern.ch/record/2281054).
* [9] ATLAS Collaboration, _Search for low-mass dijet resonances using trigger-level jets with the ATLAS detector in \(pp\) collisions at \(\sqrt{s}=13\) TeV_, (2018), arXiv: 1804.03496 [hep-ex].
* [10] CMS Collaboration, _Search for Low Mass Vector Resonances Decaying to Quark-Antiquark Pairs in Proton-Proton Collisions at \(\sqrt{s}=13\) \(\,\mathrm{TeV}\)_, Phys. Rev. Lett. **119** (2017) 111802, arXiv: 1705.10532 [hep-ex].
* [11] CMS Collaboration, _Inclusive search for a highly boosted Higgs boson decaying to a bottom quark-antiquark pair_, Phys. Rev. Lett. **120** (2018) 071802, arXiv: 1709.05543 [hep-ex].
* [12] CMS Collaboration, _Search for low mass vector resonances decaying into quark-antiquark pairs in proton-proton collisions at \(\sqrt{s}=13\) TeV_, JHEP **01** (2018) 097, arXiv: 1710.00159 [hep-ex].
* [13] ATLAS Collaboration, _Search for light resonances decaying to boosted quark pairs and produced in association with a photon or a jet in proton-proton collisions at \(\sqrt{s}=13\) TeV with the ATLAS detector_, (2018), arXiv: 1801.08769 [hep-ex].
* [14] J. Dolen, P. Harris, S. Marzani, S. Rappoccio and N. Tran, _Thinking outside the ROCs: Designing Decorrelated Taggers (DDT) for jet substructure_, JHEP **05** (2016) 156, arXiv: 1603.00027 [hep-ph].
* [15] S. A. Dudani, _The Distance-Weighted k-Nearest-Neighbor Rule_, IEEE Transactions on Systems, Man, and Cybernetics **SMC-6** (1976) 325, issn: 0018-9472.
* [16] I. Moult, B. Nachman and D. Neill, _Convolved Substructure: Analytically Decorrelating Jet Substructure Observables_, (2017), arXiv: 1710.06859 [hep-ph].

* [17] I. Goodfellow et al., _Generative Adversarial Nets_, Advances in Neural Information Processing Systems **27** (2014) 2672, arXiv: 1406.2661 [stat.ML], url: [http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf).
* [18] J. Stevens and M. Williams, _uBoost: A boosting method for producing uniform selection efficiencies from multivariate classifiers_, JINST **8** (2013) P12013, arXiv: 1305.7248 [nucl-ex].
* [19] ATLAS Collaboration, _The ATLAS Experiment at the CERN Large Hadron Collider_, JINST **3** (2008) S08003.
* [20] T. Sjostrand, S. Mrenna and P. Z. Skands, _A Brief Introduction to PYTHIA 8.1_, Comput. Phys. Commun. **178** (2008) 852, arXiv: 0710.3820 [hep-ph].
* [21] R. D. Ball et al., _Parton distributions with QED corrections_, Nucl. Phys. **B877** (2013) 290, arXiv: 1308.0598 [hep-ph].
* [22] ATLAS Collaboration, _ATLAS Pythia 8 tunes to 7 TeV data_, ATL-PHYS-PUB-2014-021, 2014, url: [https://cds.cern.ch/record/1966419](https://cds.cern.ch/record/1966419).
* [23] ATLAS Collaboration, _The ATLAS Simulation Infrastructure_, Eur. Phys. J. C **70** (2010) 823, arXiv: 1005.4568 [physics.ins-det].
* [24] S. Agostinelli et al., _Geant4--a simulation toolkit_, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment **506** (2003) 250, issn: 0168-9002, url: [http://www.sciencedirect.com/science/article/pii/S0168900203013688](http://www.sciencedirect.com/science/article/pii/S0168900203013688).
* [25] ATLAS Collaboration, _Summary of ATLAS Pythia 8 tunes_, ATL-PHYS-PUB-2012-003, 2012, url: [https://cds.cern.ch/record/1474107](https://cds.cern.ch/record/1474107).
* [26] A. D. Martin, W. J. Stirling, R. S. Thorne and G. Watt, _Parton distributions for the LHC_, Eur. Phys. J. **C63** (2009) 189, arXiv: 0901.0002 [hep-ph].
* [27] ATLAS Collaboration, _Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1_, Eur. Phys. J. C **77** (2017) 490, arXiv: 1603.02934 [hep-ex].
* [28] M. Cacciari, G. P. Salam and G. Soyez, _The Anti-k(t) jet clustering algorithm_, JHEP **04** (2008) 063, arXiv: 0802.1189 [hep-ph].
* [29] M. Cacciari, G. P. Salam and G. Soyez, _FastJet User Manual_, Eur. Phys. J. **C72** (2012) 1896, arXiv: 1111.6097 [hep-ph].
* [30] D. Krohn, J. Thaler and L.-T. Wang, _Jet Trimming_, JHEP **02** (2010) 084, arXiv: 0912.1342 [hep-ph].
* [31] ATLAS Collaboration, _Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at \(\sqrt{s}=13\) TeV with the ATLAS detector_, Phys. Rev. D **96** (2017) 072002, arXiv: 1703.09665 [hep-ex].
* [32] ATLAS Collaboration, _Jet mass reconstruction with the ATLAS Detector in early Run 2 data_, ATLAS-CONF-2016-035, 2016, url: [https://cds.cern.ch/record/2200211](https://cds.cern.ch/record/2200211).
* [33] S. Kullback and R. A. Leibler, _On Information and Sufficiency_, Ann. Math. Statist. **22** (1951) 79, url: [https://doi.org/10.1214/aoms/1177729694](https://doi.org/10.1214/aoms/1177729694).
* [34] J. Lin, _Divergence measures based on the Shannon entropy_, IEEE Transactions on Information Theory **37** (1991) 145, issn: 0018-9448.

* [35] I. Moult, L. Necib and J. Thaler, _New Angles on Energy Correlation Functions_, JHEP **12** (2016) 153, arXiv: 1609.07483 [hep-ph].
* [36] M. Dasgupta, A. Fregoso, S. Marzani and G. P. Salam, _Towards an understanding of jet substructure_, JHEP **09** (2013) 029, arXiv: 1307.0007 [hep-ph].
* [37] A. J. Larkoski, S. Marzani, G. Soyez and J. Thaler, _Soft Drop_, JHEP **05** (2014) 146, arXiv: 1402.2657 [hep-ph].
* [38] A. J. Larkoski, I. Moult and D. Neill, _Power Counting to Better Jet Observables_, JHEP **12** (2014) 009, arXiv: 1409.6298 [hep-ph].
* [39] G. Louppe, M. Kagan and K. Cranmer, _Learning to Pivot with Adversarial Networks_, Advances in Neural Information Processing Systems **30** (2017) 981, arXiv: 1611.01046 [stat.ML], url: [http://papers.nips.cc/paper/6699-learning-to-pivot-with-adversarial-networks.pdf](http://papers.nips.cc/paper/6699-learning-to-pivot-with-adversarial-networks.pdf).
* [40] C. Shimmin et al., _Decorrelated Jet Substructure Tagging using Adversarial Neural Networks_, Phys. Rev. **D96** (2017) 074034, arXiv: 1703.03507 [hep-ex].
* [41] J. Thaler and K. Van Tilburg, _Identifying Boosted Objects with N-subjettiness_, JHEP **03** (2011) 015, arXiv: 1011.2268 [hep-ph].
* [42] G. C. Fox and S. Wolfram, _Observables for the Analysis of Event Shapes in \(e^{+}e^{-}\) Annihilation and Other Processes_, Phys. Rev. Lett. **41** (23 1978) 1581, url: [https://link.aps.org/doi/10.1103/PhysRevLett.41.1581](https://link.aps.org/doi/10.1103/PhysRevLett.41.1581).
* [43] L. G. Almeida, S. J. Lee, G. Perez, I. Sung and J. Virzi, _Top Jets at the LHC_, Phys. Rev. **D79** (2009) 074012, arXiv: 0810.0934 [hep-ph].
* [44] ATLAS Collaboration, _ATLAS measurements of the properties of jets for boosted particle searches_, Phys. Rev. D **86** (2012) 072006, arXiv: 1206.5369 [hep-ex].
* [45] C. Chen, _New approach to identifying boosted hadronically-decaying particle using jet substructure in its center-of-mass frame_, Phys. Rev. **D85** (2012) 034007, arXiv: 1112.2567 [hep-ph].
* [46] J. Thaler and L.-T. Wang, _Strategies to Identify Boosted Tops_, JHEP **07** (2008) 092, arXiv: 0806.0023 [hep-ph].
* [47] ATLAS Collaboration, _Measurement of \(k_{\mathrm{T}}\) splitting scales in \(W\to\ell\nu\) events at \(\sqrt{s}=7\) TeV with the ATLAS detector_, Eur. Phys. J. C **73** (2013) 2432, arXiv: 1302.1415 [hep-ex].
* [48] S. Catani, Y. Dokshitzer, M. Seymour and B. Webber, _Longitudinally-invariant \(k_{t}\)-clustering algorithms for hadron-hadron collisions_, Nuclear Physics B **406** (1993) 187, issn: 0550-3213, url: [http://www.sciencedirect.com/science/article/pii/055032139390166M](http://www.sciencedirect.com/science/article/pii/055032139390166M).
* [49] K. Pearson, _LIII. On lines and planes of closest fit to systems of points in space_, The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science **2** (1901) 559, url: [https://doi.org/10.1080/14786440109462720](https://doi.org/10.1080/14786440109462720).

* [50] J. Snoek, H. Larochelle and R. P. Adams, _Practical Bayesian Optimization of Machine Learning Algorithms_, Advances in Neural Information Processing Systems **25** (2012) 2951, arXiv: 1206.2944 [stat.ML], url: [http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf](http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf).
* [51] D. P. Kingma and J. Ba, _Adam: A Method for Stochastic Optimization_, Proceedings of the 3rd International Conference on Learning Representations (ICLR) (2015), arXiv: 1412.6980 [cs.LG].
* [52] C. M. Bishop, _Mixture density networks_, 1994.
* [53] Y. Freund and R. E Schapire, _A Short Introduction to Boosting_, **14** (1999) 771.
* [54] A. Rogozhnikov et al., _hep_ml: Machine Learning for High Energy Physics_, [https://github.com/aroogzhnikov/hep_ml](https://github.com/aroogzhnikov/hep_ml), version 0.5.0, 2017.
* [55] B. Efron, _Bootstrap Methods: Another Look at the Jackknife_, Ann. Statist. **7** (1979) 1, url: [https://doi.org/10.1214/aos/1176344552](https://doi.org/10.1214/aos/1176344552).
* [56] F. Chollet et al., _Keras_, [https://github.com/fchollet/keras](https://github.com/fchollet/keras), version 2.1.5, 2018.
* [57] M. Abadi et al., _TensorFlow: A System for Large-scale Machine Learning_, Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (2016) 265, url: [http://dl.acm.org/citation.cfm?id=3026877.3026899](http://dl.acm.org/citation.cfm?id=3026877.3026899).
* [58] P. Baldi, K. Cranmer, T. Faucett, P. Sadowski and D. Whiteson, _Parameterized neural networks for high-energy physics_, Eur. Phys. J. **C76** (2016) 235, arXiv: 1601.07913 [hep-ex].
* [59] I. J. Goodfellow, _NIPS 2016 Tutorial: Generative Adversarial Networks_, CoRR (2017), arXiv: 1701.00160.
* [60] V. Nagarajan and J. Z. Kolter, _Gradient descent GAN optimization is locally stable_, CoRR (2017), arXiv: 1706.04156.
* [61] Y. Ganin et al., _Domain-Adversarial Training of Neural Networks_, Journal of Machine Learning Research **17** (2016) 1, arXiv: 1505.07818 [stat.ML], url: [http://jmlr.org/papers/v17/15-239.html](http://jmlr.org/papers/v17/15-239.html).
* [62] S. Ioffe and C. Szegedy, _Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift_, CoRR (2015), arXiv: 1502.03167.

## Appendix A Adversarial training details

This appendix provides additional details on the implementation of the adversarial training, the training procedures for the stand-alone and adversarially trained classifiers, the chosen input features, and the optimisation of the classifier hyperparameters.

### Implementation

The classifier and adversary neural networks are connected in a single, feed-forward architecture as shown in Figure 4. Both models are constructed as densely connected networks in Keras 2.1.5[56] using the TensorFlow 1.4.1 backend [57].

The classifier, parametrised by weights \(\theta_{\text{clf}}\) is trained to perform binary classification of the jet labels1\(Y\) based on input features \(X\) using a binary cross-entropy loss:

Footnote 1: Taken to be 1 for \(W\) jets and 0 for multijets.

\[L_{\text{clf}}(\theta_{\text{clf}})=\mathbb{E}_{x\sim X}\,\mathbb{E}_{y\sim Y }\left[-y\,\log p_{\text{clf}}(y\mid x,\theta_{\text{clf}})-(1-y)\log\left(1- p_{\text{clf}}(y\mid x,\theta_{\text{clf}})\right)\right] \tag{11}\]

The task of the adversary to infer the jet mass is implemented by having the adversary parametrise a probability density function (p.d.f.) in \(m\) given \(z\sim p_{\text{clf}}(x,\theta_{\text{clf}})\), called the adversary posterior. In order to ease this task, the network can be parametrised in the sense of Ref. [58], by providing it with a set of auxiliary inputs \(A\) from which information about the jet mass can be derived. The fact that the adversary output is parametrised by, and therefore conditional on, some number of auxiliary inputs, ensures better mass-decorrelation inclusively as well as differentially with respect to these variables. Only \(\log p_{\text{T}}\) is provided to the adversary network as an auxiliary feature, pre-scaled to the same range as the classifier output. This parametrisation yields robust results as a function of \(p_{\text{T}}\) while focusing mass-decorrelation attention towards the lower range of the \(p_{\text{T}}\)-spectrum which is the primary regime studied.

The output of the adversary is therefore the conditional probability \(p_{\text{adv}}(m\mid z,a,\theta_{\text{adv}})\), given \(a\sim A\), evaluated at the value of \(m\) for each jet in the training sample. In practice, the adversary network is constructed to specify the coefficients, means, and width of the components of a Gaussian mixture model (GMM) [39] parametrising the posterior. The adversary, tasked with decorrelating the classifier from the general set of features \(D\), is trained with the heuristic loss [59]:

\[L_{\text{adv}}(\theta_{\text{adv}})=\mathbb{E}_{z\sim p_{\text{clf}}(X, \theta_{\text{clf}})}\,\mathbb{E}_{d\sim D}\,\mathbb{E}_{a\sim A}\left[-\log p _{\text{adv}}(d\mid z,a,\theta_{\text{adv}})\right] \tag{12}\]

This loss, computed only for background jets, is seen to maximise the posterior \(p_{\text{adv}}(d\mid z,a,\theta_{\text{adv}})\) in a way which is intimately related with the decorrelation metric in Equation (2).

The challenge of adversarial training of neural networks is the non-convex nature of the problem, arising from the joint optimisation of networks with opposing objectives cf. Equation (10). Ideally, for every parameter update of the classifier, the adversary should be allowed to fully converge. In practice, the optimisation is typically done using alternating [17] or simultaneous [60] gradient updates of the classifier and adversary weights. Since the latter has been found to lead to better convergence, this is chosen asthe training method for the adversarially trained neural network jet classifiers, and gradient reversal [61] is used for the implementation of the joint optimisation. A gradient scaling operation is applied to the connection between the classifier and the adversary. In the feed-forward mode it acts as the identify operations and during back-propagation the gradient from the adversary is scaled by \(-\lambda\). The minus sign means that the gradient flowing backwards from adversary has the inverse effect for the classifier weights as for the adversary. That is, whereas the gradient acts on \(\theta_{\text{adv}}\) to minimise \(L_{\text{adv}}\), it will have the effect of maximising \(L_{\text{adv}}\) for \(\theta_{\text{clf}}\). The magnitude of the gradient scaling is controlled by \(\lambda\), leading to exactly the behaviour intended in Equation (10).

To ensure a stable joint convergence, the classifier is trained with a smaller learning rate than the adversary, resembling the effect of the standard nested optimisation. The effective learning rate is treated as an additional hyperparameter, but is scaled by \(1/(1+\lambda)\) in order to avoid too large of gradients flowing from the adversary to the classifier. The size of the learning rate ratio also reflects the high dimensionality of the adversary output space compared to that of the classifier (a single number). Since the adversary is tasked with estimating the parameters of a large number of GMM components, convergence will necessarily be slower than for the classifier, which the difference in learning rate must also account for. Finally, since the classifier is pre-trained to an optimal configuration for classification, the adversarial training should seek to identify the smallest possible perturbation around the classification optimum which satisfies the decorrelation requirement for a given value of \(\lambda\).

### Hyperparameters

The classifier is constructed as a simple, densely connected network with ten input features, some number of hidden layers all with the same number of nodes, and a single output node. The output node is equipped with sigmoid activation, to produce an output in the range \([0,1]\) with higher signal-jet likelihood towards larger classifier output values. The training is performed with the Adam[51] optimiser and to ensure high computational throughput the training batch size is fixed at 8192. Similarly, to accelerate the training of the classifier, batch normalisation [62] is applied before each hidden layer in the network to standardise the learned features. The parameters considered in the optimisation, the range of the parameters, the scale of the parameter space, and the optimal hyperparameter configuration are listed in Table 2.

The classifier is optimised according to the loss in Equation (11) using 3-fold stratified cross-validation, training for 50 epochs (pass-throughs of the entire training dataset), and shuffling the order of the training samples between each. In order to ensure stability of the result, the optimisation metric is chosen to be the mean classification loss across validation splits plus one standard deviation. The optimisation process,

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Parameter** & **Range** & **Scale** & **Chosen value** \\ \hline Learning rate & \([10^{-5},10^{-1}]\) & Logarithmic & \(10^{-2}\) \\ Learning rate decay & \([10^{-6},10^{-2}]\) & Logarithmic & \(10^{-3}\) \\ Hidden layers & \([1,6]\) & Linear & 3 \\ Nodes per hidden layers & \([2,512]\) & Logarithmic & 64 \\ Dropout regularisation & \([0,0.5]\) & Linear & 0 \\ Hidden layer activation & \(\{\text{ReLU},\text{tanh}\}\) & Choice & ReLU \\ \hline \hline \end{tabular}
\end{table}
Table 2: Neural network classifier hyperparameters optimised with Spearminet, the parameter range searched, the scale of the space samples, and the chosen hyperparameter configuration.

run for 100 Bayesian hyperparameter configuration evaluations in order to check stability for extended training duration, is shown in Figure 14.

Spearman finds a broad, shallow optimal region of hyperparameter space. Based on the optimisation minima, a classifier with three hidden layers, each with 64 nodes with hyperbolic tangent (ReLU) activation and no dropout regularisation is used. Bayesian optimisation has no convergence criteria and is not guaranteed to yield the true, optimal configuration in any finite number of iterations. However, it is capable of efficiently probing a large parameter space with few evaluations. This balance is evident in the large spread of evaluations in Figure 14. Exploration of extreme regions of the parameter space leads either to insufficient or excessive capacity of the classifier network, resulting in either underfitting or overfitting of the training data, and poor classification performance across the validation splits. Conversely, exploiting of identified minimal regions of parameter space leads to low \(L_{\text{clf}}^{\text{val}}\) in relatively few iterations. Therefore, the chosen hyperparameter configuration is deemed to be highly performant but is not guaranteed to be optimal.

The adversary is constructed as a densely connected network with one input feature (the classifier output), a number of hidden layers with the same number of nodes, and outputting the parameters for the posterior p.d.f. Batch normalisation is not used in the adversary, as it is found to yield unstable results. Similarly, no regularisation is necessary in the adversary, since overfitting in not a concern.

Mass-decorrelation and robustness with respect to the jet \(p_{\text{T}}\) is implemented by having the adversary parametrise a p.d.f. in \(m\) conditional on the auxiliary input \(\log p_{\text{T}}\). For convenience, the jet mass is scaled to the unit interval to allow for better use of output activations in the adversary. The adversary posterior GMM is constructed by \(N_{\text{GMM}}\) components, meaning that the adversary is tasked with parametrising \(N_{\text{GMM}}\) Gaussian means, widths, and \(N_{\text{GMM}}-1\) normalisation coefficients. The GMM means, widths, and coefficients are equipped with sigmoid, softplus, and softmax activation to ensure the desired properties. Similar to the classifier, the adversary is trained with flat-\(p_{\text{T}}\) jet weights, constructed to retain physical distributions in \(m\) for all \(p_{\text{T}}\). This weighted jet mass distribution plays the role of the adversary prior.

Figure 14: Neural network classifier Bayesian hyperparameter optimisation. Blue markers indicate the mean and standard deviation for each evaluation. The red line indicates the running optimisation metric minimum. Open red markers indicate improvements. Triangular markers indicate evaluation metrics outside of the axis range.

Finally, due to the scaling of the decorrelation variable \(m\), the adversary posterior is normalised to unit integral on \([0,1]\), to allow for an easier admission of the prior.

The hyperparameters of the adversary network are also optimised using Spearmint. In contrast with the stand-alone classifier optimisation, the adversary cannot be meaningfully optimised according to the loss in Equation (12) alone. This loss measures only the capacity of the adversary to construct the posterior p.d.f. for the jet mass, not the quality of the resulting decorrelated tagger. Similarly, optimising according to a combination of Equations (11) and (12) is vulnerable to breakdowns of the adversarial training procedure, where the inability of the adversary to infer the jet mass is due to an unbalanced, joint optimisation rather than the absence of correlation with the jet mass. Therefore, to tune the adversarially trained neural network tagger according to expected performance, the optimisation is performed by maximising the metric \(1/\varepsilon_{\text{bkg}}^{\text{rel}}+\lambda/\text{JSD}\) for a fixed value of \(\lambda\), chosen to be \(\lambda=10\). Throughout the optimisation, a classifier pre-trained with the chosen hyperparameters in Table 2 is used.

The adversarial optimisation is performed by 3-fold stratified cross-validation. Training is performed for 200 epochs, following 10 epochs of adversary-only pre-training, using the Adam optimiser with a batch size of 8192 and between-epoch shuffling, similar to the stand-alone classifier training. The parameters considered in the hyperparameter optimisation, the range of the parameters, the scale of the parameter space, and the optimal hyperparameter configuration are listed in Table 3.

An adversary with a single hidden layer comprising 64 nodes with ReLU activation, parametrising a GMM posterior p.d.f. with \(N_{\text{GMM}}=20\) components is found to have sufficient capacity to perform the decorrelation.

### Training procedure

#### Classifier

The training of both classifier and adversary is performed on a cluster of NVIDIA Tesla K80 GPUs. To ensure that the training converges and that no overtraining is observed, the classifier is first trained stand-alone with the chosen hyperparameter configuration using 3-fold cross-validation for 200 epochs, i.e. four times the number used for optimisation. The evolution of the classifier loss \(L_{\text{clf}}\) as a function of the number of training epochs is shown in Figure 15.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Parameter** & **Range** & **Scale** & **Chosen value** \\ \hline Learning rate & \([10^{-5},10^{-1}]\) & Logarithmic & \(5\times 10^{-2}\) \\ Learning rate decay & \([10^{-6},10^{-2}]\) & Logarithmic & \(10^{-2}\) \\ Hidden layers & \([1,6]\) & Linear & \(1\) \\ Nodes per hidden layers & \([2,128]\) & Logarithmic & \(64\) \\ GMM components, \(N_{\text{GMM}}\) & \([0,20]\) & Linear & \(20\) \\ Learning rate ratio, \(\ell_{\text{clf}}/\ell_{\text{adv}}\) & \([10^{-8},10^{-1}]\) & Logarithmic & \(2\times 10^{-7}\) \\ Hidden layer activation & \{ReLU, tanh\} & Choice & ReLU \\ \hline \hline \end{tabular}
\end{table}
Table 3: Adversary network hyperparameters optimised with Spearmint, the parameter range searched, the scale of the space samples, and the chosen hyperparameter configuration.

\(L_{\rm clf}\) decreases monotonically during training, for both training and cross-validation splits, indicating no over-training given the chosen hyperparameter configuration. In addition, the final classifier loss is comparable to Ref. [7], suggesting that the classifier network architecture is performant.

#### Adversary

The adversary's training configuration is chosen so as to find a stable, minimal perturbation around the stand-alone neural network classifier. Therefore, the training starts from the pre-trained, optimal neural network classifier. In order to provide the adversary with reasonable initial conditions, the adversarial training starts with an adversary-only pre-training period, where the adversary is allowed to condition its posterior on the pre-trained classifier. After the adversary pre-training, the two networks are trained simultaneously, with a small effective learning for the classifier, emulating full convergence on the inner optimisation in Equation (10). The evolution of the classifier, adversary, and effective losses is shown in Figure 16.

During the adversary pre-training, the adversary loss \(L_{\rm adv}\) decreases to a minimum, showing the convergence of the adversary posterior towards the true distribution of \(p(m\,|\,z,\log p_{\rm T})\). In this portion of the training, the classifier is kept fixed, and thus the classifier loss \(L_{\rm clf}\) remains constant.

After the adversary pre-training, \(L_{\rm clf}\) is seen to rise in sync with a rise in \(L_{\rm adv}\), illustrating the classifier balancing the two competing objectives. The balance is such that the effective loss seen by the classifier, \(L_{\rm clf}-\lambda L_{\rm adv}\), is minimised. As a result of the mass-decorrelation, the adversary's task becomes more difficult, and in the limit of full mass-decorrelation the adversary posterior is equal to the prior, corresponding to the jet mass distribution with training weights, assuming an adversary with sufficient capacity and full convergence of the inner optimisation in Equation (10). In this limit, the value of \(L_{\rm adv}\) will tend

Figure 15: Stand-alone neural network classifier loss during 3-fold cross validation training as a function of the number of training epochs, for training and validation splits. Lines indicate mean loss across folds. Shaded bands indicate standard deviation of losses across folds.

towards the entropy \(H\) of the prior. Any deviation from this asymptotic limit indicates a balance between classification and mass-decorrelation for a given \(\lambda\).

Relevant reference values for each loss are shown on Figure 16. In particular, the "Ideal" value of \(L_{\text{clf}}-\lambda L_{\text{adv}}\) corresponds to the case where the stand-alone NN classification power is retained along with full mass-decorrelation (\(L_{\text{adv}}\sim H(\text{prior})\)). Deviations from the ideal case reflect the information lost in order to prioritise the mass-decorrelation.

Figure 16: Classification (_top_), adversary (_middle_), and effective classifier losses (_bottom_) associated with the adversarial neural network tagger during 3-fold cross validation training as a function of the number of epochs, for training and validation splits. The first 10 epochs are spent on pre-training the adversary network. Lines indicate mean loss across folds, shaded bands indicate standard deviation of losses across folds. References for the stand-alone NN classifier loss, the entropy \(H\) of the adversary prior, and the ideal, effective loss are shown.