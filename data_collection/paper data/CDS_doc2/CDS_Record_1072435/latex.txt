###### Abstract

This note gives a summary of the lessons learned from the current generation of LHC optical links as installed for the ATLAS SCT and pixel detectors and the CMS tracker. A comparison of the costs for the different optical links systems is given. A discussion of the quality of the installed links and the methodology for long term monitoring of the links is given. A description of the technology choices and the reasons for these choices in the different systems is given. This historical summary is used to draw many important conclusions for the optical links to be used for SLHC.

**Joint ATLAS/CMS NOTE**

**ATL-COM-ELEC-2007-001**

**CMS-IN-2007/066**

**Joint ATLAS-CMS Working Group on Opto-Electronics for SLHC**

**Report from Sub-Group A**

**Lessons Learned and to be Learned from LHC**

**K.K. Gan**

_Department of Physics, The Ohio State University, Columbus, OH, USA_

Francois Vasey

_CERN, Geneva, Switzerland_

Tony Weidberg

_Department of Physics, Oxford University, U.K._

**V3.8 12/11/07**Introduction

Optoelectronics has been used at an unprecedented industrial scale for LHC detector readout and control. During the complex R&D phase which preceded the construction of the experiments, very little know how was available in the community to guide the designers. This was due to the lack of experience with large optical systems and resulted in long test campaigns and difficult decision processes in many cases. Now that the data transfer systems have finally been built and installed by all experiments, it is important to share and critically review the gathered experience and draw if possible some lessons for the future.

To this purpose, the joint ATLAS-CMS working group on opto-electronics for SLHC was formed in 2005. In September 2006, the group decided to focus its collaborative effort on three topics:

1. Lessons learned and to be learned. Collect info on successes and mistakes of the groups involved in building the optical links for the present detectors. Follow up on the technology choices made over 10 years ago. Produce a transparent account of the costs incurred. Create a repository for all publications. Monitor and follow up the performance and ageing of the installed links.
2. Radiation hardness and reliability of opto-electronic components Establish common procedures and common ways to represent the irradiation test data, share facilities and coordinate irradiation runs, avoid redundant tests and share results.
3. Common optical link reference test bench Define a reference test system for multi-gigabit/s optical links. Define test procedures and evaluation criteria. Specify the interface to the links to be tested. Develop hard, software and FPGA-IP blocks. Purchase test equipment and build reference test bench. Test proposed SLHC links on common reference bench and evaluate with common criteria.

This document reports on the results of the subgroup working on topic a: lessons learned and to be learned. It summarizes in an informal way several discussions among ATLAS SCT, ATLAS pixel and CMS tracker optical link designers. It is neither complete nor exhaustive, but highlights in a rather pragmatic way some of the salient conclusions from these meetings, in the hope of providing some guidance to the groups who will design optical links for the next generation of experiments.

In section 2 of this report, the three optical systems under review are briefly described in order to allow unfamiliar readers to understand some of their particularities. A cost comparison of the three systems is then presented in section 3; in the mid-1990's, estimated optical link cost figures were frequently presented when comparing options or even to justify technology choices. A posteriori, we demonstrate that these cost figures were systematically underestimated and often proved to be an unreliable basis to take decisions. Section 4 describes the quality of the systems as built. Impressive statistics on the quality of the installed links are shown, but careful monitoring will be required in future years to assess the availability and overall reliability of the systems in operation and measure the evolution of the fraction of dead and problematic channels. Section 5 reviews the different technology choices made by ATLAS and CMS under the light of our production and installation experience. This is of particular relevance to the SLHC designers who are now drafting the specifications for future systems. This is followed by the summary and conclusions in section 6. After section 7 that lists the references, section 8 points to several of the documents published by ATLAS and CMS. Finally, an appendix illustrates the arguments behind the discussion on technology choices with more details.

## 2 Systems Description

### CMS Tracker

The central tracker of CMS is comprised of \(\sim\)10 million silicon microstrips arranged around the proton interaction point at the centre of CMS. Data generated by the silicon detector modules must be sent to a remote counting room \(\sim\)65 m away, while timing, trigger and control (TTC) information must be passed in both directions between detector and counting room. The CMS tracker Readout and Control System is shown in Fig. 1.

Data from the silicon microstrips are processed by the APV front-end ASIC, which amplifies the signal, samplesit at the 40 MHz LHC bunch-crossing frequency and stores it in an analogue pipeline pending a trigger. Upon receipt of a Level 1 Trigger the data are time-multiplexed (256:1) and transmitted over an analogue optical link to the counting room, where the received data are digitised and formatted on the Front-End Driver (FED) VME board before being sent onto the higher-level Data Acquisition (DAQ) system. The system requirement is for an analogue data transmission system capable of pulse amplitude modulation at 40 Msamples/s with 8-bit resolution and 1% non-linearity.

The CMS tracker control system uses a token-ring-like architecture with a master control node (the Front-End Controller, FEC) located in the counting room and several Communication and Control Units (CCUs) located on the larger mechanical sub-structures of the tracker. Clock and control data are transmitted optically from the FEC to the front-end, passed sequentially around the ring of CCU modules electrically, then re-transmitted back to the FEC via the digital optical link. The digital optical link therefore carries both 40 MHz clock and 40 Mbits/s digital control data. The electrical on-detector ring allows the number of optical control links to be reduced. Redundancy is provided through full duplication of the electrical and optical signal paths in the control system.

The analogue optical readout system operates single-mode at 1310 nm wavelength. The custom-designed laser driver ASIC (LLD) directly modulates the edge-emitting laser diode drive current to achieve light amplitude modulation. Single fibres from the pig-tailed lasers are connected at the periphery of the tracker via small form-factor MU-type single-way connectors to a fan-in, which merges single fibres into a 12-fibre ribbon. There is a second break-point within the CMS detector where the transition to a rugged multi-ribbon cable (8\(\times\)12-fibre ribbons/cable) is made via 12-channel MFS-type array connectors. In the counting room each ribbon connects directly to a 12-channel analogue optical receiver (ARx) module on the FED.

Digital control and timing information generated on the FEC is output by the transmitter half of a 4-channel digital transceiver. After passing through an identical fibre system to the analogue link, the data are detected by pig-tailed InGaAs photodiodes and recovered by a custom-designed digital receiver ASIC (Rx40). Data are returned to the FEC via the same transmitting components used in the analogue link after passing around the control ring. Data are received at the FEC by the receiver half of the digital transceiver. Additional details are available in [1].

An 11,000 channel 800 MHz digital optical link with the same point-to-point architecture using identical or similar components has been implemented for the readout of the CMS ECAL [2].

Figure 1: Overview of the CMS tracker Readout (top) and Control (bottom) Systems with the optical links transmitting readout data from the detector to the counting room and control data between the counting room and the detector.

### ATLAS SCT

The ATLAS SCT consists of 6.2 million silicon microstrips arranged in cylinders and disks around the interaction region. As for CMS, data generated by the silicon detector modules must be sent to a remote counting room. TTC data is sent from the counting room to the front-end modules.

The communication between each SCT module and the off-detector data acquisition system is made by individual optical links. The system is illustrated schematically in Figure 2, with the upper part of the figure showing the data links and the lower part the TTC links. The links are based on GaAs Vertical Cavity Surface Emitting Lasers (VCSELs), emitting light around 850 nm, and epitaxial Si _p-i-n_ diodes.

#### Data links architecture and components

The binary data from each channel of an SCT module are stored in a pipeline memory of the ABCD3TA ASIC and those corresponding to a first level trigger signal are read out. The data from each side of the module are read out serially via a "master" ABCD3TA. Two data links operating at 40 Mbits/s transfer the data from the two master ABCD3TA ASICs of each module to two channels of a custom ASIC, the VDC which drives two VCSEL channels. The VDC, developed specifically for the SCT project, translates the approximate LVDS signal produced by the ABCD3TA into the drive signal required to operate the VCSEL. The VCSEL is contained within the on-detector opto-package, where the light is coupled into a step index multi-mode (SIMM) custom optical fibre with a pure silica core (50 \(\upmu\)m diameter) to ensure radiation hardness. The data are sent in non-return-to-zero (NRZ) format to a Back of Crate (BOC) card in the counting room which provides the interface between the optical signals and the off-detector electronics in the SCT Readout Driver (ROD). In the BOC, Si _p-i-n_ diode arrays provide electrical signals that are discriminated by another custom SCT ASIC, the DRX-12, which provides the LVDS data used in the ROD.

Some redundancy is built into the data links in that two independent links are provided for each SCT module. In normal operation, each link reads out one of the sides of the module, but if one link fails then all the data can be read out via the working link. The redundancy mode reduces the available bandwidth, but this will not cause any loss of data at the expected rates.

Figure 2: Overview of the ATLAS SCT Tracker Readout (top) and Timing Trigger and Control (bottom).

#### TTC links architecture and components

Optical links are also used to send the TTC data from the RODs to the SCT modules, as indicated in the lower part of Figure 2. Within the BOC, the custom BPM-12 ASIC uses biphase mark (BPM) encoding to send a 40 Mbits/s control stream in the same channel as the 40 MHz LHC bunch crossing clock. The outputs of the BPM-12 ASIC drive an array of 12 VCSELs which transmit the optical signal into 12 SIMM fibres. The signals are converted from optical to electrical form by the on-detector Si _p-i-n_ diodes within the opto-package indicated in Figure 2. Finally, these electrical signals are received by the SCT custom DORIC4A ASIC, which decodes the BPM data into a 40 MHz bunch crossing clock and a 40 Mbit/s control data stream, for transmission to the front-end ABCD3TA ASIC.

Redundancy is built into the TTC system by having electrical links from one module to its neighbour. If a module loses its TTC signal for any reason, an electrical control line can be set which will result in the neighbouring module sending a copy of its TTC data to the module with the failed signal

### 2.3 ATLAS Pixels

The ATLAS pixel detector consists of three barrel layers, three forward, and three backward disks which provide at least three space point measurements. The detector was constructed of 1,744 pixel modules, each containing a silicon sensor readout by 16 front-end chips controlled by a Module Control Chip (MCC). Each module contains 46,080 channels, yielding a total of ~80 millions channels in the system. The architecture of the optical links is similar to the SCT system and the only significant different is in the on-detector implementation as shown in Fig. 3.

#### Data links architecture and components

The binary data from each pixel are stored in a pipeline memory of the front-end ASIC and those corresponding to a first level trigger signal are read out by the MCC. After the event building, the MCC transmits the data at 80 Mbits/s via ~ 1 m of micro twisted-pair cable to a VDC to drive a VCSEL. The VDC is an updated version of the SCT ASIC but developed with IBM 0.25 \(\upmu\)m technology. As the SCT, the VDC converts the approximate LVDS signal into a single ended signal appropriate to drive the VCSEL. Unlike the SCT, we use a VCSEL array instead of a single channel device. Two 4-channel VDCs plus one 8-channel VCSEL array are mounted on one side of an optical module (opto-board). For the inner (B) layer, there are two more VDCs plus one more VCSEL array to handle the higher occupancy. The other side contains two 4-channel DORICs plus one 8-channel _p-i-n_ array (see below). There are a total of 272 opto-boards mounted on patch panels (PP0). The substrate of the opto-board is BeO with excellent thermal conductivity. Each array is housed inside a compact optical package coupled to a fibre ribbon terminated with a removable 8-channel MT ferrule. The fibre ribbon is the same radiation-hard SIMM fibre as used in the SCT. Each fibre ribbon is ~ 3 m long and extends from the PP0 to another patch panel, PP1, where it is terminated with a MT16 ferrule. This special ferrule was used to couple to two 8-channel ribbons to satisfy the stringent space constrain. Beyond PP1, there is another ~ 4 m of SIMM fibre before it is fusion spliced to 70 m of graded index (GRIN) fibre to the counting room. As in the SCT, the data are sent in the NRZ format to a BOC card and the rest of the system is identical to the SCT. Unlike the SCT, there is no redundancy in the data links.

Figure 3: Architecture of the optical links of the ATLAS pixel detector.

#### TTC links architecture and components

Optical links are also used to send the TTC data from the RODs to the pixel modules as for the SCT. The DORIC used is an updated version of the SCT ASIC but developed with IBM 0.25 \(\upmu\)m technology. The decoded clock and TTC data are transmitted to the MCC via micro twisted-pair cables. As noted above, two 4-channel DORICs plus one 8-channel \(p\)-\(i\)-\(n\) array are mounted on one side of an opto-board. The fibre ribbons used are identical to those for the data links. Unlike the SCT, there is no redundancy in the TTC system as for the data links.

## 3 Cost

It is instructive to compare the cost of the opto-links for the three systems in view of the SLHC upgrade. However, there is a large uncertainty in estimating the cost and a cost difference of 10-20% should not be viewed as significant. The manpower used is even more difficult to estimate and an uncertainty of up to a factor of two is possible. Table 1 summarizes the production and development cost. The production cost is for material only. It is evident that all links are expensive, \(\sim\)300 CHF/link. The development incurs another 10-25% of additional cost. As expected, this cost is highest for the system with the smallest number of links. However, it should be emphasized that the development costs for the three systems are not always precisely known.

The above production cost includes 30-40% of overproduction to cover the assembly yield, test systems, prototypes, installation yield, and unforeseen problems. The final spares are usually down to a few percent. This overproduction cost was usually underestimated in the initial cost estimate.

The breakdown of the material cost is shown in Table 2. About 70% of the cost is in the active components and the rest in passive components (cables and connectors). However, the latter fraction is significantly higher for the ATLAS pixel detector due to the high cost of connectorization.

The manpower deployed in the R&D, production, and installation of the opto-links are listed in Table 3. It is evident that the R&D effort is comparable to the production. The installation effort accounts for 10-20% of the total manpower consumed. This was usually underestimated or neglected in the initial estimate of the project cost. The manpower cost per link was similar for the ATLAS SCT and the CMS tracker. The cost is significantly higher for the ATLAS pixel detector because of the smaller number of links produced. Overall 200 staff-year (SY) were used over 11 years to develop and install the three systems from 1996 to 2007. As an order of magnitude estimate 1 SY of effort is required to develop 300 links.

Given the large material and manpower cost, we should reduce the number of links by maximizing the bandwidth usage and sharing as much R&D effort as possible in future systems.

\begin{table}
\begin{tabular}{l r r r}  & ATLAS Pixel & ATLAS SCT & CMS tracker \\ Quantity & 4,144 & 12,264 & 42,800 \\ Production cost (CHF) & 1,937k & 3,486k & 12,597k \\ Development cost (CHF) & 478k & 250k & 2,000k \\ Production cost/link (CHF) & 467 & 284 & 294 \\ Development cost/link (CHF) & 115 & 20 & 47 \\ \end{tabular}
\end{table}
Table 1: Summary of the production and development cost for the opto-links of the three systems.

\begin{table}
\begin{tabular}{l r r r}  & ATLAS Pixel & ATLAS SCT & CMS tracker \\ Fibre, connectors \& cable & 176 & 91 & 67 \\ On-detector opto-package & 58 & 74 & 118 \\ Off-detector arrays TX \& RX & 117 & 36 & 80 \\ Opto ASICs \& packaging & 13 & 16 & 15 \\ Opto-hybrid/flex & 103 & 67 & 14 \\ Cost/link & 467 & 284 & 294 \\ \end{tabular}
\end{table}
Table 2: Breakdown of the material cost (CHF) per link for the opto-link production of the three systems.

Table 3 Summary of the staff-years (SY) deployed in the R&D, production, and installation of the opto-links of the three systems. Also included is the number of links produced per SY.

\begin{tabular}{l r r r}  & ATLAS Pixel & ATLAS SCT & CMS tracker \\ Quantity & 4,144 & 12,264 & 42,800 \\ R\&D (SY) & 24 & 17 & 50 \\ Production (SY) & 18 & 23 & 55 \\ Installation (SY) & 4 & 5 & 10 \\ Total (SY) & 46 & 45 & 115 \\ Links/SY & 90 & 273 & 372 \\ \end{tabular}

## 4 Quality

The current quality of the links can be quantified by the fraction of dead and problematic channels. This section gives these numbers for the different systems for the current status in summer 2007. It also gives brief explanations for these problems and some conclusions about how to avoid them in future. Brief summaries of the methods that will be used to monitor the long term performance of the different systems are also given.

### CMS Tracker

#### Fraction of dead links: 0.04%

#### Fraction of problematic links: 0.38%

The quantity of optical channels installed in the three CMS tracker sub-detectors (tracker inner barrel/disk TIB/TID, tracker outer barrel TOB, tracker end caps TEC) is shown in Table 4 below. This quantity grows as one goes from pig-tails to ribbons and from ribbons to multi-ribbon cables since the system architecture did not achieve 100% cable fill factors. As a result, about 20% of the fibres in the CMS tracker are dark.

Table 4 Dead and problematic optical channel distribution in the four sub-detectors of the CMS tracker as of 28 Feb 2007.

\begin{tabular}{l r r r r r r}  & TIB/TID & TOB & TEC+ & TEC- & TOTAL & Fraction (\%) \\
**Installed** & & & & & & \\ _Opto-hybrids+pig-tails_ & 10152 & 12832 & 8128 & 8128 & 39240 & 100.00 \\ _Ribbon fanout channels_ & 11568 & 13488 & 9216 & 9216 & 43488 & 110.83 \\ _Multi-ribbon cable channels_ & 13056 & 14016 & 10176 & 10176 & 47424 & 120.86 \\
**Dead** & & & & & & \\ _Opto-hybrids+pig-tails_ & 7 & 1 & 2 & 6 & 16 & 0.04 \\ _Ribbon fanout channels_ & 0 & 0 & 0 & 0 & 0 & 0.00 \\ _Multi-ribbon cable channels_ & & & & & 0 & 0.00 \\ \hline _Total dead channels_ & 7 & 1 & 2 & 6 & 16 & 0.04 \\ \hline _Problematic_ & & & & & & \\ _Opto-hybrids+pig-tails_ & 26 & 0 & 48 & 4 & 78 & 0.20 \\ _Ribbon fanout channels_ & 4 & 5 & 32 & 36 & 77 & 0.18 \\ _Multi-ribbon cable channels_ & & & & & 0 & 0.00 \\ \hline _Total problematic channels_ & 30 & 5 & 80 & 40 & 155 & 0.38 \\ \end{tabular}

#### Explanation of dead channels:

Broken fibres due to mishandling during detector construction.

#### Explanation of problematic channels:

Channels with unexpectedly high attenuation due to dirty connector or fibre break, kink or bend.

Broken ribbon channels were bypassed or repaired with no effect on the final system thanks to the availability of redundant dark channels.

Channels with high attenuation will have little effect on final system performance thanks to gain equalization possibilities at the front-end (laser driver with selectable gain) and at the back-end.

#### Methods for long term monitoring of data links:

The analogue nature of the CMS readout link allows for easy monitoring of the transmitter operating point and link power budget. Automatic monitoring routines based on the measurement of the analog amplitude of test pulses generated by the APV front-end chip have been written and embedded into the CMS-SW framework. They will provide a homogeneous assessment of the link performance across the 3 CMS tracker sub-detectors and of its evolution with time.

Spare fibre channels have been optically looped back at the front-end to monitor from the back-end the darkening of the fibre under irradiation.

#### Methods for long term monitoring of TTC links:

The CMS tracker control system is a closed ring. The performance of the ring is constantly monitored and the occurrence of lost tokens is flagged, allowing for an early warning in case of link degradation. Bias point and gain of the front-end transmitter are user-selectable.

#### Lessons learned: CMS Tracker

1. Avoid fibre pig-tails.
2. Do not allow excessive fibre-slack without corresponding management scheme.
3. Use ruggedized ribbon/fibre only.
4. Avoid simplified and/or compact connectors which are difficult to dismount and clean.
5. Develop and distribute fibre-test tools which allow on-line channel quality testing, providing immediate feedback during construction.

### 4.2 ATLAS SCT

#### Sub-system: Data links

#### Fraction of dead links: 0.8%

##### Fraction of problematic links: 0.6%

##### Explanation of dead channels:

The main cause of dead links is believed to be due to low level ESD during the manufacture. This low level damage allowed the VCSELs to pass the manufacturer's QA and the QA at the assembly sites. The total operational time for these tests was about 30 minutes. However some of the VCSELs have died during longer running periods after installation onto the structures, at which point it was no longer possible to exchange them. Some of the single fibres in black furcation tubing were broken during the 4 barrel assembly. The fibres had to be potted in the thermal enclosure feed thorough. Once the damage was detected it was impossible to access the damaged fibres. Some of the dead data links are due to damaged tracks on the Al/Kapton power tapes that supply the control voltage that sets the VCSEL current.

#### 4.2.1.2 Explanation of problematic channels

The main cause of problematic channels was due to "slow turn-on" VCSELs. The SCT uses NRZ data so it is sensitive to VCSELs whose power output increases significantly over a timescale of micro seconds. Those channels which have such severe slow turn-on behaviour so as to be not usable are counted as "dead" and those which have a less severe behaviour and can be operated with careful tuning of the RX threshold value are classified as problematic. Data links which work but which have an anomalously low value of the RX threshold are also classified as problematic.

#### 4.2.1.3 Methods for long term monitoring of data links

Perform scans of BER versus the threshold that is used to discriminate the signal in the DRX-12 receiver chip. For each links calculate the maximum (RXmax) and minimum value (RXmin) for which there were no bit errors. In principle the RXmax values would give the best monitoring of the system performance, however in practice the RXmax values are usually equal to the maximum DAC value so only the RXmin values can be used. The values of RXmin are correlated with the optical power of the VCSELs. The long term monitoring of the data links will be made by comparing the values of RXmin in a run with the values from a reference run. This is illustrated in Figure 4 which shows a comparison between data taken for the barrel after installation in the pit with QA data from RAL taken before the opto-electronics was mounted on the barrel. The mean value is less than 1.0 and is compatible with the expected attenuation of \(\sim\) 1 dB from the long SIMM fibre cable in the pit. There is an asymmetric tail in the low side of the distribution which suggests that \(\sim\)1% of the VCSELs might be damaged. The long term monitoring will also involve looking at the variation with time of other parameters like the mean value of RXmin, the number of VCSELs with low values of RXmax and the fraction of VCSELs showing severe slow turn-on effects.

Figure 4 Ratios of RXmin from CERN pit and RAL QA data.

The VCSELs are expected to anneal sufficiently fast that only a very small decrease in light output is expected after irradiation. If necessary the VCSEL drive current can be increase from the default value of 10 mA up to a maximum of 20 mA to accelerate the annealing and to compensate for small threshold shifts.

#### Sub-system: TTC links

**Fraction of dead links: 0.2%**

**Fraction of problematic links: 37%**

#### Explanation of dead links

Broken fibres (see data links above). Broken tracks on Al/Kapton power tapes that supply the bias voltage for the on-detector _p-i-n_ diode.

#### Explanation of problematic channels

Poor coupling of light from VCSELs into the SIMM fibre, resulting in shorter than expected effective attenuation length in the fibre (can be resolved by production of better off-detector TX VCSEL arrays incorporating micro lens arrays).

#### Methods for long term monitoring of TTC links:

Measure the currents in the _p-i-n_ diodes in the on-detector opto-packages that receive light from the TTC fibres. This current is measured by the SCT power supply system so is easy to monitor. The distribution of _p-i-n_ currents from measurements after installation of the barrel SCT in the pit is shown in Figure 5. In order to operate the links with low BER a _p-i-n_ current greater than 0.02 mA is required. However to minimise SEU during high luminosity LHC operation a _p-i-n_ current greater than 0.1 mA is required. The long term monitoring of the TTC links can be performed by looking at this distribution as a function of time.

Figure 5: Distribution of the measured _p-i-n_ currents when operating the TX VCSELs at the nominal 10 mA current.

The responsivity of these _p-i-n_ diodes is expected to decrease by about 30% after a low fluence and then not show any further decrease.

#### 4.2.3 Lessons learned: ATLAS SCT

1. Better ESD precautions.
2. More longer term testing of the VCSELs at an earlier stage in the assembly to weed out any damaged devices.
3. Avoid all use of fragile single fibres on the detector.
4. Always used balanced codes.
5. Ensure that QA is performed for identical conditions to the final system.

### 4.3 ATLAS Pixel

#### Sub-system: Data links

##### Fraction of dead links: 0.06%

##### Fraction of problematic links: \(\sim\) 4%

##### 4.3.1.1 Explanation of dead links

One out of the 2032 links was found to be dead after the opto-boards became inaccessible on PP0. The link was replaced by moving the (Type0) cable to an unused channel on another opto-board. The failure is probably due to low level ESD during the production/installation and hence better ESD precaution is recommended.

##### 4.3.1.2 Explanation of problematic links

Some of the links (\(\sim\)1%) has shown sign of slow turn-on which result in some loss in the upper RX thresholds. This is not unexpected because we cannot reject all opto-boards with mild slow turn-on arrays. Some links (\(\sim\)3%) have limited range of the operation parameters and hence require manual tuning of the parameters. This is due to a combination of factors:

1. the limited dynamic range of the receiver on a BOC.
2. the slow tail in the signal from the Si _p-i-n_ diode in the RX plug-in.
3. the same VCSEL drive current has to be set for all channels on an opto-board.

The first two factors mean that if the VCSEL is too bright then the error free range of RX thresholds is greatly reduced. The third factor means that if there is a large spread in the light output, the VCSEL drive current cannot be reduced to solve this problem for the bright channel(s).

##### 4.3.1.3 Methods for long term monitoring of data links

The links will be monitored by adapting the procedure used by the SCT.

##### Sub-system: TTC links

##### Fraction of dead links: 0.06%

##### Fraction of problematic links: 0%

##### Explanation of dead link

One out of the 1744 links was found to be dead after the opto-boards became inaccessible on PP0. The link was replaced by moving the (Type0) cable to an unused channel on another opto-board. Major software changes are needed to use this link. The failure is probably due to a detached lead (cold solder) on an opto-pack. In the future, we should avoid the soldering of micro-leads (250 \(\upmu\)m width) to material with excellent thermal conductivity such as BeO.

##### 4.3.2.2 Methods for long term monitoring of TTC links

The links will be monitored by adapting the procedure used by the SCT.

#### Lessons learned: ATLAS Pixel

1. Better ESD precautions.
2. Longer term testing of the VCSELs at an earlier stage in the assembly to weed out any damaged devices.
3. Ensure that QA is for identical conditions to final system.
4. Always use balanced codes.
5. Avoid soldering of micro-leads to material with excellent thermal conductivity such as BeO.

The architecture of the opto-links of the ATLAS pixel detector allows the replacement of broken and problematic links until very late in the detector integration. This is made possible by using the micro twisted-pair cables which allow the links to be installed at a more accessible location, \(\sim\) 1 m away from the detector. We should try to preserve this architecture at SLHC if possible.

## 5 Technology Choices

The ATLAS and CMS Tracker and Pixel detectors made very different technology choices for their optical systems. The main conclusions and lessons learned from a review of these choices are summarized in **Table 5** below. Details are given in the appendix.

Table 5 Summary of technology choices and lessons learned.

\begin{tabular}{|p{142.3pt}|p{142.3pt}|} \hline \multicolumn{3}{|p{142.3pt}|}{**Components**} \\ \hline
**Laser type** & Edge Emitting Lasers (EEL) at 1310 nm excellent for analogue readout. Sufficiently radiation hard for LHC but would have large threshold shifts if used at SLHC VCSELs ok for digital readout. VCSELs show smaller threshold shifts after SLHC fluences. More consideration of transverse mode structures, thermal resistance and forward voltage drop required. VCSELs at 1310 nm would be an interesting option for SLHC (radiation hard, low power dissipation and high bandwidth in SM fibres). \\ \hline
**Wavelength choice and fibre type.** & Wide range of commercial 1310 nm Single Mode (SM) fibres are radiation resistant. Bandwidth of SM fibres much larger than Multi Mode (MM) fibres. More difficult to find array receivers at 1310 nm. \\  & 850 nm MM fibre compatible with VCSEL and cheap. 850 nm is compatible with cheap radiation hard Si _p-i-n_ diodes. \\  & Radiation hard SIMM fibre was available but only from one source. Bandwidth of SIMM fibre is very low. Radiation tolerant 850 nm GRIN fibre also available for mixed SIMM/GRIN applications as in pixels. The radiation resistance of high bandwidth GRIN fibre to SLHC doses at 850 nm wavelength needs to be confirmed. \\ \hline \multicolumn{3}{|p{142.3pt}|}{**Packaging**} \\ \hline
**Lasers and _p-i-n_ diodes** & For on-detector, a miniature pig-tail-less Optical Sub-Assembly-based package would be ideal. For off-detector, arrays would be preferred. \\ \hline
**Connectors** & Use standard connectors. Cleaning and inspection tools must be made available (together with documented procedures) at the same time as the cables are installed. \\ \hline
**Optical cables** & There were serious problems with optical cables in both CMS and ATLAS SCT. Optical cables must be tested in conditions identical to the ones they will experience during operation, installation and also manufacturing. The lengths tested must be similar to the ones of the final objects \\ \hline
**Opto-hybrid** & Should have both electrical and optical connector interfaces. Minimise number of flavours (aim for one flavour). \\ \hline \end{tabular}

**AsIC packaging**

Packaged chips should be used if space is available. The package choice is to be made in close collaboration with the hybrid designers. However it is possible to use bare die if they are potted or protected with a plastic cover.

**Architecture**

**From single to ribbon, redundancy**

Use ruggedized cables throughout the system, even in the case of space constraints. Avoid locating fibres in inaccessible regions of the detector.

**Dark fibres and spare cables**

CMS installed a large fraction of dark fibres and spare cables which were not needed.

The SCT used a more cost-effective solution; 2 spare cables were purchased but not installed. These were not yet needed in the experiment.

The pixel detector installed 4 spare cables and three of these were used because of damage to ribbons during installation.

**System capacity**

Fully use the bandwidth of the fibre to minimise the number of fibres required.

**Environmental Effects**

**Radiation**

All components must be qualified as being radiation hard but it might be possible to limit the effort in validation of production lots, based on positive experience at LHC.

**Mechanical**

Testing for mechanical reliability was very difficult and problems were seen. Refrain from using non fully-qualified products.

**Magnetic field**

Strictly adhering to the rule of no magnetic components, severely limits the packaging options. We should investigate if small magnetic packages would be acceptable.

**Reliability and Quality**

**Qualification**

Include a reasonable qualification duration in the project schedule (i.e. 6 months).

**Production supervision**

Include production ramp-up and ramp-down time in the schedule (i.e. plus 6-12 months)

**Component tracking**

Should be implemented from one centralised DB where the performance history and connectivity of all active components can be tracked.

**Reliability**

A large effort was put into verifying lifetimes after irradiation. A longer term test of a significant fraction of the final system as was done for CMS gives additional confidence in the reliability.

**Miscellaneous**

**Modulation**

Analogue modulation in CMS required more development but works very well. However to achieve higher bandwidths need to use digital modulation. We should always use balanced code and we recommend to use standard digital modulation formats in the future.

**Installation**

Ensure sufficient number of test stations are available and budgeted for during construction phase

**Cables**

At SLHC, either reuse the existing cables or minimise the number of cables by fully utilising the fibre bandwidth. From the SCT experience more care is needed in the evaluation of the lengths of fibre cables.

**Patch panels**

Avoid non-standard connector solutions. Avoid fibre pig-tails and have a clean fibre connector break at the end of the barrels and disks.

**Monitoring**

Performance monitoring is more straightforward in an analogue system. However from receiver threshold scans, the performance of digital links can also be monitored. Envisage self-test modes at SLHC.

Lessons Learned Summary

The first lesson learned is that the costs of optical links for tracking detectors are large and they also require a large staff effort for development, production and installation. In order to minimise the costs for SLHC detectors we will need to reduce the number of links by significantly increasing the bandwidth used and to share as much as possible the R&D effort between experiments.

The quality of the installed links is generally very high. The QA for analogue links requires more testing and this resulted in the CMS system having very high quality. However there were problems with a small fraction of the links for all systems as discussed in section 4.

The QA must test all the components in exactly the way they will be used in the final system. Any attempts to simplify the tests which violate this rule can result in problems only being discovered during final assembly. An example of this is given by the "slow turn-on" VCSELs in the ATLAS SCT data links (see section 4.2.1.1). After the ATLAS pixel group were informed of this problem, they improved their QA but did not find any problems. However they did discover that some of the links suffered from slow turn-on problems when they installed the final links. This was due to the bevelled edges of the MT connectors in the final system (as opposed to flat for the MT connectors on the test fibres), allowed the connectors to be positioned closer to the VCSEL array. This illustrates how small details, can have a dramatic impact on the system performance.

It is essential that an extensive system test be carried out with the final components before the full production is started. If this is not done correctly, then problems with the system design will only be understood after it is too late to change anything. A classic example of this type of problem is given by the difficulty in setting the parameters for the ATLAS pixel data links.

The detailed arguments about the pros and cons of the different technology choices are given in the appendix and were summarized in section 5. It is clear from this summary that the apparently small details can make a big difference to the system performance. A good example of this is given by the fibre cables, which turned out to be problematic for both CMS and ATLAS SCT. For CMS there were problems with single fibres until the buffer material was changed and with the cables which required a long development time. For the ATLAS SCT, there are still problems with some of the fibre cables which are not completely understood.

Overall, the different technology choices made by the two experiments did not seem to have a major impact on system quality. It is rather the Quality Assurance programmes and their shortfalls which are usually at fault when problems appear.

Finally, many lessons are still to be learned from the commissioning and operation phases of the detectors. The need to carefully monitor the evolution of the performance of our optical links with time is recognized and should be remembered in the future.

## 7 References

* [1] J. Troska, G. Cervelli, F. Faccio, K. Gill, R. Grabit, A.M. Sandvik, F. Vasey and A. Zanet, Optical Readout and Control Systems for the CMS Tracker, IEEE Transactions on Nuclear Science, Vol 50 No 4, 2003, pp1067-72.
* [2] J. Grahl, Optical Data Links in CMS ECAL, Proceedings of the 10\({}^{\text{th}}\) Workshop on Electronics for LHC Experiments, Boston, 2004, CERN/LHCC/2004-030, pp158-63.
* [3] A. Abdesselam et al., The Optical Links of the ATLAS SemiConductorTracker, accepted for publication in JINST.
* [4] K.E. Arms et al., Optical Links of the ATLAS Pixel Detector, Nucl. Instr. Meth. A554 (2005) 458.

## 8 List of Publications

* [1] CMS optical links publication list, CMS-TK-GP-0010, [https://edms.cern.ch/document/862663/1](https://edms.cern.ch/document/862663/1).
* [2] ATLAS SCT and pixel optical links publication list, EDMS, ATL-IS-QN-0001, [https://edms.cern.ch/document/863278/1](https://edms.cern.ch/document/863278/1).
* [3] Report from sub-group B on optical system irradiation guidelines: [https://edms.cern.ch/document/882783/2.6](https://edms.cern.ch/document/882783/2.6).
* [4] Report from sub-group C on optical link evaluation criteria and test procedures: [https://edms.cern.ch/document/882784/2.0](https://edms.cern.ch/document/882784/2.0).