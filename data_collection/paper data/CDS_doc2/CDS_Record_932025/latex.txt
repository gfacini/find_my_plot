###### Abstract

_Comparison of probability density estimation function with range searching algorithm (PDE-RS) versus neural network (NN) and Support Vector Machine (SVM) approaches in identification of the hadronic decays of \(\tau\) leptons_.

ATLAS Physics Communication

ATL-COM-PHYS-2006-0xx

**28 February 2006**

**Comparison of multivariate methods in identification of hadronic decays of \(\tau\) leptons**

**Tadeusz Szymocha1, Marcin Wolter**

Footnote 1: Supported in part by the Polish Government grants KBN 1 P03 09127 (years 2004-2006) and 620/E-77/SPB/CERN/P-03/DZ110/2003-2005

_IFJ PAN_

_31-342 Krakow, ul. Radzikowskiego 152, Poland._

## 1 Introduction

Optimization of the hadronic \(\tau\) identification reconstructed with tau1P3P algorithm [1, 2] with the PDE-RS method was presented in [3]. The aim of the this note is to test other multivariate analysis methods like Neural Network and Support Vector Machine on the same data and using the same discriminating variables. The results of these three methods are compared. The fact, that obtained results from all three methods are comparable, indicates that all available information from the set of discriminating variables is used and that probably the discrimination limit is reached.

## 2 Physics

Lepton \(\tau\) decays predominantly into small number of charged and neutral pions, that form well collimated jet. This jet is very narrow comparing to typical QCD jet and it is characterized by 1 or 3 tracks in the Inner Detector (left by charged pions). Thus, candidates for \(\tau\) jets are called 1 prong (tau1P) or 3 prong (tau3P) respectively. The new algorithm for studies of hadronic decays of \(\tau\) leptons was developed recently [1, 2]. It finds \(\tau\) candidates starting from relatively high \(p_{T}\) tracks in the inner detector and calculates discriminating variables from energy deposition in an electromagnetic and hadronic calorimeter in a narrow cone (\(\Delta R=0.2\)) selected by the track position at vertex. The basic four are:

* Number of strips, \(N^{\tau}_{strips}\), with energy deposition above a threshold in strip layer of the electromagnetic calorimeter* The width of the energy deposition in strips, \(W^{\tau}_{strips}\), calculated as the variance of the \(\eta\) coordinate, weighted by the transverse energy deposition in a given strip
* Fraction of the transverse energy deposited in the radius \(0.1<\Delta R<0.2\) with respect to the total energy in the cone \(\Delta R=0.2\).
* Electromagnetic radius, \(R^{\tau}_{em}\), weighted by the transverse energy deposition for given cell

In addition, as an identification variable we use also:

* Ratio of the track transverse momenta and energy deposited in the hadronic calorimeter in the vicinity of the track, \(\frac{E^{\alpha_{H}}_{r}g^{HAD}}{p^{\tau\epsilon\epsilon}_{T}}\), and for 3 prong data we take sum of \(p_{T}\) of all three tracks.
* Ratio of energy deposited in a radius \(0.2<\Delta R<0.4\), \(\frac{E^{\alpha_{T}}_{r}E^{\alpha_{T}}_{r}g^{HAD}}{E^{\alpha_{T}}_{r}}\), which represents isolation criteria.

The discriminant variables are shown in Fig. 2 for 1-prong and in Fig. 3 for 3-prong data. For more details, see [2, 1]. It is worth to mention, that shape of normalized signal and background distribution of \(N^{\tau}_{strips}\) or \(W^{\tau}_{strips}\) look very similar. This is due to reconstruction procedure of tau1P3P algorithm, that already filter QCD jets and passes only candidates similar to signal.

## 3 Data samples

The signal input consisted of a true \(\tau\) candidates from \(q\overline{q}\to Z\to\tau\tau\) and \(q\overline{q}\to W\to\tau\nu\). As a background we considered a sample of QCD di-jet events with \(p_{T}>35\,GeV\). All data were produced with Data Challenge 1 (DC1) settings and release 7.8.0 without pile-up and electronic noise. The \(\tau\) candidates for signal and background were found during reconstruction phase with tau1P3P algorithm. More details on the used data samples can be found in [1]. We have used 18.060 true 1 prong \(\tau\)-jets from Z and W boson decays and 9.737 from QCD jets. The corresponding statistics for 3 prong candidates were: 4.536 for true \(\tau\)-jets and 17.684 for QCD jets.

## 4 Pde-Rs

The standard probability density estimation technique (PDE) was used for optimization of the tau1P3P algorithm [3]. Method and implementation is based on publication [4]. The technique combines the observables into a single one, called discriminant, on which then a cut to separate signal from background is applied. Calculation of the discriminant is based on sampling the signal and background densities in a multi-dimensional phase space built from variables described in section 2. Taking number of signal events \(n_{s}\) and number of background events \(n_{b}\) in a small volume V(\(\overrightarrow{x}\)) around point \(\overrightarrow{x}\) in our 6-dimensional space, a discriminant defined as: \(D(\overrightarrow{x})=\frac{n_{s}}{n_{s}+c\epsilon n_{b}}\) is a good approximation of probability that given candidate is comming from signal event, if total number of simulated events is equal to constant c times total number of background events. The event counting is done using multi-dimensional binary trees. The data were split into two parts. One used for training and other used for analysis. As stated in [4], this method is supposed to give comparable results to neural network analysis what was checked in this note.

Signal efficiency is defined as a ratio of accepted to all signal events \(\varepsilon=N_{accepted}/N_{all}\) and background rejection as a ratio of rejected to all background events \(R=1-\epsilon_{b}=N_{rejected}/N_{all}\). For the comparison, we have chosen three configurations of so called "working points" defined by signal efficiency respectively 70%, 80% and 90%. This represent "identification efficiency" and does not include "reconstruction efficiency" with which candidate is build, see [1] for more details. When applied to the 1 prong data PDE-RS method for 80% signal identification efficiency gives 75% background rejection (see Tab. 1 and Fig. 5). For 3 prong data corresponding efficiency yields 72% (see Fig.10 and Tab. 3).

## 5 Neural Network

The neural network is a non-linear discriminating method (we refer reader to eg. [5] for detailed description of the neural network techniques). The Stuttgart Neural Network Simulator [6] was used in the analysis.

To each neuron \(j\) in the hidden layer \(n\) inputs \(x_{k},k=1,..,n\) and one output variable (the answer of the neuron) \(z_{j}\) are associated. For the first hidden layer the inputs are the discriminating variables, for next layers the inputs are the outputs of the preceding layer. All input variables are normalized to be within the range \([-1,1]\).

The architecture of the network is optimized to give the proper classification of signal and background and to avoid over-fitting at the same time. The neural network is built with 6 input nodes and two layers of hidden nodes, each with 10 nodes. After applying the skeletonization pruning algorithm [7] the number of hidden nodes was reduced down to 5 in each of two layers. The skeletonization algorithm is based on the Taylor expansion of the Neural Network around minimum and eliminating the not contributing units. The method is described in detail in [8]. Finally we obtain the following network architecture (Fig. 1):

* an input layer with 6 input nodes corresponding to 6 discriminating variables;
* two internal hidden layers, each containing 5 nodes;
* an output layer containing a single neuron, since the output is a single discriminating variable.

Figure 1: _Schematic view of the neural network (left) and the error function \(E/N_{\text{events}}\) as a function of the training cycles for the training and verification samples (right). The training was stopped after \(300\) training cycles to avoid network overtraining._

Figure 2: Distributions of discriminating variables for tau1P candidates: number of strips \(N_{strips}^{\tau}\) with energy deposition above a threshold (upper left), width of the energy deposition in strips \(W_{strips}^{\tau}\) (upper right), fraction of the transverse energy deposited in the radius \(0.1<\Delta R<0.2\) with respect to the total energy in the cone \(\Delta R=0.2\) (middle left), electromagnetic radius, \(R_{em}^{\tau}\), weighted by the transverse energy deposition for given cell (middle right), fraction of track transverse momenta and energy deposited in the hadronic calorimeter in the vicinity of the track \(\frac{E_{T}^{chrgHAD}}{p_{T}^{track}}\) (bottom left) and ratio \(\frac{E_{T}^{chrgHAD}+E_{T}^{other\,HAD}}{E_{T}^{cal\,iso}}\) (bottom right). In the plots the solid line denotes the distributions for signal samples, the dotted one for background. Histograms are normalized to give integral equal to 1. For more details, see text.

[MISSING_PAGE_EMPTY:5]

The neuron sums up the input variables \(y_{k}\), weighted by a factor \(w_{jk}\), plus a threshold \(\theta_{j}\). This defines the signal \(Z_{j}\):

\[Z_{j}=\sum_{k=1}^{N}w_{jk}y_{k}+\theta_{j}. \tag{1}\]

The output of the neuron is a function of \(Z\): \(z_{j}=a(Z_{j})\), where \(a\) is called the activation function, and is chosen to be of the form \(a(x)=\frac{1}{1+\exp-(Z_{j})}\) (logistic function). The training phase of the neural network consists in determining the weighting factors \(w_{jk}\) and the thresholds \(\theta_{j}\). This is done by minimizing the following error function:

\[E=\frac{1}{2}\sum_{i=1}^{n}\left(X_{NN}^{i}-t_{1}^{i}\right)^{2}, \tag{2}\]

where \(t_{1}^{i}\) is the expected output (0 for background, 1 for signal), \(X_{NN}^{i}\) the actual value returned by the network and \(n\) is a number of events used for training.

The training is performed using half of the available signal data and half of the background sample. The remaining data are used to estimate the signal detection efficiency and background rejection. It is also used as a verification sample to check, whether the values \(E\) obtained for training and verification samples are similar. This gives a useful information when to interrupt the network training (see Fig. 1). In this analysis training is stopped after about 300 training cycles to avoid over-learning. Distributions of the neural network output \(X_{NN}\) obtained for signal and background are shown in Fig 4.

The use of the trained neural network reduces the background by about 77% for 80% signal efficiency, the results compared with other techniques are shown in Fig. 5 and Tab. 1.

### Network pruning - reducing the number of input variables

It is interesting to check, whether all the six input variables are necessary and whether they indeed contribute to the performance of the neural network. Using again the skeletonization

Figure 4: _Normalized distributions of the discriminating function \(X_{NN}\) for signal (solid line) and background (filled histogram)._

algorithm, this time applied also to the input layer, we have obtained a network with two hidden layers: 7 nodes in the first layer and 5 in the second layer. From the input variables the third one was removed (fraction of the transverse energy deposited in the radius \(0.1<\Delta R<0.2\) with respect to the total energy in the cone \(\Delta R=0.2\)). The performance of the method remains unchanged (see Tab. 1 and Fig. 5).

According to distribution in Fig. 2 the rejected variable is a well discriminating one. However, its removal doesn't alter the network performance. It clearly indicates that information from this variable is strongly correlated with the remaining discriminating variables.

## 6 Support Vector Machine

Support Vector Machines (SVMs), developed by idea of Vapnik [9], are learning machines that can perform binary classification (pattern recognition) and real valued function approximation (regression estimation) tasks. Support Vector Machines non-linearly map their n-dimensional input space into a high dimensional feature space. In this high dimensional feature space a linear classifier is constructed. A detailed introduction to SVMs can be found in [10].

In our analysis the _libsvm_ software package was used [11] and a Gaussian kernel function was chosen. The data were divided into two subsamples, one for training and one for verification and calculating the background rejection. The performance of the SVM with radial kernel depends on two parameters, the width of the Gaussian kernel \(g\) and the cost parameter \(c\). A grid search in the space of these two parameters was performed (Fig. 6) and the optimal signal and background separation was found for \(g=1\) and \(c=256\).

The distribution for both training and verification samples are the same, which ensures us that we have no overtraining. The performance of the SVM is shown in Fig. 8 and in Tab. 1. The 80% background rejection for 77% signal efficiency is very similar to the one obtained using a Neural Network. In the region of low signal efficiencies SVM performs worse than other methods. We don't have any clear explanation for that, but we suspect, that the algorithm might have

Figure 5: _Comparison of three methods: PDE-RS (six input variables), neural network (six input variables) and a neural network with five input variables only. Performances of both neural networks are the same._

difficulties in the region, where nearly all events from one class (background) have to be rejected. Fig. 7 shows the distribution of the output value for signal and background.

Figure 6: A grid search in the space of two SVM parameters, the width of the Gaussian kernel \(g\) and the cost parameter \(c\). The parameters \(g=1\) and \(c=256\) are giving the best background rejection.

Figure 7: _Normalized distributions of the discriminating function obtained from the Support Vector Machine for signal (solid line) and background (filled histogram)._

[MISSING_PAGE_EMPTY:9]

[MISSING_PAGE_EMPTY:10]

[MISSING_PAGE_EMPTY:11]

Conclusion and Outlook

Comparison of three multivariate methods, PDE-RS, Neural Network and Support Vector Machine shows, that all of them give very similar results when applied to the \(\tau\) identification. This result is a prove, that most probably the obtained background rejection is close to the statistical limit. Information from six discriminating variables is fully exploit and no significant rejection improvement can be reached. It was also shown, that one of the input discriminating variables might be removed without effecting the performance of the method. We have shown, that after adding an additional discriminating variable \(E_{T}\) background rejection increased by about 2%.

It was also proven, that all three methods can be used in physics data analysis and are giving a similar performance. In our comparison Neural Network seems to give slightly better rejection than other methods. The advantage of the PDE-RS method is short computation time needed for both training and analysis phases together.

Another two classification algorithms were also applied for selecting hadronic decays of \(\tau\) leptons [12]. The first one was based on the Distribution Mapping Exponent (DME) [13], which is a type of a kernel method in which the sum of reciprocals of \(q\)-th power of the distances between the query point and all points from the training sample is used as the probability density estimate. The second algorithm is an improved neural network with switching units [14]. In our study we have used sub-sample of all avaiable data, so we did not pursue analysis to the level of direct comparison of the performance achievable with the NN approaches. However the background rejections reached by DME and, in lesser extend, by NN with switching units improve the PDE-RS results [3] to higher extend than a feed-forward network or SVM presented in this note. It might be a hint, that the optimized kernel algorithm is able to give the best signal and background separation for this peculiar problem.

A neural network described in this note will be used as a prototype discriminant in the implementation of the tau1P3P algorithm inside of the Athena framework [15].

## References

* [1] E. Richter-Was and T. Szymocha, _Hadronic \(\tau\) indentification with track based approach: the \(Z\to\tau\tau\), \(W\to\tau\nu\) and di-jet events from DC1 samples_, ATLAS Note ATL-PHYS-PUB-2005-005
* [2] E. Richter-Was, H. Przyssiezniak and F. Tarrade, _Exploring hadronic \(\tau\) identification with DC1 data samples: a track based approach_, Atlas Note ATL-PHYS-2004-030
* [3] L. Janyst and E. Richter-Was, _Hadronic \(\tau\) identification with track based approach: optimisation with multi-variate method_, ATLAS Note ATL-COM-PHYS-2005-028
* [4] T. Carli, B. Koblitz, "A multi-variate discrimination technique based on range-searching", Nuclear Instruments and Methods in Physics Research A 501 (2003) 576-588
* [5] C. Bishop, _Neural Networks for Pattern Recognition_, Oxford University Press 1999
* [6] A. Zell et al., [http://www-ra.informatik.uni-tuebingen.de/SNNS/](http://www-ra.informatik.uni-tuebingen.de/SNNS/)* [7] M.C. Mozer, P. Smolensky, _Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment_, in D. Touretzky (ed.), Advances in Neural Information Processing Systems 1, Morgan Kaufmann, San Monteo, California, pp. 107-115 (1989)
* [8] A. Zell, _Simulation Neuronaler Netze_ Munich, R. Oldenbourg Verlag, (2000)
* [9] Vladimir N. Vapnik, _The Nature of Statistical Learning Theory._ Springer, 1995
* [10] Christopher J.C. Burges _A Tutorial on Support Vector Machines for Pattern Recognition_ www.kernel-machines.org/papers/Burges98.ps.gz
* A Library for Support Vector Machines_ [http://www.csie.ntu.edu.tw/](http://www.csie.ntu.edu.tw/)\(\sim\)cjlin/libsvm/
* 12 p
* [13] M. Jirina, _Distribution Mapping Exponent for Multivariate Data Classification_, Proc. of the Eight World Multi-Conference on Systemics, Cybernetics and Informatics (SCI 2004), Orlando, Florida (USA), July 18-21, 2004, Vol. V., pp.103-108, CD ROM ISBN 980-6560-14-0.
* [14] P. Bitzan, J. Smejkalova and M. Kucera, _Neural Networks with Switching Units_, Neural Network World, Vol.4. (1995) 515-523
* [15] E. Richter-Was, L. Janyst and T. Szymocha, _The tau1P3P algorithm: implementation in Athena and performance for ATLAS CSC 2006 data samples_, ATLAS Note in preparation