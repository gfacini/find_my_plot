# Principles of Cost Sharing

For the Atlas Offline Computing Resources

Prepared by: R. Jones, N. McCubbin, M. Nordberg, L. Perini, G. Poulard, and A. Putzer

###### Abstract

The primary purpose of this document is to present the current ideas on the ATLAS offline computing model, to give indicative numbers on resources required, and to present first thoughts on possible cost sharing mechanisms. Discussion and reaction from the collaboration on this last point is particularly important. The numbers will evolve with time with a more detailed understanding of the detectors and of the physics programmes. Based on our present knowledge, estimates for the initial costs to be spent in the years 2006-2008 are given. (At the moment the official schedule for the construction of the accelerator, and of the experiments, is such that first physics data taking is expected in 2007.)

The level of funding is substantial, even if we extrapolate the rapidly falling prices of recent years and plan to buy everything at the very last moment. One of the major uncertainties is the estimation of the expected cost for the computing. If not mentioned otherwise, the results of the PASTA1 committee are used.

Footnote 1: PASTA, the Technology Tracking Team for Processors, Memory, Storage and Architectures, was set up by IT Division and the LHC Computing Board (LCB) to follow the progress of some of the basic technologies required for LHC. In 1996 a first report was issued: [http://wwwinfo.cern.ch/di/pasta.html](http://wwwinfo.cern.ch/di/pasta.html).

In the CERN LHC Computing Review the MONARC hierarchical model was used as basis for the resource and cost estimates for the ATLAS offline computing. The advent of the World Wide Grid triggered new ideas how to organize the offline computing in the form of a virtual worldwide distributed computing facility.

The complexity of the final system, in terms of number of boxes, networking, and organisation, is such that the construction of prototypes is necessary. These prototypes will be used to perform the Data Challenges, testing the ideas for the ATLAS world wide computing model.

Additional information related to this paper can be found on:

[http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/nch/ResPaper/appendices.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/nch/ResPaper/appendices.html)

## 2.) Executive Summary

The ATLAS World Wide Computing model has evolved significantly since the time the ATLAS Computing Technical Proposal was written2. The basic considerations that guided the ideas behind the present ATLAS Virtual World Wide Computing Model are that:

Footnote 2: ATLAS CTP, CERN LHCC/96-43 (1996)

* Every physicist in ATLAS must have the best possible access to the data necessary for the analysis, irrespective of his/her location.
* The access to the data should be transparent and efficient.

We should profit from resources (money, manpower and hardware) available in the different countries.
* We should benefit from the outcome of the Grid projects.

In Europe and US there is presently a huge research and development activity focused on distributed computing: the "Grid". The leading role and the massive participation of high-energy physics agencies are predicated on the assumption that the Grid will form the basis of the LHC computing.

The present model foresees a hierarchy of Regional Facilities and Centres (RC), with a Tier-0 located at CERN, a number of main regional facilities and lower level Tier's. The categorisation of the regional facilities is on the basis of the services they offer to the ATLAS community. The CERN computing system (Tier-0 + Tier-1) will be mainly dedicated to the data acquisition, the calibration of the detectors, the processing of the raw data and will also be part of the overall analysis facilities.

The proposed scheme envisages recording the 'raw data' (RAW), selected by the High Level Trigger (HLT), locally at CERN. A preliminary event reconstruction will be performed on all data at CERN, a few hours after data taking. For this processing, basic calibration and alignment data have to be available. The reconstructed data, called Event Summary Data (ESD), will then be replicated via the Wide Area Network (WAN), such that all data will be accessible by all ATLAS members at two ore more sites. The Monte Carlo productions will be done almost exclusively in the outside regional facilities. The simulated data will be distributed and shared (according to the analysis strategy) among the RCs.

In a common R&D project (MONARC3) the LHC experiments have performed tests and simulations to optimise their computing models. In the CERN LHC Computing Review4, the MONARC hierarchical model was used as the basis for the resources and cost estimates for the ATLAS offline computing using the basic numbers as described later in this document. The subsequent advent of the World Wide Grid has triggered new ideas how to organize the offline computing in the form of a virtual worldwide distributed computing facility. The CERN Council accepted in September 2001 a proposal5 for developing the LHC Computing Environment. The LHC Computing Grid Project (LCG) has been launched and will provide the platform for the coordination of the worldwide Grid projects with relations to the LHC experiments.

Footnote 3: MONARC PHASEII REPORT (CERN/LCB 2000-001); [http://monarc.web.cern.ch/MONARC/](http://monarc.web.cern.ch/MONARC/)

Footnote 4: CERN LHC Computing Review

Footnote 5: LHC Computing Environment

Footnote 6: [http://www.dante.net/geant/](http://www.dante.net/geant/)

The new ideas will be tested in a series of Data Challenges (DC's) to be performed during the Phase-1 of the LCG project (2001-2005). The experience gained during these exercises and in the various Grid projects will be used to formulate the ATLAS Computing TDR, which is due, according to the present planning, at the end of 2003. (This date is currently under review.) Modelling, using the MONARC tools, will help us to extrapolate the findings from the Data Challenges to the full system.

To enable physics analysis in a worldwide collaboration, good networking is a necessity. Today it is impossible to predict with any reliability the evolution of the cost and the performance of international networks up to the time of LHC running. As these are important parameters for the precise planning of an analysis scenario, we have to follow the developments and adjust our planning accordingly. During the last year the network bandwidth has significantly increased in Europe (GEANT project6) and between CERN and the USA (DATATAG project7). However, we have still some pending problems especially outside these regions.

Footnote 7: [http://www.datatag.org](http://www.datatag.org)

A precise cost estimate for the computing hardware is also impossible due to the uncertainties in both the requirements and the evolution of the technology and the market. A rough estimate based on the present model and assumptions for the start-up of the LHC puts the initial cost of the central installation of data storage and processing power at CERN for ATLAS to be 11 MCHF, to be spent in the years 2006-2008. (The hardware costs for the whole ATLAS Virtual World Wide Computing Facility, including CERN, is estimated to be about 35 MCHF in the years 2006-2008.) As stated in the CERN LHC Computing review, it is expected that, from 2009 onwards, approximately one third of the initial hardware investment will be needed annually for replacement and upgrades. For consumables and manpower (to run the facilities outside CERN) the yearly costs are estimated to be 7 MCHF (see section 4 for details). In addition we estimate that 2 MCHF are needed for ATLAS specific core computing activities, covered by Software Agreements.

The total costs for the offline computing (hardware, consumables and manpower) to be shared by the collaboration from 2006 onwards will therefore probably be ~17 MCHF per year, covered to large extent in form of in-kind contributions e.g. in the form of Regional Facilities.

The new model makes the facilities truly regional, not national, with the resources being shared worldwide. For this model to work, the access has to be open to all of ATLAS, and a suitable cost sharing must be devised, with all countries contributing at an appropriate level. A facility might choose not to make its total resources available to the whole collaboration, in which case only those resources that are made available will be taken into account for the cost sharing.

The cost sharing (hardware, network, consumables and manpower) will be part of the Computing MoU, which is due in 2005. It is foreseen that this MoU will consist of a part common to all 4 experiments, and separate parts for each experiment.

The following figure shows the proposed structure for the offline-computing facility for ATLAS; note that not all countries are displayed.

## 3.) ATLAS OFFLINE COMPUTING MODEL

### LHC design parameters

\begin{tabular}{l l} E & = 14 TeV (two 7 TeV proton beams) \\ L & = 10\({}^{34}\) cm\({}^{-2}\) sec\({}^{-1}\) (design luminosity); in the first year probably 0.5-2.0*10\({}^{33}\) cm\({}^{-2}\) sec\({}^{-1}\) \\ \(\sigma\) & = 100 mb = 10\({}^{-25}\) cm\({}^{2}\) \\ Collision rate & = L,q = 10\({}^{9}\) Hz p-p collisions \\ \end{tabular}

### Types of data

\begin{tabular}{l l} RAW & Real Raw Data, as recorded after the HLT \\ SIM & Simulated Raw Data \\ ESD & Event Summary Data (after reconstruction) \\ AOD & Physics Analysis Object Data \\ DPD & Derived Physics Data (similar to today's N-tuples) \\ TAG & Event tags \\ RAW/SIM & ESD & AOD \\ \end{tabular}

### 3.1.) The present status of the ATLAS Tier structure

The ATLAS tier resources will be built incrementally between now and 2008 in two phases. The first is between 2001 and early 2005 coinciding with the Data Challenges (DC), which are not only part of our own computing objectives but also key deliverables of the different Grid projects. This also comprises a significant part of the LHC Computing Grid (LCG) Phase-1 Computing Prototype. As such, much of the funding is now in place, and fairly definite planning has been done.

In DC1 phase1 (May-August 2002) the data needed for the High Level Trigger (HLT) TDR were generated (PYTHIA) and simulated using ATLASIM (Dice, GEANT3). Due to the huge amount of computing time needed, it was essential to make use of the computing resources available in different ATLAS institutes. Worldwide we used up to \(\sim\)3200 processors (\(\sim\)5000 NCU's8) in 39 institutes located in 18 countries9. In total the simulation part of DC1 (phase1) corresponds to \(\sim 71000\) NCUdays. For the pile-up production, which will start in October 2002, we will need another 2.000 NCUdays. The total output produced will amount to \(\sim\)100 TB in \(\sim 100000\) partitions.

Footnote 8: 1 Normalised Computing Unit (NCU) corresponds to 1 Pentium III /500 MHz equivalent to 21 SpecInt95 (SI95)

Footnote 9: ATLAS Data Challenge 1; paper in preparation.

During phase 2 of the LCG project a production system will be developed. The funding arrangements are still evolving; consequently, the planning is far less certain for this phase. The networking development is contingent on many external forces and demands. The change in the numbers compared to surveys conducted inside ATLAS about two years ago can be characterised as follows. The predicted number of users has generally fallen, especially in the next few years and to a lesser extent in 2006. One can speculate that this has partly to do with the (un)-expected migration of many ATLAS physicists to the Tevatron experiments. In terms of hardware resources, the change can perhaps be characterised by a rather steeper ramping-up of the resources, starting from a lower base and reaching roughly the previous predictions by about 2004. The changes in the disk and mass storage are not so easily summarised; in some cases, the predicted disk has gone down and mass storage increased, in others the reverse is true. This probably reflects the uncertainties in the overall computing model, which should be eliminated to a large extent by the knowledge gained in the Data Challenges.

The lower tier plans are very much in a state of flux. In some regions, such as Italy and the UK, there are fairly definite plans for lower level tiers. In the UK, the feeling is that the lower tiers should adopt a cloud model, inter-working between large local university resources, but presenting themselves to the external world as a single resource. This model has similar strengths as the cloud model for the Regional Facility resources.

## 3.2 The ATLAS Distributed Computing Facility

The advent of the World Wide Grid opens new possibilities to define the structure of the ATLAS regional facilities. The guiding idea is to make optimal use of the ATLAS computing resources distributed worldwide. The Grid middleware, presently developed as part of the Grid projects (e.g. DataGRID, GriPhyN, NorduGrid), offers the possibility to build a Distributed Virtual Offline Computing Facility with:

- distributed yet coherent computing

- coordinated sharing of geographically distributed resources

- conditional sharing (issues of trust, policy, negotiation, payment etc.)

- invisibility of the local architecture

- optimisation of the resource sharing

- definition of a virtual organisation (VO) for each experiment

The virtual computing facility (cloud model) foresees that services are offered to client consumers by service providers. All transactions are mediated through the cloud. If resources were unlimited it would be unnecessary to consider the possible structural layout. However, it makes no sense to ignore the organizational structures and geographical network structures. Therefore, it is proposed that smaller countries either use already existing regional centres or form a distributed multi-national regional facility (e.g. Nordic Cluster). In what follows, we use the term centre to imply an aggregation of computing resources at a single site, whereas a facility is an aggregation that may either be a single site or distributed and connected through fast networks.

The conditions for being part of the virtual computing facility are:

- A certain (minimal) amount of hardware and services available (see below)

- Network connection for ATLAS at least 2.5 Gbps (complying with international standards)

- Centre available 24h/7d for the lifetime of ATLAS

- Open access for ALL members of the ATLAS Collaboration (policy agreements; authentication

and authorization using Grid tools)

- User support and training provided for the region

- Agreed Grid and possibly other security methods honoured and implemented

- Sites certified

The virtual Computing Facility will be composed of:

- The CERN Tier-0

- All Regional Facilities (typically \(\sim\)200 users each)

- Some National Facilities

The role of the different categories of facilities would be defined by the services they offer to the ATLAS community.

As in the MONARC Model, the CERN computing system will be mainly dedicated to the data acquisition, the calibration of the detectors, the processing of the raw data and will be part of the analysis facilities. The ESD will be sent to the Regional Facilities, via the Wide Area Network (WAN).

The Monte Carlo production will be done almost exclusively in the regions. The raw simulated data will stay where they are produced (or moved to the closest Regional Facility). For the MC ESD the same distribution strategy will be applied as for the real data.

Therefore, each Regional Facility will hold \(\sim\)1/3 of the full ESD on disk and the full AOD. No local backup of the ESD is needed, since another copy should always be available on disk somewhere (plus one complete copy on tape in the CERN Tier-0). In the current model it is foreseen that each Regional Facility will keep also \(\sim\)1/6 of the previous version of the ESD on disk, such that each event will be directly accessible.

The new model makes the regional facilities truly regional, not national, with the resources being shared worldwide. For this model to work, the access has to be open to all ATLAS users, and a suitable cost sharing must be devised, with all countries contributing at an appropriate level.

All tasks, including those that need access to a large amount of resources, should be able to be performed in the Regional Facilities, namely:

- Access to the real data

- Access to the full set of the ESD'S (current and previous)

- Production of AOD's

- Monte Carlo production

- Large-scale analysis

- Analysis Group Activities

whereas National Facilities and below need only support a subset of these tasks, and should support tests based on smaller samples of events and analysis on DPD's/N-tuples.

The typical ATLAS user will copy small samples of the AOD and ESD to his/her local disk and use these data to test his/her programs. If access to more data is needed he/she will use the Grid middleware to access the Distributed Virtual Offline Computing Facility and make use of the necessary resources wherever it is most efficient. Unless very small samples of events are involved, it is envisaged that the programs will be sent to the data.

A normal user will have limited access to the resources. AOD production will be done under central management, and so sudden large-scale batch processing on the ESD's will not occur without warning and agreement. As mentioned above, it is assumed that the synchronization of the data, the resource brokerage and the authorization handling is taken care of by the Grid middleware.

Since this facility is quite complex, prototypes are needed to test its performance and to eventually improve the model. This will be done as part of the Data Challenges (2001-2005). It is expected that all centres will participate both in the ATLAS computing Data Challenges and the physics Data Challenges.

The cost sharing (hardware, network, infrastructure and manpower) will be part of the Computing MoU, which will be due in 2005.

[MISSING_PAGE_FAIL:7]

The manpower resources are extremely hard to estimate, as many of the support staff will also be involved in other activities. These estimates should therefore be treated with some caution, and a future ATLAS task will be to refine its manpower definitions to arrive at a more useful quantity for planning purposes. It is also a very open question as to how the manpower needs will scale with increasing hardware facilities and improving Grid management tools. The manpower needed for one **stand-alone** regional facility is estimated to be \(\sim\) 20 FTE. For the cost sharing we use the number from CERN/IT14, where it was calculated how much **additional** manpower would be needed to add a regional facility to an **existing** computing centre.

Footnote 14: Here (as in the CERN LHC Computing Review) 6 is the number of average size Regional Facilities. The total number of ATLAS Regional Facilities will probably be larger.

As for the manpower, for the costing of the hardware we calculate the costs assuming that the regional facilities are add-ons to existing centres. If we assume then a flat spending profile (LHC start-up in mid 2007) i.e. buying

30% in 2006

30% in 2007

40% in 2008

the initial hardware costs are expected to be about 35 MCHF15 (11 MCHF for the CERN part16). As stated in the CERN LHC Computing review, it is expected that, from 2009 onwards, approximately one third of the initial hardware investment will be needed annually for replacement and upgrades.

Footnote 15: Most of the Regional Facilities will be shared by 2-4 LHC experiments. Therefore, we may profit from an additional synergy effect, which could lower the number of FTE’s to be paid for.

The initial costs for the analysis part, performed in the regional facilities, as described above, are estimated to be \(\sim\) 14 MCHF. However, we foresee that investments for this part of the offline computing will be user-load driven and not a priori decided.

The manpower is budgeted as 150 kCHF/FTE/year i.e. 4.5 MCHF/year. In addition we estimate that 2 MCHF are needed yearly for ATLAS specific computing activities, covered by Software Agreements.

The costs for the consumables (e.g. tapes) are estimated to be 2.5 MCHF/year.

Up to now, the networking costs are in most cases covered by the national research networks. However, since we need a guaranteed bandwidth in 2007 of at least 2.5 Gbit/s between **all** Regional Facilities, we may be charged in the future.

## 5.) Cost Sharing

The costs for the offline computing to be shared by the ATLAS institutes include:

- the 'visible' part of the regional facilities

- the networking costs

- the consumables

- the manpower to run and maintain the centre (including Grid services)

As mentioned above, only the CPU and data intensive part of the ATLAS offline computing, which needs to be performed at the large regional facilities, is foreseen to be covered by the resources and cost sharing.17

Footnote 17: In the CERN Computing Review it was estimated that about 1/3 of the computing resources will be located in the Tier-3 and Tier-4 centres.

The ATLAS CERN-equivalent computing costs are roughly 17 MCHF/year18,

Footnote 18: These costs do not include the hardware and manpower for the CERN T0/T1 that are a host-laboratory responsibility and budgeted in the CERN cost to completion Council papers.

**8.0 MCHF : Hardware investment**

**4.5 MCHF : Manpower to run the Regional Facilities**

**2.0 MCHF : Manpower for ATLAS specific core software activities**

**2.5 MCHF : Consumables (e.g. tapes)**

proposed to be shared by all institutes, following the cost sharing principle laid out in the Maintenance & Operation MoU. The expected sharing between funding agencies will be presented by the ATLAS resource coordination in tables just like the other ATLAS costs.

Institutes can contribute in different ways to the overall costs of the ATLAS Virtual Distributed Offline Computing Facility including in-kind contributions in form of hardware, consumables and manpower

- in their own country

- at a regional facility in another country

Such in-kind contributions have to be qualified by the ATLAS NCB and will be part of the ATLAS offline computing centre, i.e. rules for accessing and using these resources - by all members of the collaboration - will be defined by the ATLAS management.

For the costing, it is proposed to use the equivalent cost at CERN. The NCB will annually review the contributions, and establish the credited value to the ATLAS computing costs.

The cost sharing will be described in detail in the Computing MoU, which is due in 2005. It is foreseen that this MoU will consist of a part common to all 4 experiments, and separate parts for each experiment.

## 6 Summary

A well-planned, organized and maintained Virtual Distributed Computing Facility will be essential for the ATLAS offline computing. Besides providing the necessary resources in an efficient way, this facility will allow all ATLAS physicists to participate in the data analysis, independent of his/her location and of the locally available resources. For the success of this model it is important that all these resources are accessible to all members of the ATLAS collaboration.

In this paper we have summarized our present knowledge on required and available resources. The overall costs can only be indicative due to large uncertainties in most of the numbers used to perform these calculations.

It is worth noting that the CPU power available to ATLAS worldwide for DC1 phase 1, as described in 3.1, corresponds to \(\sim\)110 kSI95 or 50% of the CPU power estimated for one Regional Facility at the LHC start-up. The hardware investment made by participating institutes in the last 12 months corresponds roughly to 50 % of the yearly hardware investment needed from 2006 onwards for the non-CERN part of the ATLAS Offline Computing.

The final system will be described in the Computing TDR (currently due end 2003, but could be later). Since most of the Regional Facilities will not only be used by one LHC experiment, some coordination will be needed.

We expect that the activities in the LCG project and in the various GRID projects will help to provide us with the necessary tools to successfully define, model, set-up and run this distributed computing facility.

The plans concerning the sharing of costs and resources is an important issue, which needs a detailed discussion in the collaboration and an endorsement by the collaboration board. For the cost sharing it is proposed to follow the principle laid out in the Maintenance & Operation MoU.

The total costs for the offline computing to be shared by the collaboration from 2006 (2007) onwards will probably be \(\sim\) 17 MCHF per year, covered to large extent in form of in-kind contributions e.g. in the form of Regional Facilities.

The CB is asked to endorse the principles of cost sharing for the ATLAS offline computing resources, so that further studies and discussions with the other experiments and the funding agencies can be started. As mentioned above, the final model will be part of the computing MoU.