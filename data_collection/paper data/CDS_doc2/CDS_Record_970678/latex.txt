## Appendix A

###### Contents

* 1 Introduction
	* 1.1 Summary of charge
	* 1.2 Organisation of this document
* 2 Calculating cross-sections
	* 2.1 Cross-section calculations
	* 2.2 Luminosity blocks (LBs)
	* 2.3 Relevant ATLAS time-scales and numbers
	* 2.4 TDAQ live-time corrections
		* 2.4.1 Dead-time in ATLAS
		* 2.4.2 Averaging Live-time
		* 2.4.3 Live-time estimation in the CTP
		* 2.4.4 Live-time corrections for local luminosity measurements
		* 2.4.5 Representation of luminosity with respect to live-time and pre-scales
	* 2.5 Market survey
* 3 Methods of luminosity determination
	* 3.1 Overview of luminosity methods
	* 3.2 Methods for Luminosity Monitoring
* 4 Luminosity recording in the TDAQ system
	* 4.1 Information from the Level-1 trigger
	* 4.2 Information from the HLT and dataflow
		* 4.2.1 Pre-scales
		* 4.2.2 Event losses due to failures
		* 4.2.3 Publishing counter information
	* 4.3 Transition between luminosity blocks
	* 4.4 Requirements from the luminosity sub-systems
	* 4.5 Physical clustering of luminosity blocks
* 5

## 5 Offline event selection and analysis

* 5.1 Luminosity data in the offline conditions database
	* 5.2 Sample selection and book-keeping
		* 5.2.1 Detector status, quality information, and event selection
		* 5.2.2 Use case: standard analysis samples
		* 5.2.3 Use case: event selection by individual physicists
	* 5.3 Luminosity and event-level information
* 6 Offline processing and access model
	* 6.1 Luminosity considerations for offline streaming and file merging
	* 6.2 Offline error conditions, incomplete samples, and luminosity calculation
* 7 Conclusions and recommendations
* A Full charge
* B GlossaryIntroduction

The ability to accurately determine luminosities is essential for the ATLAS physics program. Absolute luminosities are necessary for determining cross sections. Relative luminosities are required to combine different trigger chains, including those with time-dependent pre-scales. This note presents the findings and recommendations of the Luminosity Task Force on issues related to luminosity recording and calculation.

### Summary of charge

The complete charge to the Task Force is given in Appendix A. The Task Force, which reports to the TOB, has been asked to consider the implications of the measurement and monitoring of luminosity for physics analysis, offline software, database, computing and trigger. In particular, the role of the Task Force is to highlight any areas of concern and propose technical solutions.

It should be emphasized that it is not the role of the Task Force to propose new techniques for determining the luminosity or to review the status for the luminosity monitors currently under construction, but rather to insure that all necessary information is properly recorded and that the overall strategy for luminosity determination and monitoring is robust against failures in the data acquisition, trigger, and offline systems.

### Organisation of this document

This document is organized as follows: Section 2 reviews the relevant formulas that relate measurements of cross sections and luminosity. Issues associated with time dependent conditions are discussed, as are requirements on the luminosity and trigger bookkeeping. The concept of a "luminosity block," the minimum unit of time for which the integrated luminosity can be accurately calculated is introduced. Section 3 describes the methods ATLAS will use to determine the integrated luminosity. Section 4 discusses luminosity recording in the TDAQ system and provides a number of recommendations for implementing luminosity blocks and monitoring trigger live-time and pre-scales and for monitoring event losses. Section 5 describes the requirements that luminosity determination places on the offline processing and computing model and discusses what data and meta-data must be stored in ATLAS databases. Section 6 discusses issues of relevance for offline processing and data access, including the monitoring of error conditions. Section 7 briefly summarizes the Task Force's conclusions.

## 2 Calculating cross-sections

The purpose of this section is to introduce the terminology and concepts necessary for cross-section calculations and to relate the cross-section to the luminosity. A complete glossary of relevant terms is provided in Appendix B.

### Cross-section calculations

Let us begin by reminding the reader of how the number of signal events is related to the integrated luminosity and the production cross-section. For purposes of this discussion, we will limit ourselves to an analysis that is done using a single trigger chain and a single offline analysis path. (If multiple triggers or analysis paths are used, each must be analyzed separately and combined only after the cross-sections for each have been calculated). The number of observed events passing our final analysis cuts is related to the cross-section (\(\sigma\)) by the following:

\[N=A\cdot\epsilon\cdot\ell\cdot p\cdot f\cdot\mathcal{L}\cdot\sigma \tag{1}\]

Here \(N\) is the number of signal events _after background subtraction_, \(A\) is the acceptance, \(\epsilon\) is the efficiency and \(\ell\) is the live-time (see Def. B.1), \(p\) is the product of the Level 1, 2 and 3 pre-scale factors (see Def. B.8), \(f\) is a correction factor for any failures or event losses that occur during online data collection or offline processing and \(\mathcal{L}\) is the integrated luminosity. The distinction between acceptance and efficiency is somewhat arbitrary. We will define acceptance as the fraction of events that pass the geometric and kinematic cuts imposed on the sample and the efficiency as the fraction of the events within the acceptance that pass the trigger and the offline selection. Whether event-losses due to failures are corrected for in the term \(f\) or in the efficiency term \(\epsilon\) is also a question of definition. Often, losses due to the current state of the system (dead-time; memory leaks; network outage) are accounted for in the luminosity term, whereas losses due to the nature of the event data (particular event crashes processing task) are often taken care of in the efficiency term.

In practice these quantities are time dependent. We will assume that this dependence is slow enough that we can approximate their values as constant over a short time period, which we refer to as a **luminosity block**. The overall equation then becomes a sum over luminosity blocks:

\[\sum_{i}N_{i}=\sum_{i}A\cdot\epsilon_{i}\cdot\ell_{i}\cdot p_{i}\cdot f_{i} \cdot\mathcal{L}^{i}\cdot\sigma \tag{2}\]

Note that the efficiency \(\epsilon_{i}\) may have a time dependence for a number of reasons:

* The efficiency of some offline cuts (such as lepton isolation) may be dependent on the instantaneous luminosity* Parts of the detector may be inoperative or inefficient for some of the data used for the analysis
* Some offline cuts may have an efficiency that varies with run-dependent conditions (for example, if the beam-spot has a size that depends on the accelerator tune, the fraction of events passing an impact parameter of \(d_{0}\) cut may be time dependent).

In general, the efficiency will be calculated using a Monte Carlo simulation that includes the time dependent conditions. Details of such calculations go well beyond the charge to the luminosity task force.

To calculate the cross-section, we can invert equation 2

\[\sigma=\frac{\sum_{i}N_{i}}{A\cdot\sum_{i}\epsilon_{i}\cdot\ell_{i}\cdot p_{i} \cdot f_{i}\cdot\mathcal{L}^{i}} \tag{3}\]

Some comments about equation 3 are in order:

* It may seem obvious, but it is worth noting that the sums in the numerator and denominator must be over the same luminosity blocks.
* The sum in the denominator must include _all_ luminosity blocks that have been analyzed, even if a given block does not contribute any events to the sum in the numerator (i.e. because the block does not contain any events that pass the analysis cuts.)
* The sum in the numerator must include _all_ events in the examined luminosity blocks that pass the analysis cuts. We ultimately expect the systematic uncertainty on the luminosity to be a few per cent. Thus, in order to ensure that the systematic uncertainty in the numerator is negligible, we must keep the rate of lost events to less than 1 or 2 %.

These comments all indicate that book-keeping issues associated with luminosity blocks are of primary importance. Issues relevant for this book-keeping are the subject of the Subsections 2.2 through 2.3. Subsections 2.4 through 2.4.5 discuss how the time-dependent live-time is incorporated in the calculation.

### Luminosity blocks (LBs)

This document is particularly concerned the calculation of the integrated luminosity \(\mathcal{L}\) and its use in Equation 2. One important issue in this calculation is insuring that all terms in the \(\Sigma_{i}\) are determined with the same time intervals \(i\). The smallest possible time interval depends on the method of luminosity measurement and could be chosen such that the statistical uncertainty is smaller than the systematical error. Typical values from other experiments are \(\mathcal{O}\)(min).

A luminosity block (LB) is a time interval, for which the integrated, dead-time- and pre-scale-corrected luminosity can be determined (see Def. 2.2). LBs help to keep the losses to a minimum in view of failure scenarios in DAQ, data production, analysis, detector and machine operation. This is usually done by excluding from the analysis LBs in which failures occur. The biggest amount of tolerable losses due to these failures sets an upper limit on the size of a LB. A complete run of length \(\mathcal{O}\)(1 h) would, under realistic failure scenarios, be too big to be taken as a single LB. In particular, as is the case in currently running experiments, runs are often terminated just after failures have occurred. In this respect it seems difficult to find a solution without sub-dividing a run into smaller pieces of time.

**Definition 2.1: Luminosity Block (LB):** A time interval, for which the integrated, dead-time- and pre-scale-corrected luminosity can be determined. Its length is set by the LB supervisor.

The instantaneous luminosity of the machine decreases exponentially with a time constant of \(\mathcal{O}\)(6..28 h), with a nominal time constant of 14 h [15]. This would mean that under nominal conditions, the luminosity would drop by 1 % after 10 minutes. In order to open up trigger bandwidth, it is expected to change the pre-scale values at time intervals \(\mathcal{O}\)(1 h) [1]. Also, some aspects of data taking, event reconstruction or data analysis depend on the instantaneous luminosity. The size of the LB should be small enough compared to the required granularity from these aspects.

In order to calculate the corrected luminosity for each LB, a complete set of parameters needs to be determined, recorded and made available. For the analysis of a specific dataset, this complete set consists of the following information:

1. Name of the trigger chain (see Def. B.4) used to trigger the dataset.
2. At least one luminosity measurement.
3. Level-1 live-time fraction for this trigger chain.
4. Pre-scale values for the trigger chain.
5. Information (counter values) on lost events due to failures at Level-2, EF, production, and skimming. Note that if users produce their own private AOD datasets, information on lost events from the private skim will also be needed.
6. Data quality information (see Def. B.14)

Items 2 through 6 above are all time dependent. The most natural way to store this information is in a database, where the relevant numbers or retrieved as a function of a unique index that identifies the relevant luminosity block. This index is called a Luminosity Block Number (LBN).

**Definition 2.2: Luminosity Block Number (LBN):** A number, which uniquely tags a LB within a run.

It is difficult to find alternative solutions without LBs, which is reflected in the fact that major current experiments (D0, CDF, H1, ZEUS) use the concept of LBs (see Subsection 2.5).

**Recommendation 2.1:** Luminosity Blocks (LB) of length \(\mathcal{O}\)(min) should be implemented. The exact length should be tuned to match online and offline operational constraints.

### Relevant ATLAS time-scales and numbers

Tab. 1 summarizes the different constraints on the length of a LB. Details of these items will be discussed later in the document. Because it is possible to determine the relative luminosity on a time scale of less than a minute, the requirements of luminosity determination do not unduly constrain the choice of LB length. Given the expected luminosity lifetime at the LHC, no strong requirements on the LB length are imposed by the approximation that the luminosity is constant for the duration of the LB block. Thus, the two items most likely to determine how long LB should be are the mean time between system failures and the details of the computing model. If a detector ceases to operate (due, for example, to a high voltage trip or a failure of the readout system), then all analyzes that rely on that detector must exclude the complete luminosity block if they are to properly calculate the cross-section. The shorter the LB are, the less data is excluded by this consideration. If one or more files from a LB are unreadable, again the whole LB must be excluded from analysis. Again, shorter LBs lead to less excluded data. To improve the robustness of the offline data handling systems, the CDF and D0 experiments have chosen to require that all events for a given LB and trigger chain be contained in a single ESD or AOD file. This requirement, coupled with the data size and rate and the practical limits of file sizes, ultimately set the LB length at about 1 minute at the Tevatron. Implications for ATLAS are discussed in Section 6.

### TDAQ live-time corrections

In this section we discuss how luminosity estimates are influenced by the live-time (or dead-time) of the trigger and data acquisition system. In the following we use the following definitions:

**Definition 2.3: Delivered luminosity:** The integrated luminosity over a period of time (e.g. luminosity block) at the interaction region.

**Definition 2.4: Recorded luminosity:** with respect to a trigger chain \(T\). The integrated luminosity over a given period of time, but only for bunch-crossings where the trigger for trigger chain \(T\) was active. This excludes bunch-crossings that occurred during dead-time as well as bunch-crossings that did not pass the pre-scaling requirements.

The delivered luminosity is obtained by luminosity methods that are not part of the ATLAS data-acquisition and therefore do not experience the TDAQ dead-time. Methods that use Level-1 triggers or data-sets triggered by Level-1 (and HLT) are exposed to TDAQ dead-time and therefore give the recorded luminosity. Since all data-sets used for analysis were exposed to TDAQ dead-time, the final goal is to find the corresponding recorded luminosity to the data-set in question. The delivered luminosity needs to be transformed into recorded luminosity by correcting for dead-time.

#### Dead-time in ATLAS

At ATLAS dead-time is introduced only in the Central Trigger Processor (CTP) [16] by inhibiting Level-1 triggers. The dead-time has the following components:

1. **Simple dead-time:** A programmable, fixed number (default 4) of bunch crossings following each L1A.
2. **Complex dead-time:** There are two complex dead-times, implemented by two programmable leaking-bucket algorithms. Each trigger signature is assigned one complex dead-time. With this mechanism, each trigger signature can be assigned 'high' or 'low priority'.

\begin{table}
\begin{tabular}{l c c} \hline \hline Issue & Constraint & Comment \\ \hline \hline Precision on luminosity measurement & \(\mathcal{O}\)(min) & \\ \hline Minimize losses due to system failures & \(\ll\) 1 h & Depends upon projected failure rate \\ \hline Adaption to significant luminosity drop & \(\mathcal{O}\)(h) & 1 \% luminosity drop after 10 min at nominal conditions \\ \hline Counter overflow in the CTP & \(<\) 1 min & Not a hard limit, as counter read-out could be more frequent than LBs. \\ \hline Off-line: Mapping LBs onto files/file sets &? & Depends on Details of Computing Model \\ \hline \hline \end{tabular}
\end{table}
Table 1: Different considerations that influence the length of LBs.

3. **BUSY from the DAQ:** Back-pressure mechanism to throttle the generation of Level-1 triggers. Each of the 20 TTC partitions and the CTP itself send back a BUSY signal to the CTP. In addition there are two external VETO inputs in the CTP.
4. **BUSY on demand:** the CTP can make itself BUSY via VME control. Typically this would be used to _introduce operational down-time_ (see Def. B.3)between runs and luminosity blocks.

Each trigger signature will see the OR of simple dead-time, DAQ busy, busy on demand, and one (and only one) complex dead-time. Therefore, the dead-time (or live-time) needs to be estimated at least for each of the two dead-times, ORed with the other dead-times.

#### Averaging Live-time

The LHC has 3564 different slots for particle bunches, and therefore 3564 bunch crossings occur per LHC turn. The instantaneous luminosity of bunch \(i\) is denoted \(L_{B}^{(i)}\), and some of the bunches are not filled, i.e. \(L_{B}^{(i)}=0\) for these empty bunches.

The integrated delivered luminosity for a luminosity block, which spans \(n_{turn}\) number of LHC turns is given as

\[\mathcal{L}^{delivered}=25\ \mathrm{ns}\cdot\sum_{k=1}^{n_{turn}}\sum_{\mathrm{ bunches}\ i=0}^{3563}L_{B}^{(i)} \tag{4}\]

Let us consider a specific trigger chain \(T\) with a live-time function \(\ell_{ij}^{T}\) with

\[\ell_{ij}^{T}=\left\{\begin{array}{l}0\ \mathrm{for\ a\ bunch\ crossing}\ i\ \mathrm{in\ turn}\ j\ \mathrm{with\ dead-time\ for\ trigger\ T}\\ 1\ \mathrm{else}\end{array}\right. \tag{5}\]

The luminosity seen by the trigger chain \(T\) is then

\[\mathcal{L}^{T}=25\ \mathrm{ns}\cdot\sum_{k=1}^{n_{turn}}\sum_{\mathrm{ bunches}\ i=0}^{3563}L_{B}^{(i)}\cdot\ell_{ik}^{T} \tag{6}\]

**Assumption 2.1**: _Dead-time is distributed uniformly across all LHC turns, i.e. the distribution \(\ell_{ik}^{T}=\mathrm{uniform}\) for all turns \(k\)._

This assumption is probably fullfilled, which allows the definition of a **turn-averaged live-time fraction per bunch \(\ell_{i}^{T}\)**

\[\ell_{i}^{T}=\sum_{k=1}^{n_{turn}}\ell_{ik}^{T}/n_{turn} \tag{7}\]

This corresponds to the probability of bunch \(i\) of a random turn to be alive. Note, that \(\ell_{i}^{T}\) can be estimated/checked for a given run or even LB. We can write

\[\mathcal{L}^{T}=25\ \mathrm{ns}\cdot n_{turn}\cdot\sum_{\mathrm{bunches}\ i=0}^{35 63}L_{B}^{(i)}\cdot\ell_{i}^{T} \tag{8}\]At this point, no further simplifications can be done to Eq. (8) without making assumptions which might not be fulfilled. In general, the bunch luminosity of filled bunches will vary from bunch to bunch, and the dead-time will not be distributed uniformly across all filled bunches. The following effects can be relevant:

* **Shadowing:** A specific bunch with much higher luminosity than the others will trigger Level-1 more often than others. Due to simple dead-time the bunches immediately following this high-intensity bunch will experience dead-time more frequently than others. This effect is called'shadowing': A high-intensity bunch throws a shadow on its following neighbor bunches.
* **PACMAN Bunches:** Due to PACMAN bunch effects (described below), bunches near the extremities of batches in the bunch train are more likely to be unstable and therefore degrade faster than other bunches. Bunches in the middle of a bunch train experience approximately fifteen long range collisions in the common beam tube on each side of an IP. This means that there are \((2\times 15+1)\times 4=124\) possible interaction points. A bunch that encounters a bunch from the other beam at all 124 possible collision points is called'regular'. Some bunches (especially near the head and tail of a bunch train) do not always encounter bunches in the opposite beam at one or several parasitic collision points (so-called "pacman" bunches), or even at the head-on interaction points ('super pacman' bunches). These long-range collisions produce orbit distortions and tune shifts in addition to the head-on tune shift produced at the IPs. The possibility that bunches experiencing fewer than the nominal number of long-range collisions may be unstable has led to the recommendation that the luminosity be measured for each bunch individually.
* **Long-gap effects:** In addition to long-gap effects caused by PACMAN bunches, there is the effect that the first bunches after a long gap in the bunch train experience less dead-time than other bunches: The first bunch after a gap in the bunch train which is longer than the duration of simple dead-time, will never experience simple dead-time. Also, the complex dead-time algorithms, which are implemented with a leaking bucket algorithm, had more time to recover. The same is true for the BUSY signals from the read-out system: After a longer gap the RODs had time to read-out the Level-1 buffers.

It is difficult to estimate the size and relevance of these effects, but they are believed to be of second-order. In the following, we describe how the CTP can estimate the live-time by counting triggers before and after veto, and show, under which assumptions this method works. We will see that these effects are assumed to be of second order and can be therefore either neglected or treated as small corrections done in an additional correction step.

**Recommendation 2.2:** The influence of effects caused by different bunch luminosities and a non-uniform distribution of live-time per bunch should be studied and quantified, and taken into account if necessary when estimating the recorded luminosity.

#### Live-time estimation in the CTP

Under the assumption that

**Assumption 2.2**: _Dead-time is distributed uniformly across all LHC filled bunches, i.e. \(\ell_{ik}^{T}=\mathrm{const}\) for all bunches \(i\)._

the **mean live-time fraction per turn \(\boldsymbol{\ell^{T}}\)** can be defined

\[\ell^{T}=\frac{n_{BC,alive}}{n_{BC}} \tag{9}\]

which can be factored out of the sum in Eq. (8). The number of alive bunch-crossings is denoted \(n_{BC,alive}\), and \(n_{BC}\) is the total number of bunch-crossings. As previously mentioned, the validity of this assumption must be studied and checked, and if necessary, corrections must be applied.

The fraction in the last equation can be related the number of triggered bunch crossings, by the following assumption

**Assumption 2.3**: _The probability of a bunch-crossing that fired trigger \(T\) to happen during live-time is the same as for a bunch-crossing that would not fire the trigger._

This means that

\[\ell^{T}=\frac{n_{BC,alive}}{n_{BC}}=\frac{n_{T,alive}}{n_{T}}\equiv\frac{n_{ TAV}}{n_{TBV}} \tag{10}\]

where \(n_{T,alive}\) is the number of triggers during live-time (\(n_{TAV}\) in the language of the CTP: triggers after veto), and \(n_{T}\) the total number of triggers \(T\) (\(n_{TBV}\) for the CTP: triggers before veto).

This assumption is probably fulfilled, as dead-time does not depend on the bunch-crossing that just occurred, but on the earlier bunch-crossings. The recorded luminosity can now be written as

\[\mathcal{L}^{T}=25\ \mathrm{ns}\cdot n_{turn}\cdot\frac{n_{TAV}}{n_{TBV}}\cdot \sum_{\mathrm{bunches}\ i=0}^{3563}L_{B}^{(i)} \tag{11}\]

#### Live-time corrections for local luminosity measurements

As discussed in section 3 ATLAS will use several different luminosity monitor systems to determine the luminosity. In order to associate their information with the recorded integrated luminosity of a specific event sample, the dead-time imposed by the ATLAS trigger system has to be taken into account. With this respect we distinguish two types of methods:

1. **Dead-time blind methods:** These methods rely on information coming from systems independent of ATLAS TDAQ, and therefore dead-time is not taken into account.

These are typically counters running in the electronics of luminosity monitors, which count hits per bunch-crossings, and either count for each individual BCID, or integrate over all BCIDs. Also, the estimation of the instantaneous luminosity from beam parameters falls under this category.
2. **Dead-time aware methods:** These methods use information from luminosity monitoring sub-detectors in events from ATLAS luminosity data sets, that are typically triggered by a (pre-scaled by factor \(p\)) minimum bias trigger. Since these events are taken during normal operation of ATLAS in physics mode, and are part of the standard ATLAS data-acquisition, dead-time is fully taken into account. These events can be used to determine the recorded luminosity off-line (or semi-offline). In the most general case it measures the whole sum of Eq. (6), but with a pre-scale factor \(p\), which is independent of the live-time function, and therefore easily to be corrected for. It should be noted that this delivered luminosity is with respect to the trigger chain of the special trigger. In case enough statistics is acquired, the delivered luminosity per bunch can be calculated as well.

Dead-time blind methods determine the delivered integrated luminosity, and thus dead-time corrections need to be applied in order to estimate the recorded luminosity. These are not needed for dead-time aware methods, which measure the recorded integrated luminosity directly.

Dead-time blind methods often have the advantage of relying on counter values, that can be read out on-line. For this reason, these methods can be used to provide an estimate of the delivered luminosity on-line, using Eq. (4). Dead-time aware methods use events acquired through TDAQ, and can be applied already on-line with monitoring applications and, usually with better precision, off-line.

The available bandwidth for such luminosity events is likely to be a modest fraction of the full bandwidth, which limits the statistical precision of the dead-time aware methods. Precise estimates are likely to require relatively long time periods. This makes dead-time blind measurements attractive complements for fast results, and combining dead-time aware and blind methods will give many handles to arrive at a consistent, accurate and correct determination of the recorded luminosity.

With the two different complex dead-times at Level-1, high- and low-priority triggers experience different dead-times. Therefore, luminosity data sets are needed for both priorities, which then allow to cover all trigger chains in the determination of the delivered luminosity.

Two trigger chains with different Level-1 priorities should be set up for recording a luminosity data set.

The delivered integrated luminosity can be expressed as

\[\mathcal{L}^{delivered}=25\text{ ns}\cdot n_{turn}\cdot\sum_{i}L_{B}^{(i)}=25 \text{ ns}\cdot n_{turn}\cdot n_{BC}\cdot\langle L_{B}\rangle \tag{12}\]assuming the corresponding time period is short enough to only introduce a insignificant change of the individual bunch luminosities. To associate this delivered luminosity value with the recorded luminosity for a given data set, live-time corrections must be applied. We distinguish live-time corrections which depend on the bunch structure (\(\ell^{dep}_{i}\) for BCID \(i\)), and corrections which are independent (\(\ell^{indep}\)).

\[{\cal L}^{recorded}=\ell^{indep}\cdot 25\ {\rm ns}\cdot n_{turn}\cdot\sum_{i}L^{(i )}_{B}\cdot\ell^{dep}_{i}=C\cdot 25\ {\rm ns}\cdot n_{turn}\cdot n_{BC}\cdot \langle L_{B}\rangle \tag{13}\]

with

\[C=\ell^{indep}\cdot\sum_{i}\frac{\ell^{dep}_{i}\cdot L^{(i)}_{B}}{n_{BC} \cdot\langle L_{B}\rangle} \tag{14}\]

As seen from the equations the delivered luminosity value has to be corrected by a factor \(C\) which corresponds to a live-time average which is weighted by the luminosity of the individual BCs. This requires both the luminosity and the live-time for the individual BCs to be known.

The bunch luminosities \(L^{(i)}_{B}\) are foreseen to be determined from measurements by the luminosity monitoring systems, where as the monitor capabilities of live-time correlated to the BCs depend on its origin. The only dead-time contribution which is dependent on the bunch train structure is the simple dead-time introduced by the CTP after each L1A. It is introduced for specific BCs depending on the L1A occurrence of the previous BCs. Since the probability of a L1A in any BC depends on the bunch luminosity, and the possibility of one BC introducing dead-time for another depends on the LHC beam structure (e.g. long gap etc.), the knowledge of these two allows the live-time distribution between the bunches to be statistically determined. In addition the L1A rate should be monitored and preferably for individual BCs.

**Recommendation 2.3:** Luminosity monitor systems should be used for integrated luminosity determination both with and independent of the ATLAS TDAQ.

**Recommendation 2.4:** Luminosity should be determined for each BCID and, where possible, the L1A rate be monitored per BCID.

#### Representation of luminosity with respect to live-time and pre-scales

The ATLAS data will be selected by using many differently pre-scaled trigger chains. These trigger chains will also belong to one out of two trigger categories (_high_ or _low_ priority) which corresponds to different dead time periods. The dead time correction and pre-scaling is often taken into account in the luminosity values used in an analysis and in this case, depending on if single or multiple trigger chains are used, it has implications on which luminosity information has to be provided.

A simplified scenario, illustrated in figure 1, with two triggers is used to show the impact from using one data sample containing two different dead times. For simplicity it is assumed that the triggers select events from the same process, but with different trigger efficiencies.

In this case the events can be associated to two different integrated luminosities (\(L1\) (or \(\mathcal{L}_{1}\)) and \(L2\) (or \(\mathcal{L}_{2}\))) which in this example correspond to the same period of time but with different dead times according to the high and low priority trigger categories. Events from one or both of the time periods of \(\mathcal{L}_{1}\) and \(\mathcal{L}_{2}\) are then selected by one or both of the two triggers into a common event sample (\(E1\cup E2\)) as shown in figure 1.

In this scenario, where only trigger efficiency is taken into account, the relation between the number of selected events, the trigger efficiency and the luminosity becomes,

\[N=\sigma\cdot(\epsilon_{1}\mathcal{L}_{1}+\epsilon_{2}\mathcal{L}_{2}- \epsilon_{1}\cap\epsilon_{2}\mathcal{L}_{1}\cap\mathcal{L}_{2}) \tag{15}\]

This shows the fact that for a combined event sample, containing two different dead times, the luminosity and trigger efficiency parts of the expression are not factorisable, as in the case of a single dead time. For this reason the two luminosity values are proposed to be provided for data analysis separately together with their overlap1 and a combined use of the two would be implemented at the level of the individual analysis.

Footnote 1: Which is possible to monitor with the CTP.

The use of one event sample containing differently pre-scaled triggers gives rise to a similar problem. This scenario can also be illustrated by figure 1 if \(\mathcal{L}_{1}\) and \(\mathcal{L}_{2}\) are replaced by the event samples selected by the two triggers before pre-scaling and \(E1\) and \(E2\) corresponds to the samples after pre-scaling.

The number of selected events after pre-scaling is then related to the luminosity (using a single dead time) and trigger efficiency as,

\[N=\sigma\cdot\mathcal{L}\cdot(\epsilon_{1}C_{1}+\epsilon_{2}C_{2}-\epsilon_{1 }\cap\epsilon_{2}C_{1}C_{2}) \tag{16}\]

where \(C_{1}\) and \(C_{2}\) are the pre-scale factors used by the two triggers. Due to this relation of the pre-scale factors and the trigger efficiencies, the pre-scale factors for each trigger are proposed to be provided separately from the luminosity values.

Equation 16 also indicates the increasing complexity of analyzing an event sample containing multiple trigger chains. Where the number of overlaps to be determined increase fast with

Figure 1: Illustration of the relation between the events samples from two triggers with different dead times.

the number of different pre-scales/dead-times and creates a more entangled problem.

**Recommendation 2.5:** Luminosity information related to the two (complex) dead-times and the trigger pre-scales should be provided separately.

### 2.5 Market survey

Despite that the conditions for a luminosity measurement at the HERA and Tevatron experiments are different from the ones in ATLAS many elements related to obtaining a luminosity result to be used in analysis have the same importance. For this reason the procedure used at running experiments were investigated. The experiments that were mainly examined were H1, ZEUS, CDF and D0 and details of the luminosity measurements can be found in [17, 18, 19, 20].

The fact that the HERA experiments use a dedicated process (\(ep\to ep\gamma\)) with a cross-section and acceptance which can be calculated with high precision introduce additional differences with ATLAS. The Tevatron experiments on the other hand base their luminosity measurements on inelastic events in a very similar way as planned for ATLAS, where the main differences are related to the beam energy, intensity and structure.

From the overview it was observed that the following concepts were adopted by a majority of the experiments.

* **Luminosity Blocks:** All experiments associates data to time intervals smaller than the whole runs, i.e. equivalent to the luminosity block concept explained in this note. The time duration, \(\mathcal{O}\)(1 min), is normally determined empirically from operation, based on sufficient statistics for the measurement, constant running conditions etc. (see section 2.3).
* **Data Quality per LB:** Most of the experiments also use the luminosity block intervals to mask corrupt data, e.g. in case of high voltage trips or to indicate unsatisfactory operation of the different sub-systems.
* **Instantaneous \(L\) per BC:** Some methods used for luminosity measurements require that the luminosity per BC is determined, however this information is also commonly provided outside the luminosity community.
* **Dead-Time Handling:** Both methods based on dead-time correcting a luminosity measurements outside the trigger system as well as methods based on events passing the trigger are normally used. Having both methods available is valuable in order to produce fast on-line luminosity results, but at the same time having maximum control of the dead-time corrections.

* **Data Files and LBs:** Both the D0 and CDF experiments record data files which corresponds to an integral number of LBs. i.e. data from a specific LB is never separated into several files.

Most of these concepts are proposed for ATLAS in this note. The experiment survey in general also indicated that the most flexible and redundant solutions adopted have been the most successful ones. The difference between the determined value of the luminosity at the interaction point and the value used for analysis of an event sample, where especially dead-time corrections can be complicated, was also stressed.

## 3 Methods of luminosity determination

### Overview of luminosity methods

Several methods are planned for determination of both the absolute and the relative luminosity [2]. The relative measurement lack the overall normalization and has to be calibrated by absolute results obtained in parallel. Relative measurements will be calibrated at running conditions which suites a particular absolute measurement. After the calibration the relative method can be used under different conditions that are not suitable for the absolute method. During the initial running of ATLAS absolute luminosity results with high precision might not be accessible, however, it is important to keep in mind that if a relative luminosity measurement is possible during this time the results can always be re-normalized at a later stage when precise absolute results are available. The discussion about the different methods in this section is just meant to reflect the general principles. Many effects and details are left out, of which many will in principle not be known until LHC is running. The main methods are summarized below together with estimated starting date2 and accuracy.

Footnote 2: An operational year of 2007 implies that the system will be operational already at the first pilot run. The operational year of 2008 for the Roman Pot detectors is also dictated by the availability of a well behaving beam using the special high \(\beta^{*}\) optics.

* **Roman Pots (Absolute \(\mathcal{L}\))**[3] **Operational \(\sim\) 2008, expected \(\Delta\mathcal{L}/\mathcal{L}\) = 2-3%** One pair of Roman Pot stations mounted, at a distance of 240m on each side of the IP, will be used to determine the absolute luminosity primarily from elastic proton scattering in the Coulomb interference region. This measurement will only be made at so-called high \(\beta^{*}\) runs, at low luminosity, during which the luminosity monitor systems are calibrated. The Roman Pot detectors are planned to provide level 1 trigger information and data to the ROS for triggered events. Due to the low event rate (\(\sim\)30\(Hz\)) all triggered events can be recorded and the luminosity measurement will be completely based on the Roman Pot event data.
* **LHC Machine Parameters (Absolute/Relative \(\mathcal{L}\))**[4] **Operational \(\sim\) 2007, expected \(\Delta\mathcal{L}/\mathcal{L}\) = 5-10 %** The instantaneous luminosity at ATLAS can be estimated from LHC beam parameters. Ingredients are the number of protons per bunch and beam, the transverse beam dimensions (\(\beta\) function and beam emittance), and the transverse displacement of the two beams. The precision is expected to be dominated by the uncertainty of the emittance of the beam, and is estimated to be on the order of 5-10 %. This method is likely to be the first method to give a reasonable luminosity estimate.
* **W/Z Counting (Absolute/Relative \(\mathcal{L}\))**[5] **Operational from 2007, precision: 5-10 % absolute, 1-5 % relative Range: \(\mathcal{L}>10^{33}\)** Assuming a precise knowledge of the parton distributions in the proton and of thepartonic cross-section for W/Z production, the determination of the W/Z rate from leptonic decays can be used to calculate the absolute luminosity. With present pdf uncertainties and taking detector effects into account, the absolute luminosity can be determined to about 10 %. Using the LHC data to constrain the pdfs will reduce this uncertainty. Despite these large systematic uncertainties, the high rate of W/Z production will allow for online relative luminosity monitoring with high statistical precision: with standard W/Z cuts, at \(10^{34}\) cm\({}^{-2}\)s\({}^{-1}\) a statistical precision of 5 % (1 %) is expected after 10 s (3 min).
* \(\mu\mu/ee\) Counting (Absolute/Relative \(\cal L\)) **Operational late, expected \(\Delta\cal L/L=2\) % (10 fb\({}^{-1}\)) Range: \(\cal L>10^{32}\) for \(\mu\mu\), \(\cal L<10^{33}\) for \(ee\)** The cross-section of the electromagnetic process of lepton pair production from two-photon fusion is calculable to a precision of 1 %. The cross-section is small, especially in the central region: for central di-muons with \(p_{T}>3\) GeV it is about 1 pb. Some of the characteristics of the muon pair from two-photon fusion can be used to suppress background: central production (acceptance of the muon system), low invariant mass, small total \(p_{T}\) and a back-to-back requirement to suppress Drell-Yan processes. The method is difficult and will require significant analysis work. In particular backgrounds need to be well controlled and the acceptance understood.
* **LUCID (Relative \(\cal L\))**[3] **Operational \(\sim\) 2007** LUCID is a Cerenkov detector consisting of 168 tubes mounted, at a distance of 17m, on each side of the IP. The detectors are designed to have a sufficient time resolution in order to identify individual BCs and with a particle counting capability. The luminosity monitoring is based on inelastic events, where LUCID approximately will cover \(\eta=5.5-6.1\), and the method is well suited for the whole luminosity range at the LHC. The LUCID detector will provide level 1 trigger information and data to the ROS for triggered events. In addition local luminosity monitoring is foreseen at a bunch-by-bunch level by using scaler information and a private LUCID data stream.
* **Beam Condition Monitor (Relative \(\cal L\))**[6] **Operational \(\sim\) 2007** The Beam Condition Monitor (BCM) consists of four \(1\times 1\) cm\({}^{2}\), \(500\mu m\) thick diamond sensors which are mounted in modules together with the FE electronics at a distance of 1.8m on each side of the IP and at 5cm radius from the beam axis. The BCM is designed with a sufficient time resolution to identify individual BCs and with the sensitivity to a single particle transversing the active area (\(8\times 8\) mm\({}^{2}\)) of the detector. From the amplitude measurement it can also determine the number of particles (i.e. count the particles) transversing the detector at once. The luminosity monitoring is based on inelastic events, where the BCM approximately will cover \(\eta=3.9-4.1\) and the system is well suited for the whole luminosity range at the LHC. The BCM is foreseento provide monitoring information about the beam conditions, but the possibilities for additional luminosity information is under study.
* **MBTS (Relative \(\mathcal{L}\))** [7] **Operational \(\sim\) 2007** The Minimum Bias Trigger Scintillator counters consists of one plane with \(2\times 8\) scintillator segments on each side of the IP mounted in front of the LAr end-cap. The MBTS will only be used during the commissioning phase of ATLAS, but during that time, \(\mathcal{O}\)(months), it can be used to determine the luminosity by counting the minimum bias trigger rate. The luminosity monitoring is based on inelastic events and the MBTS will approximately cover \(\eta=1.9-3.8\). The MBTS will provide level 1 trigger information and data to the ROS for triggered events. Since the MBTS will experience significant radiation damage, a precise measurement of the integrated luminosity will be very difficult.
* **TileCal (Relative \(\mathcal{L}\))** [8] The ATLAS tile calorimeter has a minimum bias monitor system which is based on the integrated anode current of the PMTs. This information will also be used for monitoring the luminosity and is best suited for operation at high luminosity. The tile calorimeter will provide relative luminosity information from the local monitor system outside the event stream.
* **LAr (Relative \(\mathcal{L}\))** [9] The ATLAS LAr calorimeter have the possibility to measure the relative luminosity by monitoring the high voltage current in the LAr. The LAr calorimeter will provide relative luminosity information from the local monitor system outside the event stream.

Most of the methods planned to measure absolute luminosity are based on specific final states and all the information for the measurement is present in the event data. The absolute luminosity measurements, however, have to be complemented by luminosity monitor measurements. One main reason for this is that the absolute methods in general are less well suited to obtain online luminosity results during the running of ATLAS. Another reason, as mentioned above, is that absolute methods often are not suitable for the whole dynamic range of ATLAS. For example the measurement by the Roman Pot detectors, with the highest expected precision, will be done at special runs with \(L=10^{27}\) cm\({}^{-2}\)s\({}^{-1}\), which is seven orders of magnitude smaller than the LHC design luminosity.

All the methods to determine absolute luminosity using the ATLAS detector are based on the known cross section of a particular process which translates the number of corresponding events in a data sample into its integrated luminosity. This is for example the case of the methods using the number of measured \(Z\), \(W\), \(\mu\mu\) or \(ee\) events where the main difficulties are related to understanding the detector response and efficiencies. The Roman Pots will primarily be used to determine the luminosity from a fit of the well known \(t\)-distribution of elastic proton scattering in the Coulomb interference region to data. This method givesthe best precision, but requires that the Roman Pot detectors can operate at a 1-\(2mm\) distance from the LHC beam. The Roman Pot detectors also provide a second possibility to measure the luminosity by means of the Optical Theorem. This measurement links the luminosity to the total event rate and the elastic event rate in the forward direction. The method however implies larger uncertainties on the measurement than the former, due to the stringent requirement on acceptance coverage (\(\eta\) up to 7-8) in order to make a precise measurement of the inelastic event rate.

By using the luminosity monitors in parallel to the absolute luminosity measurements the relative measurements by the monitors can be properly normalized. Most of the luminosity monitor methods can then provide a measurement of the integrated luminosity for the individual BCs (\(\mathcal{L}_{B}^{i}\)) at a fast pace and these can then be used both to determine the over all integrated and instantaneous luminosity.

Assuming that the time interval is small enough, \(\mathcal{O}(1\) min) or less, to approximate the integrated luminosity of a specific BC, \(BCID=i\), to be constant each LHC turn, the integrated luminosity is given by,

\[\mathcal{L}=n_{turn}\sum_{i}\mathcal{L}_{B}^{i} \tag{17}\]

where \(n_{turn}\) represent the number of LHC turns that has passed. The bunch luminosities can also be directly translated into instantaneous luminosity by the BC frequency, \(f_{BC}\),

\[L^{i}=\mathcal{L}_{B}^{i}\cdot f_{BC} \tag{18}\] \[\langle L\rangle^{Total}=\langle\mathcal{L}_{B}\rangle\cdot 3564 \cdot f_{BC} \tag{19}\]

where the nominal BC frequency for a specific BCID is \(f_{BC}=1/3564\times 40MHz=11.2kHz\). In this section the average bunch luminosity, \(\langle\mathcal{L}_{B}\rangle\), is based on all the 3564 BCIDs, of which only 2808 will be filled. Since the decrease of luminosity with time is not significant until periods larger than \(\mathcal{O}(1\) min) the \(\mathcal{L}_{B}^{i}\) measurement can be done once per LB without introducing a significant error when interpolating values in between.

### Methods for Luminosity Monitoring

Most of the relative luminosity measurements will be based on measuring inelastic events. Using the LUCID or BCM system the monitoring can be done at the level of individual BCs at 40MHz. This removes two orders of magnitude from the dynamic range between \(L=10^{27}-10^{34}\) cm\({}^{-2}\)s\({}^{-1}\) since the luminosity increase due to a larger number of bunches in LHC do not affect the monitoring capabilities. For the MBTS the foreseen way of counting colliding BCs will also be done at the level of individual BCs during the startup of ATLAS. The methods to monitor luminosity using the Tile and LAr calorimeters do not have a BC time resolution, but are based on their average currents. Work is ongoing with respect to the implementation of these methods and no further details will be discussed here.

Since the luminosity monitoring is based on high rate events, the monitors with sufficient time resolution to measure individual BCs have to be read out separately of the event stream or highly pre-scaled. This is because even at modest luminosities the collision rate will be very high (\(L=4\times 10^{32}\) cm\({}^{-2}\)s\({}^{-1}\rightarrow\mu=1\), where \(\mu\) is the mean number of inelastic interactions per BC). Due to the high inelastic event rate at design luminosity the measurement can be made very fast, where in some cases a sample with a 1% statistical uncertainty can be recorded in \(\mathcal{O}\)(1 ms). Depending on the design of the monitor system the luminosity, \(\mathcal{L}_{B}^{i}\), can be determined from inelastic collisions mainly using two different methods [10].

The first method (I) relates the luminosity to the total collision rate of the BCs (equivalent to the _zero counting method_ which for example is described in [10]). Using this method the luminosity for a specific \(BCID=i\) can be determined during a defined time interval from,

\[\frac{n_{pp}^{i}}{n_{BC}^{i}}=1-e^{-A\cdot\mathcal{L}_{B}^{i}}\ \ \left\{\begin{array}{l}A=\epsilon_{pp}\cdot\sigma_{Inel}\\ \mathcal{L}_{B}^{i}=L^{i}/f_{BC}\end{array}\right. \tag{20}\]

where \(n_{pp}^{i}\) is the number of times one or more interactions have been detected for \(BCID=i\) by the luminosity monitor. \(n_{BC}^{i}\) is the total number of crossings of \(BCID=i\), \(A\) a calibration constant and \(\mathcal{L}_{B}^{i}\) the bunch luminosity for \(BCID=i\). \(\epsilon_{pp}\) refers to the detection efficiency of the luminosity monitor, \(\sigma_{Inel}\) the inelastic cross section, \(L^{i}\) the instantaneous luminosity and \(f_{BC}\) the BC frequency. Equation 20 express, using Poisson statistics, the probability of detecting a BC as the complement to the probability of not detecting a BC, using a detector with \(\epsilon_{pp}\) detection efficiency of inelastic events. The fact that this method does not require particle count capability from the luminosity monitor makes it possible to use with any of the LUCID, BCM or MBTS systems. Method I is, however, less well suited for luminosities with a large amount of pile-up since the difference \(n_{pp}^{i}-n_{BC}^{i}\) can become very small.

It is also important to keep in mind the implications from the non-linear relation between the luminosity and the measured fraction of colliding BCs. This is especially important in the case of high luminosity, where equation 20 becomes highly non-linear, since the integrated luminosity expressed as the sum of the contributions from each BC do not correspond to the mean number of collisions obtained from all the BCs3,

Footnote 3: In this section the fact that the time interval of an integrated luminosity value does not exactly correspond to an integral number of turns is neglected.

\[\mathcal{L}=n_{turn}\sum_{i}^{3564}\mathcal{L}_{B}^{i}=-\frac{n_{turn}}{A} \sum_{i}^{3564}\ln\left(1-\frac{n_{pp}^{i}}{n_{BC}^{i}}\right)\neq-\frac{n_{ turn}\cdot 3564}{A}\ \ln\left(1-\frac{\sum_{i}n_{pp}^{i}}{\sum_{i}n_{BC}^{i}}\right) \tag{21}\]

This imply that in order to use method I at high luminosities, \(n_{pp}/n_{BC}\) must be determined for each BC separately unless all the bunch luminosities are identical. This is however not important at low luminosities where the exponential expression becomes linear, \(n_{pp}^{i}/n_{BC}^{i}=[\mu\ll 1]\sim A\cdot\mathcal{L}_{B}^{i}\).

**Recommendation 3.1:** Luminosity monitor systems that determine luminosity from collision rates should determine \(n_{pp}/n_{BC}\) separately for each BCID.

The second method (II) relates the luminosity to the mean number of particles per BC,

\[\langle M\rangle=\sum_{i,j}^{3564,n_{turn}}\frac{n_{part}^{ij}}{3564\cdot n_{turn} }=\langle C\rangle\cdot A\cdot\langle\mathcal{L}_{B}\rangle \tag{22}\]

where \(\langle M\rangle\) is the measured average number of particles per BC, \(n_{part}^{ji}\) the number of particles obtained from \(BCID=i\) at turn \(j\) and \(\langle C\rangle\) the average number of particles per detected interaction (obtained at low luminosity where one interaction per BC in principle is guaranteed). Unlike method I this method do not saturate when pile-up is introduced and it is for this reason better suited for high luminosities. The linear relation ship between \(\langle M\rangle\) and \(\mathcal{L}_{B}\) allows for an integrated luminosity measurement which does not have to treat each BCID separately,

\[\mathcal{L}=n_{turn}\cdot 3564\cdot\frac{\langle M\rangle}{\langle C\rangle \cdot A} \tag{23}\]

The method can, however, also be used to determine the luminosity for individual BCIDs. Method II does require particle count capability and for this reason it can only be used properly by LUCID and the BCM.

## 4 Luminosity recording in the TDAQ system

The luminosity needs to be corrected for event losses of all kind in the whole TDAQ system: event losses due to pre-scaling, dead-time, and system failures. This section describes which relevant information needs to be gathered from the various TDAQ components (Level-1 trigger, High-Level Trigger and Dataflow), and how LBs can be introduced in order to obtain this information coherently per LB.

### Information from the Level-1 trigger

The Level-1 Central Trigger Processor (CTP) has for each of its 256 trigger signatures (= item) a 31-bit counter (plus one overflow bit) which counts the number of triggers before pre-scaling (TBP), before veto (TBV) (sometimes called after pre-scaling (TAP)) and after veto (TAV). The read-out of the counters is synchronous to the LHC revolution and an LHC turn counter is always read out synchronously with the TBP and TBV/TAV counters. The exact length of the time interval can be determined from the turn counter value.

The Level-1 pre-scale factor for each trigger signature can be determined either from the configuration of the CTP, or from comparing the counter values for TBP and TBV:

\[p^{i}=\frac{n^{i}_{TBV}}{n^{i}_{TBP}} \tag{24}\]

The Level-1 average live-time for each trigger signature can be determined using the counter values for TBV and TAV:

\[l^{i}=\frac{n^{i}_{TAV}}{n^{i}_{TBV}} \tag{25}\]

The read-out of the counters is done via VME with the sequence: set BUSY, stop counters, read-out, reset, re-start, release BUSY. This can be done quasi-synchronously and with negligible error. The counters for TBV, TAV and the corresponding LHC turns can be stopped (and started) at the same time, whereas the counters for TBP and its corresponding LHC turns are controlled by a separate VME cycle. This could lead to situations where the counter values are off by one LHC turn, which does not pose a problem: first, the turn counter values indicate this mismatch, and a correction is trivial. Second, one LHC turn is only \(10^{-6}\) compared to a LB of length \(\mathcal{O}(\mathrm{min})\) and therefore negligible anyway.

The read-out frequency of the CTP counters must be high enough such that the counters will not overflow. As the counter values of all trigger signatures are read out at the same time, the highest-rate trigger signature determines the read-out frequency. Under the assumption that the highest-rate trigger signature triggers every bunch-crossing, a value of 53 seconds4 is obtained. Note, however, that this does not necessarily pose a hard limit on the length of a LB, as the read-out of the counters can be much more frequent than the LB length. Theseintermediate read-outs could be done without introducing dead-time. The counters would only miss about one LHC turn per read-out, which would lead to a negligible error.

**Recommendation 4.1:** The Level-1 TBP, TBV and TAV values should be obtained per LB. The actual counters might need to be read out more frequently (more than once per minute) to prevent overflow. The information, together with the Level-1 prescale configuration values, should be published per LB in the TDAQ Information Service, from where it is available on-line and will be stored in the conditions database.

### Information from the HLT and dataflow

Fig. 2 shows a block diagram of the TDAQ system. For the determination of corrected luminosity, information on the pre-scaling and on event-losses is needed, which could be obtained by having scalers at various stages of the HLT and Dataflow. We have noticed that currently no coherent concept exists for obtaining this information. In the following, we discuss a possible strategy to obtain the necessary information, with emphasis on which information to use - concerning pre-scales and event losses - and how to make it available on-line and off-line.

#### 4.2.1 Pre-scales

The Level-2 and the EF will pre-scale5 each Level-2/EF chain, which will be implemented in each trigger processing task. Therefore a set of counters is needed in each processing task of Level-2 and the EF, which count for each trigger chain the number of events before the pre-scaler and after (see Table 2). These measured values can be used to do the pre-scale corrections. It should be noted that alternatively the pre-scale configuration parameters could be used, but it is more accurate to measure the pre-scaling directly by counting. The counter values reflect what was really happening and will be free of potential biases due to short time intervals. Nevertheless, the pre-scale configuration parameters will allow for consistency checks with the measured values, and will still be useful for correcting rates in monitoring applications.

Footnote 5: Please note, that this includes using a pre-scale value of 1, which effectively means _no pre-scaling_

**Recommendation 4.2:** In HLT, for each LB, scalers should be provided that count for each trigger chain the number of triggers before pre-scale, and after pre-scale. The scaler values, as well as the pre-scaling configuration parameters, need to be made available on-line with a latency of a few LBs, and need to be recorded for later use in off-line analyzes.

[MISSING_PAGE_EMPTY:26]

#### 4.2.2 Event losses due to failures

There are two categories of failures, whose event losses affect the LBs in a different way:

Unrecoverable failures:The TDAQ system depends fully on the proper functioning of a number of components, like the Level-1 trigger, RoIB, and DFM, and to a lesser but still significant extent the L2SVs, SFIs and SFOs.6 The run is likely to be stopped if one of these components breaks down. All affected LBs would be excluded from any cross-section analysis and therefore need to be flagged off-line.

Footnote 6: It should be noted that the applications that handle dataflow (DFM, L2SVs, SFIs, SFOs) are much less likely to fail, because they do not process event data.

Tolerable (or correctable) failures:Failures of the L2Ps or EFPs could cause event losses, but also event duplication is possible: for example time-outs due to processing delays could cause the L2SVs/DFM to duplicate events. These losses/duplications will probably happen with a fraction much smaller than the precision of luminosity determination \(\mathcal{O}\)(1 %). LBs which include such failures can still be used in a cross-section analysis, provided it is proven that the losses can be either neglected or be corrected for.

The best way to address both kinds of failures is by having event counters in each system component for each LB. This would detect LBs in which unrecoverable failures have happened, and quantify the extent of tolerable failures. In addition, having some redundancy in the counters would allow for cross-checks, which is particularly important in the start-up phase of ATLAS. Tab. 3 lists possible counters to be implemented in the various software applications of the TDAQ components.

\begin{table}
\begin{tabular}{l l l} \hline \hline Component & Counter & Comment \\ \hline L2P \(l\) & \(N_{TBP,ij}^{L2P,l}\) & Trigger before pre-scale per trigger chain \(j\) \\  & \(N_{TAP,ij}^{L2P,l}\) & Trigger after pre-scale per trigger chain \(j\) \\ EFP \(n\) & \(N_{TBP,ij}^{EFP,n}\) & Trigger before pre-scale per trigger chain \(j\) \\  & \(N_{TAP,ij}^{EFP,n}\) & Trigger after pre-scale per trigger chain \(j\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Proposed counters for Level-2 and EF processes, counting trigger before and after pre-scaling for each LB \(i\) and each trigger chain \(j\).

[MISSING_PAGE_EMPTY:28]

The following equations hold in case of a correct functioning of the TDAQ system:

\[N_{L1A,i}^{CTP} = N_{L1A,i}^{RoIB} \tag{26}\] \[N_{L1A,i}^{RoIB} = \sum_{k}N_{L1A,i}^{L2SV,k}\] (27) \[N_{L1A,i}^{L2SV,k} = N_{L2R,i}^{L2SV,k}+N_{L2A,i}^{L2SV,k}\] (28) \[\sum_{l}N_{L2R,i}^{L2P,l} = \sum_{k}N_{L2R,i}^{L2SV,k}\] (29) \[\sum_{l}N_{L2A,i}^{L2P,l} = \sum_{k}N_{L2A,i}^{L2SV,k}\] (30) \[N_{L2A,i}^{DFM} = \sum_{k}N_{L2A,i}^{L2SV,k}\] (31) \[N_{L2R,i}^{DFM} = \sum_{k}N_{L2R,i}^{L2SV,k}\] (32) \[N_{L2A,i}^{DFM} = \sum_{m}N_{i}^{SFI,m}\] (33) \[\sum_{m}N_{i}^{SFI,m} = \sum_{n}\left\{N_{EFA,i}^{EF,n}+N_{EFR,i}^{EF,n}\right\}\] (34) \[\sum_{n}N_{EFA,i}^{EFP,n} = \sum_{r}N_{i}^{SFO,r} \tag{35}\]

**Recommendation 4.3:** In order to quantify event losses due to failures in each LB, event scalers for each LB should run in all applications that control TDAQ components, and if possible, also in the CTP and RoIB. Applications for on-line consistency checking and event loss monitoring should be implemented.

In case of crashes of trigger processes, the Level-2 and the EF react in a different way:

* The Level-2 trigger is designed in such a way, that there will not be any event-losses due to crashes of L2Ps, because the data is stored in the ROSs, and the L2SV can detect timed-out events. In this case, it can re-assign the event to a new L2P or flag it to follow a special debug stream, where the event can be analyzed off-line. In the latter case, the 'trouble-maker' event will be treated late, but will not be lost. In case buffers fill up in the Level-2 system, dead-time will be generated at Level-1.
* For the EF it is important to distinguish between crashes of an EFP and EF machines. Once an event is assigned and copied to an EF machine, it is deleted from the SFIs. In case only the EF process crashes, then the event still resides on the machine and can get reprocessed or streamed out on the debug stream. If the EF machine crashes, then the event might get lost.

**Recommendation 4.4:** For events that cause on-line failures in the Level-2 trigger or in the EF, and that are streamed out on the debug stream, there should be tools available for later - after the bug fixing - inclusion into the appropriate off-line data-sets and updating the luminosity-relevant information.

#### 4.2.3 Publishing counter information

Since processes in HLT can potentially crash on certain events, the information from their counters should be put to a safe place in regular, short time intervals.

**Recommendation 4.5:** All counter values in HLT and dataflow previously mentioned, for each LB, should be made available on-line with a latency of a few LBs, and should be recorded for later use in off-line analyzes.

The TDAQ Information Service (IS) [11] could be used to perform this task, as it allows the publishing of information to a central service, from where monitoring applications can retrieve it, and which also has an interface to the conditions database for storage. The actual implementation of publishing the counter values would go beyond the scope of this report. We only discuss two possible solutions:

1. For each LB, a new set of counters could be created in the applications, and with a frequency of \(\mathcal{O}\)(min) the current counter values could be published to IS, using LBN, name of the process and type of counter to identify the IS entry. Obviously, a huge number of different IS entries would exist: per LB, per process, per counter. The advantage would be that every process would have a separate entry in IS, which facilitates the gathering with existing tools and would also allow debugging for each individual process.
2. Only the LBN and the type of counter would be used to identify the IS entry. A process could delete a counter after its value has been published. This should happen when it is likely that a new LB has begun, typically after a few minutes. In case a process receives an event with a LBN that does not have a corresponding scaler running already, a new scaler can be instantiated. This case could happen either when a new LB has started, or when the scaler value of an old LBN has already been published and deleted. This would reduce the number of different IS entries, because for a given LBN and for the same type of counter, all processes would publish to the same IS entry. The sum of all these entries could be taken by using the history function in IS. The disadvantage of this implementation is that the information on the origin of the counter value is lost, and debugging on the process level would be impossible.

**Recommendation 4.6:** The implementation options for counters in HLT and dataflow should be studied, in particular its performance using IS, and the constraints IS poses on it.

Another type of failure in the TDAQ system would be the break-down of critical applications in the on-line software, such as IS servers, controllers, etc.

**Recommendation 4.7:** Applications in the on-line system which are critical for a correct determination of the luminosity should be identified, and mechanisms to identify failures of these applications and allow to correlate them with LBs should be implemented. This could be done on a time-stamp basis, with a time precision \(\mathcal{O}\)(LB length).

### Transition between luminosity blocks

There are 3 issues concerning the transition to a new LB that need to be addressed, which are related to one another:

1. **Consistent read-out of all luminosity-relevant information.** This includes scaler values at Level-1, Level-2 and at the level of the EF, scalers in the dataflow, as well as scalers in luminosity monitor systems. Also, time-stamped information from the LHC machine (available through DCS and IS) has to be matched to the LB boundaries.
2. **Tagging of LBs** in a way that every event can be assigned unambiguously to one LB.
3. On-line supervision of LB transitions (see Def. B.13)

Fig. 2 shows the different parts of the ATLAS TDAQ system. The components LVL1 and RoIB operate (quasi-)synchronously and serially, which means they process events quickly (at very short time-scales \(<10\)\(\mu\)s) and in time-order. The subsequent components operate asynchronously and in parallel: events are assigned to individual processing tasks, which process them in parallel and at much longer time-scales (\(\gg 10\mu\)s, up to minutes in some cases). At this stage the events will loose time-order.

A variety of ways have been discussed in order to implement LBs in such a system: time-stamps, Checkpoint mechanism [12], Event Counter Reset, or introducing a LBN in the CTP event data. All these solutions but the last one have been found problematic. The last solution, however, seems to avoid all drawbacks of the others and is considered to serve well its purpose. In the following, we will recommend and discuss only this solution.

The solution is based on tagging LBs with a number:

**Recommendation 4.8:** A LB should be tagged with a LB number (LBN), which uniquely identifies the LB within a given run.

This LBN needs to be transferred to the Level-2 trigger, EF and dataflow applications, and to luminosity detectors with local scalers. The idea is to have the CTP put the LBN in its data for the RoIB and its event data. Level-2 applications would see the LBN in the RoI data of the CTP; the EF and dataflow applications would see it in the CTP event data fragment.

**Recommendation 4.9:** In the CTP, a read/writable VME register should be implemented to hold a LBN (at least 14 bits), which will be put into the CTP event and RoI data fragment for each event accepted by Level-1.

Note: Assuming worst cases, a 30 h-long run with LBs of length 10 s, gives about 10000 LBs, corresponding to 14 bits. As the CTP internally works with 16-bit words, 16 bits would be the natural choice. This would require a change in the CTP firmware, which we clarified to be technically feasible [13]. This would result in a 16-bit LBN.

**Recommendation 4.10:** The Level-2 processing tasks should be able to read the LBN in the CTP RoI data fragment and increment the appropriate scalers for this LBN.

**Recommendation 4.11:** EF and dataflow applications should be able to act on the LBN and increment the appropriate scalers. The LBN should be made available in the CTP event data fragment and in the event header.

It is particularly attractive to put the LBN into the event header, which could happen at the level of the SFI. According to dataflow guidelines, the SFIs should do event building only based on header information, not using any event data. A possible solution could be to include the LBN in the messages that the L2SVs send to the DFM, and the DFM could pass on the LB to the SFI for each event [14].

**Recommendation 4.12:** A mechanism should be invented such that the CTP control software can issue a hardware pulse (e.g. NIM pulse of width \(W\) with 25 ns \(<W\ll\) duration of LB transition) at any moment.

This signal could be triggered during a LB transition and be fanned out to all systems which locally count luminosity-relevant information. One possible implementation could be based on using additional hardware (e.g. additional VME module) in the CTP crate.

In addition to the LBN, there will be time-stamped information (e.g. from DCS, LHC) that needs to be matched with the LBs. For this reason, we recommend:

**Recommendation 4.13:** The start and end time of a LB should be recorded.

A precision of order ms would be enough. This can be easily achieved by polling the VME register in the CTP that contains the GPS time (see Tab. 4). Note, that the duration of a luminosity block can precisely be determined using the turn counter of the CTP.

**Operational down-time** will be introduced by the CTP and by the on-line LB supervisor.

**Recommendation 4.14:** The operational down-time introduced by a LB transition ought to be kept to a minimum \(\mathcal{O}\)(ms).

In the following, we estimate the down-time introduced by the CTP. Typical VME access times for single cycles are \(\mathcal{O}\)(1 \(\mu\)s), and \(\mathcal{O}\)(20 MB/s) for D32 block transfer. Table 4 shows the proposed sequence of steps for the CTP during a LB transition, which can be done in less than one millisecond, under normal conditions, which means that the CPU controlling the CTP will be free to do its tasks without interruption. It needs to be guaranteed that no significant delays are introduced, and that there are no triggers during the VME read-out.

The CTP will technically implement LBs, but does not necessarily play the role of the **on-line LB supervisor**. There are good reasons to defer this task to the Run Control application, which centrally controls the whole data-taking process. Run Control could send messages to the CTP to initiate a LB transition. This has the advantage that during a run slight adjustments to the configuration (disabling channels, etc.) could be done on LB boundaries, and that monitoring applications could be notified of LB changes as well.

**Recommendation 4.15:** Run Control should be the on-line LB supervisor and initiate LB transitions by sending a message to the CTP. Slight adjustments to the configuration during a run should only be done on LB boundaries.

### Requirements from the luminosity sub-systems

Notification of the luminosity monitor sub-systems about the LB transitions is a necessity in order to have maximum flexibility and control of the luminosity measurement. This information is not important for measuring the instantaneous luminosity, which is not expected to change at the order of one minute, but for the integrated luminosity very little can be estimated locally at the luminosity monitor system without the information about the LB transitions.

Without the ability of identifying LBs locally the integrated luminosity can in principle only be determined by two methods with very limited possibilities for cross checks and consistency studies. These main two methods would then be to either use pre-scaled events

[MISSING_PAGE_EMPTY:34]

from a luminosity trigger or the collision rate measured from minimum bias triggers provided to the CTP. Beside the limited number of methods, this scenario would also allocate common resources of the event stream and CTP which would not be necessary to the same extent if the LB information is sent to the luminosity monitor sub-system. Also previous experiments have stressed the importance of flexibility and keeping the assumptions entering the luminosity measurement to a minimum.

In the following it is assumed that a LB will correspond to about 1 minute as discussed in section 2. Since the luminosity is believed to be measured at a 1-2% level at best, the uncertainty related to the LB time period should be well below that. Since 1 permille uncertainty is equal to a 60 ms time uncertainty of the LB, this implies a quite relaxed timing constraint for the LB mechanism with respect to the ATLAS time scales. This could be used as a guide line for the required absolute time precision of notifying the luminosity sub-detectors about a LB transition.

Since the ratio \(n_{pp}/n_{BC}\) in method I, described in this section, becomes close to 1 at luminosities with a large amount of pile-up, care has to be taken with respect to how the \(n_{pp}\) and \(n_{BC}\) values are obtained. If for example \(n_{pp}\) is counted by LUCID and \(n_{BC}\) is determined at the CTP together with the LB, a small relative miss match between the counting periods used to obtain \(n_{pp}\) and \(n_{BC}\) can give an significant error on the luminosity measurement. For example when the luminosity is approaching \(L=10^{34}\) cm\({}^{-2}\)s\({}^{-1}\) and the full LUCID acceptance is used, a difference in the order of \(\mu s\) can give a significant error on the derived luminosity. The \(n_{pp}\) and \(n_{BC}\) counter functionality should therefore be located at the same place in order to be implemented in a way which guarantee that they are read out with respect to the same period. This is the case for the scalers in the CTP, which can be used to provide the required information for method I. For example the so called \(TBP\)- and \(turn\)-counters, which can be used to determine \(n_{pp}\) and \(n_{BC}\), have their enable/disable synchronized with the orbit and can therefore be associated with each other at a BC precision.

Using method II the relative precision between \(n_{part}\) and \(n_{BC}\) have the same linear effect on the luminosity uncertainty as the absolute precision of the LB transition at the luminosity sub-detector. Without the information about the LB transition locally at the luminosity detector method II could only be used with recorded data and assuming that luminosity events can only allocate a small fraction of the recorded events, results with high statistical precision of the integrated luminosity from method II (preferred at high luminosity) can only be obtained for data samples extending over many LBs.

For these reasons the LB transition is necessary at the luminosity sub-detectors to disallocate resources from the CTP and the event stream, maximize the redundancy of the luminosity measurement and to have the possibility of obtaining high statistics results in a short time from both method I and II. For local measurements of the integrated luminosity, e.g. using scaler values or a local event stream, the LB transition only has to be known at a 60 ms precision (assuming a 60-second LB and that the scaler functionality is implemented as described above) to introduce a 1 permille error.

**Recommendation 4.16:** The luminosity detector systems should be notified of the LB transition with a latency well within one permille of the LB duration.

### Physical clustering of luminosity blocks

For analysis it would be of great advantage if the events belonging to one LB could be physically clustered into files or file sets. Other experiments cluster the events physically in the file, such that each LB can only occur in one single file, although many LBs per file are possible. At ATLAS, this will not be possible, because the events are written by the SFOs to many files simultaneously. Even if only one file would be filled at the time, a file would fill up after 6 s at 200 Hz, too short to contain a LB of size \(\mathcal{O}\)(min) (a raw event size of 1.6 MB is assumed, and the file size limit is 2 GB). But physically clustering events of one LB into file sets might be possible. In principle, this could be done either on-line with the raw data, or in future steps of data processing (ESD, AOD).

It could be useful to already on-line write events into files only with increasing LBN. This would considerably simplify the clustering of events into file sets at sub-sequent production steps. In addition, it could be useful if raw date files would respect LB boundaries, such that each file contains events of only one LBN, but a given LBN corresponds to several files. Perhaps this could be implemented in the TDAQ system with a combination of queuing events and assigning them to processors based on the LBN.

**Recommendation 4.17:** Feasibility and performance issues (trigger rate, dead-time) of on-line writing events into files only with increasing LBN should be studied. In addition, the feasibility of raw data files respecting LB boundaries should be explored.

Currently there are discussions about event streaming - a topic which is coupled to the physical clustering of events according to LBN.

**Recommendation 4.18:** The possibility of physically clustering events of the same LB to files/file sets should be reviewed and evaluated during the discussions on event streaming.

## 5 Offline event selection and analysis

When an event sample is selected for analysis, it is essential that one be able to obtain the information necessary for integrated luminosity and cross section determination. This section discusses the requirements that luminosity determination places on the database and book-keeping systems.

### Luminosity data in the offline conditions database

A great deal of data - from the LHC, from the TDAQ system, from specialized detectors, and from physics processes - will be available offline for luminosity experts to use in determining the numbers that should be used by the collaboration for understanding sample cross sections and integrated luminosity. Here we list only the quantities that we expect physicists to access in cross section and luminosity determination for their samples, not all the data used by experts in computing those quantities.

For each luminosity block, we expect to retain in the offline conditions database

* \(t^{start}\), the start time of the luminosity block;
* \(t^{end}\), the end time of the luminosity block;
* \(\mathcal{L}\), a measured or computed luminosity for the corresponding time interval, endorsed for use in offline analysis. (The conditions database may in fact contain several luminosity estimates computed by a variety of methods for any given luminosity block. In the discussion that follows, it suffices to assume that one such estimate has been identified and endorsed for use in a given analysis.)

For each trigger chain \(T\), the offline conditions database will also retain, for each luminosity block,

* \(\ell^{T}\), the trigger live time;
* \(p^{T}_{1,2,3}\), the corresponding Level 1, Level 2, and Event Filter pre-scales associated with the trigger signature;

### Sample selection and book-keeping

We restrict our attention here to samples drawn from events that satisfy a single trigger chain.

Any sample selection from a single trigger chain should have an associated temporal component, explicitly or implicitly (a range of runs, for example). For such a sample \(S\), let \(T\) be thetrigger chain from which it is derived, and \(LB^{S}\) the set of luminosity blocks corresponding to the temporal component of the selection.

Let \(N\) be the number of events in a derived sample; then the corresponding cross section \(\sigma\) may be computed using Equation 3. Writing that equation explicitly in terms of \(S\) and the three levels of the trigger chain \(T_{1,i}\) through \(T_{3,i}\):

\[\sigma=\frac{N}{A\sum_{i\in LB^{S}}\epsilon_{i}^{T}p_{1,i}^{T}p_{2,i}^{T}p_{3, i}^{T}\ell_{i}^{T}\mathcal{L}^{i}} \tag{36}\]

Calculation of the denominator requires that, as successively derived samples are produced, knowledge of the temporal component of the original selection (\(LB^{S}\) or its equivalent) be retained.

The luminosity data available in offline databases, described in Section 5.1 make cross section calculation straightforward - each term in the denominator is readily computed - provided that:

* one always processes data from an integral number of luminosity blocks;
* one always processes every event satisfying a given trigger chain within a luminosity block;
* one always tracks, throughout the process of successive sample filtering and refinement, the list of luminosity blocks from which one's sample originated.

The recommendations that follow are all intended to ensure that these conditions may robustly be met, even in the presence of errors or missing data.

**Recommendation 5.1:** Standard samples that form the basis of subsequent analysis by physics working groups should correspond to an integral number of luminosity blocks. When subsets of standard samples are used, such subsets should also correspond to an integral number of luminosity blocks.

**Recommendation 5.2:** Event selection and data management infrastructure should: (a) provide a means to ensure that selections do indeed correspond to an integral number of complete luminosity blocks, and (b) provide a means to determine the luminosity blocks corresponding to such a selection, so that the denominator in equation 36 may be computed.

**Recommendation 5.3:** Time interval specifications associated with a query (events taken between \(t_{1}\) and \(t_{2}\), for example) that do not correspond to run boundaries should, if intended for physics analysis, be constrained to begin and end at endpoints of luminosity blocks.

#### 5.2.1 Detector status, quality information, and event selection

Occasionally, a sub-detector will suffer a substantial change in status (e.g., an error condition) at some time \(t\). This will, in general, occur in the middle of a luminosity block \([t_{i},t_{i+1}]\). If detector status is used in defining a subsequent event selection, a sample may include events from the interval \([t_{i},t]\) or the interval \([t,t_{i+1}]\). It may not be possible to accurately estimate trigger cross sections in such subintervals.

**Recommendation 5.4:** In general, event selections for physics analysis that rely upon quality and detector status information should omit luminosity blocks for which such information is not constant for the duration of the block. A possible implementation is to associate quality and detector status information directly with the luminosity blocks themselves.

Sometimes it will happen that particular runs or run segments (ranges of luminosity blocks within a run) will be marked as bad or inappropriate for physics because of quality or detector status issues only after an analysis has already begun. In this case it may be necessary to remove or disregard event data from specific luminosity blocks within an already-defined sample.

**Recommendation 5.5:** Mechanisms to support identification of events within a sample from a given luminosity block, and their removal or omission from an analysis sample, should be provided.

#### 5.2.2 Use case: standard analysis samples

We posit that standard analysis samples will consist of events that satisfy criteria of the form (\(temporal\ constraints\ (e.g.,\ run\ range)\)) \(AND\ (quality\ and\ detector\ constraints)\)

\(AND\ (trigger\ constraints)\ AND\ (additional\ physics\ constraints)\).

If the temporal constraints are appropriate (runs, or sets of time intervals that correspond to luminosity block boundaries, the list of luminosity blocks included is well defined. If the quality and detector constraints are constant over luminosity blocks, or care is taken to omit luminosity blocks for which these conditions are not constant, then again the list of luminosity blocks is well defined. If the trigger constraints correspond to a single trigger or a subset of a single trigger stream, then it is possible to compute the denominator in equation 36 unambiguously.

If a subsequent selection is based upon the complete resulting sample, the cross section of the remaining events remains unambiguous: the event count may be divided by the same denominator. This remains true for subsequent sample refinements, as long as at each stage the complete sample from the previous stage is used.

Incomplete samples at any stage in the analysis chain may pose problems. These are discussed in a subsequent section.

#### Use case: event selection by individual physicists

In principle, an analysis that begins with an individual physicist's event selection (for example, via a query to the tag database) is no different than an analysis that begins with a standard production sample, so long as the same care is taken to ensure that the sample corresponds to an integral number of clearly defined luminosity blocks. Tools that assist in mapping the temporal and quality and detector constraints of an _ad hoc_ query to luminosity block boundaries will be necessary.

### Luminosity and event-level information

The cross section computations described above do not require event-by-event knowledge of the luminosity blocks from which each of the events in the final sample have come. It may nonetheless be helpful to have a means to determine, for a given event, the luminosity block with which it is associated.

**Recommendation 5.6:** From a given event, it should be possible to determine both, all trigger chains that triggered it and the luminosity block to which it belongs. A means to retrieve trigger- and luminosity-related data corresponding to the given luminosity block should also be provided.

Recent discussions with the TDAQ community suggest that a luminosity block number will be written into event headers. This would presumably be propagated through offline processing stages into ESD and AOD, along with trigger chain information. An offline alternative might be to use the event time-stamp to determine the luminosity block to which the event belongs. In either case, offline conditions database access should support retrieval, beginning with such event header information, of corresponding luminosity-block-level data.

Offline processing and access model

This section describes luminosity considerations in offline data organization and processing. ATLAS data rates and Event Filter I/O organization together suggest that raw data for a given luminosity block will be spread across several files. The number of such files is likely to correspond to the number of event data writers operating in parallel, i.e., to the number of Event Filter sub-farm output nodes (SFOs).

The offline data management supports the concept of a _data block_, a set of files treated as a unit for purposes of data transfer and cataloging above the site level.

**Recommendation 6.1:** The set of raw event data files associated with a given luminosity block should belong to a single data block. If raw data are streamed by trigger at the SFO level, the set of raw event data files associated with a given luminosity block and trigger should belong to a single data block.

### Luminosity considerations for offline streaming and file merging

Derived data products (ESD and AOD) are successively smaller than the corresponding RAW data. Current estimates are 1.6 MB/event, 0.5 MB/event, and 0.1 MB/event for RAW, ESD, and AOD, respectively. A single raw data file therefore will, because of an overall 2-GB file size limit, contain on the order of 1200 events (equivalent to six seconds of data taking with a single SFO, or one-tenth of sixty seconds of data taking with 10 SFOs). A 1200-event RAW file will yield, in the absence of streaming, a 600-MB ESD file and a 120-MB AOD file.

Current offline plans are to stream at the AOD level, and very possibly at the ESD and RAW levels. The estimated number of streams is approximately 10, but decisions await the recommendations of the Streaming Study Group and decisions of the computing model group.

Because of the successively smaller sizes of derived event data products, offline processing is expected to include a _merge_ step, in which the output of several ESD- or AOD-producing jobs are concatenated into a single file. 7 Alternatives to merging are also under consideration; for example, one might instead use multiple ESD files as input to each AOD-producing job.

Footnote 7: Without some kind of aggregation or merge step, sequential Tier 0 processing of a single 2-GB RAW file would yield, for example, approximately 10 AOD files of average size 12 MB. This proliferation of small files would likely prove unmanageable in a multi-petabyte event store, and is not well matched to the capabilities of data handling systems in any case.

Figure 7 provides a pictorial view of data handling from the Online system through the creation of customized AOD by the physics groups or users. At each stage of the processing,

the order of data delivery is determined by the constraints of efficiency for the data handling system. Unless the data delivery system is designed to be aware of LB boundaries, events from a single LB are likely to be places in a large number of files. Since a requirement of Equation 36 is that all events from a LB be read in order to calculate the cross section, such a chaotic model would place significant requirements on the robustness of the data delivery system.

**Recommendation 6.2:** When merging is done, files corresponding to a given luminosity block should all be merged into the same file, if possible. If, as an alternative to merging, multiple ESD files are used, for example, as input to each AOD production job, the input ESD files should correspond to an integral number of complete luminosity blocks, to ensure that AOD output files share that property.

Note that, by current estimates, ensuring that AOD files contain data from an integral number of complete luminosity blocks should always be possible, even in the absence of streaming: a luminosity block's estimated minute of data taking yields approximately 12,000 events, or 1.2 GB, of AOD-well within file size limits.

With streaming by trigger, multiple complete luminosity blocks should fit into a single AOD file.

### Offline error conditions, incomplete samples, and luminosity calculation

Perhaps the greatest potential luminosity-related pitfall in offline analysis arises from the possibility of processing incomplete samples. For an appropriately-defined event selection from a single trigger stream, the integrated luminosity is well defined, as described above.

We consider two cases of incomplete samples: one in which all data from one or more luminosity blocks are omitted, and the other in which a job processes some but not all of the events from a given luminosity block.

**Case I: A file or data block containing data corresponding to an integral number of luminosity blocks is unavailable.**

This case is not conceptually difficult--one simply omits the corresponding terms from the sum in the denominator of the cross section calculation. The important proviso is this:

**Recommendation 6.3:** The ATLAS meta-data system must be able to determine the luminosity blocks and trigger chains associated with a given production data file or event collection. This capability must be external to the files or collections themselves, i.e., one needs to be able to obtain this information even when the files themselves are unreachable or unreadable.

### Offline error conditions, incomplete samples, and luminosity calculation 43

Figure 3: Pictorial view of data processing

[MISSING_PAGE_EMPTY:44]

### Offline error conditions, incomplete samples, and luminosity calculation 45

**Recommendation 6.5:** As ATLAS begins to address the issue of provenance tracking and what a file or event collection internally "knows" about its contents, history information regarding the luminosity blocks used as input to its production should be included in any provenance record associated with or contained within that collection or file.

## 7 Conclusions and recommendations

A complete set of TF recommendations can be found in Appendix 7. We summarize here only the most important features of these recommendations:

* The TDAQ system should define **luminosity blocks*
* (LB), time intervals with well defined starting and ending times that represent a period of constant luminosity and trigger conditions.
* The length of these LB is expected to be of order 1 minute, but will in practice be determined by TDAQ and computing operations issues.
* The LB number should be recorded in the event header by the TDAQ system.
* Analyzes where the integrated luminosity is needed must be performed on all events from a specified, integral number of LB.
* Luminosity, trigger pre-scales, TDAQ dead-times and detector status should be accessible through the database system as a function of LB number.
* It is essential to monitor the performance of the TDAQ and Offline systems and to record the results of this monitoring. Quantities that must be monitored and recorded include TDAQ dead-times, trigger pre-scales and event loss during Online and Offline data processing.
* To the largest extent possible, data should be physically clustered by LB into single files or single data blocks. If the data are streamed to multiple output streams by trigger chain, this requirement only holds for data within a given stream.

The purpose of these recommendations is to insure not only that all necessary information is available for calculating luminosity and cross sections, but also that the model for calculating cross sections is robust. Steady and reliable operations require large scale testing. The Task Force therefore strongly recommends:

**Recommendation 7.1:** ATLAS commissioning and computing system commissioning become trigger and luminosity block aware as soon as possible. Cross section analyzes that require trigger, luminosity and data quality information should be incorporated into the testing of the trigger, data distribution and analysis systems planned for the next year ("Dress rehearsal").

[MISSING_PAGE_FAIL:47]

* 4.7 Applications in the on-line system which are critical for a correct determination of the luminosity should be identified, and mechanisms to identify failures of these applications and allow to correlate them with LBs should be implemented. This could be done on a time-stamp basis, with a time precision \(\mathcal{O}\)(LB length).
* 4.8 A LB should be tagged with a LB number (LBN), which uniquely identifies the LB within a given run.
* 4.9 In the CTP, a read/writable VME register should be implemented to hold a LBN (at least 14 bits), which will be put into the CTP event and RoI data fragment for each event accepted by Level-1.
* 4.10 The Level-2 processing tasks should be able to read the LBN in the CTP RoI data fragment and increment the appropriate scalers for this LBN.
* 4.11 EF and dataflow applications should be able to act on the LBN and increment the appropriate scalers. The LBN should be made available in the CTP event data fragment and in the event header.
* 4.12 A mechanism should be invented such that the CTP control software can issue a hardware pulse (e.g. NIM pulse of width \(W\) with 25 ns \(<W\ll\) duration of LB transition) at any moment.
* 4.13 The start and end time of a LB should be recorded.
* 4.14 The operational down-time introduced by a LB transition ought to be kept to a minimum \(\mathcal{O}\)(ms).
* 4.15 Run Control should be the on-line LB supervisor and initiate LB transitions by sending a message to the CTP. Slight adjustments to the configuration during a run should only be done on LB boundaries.
* 4.16 The luminosity detector systems should be notified of the LB transition with a latency well within one permille of the LB duration.
* 4.17 Feasibility and performance issues (trigger rate, dead-time) of on-line writing events into files only with increasing LBN should be studied. In addition, the feasibility of raw data files respecting LB boundaries should be explored.
* 4.18 The possibility of physically clustering events of the same LB to files/file sets should be reviewed and evaluated during the discussions on event streaming.
* 5.1 Standard samples that form the basis of subsequent analysis by physics working groups should correspond to an integral number of luminosity blocks. When subsets of standard samples are used, such subsets should also correspond to an integral number of luminosity blocks.
* 5.2 Event selection and data management infrastructure should: (a) provide a means to ensure that selections do indeed correspond to an integral number of complete luminosity blocks, and (b) provide a means to determine the luminosity blocks corresponding to such a selection, so that the denominator in equation 36 may be computed.
* 5.3 Time interval specifications associated with a query (events taken between \(t_{1}\) and \(t_{2}\), for example) that do not correspond to run boundaries should, if intended for physics analysis, be constrained to begin and end at endpoints of luminosity blocks.

* 5.4 In general, event selections for physics analysis that rely upon quality and detector status information should omit luminosity blocks for which such information is not constant for the duration of the block. A possible implementation is to associate quality and detector status information directly with the luminosity blocks themselves.
* 5.5 Mechanisms to support identification of events within a sample from a given luminosity block, and their removal or omission from an analysis sample, should be provided.
* 5.6 From a given event, it should be possible to determine both, all trigger chains that triggered it and the luminosity block to which it belongs. A means to retrieve trigger- and luminosity-related data corresponding to the given luminosity block should also be provided.
* 6.1 The set of raw event data files associated with a given luminosity block should belong to a single data block. If raw data are streamed by trigger at the SFO level, the set of raw event data files associated with a given luminosity block and trigger should belong to a single data block.
* 6.2 When merging is done, files corresponding to a given luminosity block should all be merged into the same file, if possible. If, as an alternative to merging, multiple ESD files are used, for example, as input to each AOD production job, the input ESD files should correspond to an integral number of complete luminosity blocks, to ensure that AOD output files share that property.
* 6.3 The ATLAS meta-data system must be able to determine the luminosity blocks and trigger chains associated with a given production data file or event collection. This capability must be external to the files or collections themselves, i.e., one needs to be able to obtain this information even when the files themselves are unreachable or unreadable.
* 6.4 The ATLAS meta-data system should, given luminosity block and trigger chain information, be able to determine which production data files and event collections at a given processing stage contain events from that block and trigger chain. This is the converse of the previous recommendation.
* 6.5 As ATLAS begins to address the issue of provenance tracking and what a file or event collection internally "knows" about its contents, history information regarding the luminosity blocks used as input to its production should be included in any provenance record associated with or contained within that collection or file.
* 7.1 ATLAS commissioning and computing system commissioning become trigger and luminosity block aware as soon as possible. Cross section analyzes that require trigger, luminosity and data quality information should be incorporated into the testing of the trigger, data distribution and analysis systems planned for the next year ("Dress rehearsal").

## Appendix A Full charge

Consider the issue of luminosity measurement and monitoring from the points of view of physics analysis, offline software, database and computing, and trigger. Address in particular the information that needs to be recorded and accessed to determine the luminosity of any ATLAS data sample, and the main implementation issues, and spot technical problems and missing information (if any).

Consider both integrated luminosity (per interval of data taking) and absolute luminosity which is relevant for pile-up corrections. Note that luminosity will vary from bunch-to-bunch in the machine and that pileup may depend on the activity in bunches around the triggered one (in particular the first bunches after gaps in the bunch-train may be different from a pile-up point of view).

Address the issue of what information needs to be recorded and how intervals of validity (time and/or run and/or run-fragment) will be specified:

* Review information available from the LHC machine.
* Review information that ATLAS will make available to the machine.
* Review scaler information available from the LVL1 CTP.
* Review scaler information from LVL2 (ROIB, supervisors, processors) and from DAQ and EF (DFM, SFI, SFO, processors).
* Review plans for handling of pre-scales at each trigger level, considering also the overlaps between different triggers.
* What information needs to be displayed online?
* What granularity in time is needed in recording the information?

Address the issue of how a physicist doing an offline analysis can calculate the integrated luminosity for any event sample:

* is that the right granularity?).
* What tools are required to identify the integrated luminosity per raw data file, recognizing how many parallel files there were being written simultaneously, with open/close times.

Address the consequences of real-life scenarios:

* Data-taking runs will sometimes not terminate cleanly.

* Machine coasts will sometimes be lost unexpectedly.
* Events will be lost due to deadtime in LVL1.
* Events may be lost in the LVL2 system (e.g. if a processor fails).
* Events may be lost in the EF system (e.g. if a processor fails).
* Events may be lost in the DAQ system (e.g. if an SFI fails).
* Events may be lost in the Offline computing system.
* Events may be unusable for various reasons (e.g. cannot be reconstructed because of corrupted data).

Consider how the luminosity monitors will be calibrated via dedicated luminosity measurement techniques.

Consider the issue of possible revisions to the calibration of the luminosity monitors during the lifetime of the experiment.

## Appendix B Glossary

In this section we try to establish a common, well-defined language, in order to avoid possible confusion. Some of the terms are already defined elsewhere (where indicated), and some needed to be refined for the purpose of this document. It should also be noted that some of the definitions could also depend on the future implementation of the system and might therefore not be 100 % unambiguous.

Throughout this document the term luminosity is used as integrated luminosity. In the few cases, where we talk about instantaneous luminosity, we make it clear by using the word "instantaneous".

**Definition B.1**: **Live-time fraction (of a trigger chain):** _1 minus dead-time._

**Definition B.2**: **Dead-time fraction (of a trigger chain):** _The fraction of time during a data-taking run during which triggers issued by the trigger system had to be ignored due to back-pressure from the DAQ indicating data overflow, and due to special dead-time algorithms running at Level-1._

**Definition B.3**: **Operational down-time** _is the time between periods of data-taking, when the detector is not operational to take data._

**Definition B.4**: **Trigger chain [1]:** _A trigger chain is a set of trigger signatures at a given trigger level: Level-2 chain, EF chain. Alternatively, a (global) trigger chain is a set of one Level-1 trigger item, one Level-2, and one EF chain. A trigger chain is canceled if one of its signatures is not fulfilled._

**Definition B.5**: **Trigger element [1]:** _Active trigger elements are objects produced during the stepwise process of events in the HLT in case an event fulfilled the requirements of certain trigger algorithms in a trigger sequence. A trigger sequence is an HLT expression and describes the way a trigger element is transferred to another trigger element by the use of trigger algorithms._

**Definition B.6**: **Trigger condition [1]:** _At Level-1, a "trigger condition" is composed of a trigger threshold and a corresponding multiplicity, e.g. 2TAU50._

**Definition B.7**: **Trigger signature [1]:** _A trigger signature is a logical combination of trigger elements/conditions (e.g. e20i AND \(\mu\)20i). For each level in the trigger system a list of trigger signatures (the trigger menu of that level) exists. The trigger decision of a level is the logical OR of all trigger signatures of that level. A trigger signature also has a pre-scale factor. The Level-1 term for a signature is "item"._

**Definition B.8**: **Pre-scale [1]:** _A pre-scale factor is specified for each trigger signature at each of the three trigger levels. It reduces the amount of events accepted by this signature. For instance, for a pre-scale of 10, one event out of 10, fulfilling the signature, leads to an accept of the event. Pre-scale factors are implemented via scalers. There will be pre-scales at all three trigger levels._

**Definition B.9**: **Luminosity measurement:** _An estimate of the luminosity with an appropriate method. This could be a complete set of LHC beam parameters for the given time period, or a well-defined data-set suitable for luminosity determination (well determined cross-section acceptance, efficiency, and background)._

**Definition B.10**: **Corrected luminosity:** _The corrected luminosity of a trigger chain is a luminosity measurement, corrected for the pre-scales and the dead-time of the trigger chain, as well as corrected for possible failures of the system._

**Definition B.11**: **Relative luminosity measurement:** _A luminosity measurement derived without proper normalization. Often, this information comes from on-line luminosity monitors and does not include off-line corrections._

**Definition B.12**: **Absolute luminosity measurement:** _Normalized luminosity measurement (in units of \(\mathrm{cm^{-2}s^{-1}}\)). Typically, the scale is set by a physical process with accurately calculated cross-section._

**Definition B.13**: **Luminosity Block Supervisor:** _The online component that controls the transition from one LB to the next._

**Definition B.14**: **Data quality information:** _Meta-data associated with periods of data-taking (run, LB, or others) containing information on the data quality. This could range from on-line information on the status of sub-systems, to results from off-line data validation analyzes._

## List of definitions defined in the text

### Luminosity Block (LB):

A time interval, for which the integrated, dead-time- and pre-scale-corrected luminosity can be determined. Its length is set by the LB supervisor.

### Luminosity Block Number (LBN):

A number, which uniquely tags a LB within a run.

### Delivered luminosity:

The integrated luminosity over a period of time (e.g. luminosity block) at the interaction region.

[MISSING_PAGE_EMPTY:54]

## References

* [1] J. Haller, S. Tapprogge, _"Guidelines for Trigger Operation in Physics Running,"_ ATL-COM-DAQ-2005-032.
* [2] ATLAS Collaboration, _ATLAS Detector and Physics Performance TDR_, CERN-LHCC/99-14, ATLAS TDR 14, Volume I, Chapter 13; ATLAS Collaboration, _ATLAS Detector and Physics Performance TDR_, CERN-LHCC/99-15, ATLAS TDR 15, Volume II, Chapter 15.
* [3] ATLAS Collaboration, _ATLAS Forward Detectors for Luminosity Measurement and Monitoring_, CERN-LHCC/04-10, LHCC I-014.
* [4] Jorg Wenninger, private communication.
* [5] A. M. Cooper-Sarkar, _"Low-x physics and W and Z production at the LHC,"_ arXiv:hep-ph/0512228.
* [6] A. Gorisek, _Recent tests and integration plan for Beam Conditions Monitor (BCM)_, ATLAS Inner Detector Week, June 2005, CERN, Switzerland; A. Gorisek private communication.
* [7] J. Huston, _Status of Minimum bias trigger counters_, Luminosity and Forward Physics WG Meeting, February 2006, CERN, Switzerland; J. Huston private communication.
* [8] ATLAS Collaboration, _Tile Calorimeter TDR_, CERN-LHCC/96-42 (1996).
* [9] ATLAS Collaboration, _Liquid Argon Calorimeter TDR_, CERN-LHCC/96-41 (1996).
* [10] S.Ask, _Simulation of Luminosity Monitoring in ATLAS_, ATL-LUM-PUB-2006-001, Sept. 2005
* [11] Sergei Kolos, _"Information Service, "_ ATLAS Online Software [http://atlas-onlsw.web.cern.ch/Atlas%2Donlsw/components/is](http://atlas-onlsw.web.cern.ch/Atlas%2Donlsw/components/is) /doc/userguide/is-usersguide.pdf
* [12] P. Farthouat, G. Mornacchi, _"Run checkpoint, Run Number and RODs,"_ ATL-D-EN-0002, 1 March 2004, [https://edms.cern.ch/document/450857/1](https://edms.cern.ch/document/450857/1)
* [13] Georges Schuler, private communication.
* [14] Hans-Peter Beck, Szymon Gadomski, private communication.
* [15] LHC Design Performance, EDMS CERN-000002013 v.0.