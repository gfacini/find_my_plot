# **Atlas Note**

ATL-PHYS-PUB-2009-000

March 17, 2009

**Data Preparation for the High-Level Trigger Calorimeter Algorithms**

The ATLAS Collaboration1

Footnote 1: This note is prepared by I. Aracena, A. Bazan, M.P. Casado, K. Cranmer, D. O. Damazio, R. Lafaye, B. Laforge, W. Lampl, S. Laplace, F. Ledroit, H. Ma, A. Ruiz-Martinez, B. Salvachua, C. Santamarina-Rios, R. Seuster, A. Solodkov, G. Unal, M. Wielers, H. Wilkens, I. Wingerter-Seez, X. Wu.

_This note is part of CERN-OPEN-2008-020. This version of the note should not be cited: all citations should be to CERN-OPEN-2008-020._

**Abstract**

This note describes the data preparation necessary to enable the ATLAS High Level Trigger Calorimeter Algorithms. An overview of the infrastructure, which provides the transition from the calorimeter electronics to data reconstruction and trigger algorithm implementation, is given. This infrastructure is detailed as a separate note since it is relevant to all trigger algorithms requiring calorimeter information (electrons, photons, taus, jets, missing \(E_{T}\) and muons).

Introduction

The calorimeters and part of the muon system were designed to participate in the ATLAS first level hardware-based trigger (L1) [1, 2, 3, 4], while all sub-detectors participate in the software-based high level trigger (HLT) [5], comprised of Level 2 (L2) and the Event Filter (EF). One important phase for any trigger software algorithm is the data preparation step which provides the conversion of the bytes of data produced by the detector electronics into a convenient form for the trigger algorithms. In the case of the calorimeters, the digital information provided by the detector must be converted into calorimeter cells as input to the reconstruction algorithms. A good data preparation step will provide the input to the trigger software in an organized manner, so that access to the prepared data is optimized. This note describes this step for the calorimeter trigger software in the HLT. The same software data preparation layer is used in algorithms that are used to identify electrons, photons, taus, jets and muons [6].

### Calorimeter readout

The fundamental LAr calorimeter readout unit is the calorimeter cell. The cell electrodes receive the current due to the drift electrons in the liquid argon and form a triangular shaped signal [1]. The shaping and readout of this signal is performed by the Front-End Electronics. To preserve the dynamic range and the energy resolution, the signal is shaped with three possible gains. The Front-End Boards (FEBs) save analog samples of the signals coming from the detector at the bunch crossing rate (25 ns). Each FEB can process up to 128 LAr calorimeter cells.

The signals are converted by the FEBs to a digital format if the event is accepted by the L1 Trigger. The digital information is sent to the ReadOut-Drivers (RODs). These are Digital Signal Processor (DSP) based machines, fast enough to deal with a number of input channels (2 FEBs feed one ROD DSP). From the pulse shape digitized at the FEB, the energy deposited in any cell can be calculated.

Data from each ROD are sent to a ReadOut Buffer (ROB). The ROBs keep this data fragment until it is requested by L2 or the Event Builder (EB). While L2 only requests a limited amount of data fragments, the EB will request fragments from the whole detector for events approved by L2 for subsequent processing in the EF computer nodes.

In the Tile Calorimeter [2], the photons produced in the scintillators are measured by photomultipliers, which produce a negative shaped pulse. The digitized electronic signal is saved into an on-detector memory waiting for the accept signal from the L1 trigger. For each of the 256 Tile Calorimeter modules, a so-called drawer (inserted in the back of the calorimeter structure) contains up to 48 photomultipliers and all the readout electronics.

The analog signals from the detector cells are also summed up by dedicated hardware by detector regions in depth. Trigger Towers (TT) are coarse granularity combinations of the detector cells and can be provided in analog mode to the hardware L1 processing. Except for the very forward regions, the TT size is \(0.1\times 0.1\) in \(\eta\times\phi\). The L1 hardware algorithm uses some minimal TT energy and isolation quantities to define a possible L1 calorimeter candidate. A pointing to the found candidate \(\eta\times\phi\) position is sent as a seed for software trigger processing. This seed is used to open a region (usually defined in terms of TT coordinates) called Region of Interest (RoI), where the full detector granularity ca be accessed by the reconstruction algorithms.

## 2 Data preparation

From a general point of view the preparation of the LAr and Tile Calorimeters data is similar. Figure 1 depicts the global scope of the data preparation for the L2 calorimeter algorithm as described below. The EF structure is similar and is discussed briefly later in the note (see Ref. [7] for additional details of the full HLT data preparation).

The L2 software component called Steering receives the L1 information on the acceptance of an event along with the \(\eta\) and \(\phi\) coordinates corresponding to the L1 triggered object. The reconstruction algorithm gathers a list of ROB identifiers which contain data for a given RoI. Each ROB may partially contain data from TTs not pertaining to the RoI (ROB data access is not usually defined by the RoI, but rather by the hardware cabling). An optimal way to map cells to the towers and to the addresses of the ROB must be provided.

The mapping of the ROBs and TT are part of the geometry description and are also used in the offline reconstruction software framework. The access to the offline detector description databases which maps any physical position into a set of identifiers is typically very slow, as the full detector description is comprised of a great amount of data. In order to provide faster access, compatible with the L2 speed requirements, a look-up table called the Region Selector is prepared in the initialization phase of the trigger software.

The ROB identifiers are translated to network addresses of the ROB machines and the data are subsequently requested. The processing of the trigger selection algorithms is inhibited while the network acquires the data. Operating in a multiprocessing environment, as forseen for the ATLAS trigger, reduces this dead time [5, 8]. The ROB data provider receives the list of ROB identifiers and returns the detector data to the algorithms.

When data are received, pointers to the beginning of the different fragments are passed to the detector-specific bytestream conversion code. Bytestream conversion is the decoding of the data format produced by the detector RODs, and packaging of the data into an accessible format for the algorithms, in this case calorimeter cells. The last part of the data preparation is to provide the cells in a manner organized for the reconstruction algorithms. For instance, cells are provided by detector layer.

### Data processing in the Read-Out Drivers

The LAr Digital Signal Processors (DSPs) are able to prepare data in different formats, the most important one is termed "physics mode." In this mode the DSPs process the nominal 5 samples per cell provided by the front-end electronics. These samples are used to compute the energy deposited in the cell by the particles using an optimal filtering (OF) [9]. This processing is a simple weighted sum of the samples. The weights include the noise autocorrelation, electronics calibration constants, and a normalization factor that converts ADC counts to MeV. For cells with energy above a programmable threshold the timing of the signal and the quality of the pulse shape compared to the expectation are calculated. Finally, for each cell, the choice of electronic gain applied to the analog signal in the FEB is recorded.

Beyond the cell-based data, the DSP can also extract global information at the FEB or TT level,

Figure 1: Different parts of the data preparation processing and their relation to the calorimeter algorithm at the L2. For details, see text.

which can be used to improve the L2 and EF processing speed. The DSP can sum up the energy in a given region in space providing \(E_{x}\), \(E_{y}\), and \(E_{z}\) sums for these regions. The cell energies are added within these regions using cell-position-based projection coefficients loaded in the DSP from a database. Zero suppression is applied for cells below a given threshold. FEB's or TT's can be used to reconstruct jets or missing \(E_{T}\) at L2 and EF if unpacking the full detector is too time consuming. Currently FEB's are being used to provide the energy sums, however, using the TT information instead is under evaluation to improve jet and missing \(E_{T}\) resolutions.

The pulses from the Tile Calorimeter photomultipliers are also sampled and digitized by 10-bit ADCs. During a physics data taking, 7 samples (175ns) of the signal pulses are acquired and transmitted to the RODs. The information is processed using DSPs which also apply optimal filtering for cell energy reconstruction [10].

### Region selector

As mentioned earlier, part of the information is stored in lookup tables for fast access to the detector description. In the LAr calorimeter case, the information unit to be correlated to the L1 position is the Trigger Tower. The \(\eta\times\phi\) minimum and maximum and the ROD identifier for each TT is arranged in a large matrix. Multiple tables corresponding to the different calorimeter layers are available. For the transition region from barrel to endcap calorimeter the data of the fiducial volume covered by a TT may be provided by more than one ROD. In the Tile Calorimeter case, the geometry information is associated with the calorimeter module identifier and, again, the ROB identifiers. The Look-Up tables with geometry information for LAr and Tile are prepared by accessing the relevant conditions database.

### Data containers

The data structure of a calorimeter cell includes a part common to LAr and Tile, and parts specific to both subdetectors. In the software these cells are organized in vectors, called collections. For the Liquid Argon Calorimeter each cell collection holds data for a LAr ROD, corresponding to two FEBs or, at most 256 cells. In the case of the Tile cell collection, there are either 23 cells (in the Barrel) or 13 cells (in the Extended Barrel) per collection. Data for four cell collections are associated with a single ROD. A Tile Calorimeter ROD has data for at most 92 Tile cells. Finally, the collections are organized in a vector which is called a container.

The containers for LAr and Tile are stored permanently in memory and the cells and collections are never deleted. This way, on-the-fly memory allocation, which is typically a slow operation in a computing system, is avoided. One problem with reusing collections is that the container must keep track of which collections have already been decoded in a given event. This information is provided by the tools that access the container. If requested subsequently in the same event, the collection will not be decoded again.

### Bytestream conversion

The ROD fragments, containing the energy encoded information are provided to the appropriate HLT bytesream conversion code. Based on the ROD fragment identifier, the corresponding cell collection is located by the proper container (LAr or Tile containers). Subsequently, subdetector specific code is used to perform the data unpacking.

The LAr bytestream conversion code automatically identifies the fragment type using the ROD version encoded in the bytestream itself. Depending on the detected format, the corresponding internal infrastructure is selected.

The bytestream conversion software unpacks the energy information using the DSP physics output format as described in Section 2.1. The conversion provides the cell energy, hardware gain, pulse peak time, and pulse-fit quality information (if available) for each of the ROD fragment channels. The channel number is used as an index to the cell position in the cell collection, so that each LAr channel is associated to a single predefined cell object in the collection. Each cell is updated with the current values of energy, time, quality and hardware gain. In the unpacking step, typically more cells are requested than those contained in the RoI as data from one FEB may extend over several TTs. Furthermore, the trigger reconstruction algorithms require data access on a layer-by-layer basis. This results in a very complex operation with many checks of cell layer and position. Maps between TT identifiers and the associated groups of cells are prepared prior to algorithm execution, to speed up the process. Using the TT identifier list obtained from the Region Selector, a chain of cells for those TTs can be obtained, simplifying the algorithm code.

The bytestream conversion software for the Tile calorimeter data also checks the ROD format, ensuring that the correct method of unpacking the data is chosen. The data is decoded and the energy values are stored in a pre-allocated raw data structure. This is again used to avoid online memory allocation. The energy, time, and quality are stored together with the ADC identifier for each cell in a Tile Calorimeter drawer. This raw data is copied into the cell structure. The mapping of raw data to cells is the same for every drawer in a given calorimeter sector. To speed up the processing a mapping is built to the indices of the cells that correspond to each raw data.

Each Tile drawer is unpacked into a cell collection. The data providing in this case is much simpler than in the LAr case. Algorithms are able to iterate through the whole collection after the unpacking is done.

### Data preparation in the EF

The Data Preparation tools in the EF make use of the same data unpacking approach that is used by L2, dumping this information into an offline cell container. As the EF has a larger time budget, however, more sophisticated algorithms and tools developed for offline reconstruction are used to process the cells stored in this container.

For each of the subdetectors (EM, HEC, FCal and Tile), the EF cell container is filled. This provides the possibility of unpacking only selected calorimeter sections, as needed. Once the container has been filled with the corresponding calorimeter cells, a set of software tools are executed to organize and check the container, and to perform cell-based calibrations.

## 3 Algorithm performance

In this section, the performance of representative HLT algorithms (e/\(\gamma\) for L2 and missing \(E_{T}\) for EF) and data preparation is studied. The results are based on bytestream files prepared with a format similar to the ATLAS raw output data.

The primary performance issue is processing time. The processing time depends on the number of cells required, which is a function of \(\eta\). Figure 2 shows the number of cells separately for LAr (left) and Tile (right) calorimeters; the overlapping bins in the figure are due to variable \(\phi\) segmentation in transition regions between different calorimeter modules. The distribution on the left shows that the number of active cells in the barrel is quite uniform; since the endcap granularity is smaller, the number of unpacked cells decreases with increasing \(\eta\). The distribution of the number of cells unpacked for the Tile Calorimeter depends on the number of drawers to be unpacked. In the very central region (\(|\eta|<0.4\)), data from negative and positive rapidities must be accessed to complete the RoI, doubling the amount of data to be unpacked. A similar effect is observed in the region between the TileCal Barrel and Extended Barrel.

For the standard L2 e/\(\gamma\) selection based on a RoI size of \(0.4\times 0.4\) in \(\eta\times\phi\), the execution time for each of the processing steps was measured. The results, based on a sample of about 15,000 single electron events are shown in Fig. 3 separately for the EM (left: LAr) and for the hadronic (right: Tile and HEC) sections. It is important to stress that the processing time per RoI does not depend on event type, since the cluster sizes are constant for a given RoI. It can be seen that the Region Selector comprises a small portion of the total time, and that the bytestream conversion is the dominant source of time, with the algorithm itself taking only about 35% (10%) of the total time for the EM (Hadronic) calorimeters. Even though fewer cells are used in the EM calorimeter crack region (around \(\eta=1.5\) as shown in Fig. 2), these cells are distributed in two ROBs (one from the Barrel and another from the EM endcap), resulting in an overall increase in the processing time. Finally, we note that the conversion times are especially large in the regions covered by the Tile calorimeter and in proportion to the number of Tile calorimeter modules accessed. Work is ongoing to reduce these large processing times. The timing results averaged over \(\eta\) are also summarized in Table 1. ROB data retrieval times are not included, since they can only be evaluated during real data taking.

These time measurements indicate that the preparation of the Tile calorimeter data needs to be improved. Even though about six times fewer cells are accessed for the barrel region, the data preparation time to run the hadronic part is comparable to the EM part.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Reco. step & Region Selector & bytestream conversion & Algorithm & Total \\ \hline EM \(2^{nd}\) layer & 29\(\mu\)s & 169\(\mu\)s & 146\(\mu\)s & 347\(\mu\)s \\ \hline EM \(1^{st}\) layer & 13\(\mu\)s & 171\(\mu\)s & 113\(\mu\)s & 301\(\mu\)s \\ \hline EM other layers & 21\(\mu\)s & 158\(\mu\)s & 56\(\mu\)s & 243\(\mu\)s \\ \hline Hadronic & 46\(\mu\)s & 334\(\mu\)s & 43\(\mu\)s & 438\(\mu\)s \\ \hline Total & 109\(\mu\)s (8\%) & 833\(\mu\)s (63\%) & 358\(\mu\)s (27\%) & 1.33 ms \\ \hline \end{tabular}
\end{table}
Table 1: Processing time for different algorithm steps and for different actions. Improvements for the Tile calorimeter data preparation are envisaged. Time measurement excludes ROB data retrieval time (a 2.3 GHz machine was used).

Figure 2: Number of cells used in the L2 e/\(\gamma\) selection algorithm as a function of \(\eta\) for the LAr (EM, EM endcaps and HEC) Calorimeters (left) and for the Tile Calorimeter (right). The RoI size was \(0.4\times 0.4\) in \(\eta\times\phi\).

Other trigger algorithms such as tau or jet identification need larger RoI sizes and consequently require more time. As an example, a jet algorithm which uses a \(1.0\times 1.0\) RoI size takes 10 to 12 ms.

### EF missing \(E_{\rm T}\) performance

The EF missing \(E_{T}\) reconstruction algorithm accesses data from all the calorimeters and computes the missing \(E_{T}\) with its \(E_{x}\), \(E_{y}\) components as well as the total scalar energy sum. In addition, corrections due to energy deposits from muons can be taken into account by including the results from the EF muon reconstruction.

To access the calorimeter data the algorithm uses the same data preparation layer used by L2. Since the ATLAS calorimeters contains about 200,000 cells, the access to every single cell can become too time consuming at the trigger level. A faster option is to use energy sums at the FEB level (discussed earlier in Section 2.1). FEB unpacking has only been implemented for LAr data, where the impact on the unpacking time is most significant.

In Fig. 4 the processing time of the EF missing \(E_{T}\) algorithm is shown for full unpacking and FEB

Figure 4: Processing time of the EF missing \(E_{T}\) algorithm. The timing distributions for the cell and FEB method are shown. A 2GHz machine was used for this test.

Figure 3: Cumulative time spent in the different phases (Region Selector, bytestream conversion, and algorithm) of the L2 e/\(\gamma\) selection as a function of \(\eta\) for the electromagnetic part (left) and for the hadronic part (right) for an RoI size of \(0.4\times 0.4\) in \(\eta\times\phi\) (a 2.3 GHz machine was used to perform these measurements).

based unpacking. On average, the time to process the whole calorimeter is dramatically reduced from 57 ms for the cell method to 2.4 ms for the FEB method.

The total scalar sum and the Missing \(E_{T}\) calculations were performed using the two unpacking methods. The missing \(E_{T}\) calculation does not depend on the method, while the scalar sum is systematically reduced in the FEB calculation due to the effect of the zero suppression. However, due to the drastic improvement in speed, the FEB algorithm is a valid option for the missing \(E_{T}\) reconstruction.

In addition to the timing studies detailed here, a thorough study of the memory usage and initialization time was performed. A substantial fraction of the initialization time is taken by the detector geometry preparation, including the filling of the cell coordinates and the Region Selector tables. This initialization step requires access to databases containing information on detector conditions, and possibly files with complementary information. It was determined that the initialization time is acceptable and does not inhibit the running of any of the desired algorithms. Furthermore, the memory usage of the algorithms was measured to be stable and within acceptable operating limits.

## 4 Data preparation summary

This note describes the implementation of the whole data preparation step for the HLT calorimeter trigger from the detector electronics up to the reconstruction level. It is fundamental that a data preparation layer is efficient and fast, leaving time for the real physics algorithms. The High-Level Trigger Calorimeter tools described here have been used extensively with simulated data to commission the ATLAS trigger. A unique interface provides access to detector physics quantities (calorimeter cells) obtained with complex computations from the readout data. Knowledge of the detector details is, of course, a fundamental input into optimizing the strategy to be followed in this unpacking procedure. The critical performance issue for calorimeter data preparation is that it be accomplished within the online time budget, and this goal has been achieved with the current system. Even for special algorithms, like the missing \(E_{T}\) which process cells from the whole detector, the data preparation performance is still within the required processing interval restrictions. Whenever FEB summary information can be used, significant timing reductions can be achieved. Further optimization studies are still in progress.

In addition to studies with simulated data, the tools and algorithms discussed here have been applied to commissioning runs of the ATLAS detector using cosmic rays. Until the LHC begins taking data, this is the only exercise that can approximate the real trigger usage in LHC conditions. Many trigger objects, such as taus, jets, and missing \(E_{T}\) are being successfully debugged in this manner, providing important feedback to the algorithm developers.

## References

* [1] ATLAS Collaboration, Liquid Argon Calorimeter Technical Design Report, CERN/LHCC/96-041 (1996).
* [2] ATLAS Collaboration, Tile Calorimeter Technical Design Report, CERN/LHCC/96-042 (1996).
* [3] ATLAS Collaboration, Muon Spectrometer Technical Design Report, CERN/LHCC/97-022 (1997).
* [4] ATLAS Collaboration, Atlas First Level Trigger Technical Design Report, CERN/LHCC/98-14 (1998).
* [5] ATLAS Collaboration, High-Level Trigger, Data Acquisition and Controls Technical Design Report, CERN/LHCC/03-022 (2003).