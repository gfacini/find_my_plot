# Design and Implementation of the ATLAS Detector Control System

H. Boterenbrood, H.J. Burckhart, J. Cook, V. Filimonov, B. Hallgren, W. Heubers, V. Khomoutnikov,

Y. Ryabov, F. Varela

Manuscript received May 28, 2003; revised October 3, 2003

F. Varela is with the European Laboratory for Particle Physics, CH-1211 Geneve 23 (eMail Fernando.Varela.Rodriguez@cern.ch).H. Boterenbrood and W. Heubers are with The National Institute for Nuclear Physics and High Energy Physics, 1098 SJ Amsterdam, The Netherlands.H. J. Burckhart, J. Cook and B. Hallgren are with the European Laboratory for Particle Physics, CH-1211 Geneve 23.V. Filimonov, V. Khomoutnikov and Y. Ryabov are with the Petersburg Nuclear Physics Institute, Gatchina, Leningrad district 188350, Russia.

###### Abstract

The overall dimensions of the ATLAS experiment and its harsh environment, due to radiation and magnetic field, represent new challenges for the implementation of the Detector Control System. It supervises all hardware of the ATLAS detector, monitors the infrastructure of the experiment, and provides information exchange with the LHC accelerator. The system must allow for the operation of the different ATLAS sub-detectors in stand-alone mode, as required for calibration and debugging, as well as the coherent and integrated operation of all sub-detectors for physics data taking. For this reason, the Detector Control System is logically arranged to map the hierarchical organization of the ATLAS detector. Special requirements are placed onto the ATLAS Detector Control System because of the large number of distributed I/O channels and of the inaccessibility of the equipment during operation. Standardization is a crucial issue for the design and implementation of the control system because of the large variety of equipment and the high number of different groups involved working all around the world. The functions of the two main components of the DCS, namely the distributed Back-End software system, which will be based on a commercial SCADA package, and the sub-detector's Front-End systems, with extensive use of the CAN fieldbus, are explained. The standard readout chain of the Detector Control System, which comprises both Back-End software and general-purpose Front-End equipment, is described and its performance is discussed.

## I Introduction

ATLAS[1] is a general-purpose particle detector designed to study the _p-p_ collisions at the Large Hadron Collider (LHC) at CERN, which will start operation in 2007. ATLAS will be the largest particle detector ever built. The project is an international collaboration involving institutions in 34 countries. The ATLAS detector has a cylindrical symmetry with a total length of 42 m and a radius of 11 m. The detector is divided in nine different specialized sub-detectors that perform different tasks like particle identification and track reconstruction, energy measurement and muon spectrometry.

The LHC experiments represent new challenges for the design of the control system. The very large number of elements and the high complexity of the project forces the construction of the detector components and related systems to be performed well ahead of their use. The long lifetime of the LHC experiments imposes the use of evolving technologies. The overall dimensions of the detector and the high number of I/O channels call for a control system with processing power distributed over all the elements of the experiment while keeping a low cost. The environmental conditions require the utilization of magnetic field and radiation tolerant equipment. A key concern is to achieve homogeneity throughout the system in spite of the diversity of the equipment and the number of people involved in the construction of the system. For this reason, the usage of commercial components and industry standards like fieldbuses or communication protocols is envisaged wherever possible. This approach will lead to a significant reduction in the design and maintenance efforts.

## II Experiment Control

The overall control of the ATLAS experiment includes the monitoring and control of the operational parameters of the detector and of the experiment infrastructure, as well as the supervision of the software involved in the event readout. This functionality is provided by two independent, but interacting systems that perform complementary functions, as shown in Fig. 1. The Online Software [2] controls all Trigger and Data AcQuisition (DAQ) processes needed for physics data taking. The DCS, which is the subject of this contribution, handles the control of the detector hardware and the related infrastructure.

The DAQ control and the DCS systems have different operational requirements. Whilst the DAQ control is only required during physics data taking or calibration procedures, the DCS has to function with no interruption to ensure the safe operation of the detector. However, an intense interaction between both systems is of prime importance for the coherent overall operation of the experiment during data taking or calibration periods. The operation of the detector outside data-taking periods will be entirely handled by the DCS.

## III Connection DAQ-DCS

The DAQ system and the DCS are complementary. The former treats all aspects of the physics event data, which are identified by an event number, whereas the latter deals with the operational parameters of the detector, which are normally categorized with a timestamp. Although the ATLAS collaboration has decided to separate the functionality of both systems, an intense bi-directional communication between the DAQ control and the DCS is needed to ensure the coherent operation of the experiment. Physics data taking and calibration where both systems are required will be driven by the DAQ system. The overall synchronization of both systems will be performed by means of data, message and command exchange. The DAQ-DCS Communication (DDC) [3] software will handle this run-time information exchange between both systems. The DDC has been successfully operated in several test-beam periods of the ATLAS sub-detectors [4].

## IV Score of the DCS

The principal task of the DCS is to enable the coherent and safe operation of the detector and to serve as a homogeneous interface to all sub-detectors and to the technical infrastructure of the experiment. The DCS has to continuously monitor all operational parameters, signal any abnormal behavior to the operator and allow for automatic or operator initiated corrective actions to be taken. The DCS has also to provide a homogeneous interface between ATLAS and external systems. The term external indicates that the control of these systems is outside of the scope of the DCS although communication with them is required. These systems like the LHC accelerator, the CERN technical services (e.g. cooling, electricity, ventilation), the ATLAS magnet and the Detector Safety System, will be interfaced by means of a dedicated workstation called, DCS Information Server (DCS_IS) as shown in Fig. 1.

## V ATLAS Environment

The architecture of the DCS and the technologies used for its implementation are constrained by environmental and functional reasons. The main requirements for the DCS equipment are the following:

Radiation tolerance: The radiation levels vary by many orders of magnitude, from up to 100 kGy/year close to the interaction point and in the forward region, down to 1 Gy/year in areas shielded by the calorimeters. DCS equipment will only be placed in such shielded regions where radiation tolerant electronics will be sufficient. For this electronics, which will be installed at accessible places, it is envisaged to use selected Components-Of-The-Self (COTS) previously verified to operate in such an environment of ionizing radiation. In some cases, the possibility to replace the electronics during the lifetime of the experiment is foreseen.

Operation in magnetic field: The magnetic field in the cavern will vary in direction and magnitude reaching values of up to 1.5 T. Therefore electronics components like coils, chokes, transformers and some types of DC/DC converters must be avoided. Moreover, since power supplies may be sensitive to magnetic field or to effects caused by radiation, it is foreseen to supply the power remotely via long cables requiring electronics to have low power consumption.

Inaccessibility of the ATLAS cavern during data taking: Remote operation, configuration and monitoring, as well as remote diagnostics and automatic error recovery procedures must be implemented.

Experiment size: This requires a very large number of highly distributed channels with cable lengths of up to 200 m. Therefore, high-density I/O concentrators must be employed in order to keep cabling at a reasonable cost.

Large lifetime of the experiment: ATLAS will be operated for more than a decade. In order to reduce the maintenance effort, the DCS must be implemented out of well-defined and common building blocks to promote ease of repair and replacement of hardware. In addition, the DCS will undergo different software upgrades during the lifetime of the experiment and therefore in-system re-programming capability must be provided.

## VI DCS Organization

The DCS consists of two main components: a distributed supervisor software called Back-End (BE) system and the different Front-End (FE) systems of the sub-detectors. The equipment utilized in the implementation of the FE ranges from simple sensors and actuators up to complex computer-based devices like Programmable Logical Controllers (PLC). The functionality of the BE system is two-fold: it acquires data from the FE and it offers supervisory and control functions such as displaying, archiving or alert handling.

The organization of the DCS includes the geographical distribution of the DCS components and the functional architecture of the system. These two views of the DCS are

Fig. 1: Overall control system of the ATLAS experiment showing the connection between the DAQ control and the DCS.

constrained by different criteria. The physical deployment of the components is imposed by the ATLAS environment. On the other hand, the functional architecture is determined by the operational requirements of the ATLAS detector, which call for the support of different partitioning modes of the DCS. These two views are described in the following sections.

### _Geographical deployment_

The DCS equipment will be geographically distributed over three different areas as shown in Fig. 2. The operator workstations will be installed in the main control room, at the surface of the installations. These stations will communicate via a LAN with the equipment installed in the underground electronics rooms, USA15 and US15. These rooms will house the equipment that has to be protected against particle radiation and magnetic field. The different sensors and actuators will be distributed over the whole volume of the detector in the experiment's cavern, UX15. This equipment will be exposed to particle radiation and strong magnetic field. The devices placed at this location will be interfaced by means of field-buses to the equipment located in the underground electronics rooms. In particular, ATLAS has selected the industry standard CAN bus [5] on the basis of its reliability, robustness and determinism.

### _DCS Logical Architecture_

The DCS system must provide the functionality required to operate the different ATLAS sub-detectors in stand-alone mode, as well as the coherent and integrated operation of all sub-detectors for concurrent physics data taking. In order to provide this versatility, the system must be capable to dynamically map the partitioning schema of the DAQ system. The finest granularity of the DAQ system is given by the segmentation of the detector in different Timing, Trigger and Control zones.

From the point of view of controls, the detector is composed of largely independent systems arranged in a tree-like structure of several levels. These systems will be controlled by self-contained applications called control units. In addition, the states of the external systems, which may have influence on the operation of the detector, will be controlled by means of similar units. The hierarchy of control units will be operated as a Finite State Machine (FSM). The FSM approach allows for sequencing and automation of actions and supports different partitioning and ownership modes. Each control unit, and in particular, the sub-detector units, will be characterized by a well-defined set of states and by transitions possible between these states, which can be triggered either by command or by incidents. The main flow of information in the hierarchy, both data and commands is normally only vertical.

## VII Front-End Systems

The various components of the FE systems will be installed in the detector's cavern or in the underground electronics rooms depending on their operability in the ATLAS environment. All equipment, both commercial and purpose-built, to be installed in the cavern will have to be qualified for the operation in the environment described according to the ATLAS Policy on Radiation Tolerant Electronics [6].

Although the FE systems are the responsibility of the ATLAS sub-detectors, the DCS central team has developed a general-purpose I/O module called Embedded Local Monitor Board (ELMB) [7] to provide a common solution to all sub-detector groups for standard digital and analog I/O. The ELMB is a credit card sized electronics board, which provides 24 digital I/O lines and 64 high precision (16-bit) analog input channels at a low cost. The module is based on the industry standard CAN and CANopen [8] has been selected as the high-level communication protocol. It satisfies the requirements of most of the control applications and it has been qualified to be used in the ATLAS radiation environment [7]. The ELMB will be a key element in the implementation of the FE systems of the ATLAS sub-detectors with a total number of about 5000 ELMB nodes. This will provide homogeneity throughout the systems of the sub-detectors and it will ease the integration of the FE equipment with the BE software.

## VIII Back-End System

The BE system of the ATLAS experiment will be implemented using a commercial Supervisory Control And Data Acquisition (SCADA) package. SCADA products constitute a standard framework to develop control applications. A major evaluation effort of SCADA products [9] has been carried out at CERN in the frame of the Joint COntrols Project (JCOP) [10]. This evaluation concluded with the selection of PVSS-II [11] from the Austrian company ETM, to be used for the implementation of the BE systems of the four LHC experiments. PVSS-II is a device-oriented product were process variables that logically belong to a device are combined in hierarchically structured data-points. PVSS-II has been designed as a distributed product, where the different tasks are performed by specialized programs called managers. These managers, as well as the internal PVSS-II database, can be distributed over several

Fig. 2: Geographical deployment of the DCS equipment.

PCs running either Microsoft Windows or Linux. The communication between the different managers is handled by the SCADA. The internal architecture of PVSS-II is entirely event-driven, which allows a significant reduction of the data traffic leading to better performance, as compared to the SCADA products based on polling. PVSS-II supports a wide set of standards to interface hardware and software. It also provides an open Application Program Interface (API) library, which allows the access to the run-time database of the product facilitating the interface between the DCS and the external systems like the LHC accelerator, or the DAQ system.

An engineering framework on top of PVSS-II is being developed in the frame of JCOP to guarantee homogeneity of the BE system. It comprises a set of tools and components commonly used by the four LHC experiments. The ELMB has been integrated into PVSS-II as a component of the JCOP framework to ensure the standardization of the ELMB SCADA software throughout the experiment.

### _Functions of the BE_

The BE system of the ATLAS DCS will consist of the order of 100 PVSS-II stations. These are arranged in three functional layers as shown in Fig. 3. The actions at the operator level will be performed by the PC of the upper layer of the hierarchy shown in the figure, while the real time actions will be carried out by the PVSS-II stations at the lower levels. Common functionality like archiving, alert handling and logging of incidents will be provided at all levels of the hierarchy.

The overall control of the detector equipment will be performed from the Global Control Stations (GCS). These workstations will perform high level monitoring of the different sub-detectors and display summary information. The communication with the external systems will also be handled at this level by means of a dedicated PVSS-II system called DCS_IS.

The full control of the sub-detectors will be provided at the Sub-detector Control Station (SCS) level. There will be a unique SCS per sub-detector and an additional station for the supervision of the Common Infrastructure Controls (CIC). The CIC controls all equipment which is not the responsibility of the sub-detectors, like the rack system and power distribution. It also monitors the environmental parameters over the whole installation, e.g. temperature and pressure. The information monitored by the CIC is published to the DCS_IS. The SCS are also directly interfaced to the DCS_IS in order to retrieve the information of the external systems and of the CIC, which may have an impact on the operation of the detector. The SCS will also handle the coordination of the various services of this sub-detector, which are controlled by the Local Control Station (LCS) in the layer below. For these reasons, the SCS represents the natural place to validate commands issued by either the operator or the DAQ system.

The LCS will be directly connected to the FE equipment and will perform the low-level monitoring and controls of the different systems or detector services. Besides the equipment readout and control, these PVSS-II stations will carry out fine calibration of the raw data and will build up summary information of the system status to be reported to the SCS in the layer above. The LCS stations at this level can be arranged either following functional or geographical criteria. In the former approach, each PC controls one or more services of the sub-detector (e.g. cooling, voltage), whereas in the later, the segmentation of the sub-detector is modeled and each of the stations controls all DCS services for a segment. Both arrangements are shown in Fig. 3. In this picture, a functional arrangement has been followed for the Tile Hadron Calorimeter whereas the Pixel detector is organized geographically.

## IX Interface FE-BE

All control equipment of the experiment will be interfaced to PVSS-II. Out of the different methods possible to connect the FE devices to SCADA, JCOP has chosen OPC (OLE1 for Process Control) [12] as the preferred solution. OPC is becoming the industry standard to interface hardware to control applications or distinct software processes. It defines middle-ware, based on the Microsoft technology DCOM2, which allows separation of the hardware specific part from the software used for control. PVSS-II, as many other SCADA products, supports the OPC protocol. OPC implements a multi-client/multi-server architecture, where clients may access the process data held in the so-called address space of the servers through a set of standardized interfaces. The server may organize the different items in groups on behalf of the client assigning common properties to them, like update rate or dead-band. OPC also defines several data access mechanisms such as synchronous or asynchronous readout, refresh or subscription. Despite the internal architecture of OPC being based on polling, the servers may be configured to transmit only data that have changed to the client, leading to a significant reduction of data traffic.

Footnote 1: OLE stands for Object Linking and Embedding.

Footnote 2: DCOM stands for Distributed Component Model.

Fig. 3: Functional hierarchy Back-End system.

Although there are several firms providing CANopen OPC servers, those evaluated only implement a limited sub-set of the CANopen functionality required to work with the ELMB. For this reason, a custom CANopen OPC server has been developed. The server complies with the OPC Data Access specification 2.0 and the CANopen Device Specification Profile 301.

Purpose-built devices or stand-alone systems like the Muon or Tile Calorimeter calibration systems will be interfaced to PVSS-II by means of dedicated drivers or using the PVSS-II API.

## X Readout Chain

The standard DCS readout [13] ranges from the sensor to the operator interface comprising the elements described above: the ELMB, the CANopen OPC server and PVSS-II. The ELMB network topology, in terms of buses, nodes and channels, is modeled in the internal database of PVSS-II by means of data-points. This information at the SCADA level is used to configure the OPC server, which acts as a driver for the ELMB network. The PVSS-II data-points are connected to items in the address space of the CANopen OPC server.

The ELMB digitizes the analog values and sets or monitors the digital output and input lines. The ADC can be configured to be operated in different ranges and at different conversion rates depending on the application. The ELMB can send data either on request from the SCADA application or at regular time intervals. It can also be set up to send data immediately when they have changed either by a pre-set amount or by exceeding pre-defined thresholds.

The OPC server assembles or de-multiplexes the CANopen frames to be transmitted to or to be received from the bus. It may also be configured to perform conversion from raw data to physical units before sending the values to the client.

Fine calibrations for the individual channels are performed by PVSS-II. At this level, data is displayed and then archived. The SCADA application performs the alert handling and enables corrective actions to be taken either automatically or triggered by operator intervention. Setting of data-points at the PVSS-II level will imply a change of the value of the OPC items with the consequent CANopen frame being sent to the CANbus by the server. In turn, when a message is sent by an ELMB, it is decoded by the OPC server, which sets the corresponding OPC item in the address space of the server and transmits the result to the PVSS-II data-point.

### _Performance of the readout chain_

In order to study the performance and scalability of the DCS readout chain, a real size ELMB network has been set up [14]. A system of six CAN buses, each 350 m long, with 32 ELMB was operated from PVSS-II at a bus speed of 125 kbaud, using the CANopen OPC server. All buses were connected to a single PC where two PCI CAN interface cards, manufactured by the Swedish company Kvaser [15], were installed. The number of channels in this setup was of the order of magnitude of some applications in ATLAS (12288 analog input channels and 1536 and 3072 digital input and output lines respectively). The setup employed in the test is shown in Fig. 4.

The remote powering of the ELMB nodes via the CANbus as required in ATLAS was studied in this test. The ELMB nodes in ATLAS may be susceptible to Single Events Upset (SEU) and in some cases, power cycling (OFF/ON) of the nodes will be required to recover from these effects. For this reason, it is preferable to install the power supplies in the underground electronics rooms and to feed the power to the nodes remotely via the CAN bus.

The performance of the systems was studied for two different situations:

* _Steady run_, where all messages sent by the ELMB are processed in real time and are stored in the PVSS-II database i.e. no buffering of messages is performed at the PVSS-II level.
* _Avalanche run_, where the fastest readout possible is performed for a short period, typically a few minutes. Under these circumstances, although all data are stored in the PVSS-II database, an increase of the RAM memory usage is observed as a consequence of the buffering of message in PVSS-II. It is important to note that although long-term operation under these conditions would not be possible, this situation can arise in ATLAS in case of major problems of the equipment and it must be handled by the system for a short period of time.

A readout rate of 30 s is required to have _steady run_ conditions in the system studied in this test, while the a readout interval of 8 s is achieved for several minutes in _avalanche_ mode.

The study of the bus-load for a system of these characteristics showed a peak of the bus occupancy of about 65%. In order not to exceed this value, the maximum number of ELMB nodes per bus will be limited to 32. These results permitted to define the granularity of the ELMB networks in ATLAS in terms of the number of nodes per bus and the number of buses per PVSS-II system, and to optimize the work balance amongst the different elements of the readout chain.

Fig. 4: Full size ELMB/CAN system.

### _Long term operation in radiation environment_

Some parts of the detector will be operated continuously because any interruption is costly in time or money or may even be detrimental to the performance of the detector requiring that the DCS operates without any interruption. On the other hand, the harsh environment in the detector's cavern may affect the normal operation of the control system. For these reasons, the long-term operation of the DCS readout chain in presence of particle radiation was studied at the TCC2 [16] test beam area at CERN. The particle composition of the radiation is similar to that expected in ATLAS, the dose rate is however much higher allowing for adequate statistics to be obtained in a shorter time interval. Radiation may induce different effects on the electronics, which require different recovery actions such as software reset if the registers were affected or power cycling of the node in case of Single Event Upsets. The environment in TCC2 allowed the investigation of different procedures to correct effects caused by radiation at the different levels of the readout chain. Some of the effects are internally handled at the ELMB level by the CAN controller, e.g. by re-sending the CAN message if errors are detected during the transmission of the frame, or by the ELMB firmware, e.g. for bit-flips. The recovery procedures at the OPC server level were based on the CANopen network management and node guard protocols. PVSS-II scripts were used to monitor the current consumption of the modules and to issue a power cycle if the current consumption of the nodes increases as a consequence of SEU. The system, consisting of three ELMB nodes on a 100 m long CANbus, was successfully operated with no interruption for two months, which is equivalent to approximately 300 years of operation in ATLAS in terms of Total Ionizing Dose (TID).

## XI Interface to External Systems

Information from the external systems namely LHC, CERN Technical Infrastructure, Detector Safety System and the magnet will be made available to ATLAS via the DCS. Although these systems are designed to react autonomously in case of problems, their status and early indications of possible problems must be transferred to the DCS since this may have consequences on the detector and actions initiated by the DCS may be required. In the case of the LHC, the DCS will receive information online about relevant parameters of the accelerator and will send back data about the quantities of the beam as observed in the ATLAS detector. All four LHC experiments have common requirements to interface the external systems. In order to reduce the development effort it was decided to define a single data exchange mechanism.

## XII Conclusions

Standardization and homogeneity are key issues in the overall design of the ATLAS DCS due to the large dimensions of the system, the diversity of equipment and the long lifetime of the experiment. Selected commercial equipment will be used whenever possible. These components have been qualified for the operation in the harsh ATLAS environment.

The operational requirements of the ATLAS experiment call for a highly flexible DCS, which allows to model the partitioning schema of the DAQ system. In order to operate the different sub-detectors in stand-alone or integrated modes, the DCS will map the hierarchical organization of the detector, where the different components will be arranged in a tree-like structure.

The DCS consists of two major systems: a distributed BE software, which will be based on the SCADA product PVSS-II, and the FE systems of the sub-detectors. Homogeneity at the BE level is ensured by the usage of the same commercial software product throughout. Standardization of the FE systems is achieved by the usage of the ELMB and the software employed in the readout chain. The ELMB offers the digital and analog I/O functionality required for most applications at a very low cost per channel. The functions performed by the elements in the readout chain, such as data conversion and calculation, data reduction, threshold comparison, detection and correction of radiation effects, are well balanced over the components. Its performance and the long-term stability of operation satisfy the requirements of ATLAS.

## XIII References

* [1] ATLAS Collaboration, "ATLAS: detector and physics performance technical design report", CERN/LHC/99-14, TDR 14 and 15, 1999.
* [2] ATLAS Collaboration, "ATLAS DAQ, EF, LVL2 and DCS technical proposal", CERN/LHC/2000-17, 2000.
* [3] H.J. Burckhart, R. Hart, R. Jones, V. Khomoutnikov, Y. Ryabov, "Communication between trigger/DAQ and DCS in ATLAS", in Proc. _Computing in High Energy and Nuclear Physics Conference_, China, 2001.
* [4] F. Varela Rodriguez, "The detector control system of the ATLAS experiment at CERN: An application to the calibration of the modules of the Tile Hadron Calorimeter", PhD Thesis, University of Santiago de Compostela, Spain, June 2002.
* [5] CAN in Automation (CIA), D-91058 Erlangen, Germany, [Online], Available: [http://www.cana-cle.de/](http://www.cana-cle.de/)
* [6] M. Dentan, "ATLAS policy on radiation tolerant electronics", CERN ATC-TE-QA-0001, 2000.
* [7] B. Hallgren, H. Botcherhood, H.J. Burckhart, H. Kvedalen, "The Embedded Local Monitor Board (ELMB) in the LHC front-end I/O control system", in Proc. _" Workshop on Electronics for the LHC Experiments_, Stockholm, Sweden, 2000.
* [8] G. Grubler and B. Dreider, "CANopen implementation guidelines", STA Reuellingen, Germany 1997.
* [9] A. Daneels and W. Salter, "Selection and evaluation of commercial SCADA systems for the controls of the CERN LHC experiments", in Proc. _International Conference on Accelerator and Large Experimental Physics Control System_, Trieste, Italy 1999.
* [10] D. R. Myers, "The LHC experiments Joint Controls Project, JCOP", in Proc. _International Conference on Accelerator and Large Experimental Physics Control Systems_, Trieste, Italy, 1999.
* [11] ETM, ProzeWisualisiearungs- und Steuerungs-system (PVSS-II), [Online], Available: [http://www.pvss.com](http://www.pvss.com)
* [12] The OPC foundation, OLE for Process Control, [Online], Available: [http://www.ep/foundation.org/](http://www.ep/foundation.org/)
* [13] H. Boterenbrood, H.J. Burckhart, J. Cook, V. Filimonov, B. Hallgren, F. Varela, "Vertical slice of the ATLAS detector control system", inProc. 7th Workshop on Electronics for LHC Experiment", Stockholm, Sweden, September, 2001
* (14) F. Varela Rodriguez, "Systems of ELMB buses using the Kvaser PCI CAN card", CERN ATLAS DCS-IWN17, September 2002.
* (15) Kvaser, [http://www.kvaser.com](http://www.kvaser.com)
* (16) H. Boterenbrood, H.J. Burckhart, J. Cook, V. Filimonov, B. Hallgren and F. Varela-Rodriguez, "Results of radiation tests of the ELMB (ATmega128L) at the CERN TCC2 area", CERN ATLAS DCS-IWN16, September 2002.