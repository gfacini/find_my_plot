ATLAS DAQ note 48

2 November 1995

R.Bock, F.Chantemargue, R.Dobinson, R.Hauser

Benchmarking communication systems

for trigger applications

### Abstract

We propose a number of measurements, to be executed on different existing networked parallel processing systems, in order to obtain comparable performance numbers in view of future realisations of high-frequency 'trigger farms'. We assume that interconnected processors will potentially play an important role in trigger implementations of LHC experiments, and that the proposed measurements will allow individual systems to be assessed for different applications.

### Farms in trigger applications

General-purpose processors in conjunction with communication networks can potentially provide high computing power for real-time applications in a largely configurable and scalable way. This has been recognised in the high-energy physics community at an early date. The most challenging such application can be found in the high-frequency (\(\sim\)100 kHz) triggering of events in proton-proton experiments; the Technical Proposals for both CMS and ATLAS express the confidence that such systems will be available at the timescale of LHC, with performance and economic parameters that match the experiments' requirements and resources.

The targets for marketing such systems, or components thereof, are quite different, however. Some cater for pure communication systems (Telecom), others compete in the market for what used to be supercomputers (presently called HPCN systems, for High-performance computing and networking), yet others are attempting to become a substitute for LAN-s. It is not clear, today, which components or which communication standards will eventually become market leaders, and have a chance of surviving the stiff competition.

In ATLAS parlance, the trigger problem of level 2 offers several applications for networks with associated intelligent nodes:

- local data concentration and preprocessing, within a group of individual readout buffers,

- collection of RoI data from readout buffers and feature extraction, within a detector subsystem ('local processing farm'),

- collection of features from multiple subsystems and RoI-s, and global decision taking ('global processing farm'),

- supervision and resource allocation for some or all of the above operations,

- monitoring with a suitable user interface.

### Benchmarking communications

Ideally, the problems arising in high-frequency triggering should be defined, and then ported onto multiple potential implementations to demonstrate their feasibility. Today's systems, however, are certainly not sufficiently performant to cope with all of the above applications; if they were so, medium- to large-scale demonstrators would cost too much to implement. The algorithms to be executed have not been defined in any detail, experiments are still optimising the detectors. On the other hand, the trigger structure will critically depend on the available components, and can not be defined in abstract.

It seems necessary, therefore, to define a sequence of measurements that can be performed with existing equipment at acceptable cost, and yet allows to judge the suitability for different application areas. We assume that it will be possible to execute these benchmarks in collaboration with manufacturers; we will not be able to afford a purchase of all potential communication and parallel systems, to learn about their possibilities of fine-tuning, or to interface them to data sources like emulators. This may eventually be done with a very few selected candidates, at a time when the pressures of final decisions on technology can no longer delay serious prototyping.

We propose below a sequence of basic measurements; the following additional comments apply to all benchmarks:

- all performance numbers measured must be end-user-to-end-user, including all overheads of interfaces with software, operating system kernel, context switching, protocol wrapping/unwrapping, etc. They must correspond to system conditions that can be extrapolated to several hundred nodes with a long lifetime. Bare or hardware results may be supplied in addition, but the system aspects must be addressed in detail;

- in many cases, communication performance can be expressed by simple parameters: an _overhead_ (often referred to by the very ambiguous term latency), a maximum _throughput_ (assuming very large packets), and a rising parameter indicating at which packet size half the maximum throughput is reached (usually dependent on the two others). Another simple measure, particularly important for trigger applications, is the achieved _frequency_. We realise that these simple concepts will not cover all situations, and suggest therefore multiple measurements which implicitely define them;

- all benchmarks are to be executed for the constant _packet sizes_ 0, 64, 256, and 1024 bytes; more complete measurements, like a full frequency or bandwidth curve versus packet size, are encouraged, but they need to be interpreted;

- we assume that communication performance depends only on traffic patterns, packet sizes, and processor occupation, but is independent of the information content of packets;

- we assume that algorithms have no detector or physics content (the benchmarking of trigger algorithms is a separate exercise);

- interfacing to the specific hardware of experiments need not be discussed, all data streams can originate inside a node's memory.

The benchmarks come in two groups, the first measuring basic parameters, the second being closer to possible implementations of parts of second-level trigger architectures in ATLAS. The way in which processor occupation is introduced, is different in the two groups (we assume that the environment provides tasks running with different priorities):

In the first, basic group (benchmarks 1.1 to 1.7), processor occupation is measured by running with and without a continuous computing task, running in parallel with lower priority. Computing is introduced separately in sending and receiving processors, where this applies. To be measured is the realtive slowdown both in communication and in computing performance.

In the second group (benchmarks 2.1 to 2.3), three distinct processor occupation assumptions are made for receiving processors ('zero', 50, and 500 microsec/packet).

The 'zero' algorithm in a processor is one which just avoids that the (too clever) systems are not effecting any transfer, because data are not referred to.

**Benchmark 1.1 (one-way):** one sender, one receiver. Measured by bouncing back the message (ping-pong), and dividing the time by two. Objective: measure basic one-directional communication (this benchmark corresponds to Parkbench COMM1).

**Benchmark 1.2 (two-way):** two identical nodes communicate, both acting as sender and receiver. Objective: measure basic two-directional communication (this benchmark corresponds to Parkbench COMM2).

**Benchmark 1.3 (all-to-all):** all processors are simultaneously senders and receivers. The number of processors depends on availability, but should include at least the numbers 4, 8, and 16. Objective: measure switch congestion and scaling (this benchmark corresponds to Parkbench COMM3).

**Benchmark 1.4 (pairs):** an even number of processors is split into two group of same size, one of senders and one of receivers. The number of processors depends on availability, but should include at least the numbers 4, 8, and 16. Objective: measure switch scaling without contention.

**Benchmark 1.5 (outfarming):** one sender, different data are sent to multiple receivers, defined by a list. The number of receivers depends on their availability, but should include at least the numbers 4, 8 and 16. Objective: measure the basic data distribution capability.

**Benchmark 1.6 (multicasting):** one sender, identical data are sent to multiple receivers, defined by a list. The number of receivers is defined as in benchmark 1.5. Objective: measure the basic broadcasting capability. (Comments desirable on the effect of changing the list for each broadcast.)

(Benchmarks 1.5 and 1.6 have the same diagram)

**Benchmark 1.7 (funnel):** multiple senders, one receiver. The number of senders depends on their availability, but should include at least the numbers 4, 8 and 16. Objective: measure basic event building capability.

**Benchmark 2.1 (push farm with supervisor):** a supervisor processor communicates to all sources a receiver (destination) address (round-robin); the source processors then send data packets to the indicated destination processors. The destination processors inform the supervisor of a 'decision', no action ensues in the supervisor. All data packets are of same (variable) length; 4, 6 and 8 sources are combined with one destination each, all packets exchanged with the supervisor are 16 bytes. Objective: measure basic push-farming performance, assuming processor control is via the supervisor through the switch.

**Benchmark 2.2 (pull farm with supervisor):** as benchmark 2.1, but the supervisor processor communicates to the destinations (round-robin) the source addresses, and the destination processors fetch their data from the sources; the destinations again inform the supervisor of their decision, and no action ensues in the supervisor. All packet sizes exchanged are as before. Objective: measure basic pull-farming performance, assuming processor control is via the supervisor through the switch.

**Benchmark 2.3 (pipeline):** source processors send data to destination processors, in a data-driven pattern. Some synchronisation is necessary in the destination processors. In this benchmark, only the overall achievable frequency is measured, with different (equal) packet sizes and processor occupations. Two levels of pipeline are assumed, a limited funnel effect (2:1 or 3:1) is assumed. Processor topologies: 4-2-1 (2:1 / 2:1), 6-2-1 (3:1 / 2:1), and 9-3-1 (3:1 / 3:1). Objective: measure the capability of processors to be part of a high-frequency data-driven architecture, running short algorithm parts.

In ATLAS language, benchmark 2.3 is a global decision architecture fed by data-driven 100 kHz processors. A similar benchmark was the objective of past measurements; cf. R.Hauser and I.Legrand: A real-time application for the CS-2, Proceedings of HPCN Europe, May 1995, Milano, Springer Lecture Notes in Comp.Science 919.

_Discussion of results_

The output of a benchmark sequence should contain the achieved numbers, and further include a discussion of the following points:

- details of hardware platform and configuration used

- programming language used

- O/S version used, kernel tuning used

- communication method(s) used (library, protocol)

- measuring method used

- possible effects of caching

- possible effect in measurements of other users (in non-dedicated systems)

- discuss the scaling possibilities to 100 and 1000 sources/destinations

_Example algorithms, References_

A file with algorithms as examples of implementation for the above benchmarks is in preparation, see [http://www.cern.ch/RD11/combench.html](http://www.cern.ch/RD11/combench.html).

The Parkbench algorithms are described in:

[http://www.epm.ornl.gov/](http://www.epm.ornl.gov/)\(\sim\)walker/parkbench