Atlas INDET-NO-116

November 1995

**b-tagging with Atlas Inner Detector using Fast Simulation**

**Stephen Haywood - RAL**

## 1 Introduction

This note should be read in conjunction with **INDET-NO-092** where the basic methods are outlined. In this note, the methods will not be reiterated, except to highlight significant changes.

While it is clear that the work carried out for these studies is less sophisticated than can be achieved with full simulation, nevertheless, the level of agreement between this work and Igor Gavrilenko's work using full simulation and superior pattern recognition confirms that this work is not such a bad description of what may be possible and probable for real data.

### Definitions

The following abbreviations are used:

**MC**: Monte-Carlo. MC tracks are referred to as KINE tracks.
**IP**: Impact parameter, primarily in the \(r\)-\(\phi\) plane.
**PV**: Primary vertex - where \(p\)-\(p\) collisions occur.
**ID**: The Atlas Inner Detector.
**PR**: Pattern-recognition.
**MS**: Multiple-scattering.
**PVL**: Pixel Vertexing Layer.

The helix parameters fitted are:

\(R^{-1}\)Inverse radius of curvature, proportional to \(p_{r}^{-1}\).

\(\phi_{0}\)Direction of trackin \(x\)-\(y\) at point of closest approach.

tan\(\lambda\)Tangent of dip-angle.

\(d_{0}\)Impact parameter, defined as distance of closest approach to the beam-line.

Sign is positive if track has positive angular-momentum around beam-line.

\(z_{0}\)'impact parameter', defined as the value of \(z\) at point on track where \(d_{0}\) is evaluated.

## 2 New Developments

### Layouts

By default, I have studied the **Morges Layout**, where Si is chosen in the Forward region rather than MSGC's. My work of the last year has considered more carefully the \(b\)-tagging capabilities of the detector over the _whole rapidity range_. This was of importance for the technology choice in the Forward region. However, since with the Morges Layout, the performance is arranged to be more uniform as a function of rapidity, here the differences between different rapidity intervals are less marked, and it generally suffices to look at the Barrel region (\(|\eta|<0.9\)).

At low luminosity, I have chosen to use a **Pixel Vertexing Layer** because

* see section 3.3.2.
* It has greater rapidity coverage.
* The presence of space-points with low probability for being merged allows my PR algorithm to converge much faster than for a Strip Vertexing Layer. (I estimate 15% of hits in \(b\)-jets from \(H\to b\bar{b}\) are merged in the proposed Strip Vertexing Layer.)

### Merged hits

Previously, if two hits were too close together, I removed both hits. Now, if two hits are within \(3\times pitch\), I replace them with a single hit half-way between the two. Since I do not simulate individual strips or the response of the detectors, I am unable to do more. This procedure is applied to strip layers or the individual projections (\(\phi\), \(u\) or \(v\)) of Stereo layers. This new procedure significantly reduced my \(b\)-tagging capability but was compensated by other improvements which I made.

### V\({}^{0}\)'s

Originally I was interested in measuring efficiencies for reconstructing tracks in a jet, and simply used \(b\)-jets as a test sample. Because I had not developed the techniques to handle charged particles from V\({}^{0}\) decays being produced beyond the sensitive layers, I forced all V\({}^{0}\)'s to decay within a cylinder \(r<10\)cm and hits were generated on all layers even if they were inside the decay radius. This enhanced the number of V\({}^{0}\) daughters in a \(b\)-jet which could contribute to the \(b\)-tagging (moderate impact parameter, but not so large as to be removed by cuts) by a factor of around 3, and unfairly enhanced the \(b\)-tagging capability compared to _prompt_-jets.1 Conversely, since the daughters of V\({}^{0}\)'s wereidentified as limiting the Rejection of light-quark jets (\(u\), \(d\) and \(s\)), this procedure degraded the Rejection which could be obtained against the light-quark jets.

Footnote 1: _prompt_-jets are \(b\)-jet where the impact-parameters of all tracks are set to zero.

Now, the decay point of V\({}^{0}\)'s is not artificially limited and hits are only generated beyond this point. By (my) definition, a track in a _prompt_-jet which corresponds in the original \(b\)-jet to a V\({}^{0}\) daughter is such that only the impact-parameter is set to zero, but the track'starts' (leaving hits in detectors) at the same distance from the collision point as the original track. To conclude, the degradation of the _prompt_-jet Rejection resulting from this more correct treatment is not huge, although there is a more significant improvement in the Rejection of light-quark jets.

It was pointed out to me that since my _prompt_-jets are supposed to represent light-quark jets _after_ all V\({}^{0}\)'s have been identified and removed, it is cheating to allow V\({}^{0}\)'s to be used for the \(b\)-jets. Therefore I have tried tagging the V\({}^{0}\) daughters from the MC truth and not using them for tagging. However, after the changes indicated above, this actually improves the _prompt_-jet Rejection because the benefit to the \(b\)-jets is minimal, but since the corresponding tracks in the _prompt_-jets tend to have missing hits at the inner radii, they tend to incorrectly get associated to hits from other tracks and lead to tails in the IP distribution. These tails are reduced by removing the V\({}^{0}\) daughters. This is a small effect and I ignore it.

### Multiple-scattering fits

MS is particularly important in the Forward region. Therefore I have upgraded my fitting procedure to allow for MS. This is done by introducing a correlation matrix2 into the fitting procedure and the fitted parameters are expressed inside the last observed scattering plane. The same fit is used through the PR phase of the reconstruction.

Footnote 2: MS is simulated and fitted as if it were simply described by a Gaussian parameterisation.

The fit yields parameter errors which correctly describe the measurements and which I use for tagging rather than the parameterisations which I had previously take from Szymon Gadomzki. An example of a normalised distribution ('pull') can be seen in Figure 1a and 1b. Correct errors result in meaningful \(\chi^{2}\) distributions for the track fit which permit quality cuts to be made - see Figure 1c. Also fitted tracks can be matched to MC tracks by \(\chi^{2}\) - see Figure 1d - and this permits track-finding efficiencies to be estimated in a more Physics oriented way, as opposed to counting hits which is better suited to checking the PR algorithm. The probability distributions show spikes at low values due to PR problems; the spike as high values arise from the fact that although an MS fit is performed for all fitted tracks, not all particles have MS applied when generating hit positions. All particles are tracked through the detector in the absence of MS (perfect helix) and hits are subsequently displaced according to MS only for those particles within acone of \(\Delta R<0.4\) around the \(b\)-quark direction. Track-finding is performed in a slightly larger cone, but only tracks within the \(\Delta R<0.4\) cone are subsequently considered for \(b\)-tagging.

### Cone size

Previously track-finding was performed in a 'cone' in \(\Delta\phi\times\Delta\theta\) around the jet direction. Now it is done a 'cone' in \(\Delta\phi<(0.4+0.15)\times\Delta\eta<0.4\) and then only tracks with \(\Delta R\equiv\sqrt{\Delta\phi^{2}+\Delta\eta^{2}}<0.4\) are used for \(b\)-tagging. (The term 0.15 is added to allow for magnetic deflection of particles \(p_{T}\geq 1\)GeV.)

### Physics

Previous work concentrated on tagging \(t\to bW\) (\(m_{t}=1\,60\)GeV). This is challenging because of the close proximity of tracks within the \(b\)-jet, and hence provides a good test of the ID. However, since IP tagging may not be required for this process, a physics process which may be of more direct interest is \(H\to b\overline{b}\), (\(m_{H}=80\)GeV).

_Prompt_-jets have been used extensively in this work since they provide a sensitive test. However, they only provide an upper limit on what is obtainable. In practice, the background to \(b\)-jets from \(H\to b\overline{b}\) is more likely to come from gluon jets. To compare compatible kinematics, background jets have been generated with the process \(H\to gg\). This is not to say that this process is an actual background to 

[MISSING_PAGE_EMPTY:4]

Probabilistic methodThe offset at high Efficiency before any significant Rejection is achieved arises from a) the need to have tracks in the jet to determine the jet axis and b) the requirement that there must be at least one track passing the cuts to provide a tag. This is implicitly included in the counting method. The **steep rise** in the Rejection which follows quite closely the curve from perfect PR corresponds to the Rejection of _prompt_-jets where the tracks are described by the nominal IP resolution. The steepness of the curve is determined by this resolution compared to the _b_-lifetime. The **shoulder** occurs where the nominal resolution is no longer applicable because PR problems have arisen. Often, this is the result of just one or sometimes two tracks being incorrectly reconstructed. The last two features can be anticipated from the IP distribution of tracks from a non-_b_-jet - see Figure 5.

Counting methodA cut at \(K\sigma\) yields a probability \(\omega\) for one track to be significant. Very crudely, one can expect that the probability of finding \(n\) significant tracks will be roughly Poisson and will decrease approximately as \(\omega^{n}\). For _b_-jets, \(\omega\) will be close to unity, and the changes to _b_-jet Efficiency will be approximately linear with \(n\). For non-_b_-jets, \(\omega\) will be much less than unity and changes to efficiency (reciprocal of Rejection) will be geometrical (ie. as a power of \(n\)). So the Efficiency-Rejection curve will be approximately exponential - or linear on a log-linear graph.

Figure 3 shows what can be achieved for _prompt_-jets. The situation for real jets which contain significant numbers of daughters from long-lived parents (heavy quarks or V's) is different - this is illustrated for gluon jets in Figure 4. There is much less distinction between real and perfect PR, because the Rejection which can be achieved is limited by particles truly carrying lifetime information. This is discussed more in section 3.1.

It can be seen that in principle, with excellent PR and the removal of daughters from long-lived parents, the probabilistic method can provide better Rejection than the counting method. However if this cannot be attained, then the presence of just one or two tracks in a jet with large IP can limit the Rejection, and the counting method naturally avoids this limitation by demanding more significant tracks. So in many situations, the counting method may provide greater Rejection for Physics analysis. However, as a test of the PR capabilities of a given design of the ID, the probabilistic method may provide a more useful tool since it explicitly separates the role of resolution and PR in the Rejection.

As a general observation, the Rejection curves for the two methods are very close around the position of the shoulder on the probabilistic curve. I believe the explanation is as follows: If one considers a single track, then the probability distribution is directly related to the 'pull' distribution \((d_{\parallel}/\sigma)\) - see Figure 5. In the _probabilistic_ method, a cut is made on the probability distribution yielding an efficiency equivalent to integral to the right of the cut. The Efficiency-Rejection curve is mapped out by sweeping this cut simultaneously across probability distributions of the \(b\) and non-_b_-jets. When one asks for \(\geq 1\) significant tracks in the _counting_ method, then a fixed cut is made on the _same_ probability distributions at \(K\sigma\) and yields a point on the curve derived from the _probabilistic_ method. So the two methods will share a common point. This explanation is complicated by the fact that there are of course several tracks in a jet and certain correlations between tracks: for a _b_-jet, there are likely to be several tracks with large IP all of which will contribute to the probability \(\Omega\); for a non-_b_-jet, if one track is confused, it is likely that so is another.

The point in common between the two curves is determined by the value of the cut \(K\) in the counting method. This is chosen to be 3 which is close to the value where PR tails in the 'pull' distribution start to dominate over the Gaussian portion of the distribution which is determined by the nominal IP resolution. It is the onset of these tails which lead to the _shoulder_ in the probabilistic curve. Hence the point in common for the two methods will be somewhere on the shoulder. Therefore, it can be seen that choosing \(K\approx 3\) provides the optimal Rejection curve for my PR algorithm (I only appreciated this retrospectively). In deed, if I choose \(K=5\), I get a curve whose first point is at lower Efficiency than the position of the'shoulder' and consequently has a poorerslope.

### Final comments

In this paper, many Efficiency-Rejection curves are shown.

* In some cases, it may be difficult to visually identify some of the curves (because I have tried to preserve trees), but in cases of difficulty, common sense normally will provide the correct answer!
* Frequently \(\sim 10^{4}\) events have been used leading to a comparable number of jets in the Barrel. In some cases, more jets have been processed for the non-\(b\)-jets, and in other studies, the PR is so slow that fewer have been processed. The number of non-\(b\)-jets used is equivalent to the maximum Rejection which is shown on the plots.
* It will be seen that just below the maximum Rejection, the curves are stepped corresponding to the rejection of individual jets. These observations provide an estimate of the statistical errors on the curves which become questionable within a factor of 10 of the maximum Rejection shown.
* While it will not often be stated, the Efficiencies on these graphs are the efficiencies to tag \(b\)-jets from \(H\to b\overline{b}\) (or \(t\to bW\)). It is usually only the non-\(b\)-jets which are indicated, since these may be of several types.
* By default, I will tend to focus on the Barrel (\(|\eta|\leq 0.9\)), since the effects at other rapidities are similar, and the comparison of \(H\to b\overline{b}\) with \(H\to prompt\).

There are many parameters which enter into my analysis:

1. In the detector description.
2. In the simulation of digitisings - to speed up simulation.
3. In the PR.
4. In the \(b\)-tagging algorithms.

It is very likely that optimal choices may not have been made. In particular, the PR was tuned over a year ago. For any given detector design, there may be optimal algorithms and parameter choices. I have written my simulation/reconstruction to be fairly general and to work in most situations. However, this does mean that it may not be ideal for some variations. Nevertheless, it is hoped that the choices made are reasonable and permit meaningful studies.

Enough of the apologies! Now for some results!

## 3 Various studies

Important studies are contained in INDET-NO-115 [2]. In particular:

* The role of the calorimeter in determining the jet-axis.
* The role of the Vertexing Layer.
* Basic distributions for efficiencies and fake rates.

### Background Jets

Figures 6 and 7 indicate the Rejections which can be obtained against Higgs decaying to _prompt_, \(d\)-quark and gluon jets (see comment in section 2.6). Also comparison is made for \(t\)-quarks decaying to _prompt_-jets. Comparing jets from \(t\)-quarks (mean \(p_{T}\left(b\right)=68\) GeV) and Higgs (mean \(p_{T}\left(b\right)=40\) GeV), it can be seen that PR problems are worse for the \(t\)-jets. However, since the \(t\)-jets are better defined (more high \(p_{T}\) particles in a restricted cone), there is a smaller Efficiency offset at high Efficiency.

As was seen in section 2.7, the Rejection of real jets is limited. The Rejection is close to that obtainable with perfect PR (and hence not limited by PR) (see Figure 4) and is close to that which can be obtained at the KINE level - see Figure 8. Examining gluon jets which have \(\geq 2\) tracks with significant IP, I find that in the first 6 I considered, 1 has a pair of tracks from a V\({}^{0}\), 2 have a gluon splitting into a \(b\overline{b}\) pair and 3 have a gluon splitting into a \(c\overline{c}\) pair. The Rejection of \(u\), \(d\) and \(s\)-quark jets is limited by the production of V\({}^{0}\)'s, in particular, \(K_{s}^{0}\)'s and \(\Lambda\)'s.

The heavy-flavour content of the gluon jets is illustrated in Figure 9. The plots show the signed pulls \(d_{u}/\sigma\) for

* non-\(b\) particles. These come primarily from the fragmentation of the string, excluding the \(b\)-quark. They may include a few daughters of V\({}^{0}\)'s.
* Tracks which come from a gluon jet.

The first plot is dominated by the lifetime of \(b\)-hadrons. The second by the IP resolution, with tails from PR problems. Superimposed on the gluon jet plot is an admixture of 97% of the non-\(b\) particles and 3% of the \(b\)-daughters (after appropriate normalisation). One can conclude that roughly 3% of tracks in gluon jets arise from heavy-flavour. In a gluon jet, even when \(b\overline{b}\) or \(c\overline{c}\) pairs are created, there will be plenty of other particles produced which do not come from the heavy-flavour decays, hence the actual probability of heavy-flavour production in a given jet will be greater than the value of 3%. Hence a Rejection of less than 1/0.03 seems very plausible when one demands \(\geq 1\) significant track.

### Rapidity

Efficiency-Rejection curves for three different rapidity intervals are shown in Figures 10 and 11. Looking at the curves derived from the probabilistic method (Figure 10), in the resolution dominated region at high Efficiency, there is decreasing performance going to higher rapidity due to the degraded IP resolution which is inflated by MS (see Figure 24). However, in the PR problem dominated region at high Rejection, somewhat surprisingly the Rejection is greater at higher rapidity.

Reasons to expect things would be worse at high rapidity are:

* PR is more difficult in the presence of edges, which are present in the Forward wheels.
* Low \(p_{T}\) particles are curled into the wheels by the magnetic field.
* The charged particle distribution \(dN/d\eta\) peaks around \(|\eta|\sim 2.5\).

Reasons to expect the PR to be easier are:

* In the Morges layout, beyond \(|\eta|\sim 1.5\), there are 7 space-point measurements + 1 vertexing hit. Removing a layer reduces the Rejection, but does not account for all the effect.

* In the Morges layout, the GaAs modules, at high rapidity, have better resolution and smaller striplength than their Si counterparts. Degrading the GaAs modules reduces the Rejection, but does not account for all the effect.
* Heavy objects (Higgs or \(t\)-quarks) do not produce jets which have a constant cone-size measured in rapidity units as a function of rapidity. Rather the cone size tends to grow in \(\Delta\eta\).4 Since the wheels measure \(\Delta\eta\times\Delta\phi\) (in the approximation that \(\cos\lambda\ll 1\)), the ratio of separation to resolution between particles in a \(b\)-jet tends to increase at higher rapidity. The effect is even stronger for the heavier \(t\)-quark, leading to an even greater difference between the Rejection at low and high rapidity. Footnote 4: A very heavy object which would be produced at rest in the lab-frame would produce jets which have a constant cone-size measured in solid angle.
* Loopers cross the wheels only once (and \(\mathrm{Id}\) not simulate large cluster sizes) whereas they can cross barrels many times giving close correlated hits leading to ghosts.

### Layout

#### 3.3.1 Barrel radii

Figure 12 compares the TP Barrel (\(r_{{\it SCT}}=30,40,50,60\) cm) with the Morges Barrel (\(r_{{\it SCT}}=30,37,45,52\) cm). This plot is an old one (not comparable with Figure 6). While the IP resolution is unchanged, the larger radii of the TP Barrel allow greater separation of hits and reduced PR problems.

#### 3.3.2 Vertex layer

Figure 13 compares a Pixel (\(r=4\) cm) and Crossed-strip (\(r=6\) cm) Vertexing Layer. In IND ET-NO-115 [2] it was shown that for Rejection of gluon jets, there was little difference between the two options. However, the Rejection of gluon jets is limited by the physics, rather than the PR and the _prompt_-jets provide a more sensitive test. The PVL _which I have used_ has

1. Full rapidity coverage (with \(z=0\), \(\eta_{{\it m}\,{\it m}}=2.9\) vs \(\eta_{{\it m}\,{\it m}}=2.1\)).
2. Smaller inner radius (\(r=4\) cm vs \(r=6\) cm).
3. Worse resolution (\(\sigma_{{\it\phi}}=10\oplus 10\mu\) m vs \(\sigma_{{\it\phi}}=7\oplus 7\mu\) m).
4. Better 2-hit resolution (500\(\mu\) m pixels vs 23cm strips).
5. More material (1.1%\(X_{{\it 0}}\) vs 0.5%\(X_{{\it 0}}\)).

Nevertheless, the IP resolution of the layouts incorporating these two layers is almost the same: \(\sigma(d_{{\it 0}})\approx 17\oplus 54/p_{{\it T}}\).

#### 3.3.3 Replace pixel layers

Figure 14 shows the effect of replacing the pixel layers (\(r=11\) and 16 cm and wheels) by SCT stereo layers and the Vertexing Layer by crossed-strips. The performance is manifestly deteriorated, as is the speed of the PR.

### Resolution

#### 3.4.1 SCT resolution

Figure 15 shows the effect of replacing the SCT stereolayers with detectors of double the pitch, so that the effective space-point resolution increases from \(\sigma_{r,\phi}=16\mu\)m to \(30\mu\)m. While this has a negligible effect on the Rejection of gluon jets, there is a significant effect for _prompt_-jets.

#### 3.4.2 Pixel resolution

With analog electronics, it may be possible to increase the resolution of the pixels - in particular those in the Vertexing Layer. Figure 16 shows the result of improving the resolution of this layer from \(\sigma_{r,\phi}=10\oplus 10\mu\)m to \(5\mu\)m with no systematic. The improvement is disappointingly small. With this change, the IP resolution improves from \(\sigma(d_{\oplus})\approx 17\oplus 54/p_{\mathrm{r}}\) to \(8\oplus 55/p_{\mathrm{r}}\). These expressions have to be combined in quadrature with the error on the beam-spot of \(15\mu\)m, so the gain is not tremendous. Where PR problems dominate, the improved resolution of the Vertexing Layer does not appear to help much. This may be a feature of the algorithm used, in particular starting from the outer SCT layers.

### Detector Efficiency

My simulation works with space-points (except for the consideration 2-hit resolution and ghosts in stereo layers) and it is the efficiencies for the creation of these which are the inputs used. My model for the efficiencies is:

* Pixels consist of a single layer of silicon of efficiency \(\epsilon_{1}\) for seeing a hit. They have a geometrical acceptance of \(97\%\).
* SCT stereolayers consist of two layers of silicon each of efficiency \(\epsilon_{1}\) for seeing a hit. They have a geometrical acceptance of \(98\%\).

I choose a single layer efficiency \(\epsilon_{1}=1-1\%\) (threshold) \(-1\%\) (defects) = 0.98. Therefore the default efficiencies for obtaining a space-point (I do not use single measurements in stereo superlayers - only in crossed-strip layers) are:

\(\epsilon(\mathrm{pixels})\)\(\epsilon_{1}\times 0.97\)\(=\)\(0.95\)\(\epsilon(\mathrm{stereo})\)\(\epsilon_{1}\times\epsilon_{1}\times 0.98\)\(=\)\(0.94\)

In a degraded scenario, I have chosen \(\epsilon_{1}\) to be 0.91, leading to \(\epsilon(\mathrm{pixels})\) = 0.88 and \(\epsilon(\mathrm{stereo})\) = 0.80.

With the above changes in detector efficiency, the track-finding efficiency falls from 96.0 to 87.4%. The effect on \(b\)-tagging is shown in Figure 17. The differences are surprisingly small. If one sums the efficiencies over the 7 planes, then for the two scenarios, one obtains 6.61 and 5.84. Hence, in degrading the efficiency, the effective number of measurements falls by 12%. I observe that both the high-\(p_{\mathrm{r}}\) and MS terms of the average IP resolution increase by about 16%.

I have spent considerable effort trying to understand the region of the curves dominated by PR problems - where there seems to be very little sensitivity to detector efficiency (at least within the considered ranges).5 Concentrating on _prompt_-jets which survive beyond Rejections of \(\sim 10^{3}\), I find that about half the jets are in common between data samples corresponding to the two different efficiencies. These represent 'difficult' jets where tracks are very close together and PR problems are inevitable, irrespective of the detector efficiency. As for the remainder, I have failed to find any coding errors or logical errors in the way in which hits are generated which could explain the observations. Debugging events is very difficult and depends crucially on the random numbers used - this implies that for theseevents, the PR is on the 'edge'. For such events, a _global_ approach6 would be advantageous. It is quite possible that my PR algorithm is not optimal for investigating the dependence on detector efficiency. In particular, my algorithm does not permit branching, but instead (to decrease execution time) tries to pick up a _single_ hit on each layer.

Footnote 6: A _global_ approach considers the allocation of a set of hits to a set of tracks rather than a ‘first come, first served’ approach where only one track at a time is considered.

### Noise

The default noise levels which I have considered are: \(10^{-3}\) for \(\phi\), \(u\) and \(v\)-strips in stereo layers and \(10^{-7}\) for pixels. Figure 18 shows the effect of increasing this noise from a) the default to b) \(10^{-2}\) on the strips and c)\(10^{-2}\) on the strips and \(10^{-5}\) on the pixels. The biggest loss is in processing speed and consequently statistics (63K, 5K and 3K background events respectively). However, it can be seen that although my PR algorithm struggles, it does actually succeed in reconstructing tracks and there is little loss in Rejection power. Although this amount of noise is of a similar order to the occupancy from pile-up at high luminosity, at least in the stereo superlayers, the \(\phi\)-hits and the \(u/v\)-hits are not correlated and tend not to form space-points.

### Luminosity

Figures 19 and 20 show the effect of going to high luminosity (\(\mathcal{L}=10^{34}\) cm\({}^{-2}\)s\({}^{-1}\)). It is very likely that as the luminosity increases, so the Vertexing Layer will get fried and have to be removed. Because the Rejection of gluon jets is dominated by Physics effects, it is less affected by the changes than the _prompt_-jet Rejection.

From the Figures and Table 1, it can be seen that in the absence of a Vertexing Layer, the addition of pile-up does not seriously affect the \(b\)-tagging performance. If the layer survives, the effect of pile-up is more significant, presumably because it is more difficult to cope with the high density of hits close to the PV.

### Staging or Missing Layers

This study was performed before the release of the Atlas staging document [4], which proposes staging wheels only at higher rapidity. The consequences of this are fairly clear. I have considered what happens if one seeks to reduce the performance more uniformly in rapidity. This study also serves to indicate the effect of losing specific layers.7

Footnote 7: Note: layers are completely removed and so do not contribute to any MS.

I considered two conceivable staging scenarios for the Morges layout:

1. Remove barrel at \(r=30\)cm and wheels 5 and 9 (with corresponding GaAs)
2. Remove barrel at \(r=52\)cm and wheels 1, 6 and 9 (with corresponding GaAs).

\begin{table}
\begin{tabular}{|l|r r r|} \hline Rejection & Low \(\mathcal{L}\) & High \(\mathcal{L}\) & \\ \hline + Vtx layer & 300 & 100 & \(\pm 50\) \\ No Vtx layer & 75 & 50 & \\ \hline \end{tabular}
\end{table}
Table 1: Rejection of _prompt_-jets from Higgs for a \(b\)-jet Efficiency of 50%.

It turns out that the former does not perform as well (probably due to the difficulty of linking hits in SCT to pixels)8 and saves less money. Therefore I have concentrated on the second scenario. At low luminosity, I have retained the PVL. Figure 21 shows the Staged layout and the 7 layers removed. Also the number of space-points is shows as a function of rapidity.

Footnote 8: It turns out that for a \(b\)-jet Efficiency of 50%, the _prompt_-jet Rejection in the Barrel falls by a factor of 6 for the first scenario, while it falls by a factor of 3 for the second.

Figures 22 (low \(\mathcal{L}\)) and 23 (high \(\mathcal{L}\)) show ratios of the Rejection curves for different rapidities for the Staged layout vs. the complete Morges layout. The flatness of the ratios for high Efficiencies indicates that the IP resolution is hardly degraded by the staging; the main change is in the region of high Rejection dominated by PR problems. There is a more significant effect at low luminosities (although Rejections are already much higher to start with).

### 3-D Impact Parameter Tagging

It is interesting to consider 3-D IP tagging to a) see if there are potential gains for Physics with the current Morges design and b) see if there are (affordable) changes which might be made which would significantly improve the \(b\)-tagging capability of the ID.

When it comes to using IP's in the \(r\)-\(z\) projection, the relevant quantity is the \(z\) IP projected into the plane normal to the jet direction, ie. the quantity \(\cos\lambda\cdot z_{b}\). Plots of the resolutions for \(d_{0}\) and \(\cos\lambda\cdot z_{b}\) are shown in Figure 24. The rapidity dependence can be explained as follows:

At high \(p_{T}\)

* \(\sigma(d_{0})\) is constant because the precision tracker is designed to have uniform radial measurements at all rapidities.
* hence a tendency for \(\cos\lambda\cdot\sigma(z_{0})\) to fall like \(\cos\lambda\). This is moderated by the fact that the wheels measure \(r\).

At lower \(p_{T}\) (the average for a \(b\)-jet), MS is important and if the resolution is dominated by MS at the Vertexing Layer, then \(\sigma(d_{0})\) and \(\cos\lambda\cdot\sigma(z_{0})\) should be similar and behave like \(\sqrt{\sec\lambda}\) (sec \(\lambda\) from projection, \(\sqrt{\sec\lambda}\) from increasing material and \(\cos\lambda\) from increased momentum to preserve the \(p_{T}\)). For \(\sigma(z_{0})\), MS does not dominate until higher rapidities.

#### 3.9.1 Algebra for 3-D \(b\)-tagging

In 2-D, the IP is defined as _the distance of closest approach to the nominal beam-spot in the \(r\)-\(\phi\) plane._ In 3-D, I have chosen to consider _the point of closest approach to the jet-axis_. The distance of this point from the estimated collision point is an estimate of the decay length of the parent. The jet-axis will be close to the direction of the \(b\)-quark and hence the \(b\)-hadron. Therefore particles produced from \(b\)-decays (and prompt particles coming from the collision point itself) should originate from the jet-axis and therefore the _distance of closest approach to the jet-axis_ should be approximately zero. This is spoilt by a) the jet-axis being slightly different from the \(b\)-hadron direction, b) the jet direction resolution, c) the sequential decay of \(B\)'s to \(D\)'s. Nevertheless it provides some discrimination against reconstruction problems and tracks from V\({}^{0}\)'s.

_The point of closest approach to the jet-axis_ is determined separately for each track by forming a \(\chi^{2}\) in the fitted variables, ie. in the \(r\)-\(\phi\) plane perpendicular to the track direction and in the \(z\)-direction. Inthe approximation that tracks are close to the jet-axis, the \(\chi^{2}\) becomes:

\[\chi^{2}=(\frac{L-L_{\phi}}{\sigma(L_{\phi})})^{2}+(\frac{L-L_{z}}{\sigma(L_{z}) })^{2} \tag{4}\]

where \(L\) is the estimated transverse decay length and \(L_{\phi}\equiv d_{\parallel}/\Delta\phi\) and \(L_{z}\equiv-z_{\parallel}/\Delta\tan\lambda\) are the \(r\)-\(\phi\) and \(r\)-\(z\) estimates of the transverse decay length respectively. \(\Delta\phi\) and \(\Delta\tan\lambda\) are the angular differences between the track and jet directions. \(\sigma(L_{\phi})\equiv\sigma(d_{\parallel})/\Delta\phi\) and \(\sigma(L_{z})\equiv\sigma(z_{\parallel})/\Delta\tan\lambda\).

Minimisation yields

\[L=\frac{L_{\phi}/\sigma(L_{\phi})^{2}+L_{z}/\sigma(L_{z})^{2}}{1/\sigma(L_{ \phi})^{2}+1/\sigma(L_{z})^{2}} \tag{5}\]

with

\[\sigma(L)=\frac{1}{\sqrt{1/\sigma(L_{\phi})^{2}+1/\sigma(L_{z})^{2}}} \tag{6}\]

and the \(\chi^{2}\) becomes

\[\chi^{2}_{axis}=\frac{(L_{\phi}-L_{z})^{2}}{\sigma(L_{\phi})^{2}+\sigma(L_{z}) ^{2}} \tag{7}\]

It is easy to see that these expression are correctly normalised for prompt tracks. These expressions do not blow up for small \(\Delta\phi\) or small \(\Delta\tan\lambda\) and if one tries to formulate them in terms of the quantities perpendicular to the jet-axis in \(r\)-\(z\) ie. \(\cos\lambda\cdot z_{0}\), then one ends up with the same expressions.

I looked briefly at the KINE level at the discrimination provided by the quantity

\[\chi^{2}\,(2DF)=(\frac{d_{\parallel}}{\sigma(d_{\parallel})})^{2}+(\frac{z_ {0}}{\sigma(z_{0})})^{2} \tag{8}\]

(has 2 degrees of freedom) and concluded it was similar to that provided by my approach.

The \(\chi^{2}\) (1 degree of freedom) is converted to a probability, and a cut is made at 10\({}^{-20}\) (corresponding to about 9\(\sigma\)). The 'pull' \(L/\sigma(L)\) is used in similar way to the quantity \(d_{\parallel}/\sigma(d_{\parallel})\) in the 2-D analysis, and is used to form either a jet probability or to count the number of significant tracks. The errors \(\sigma(d_{\parallel})\) and \(\sigma(z_{0})\) should be increased to contain the uncertainty on the PV. As before, \(\sigma(d_{\parallel})\) is increased by the transverse beam-spot size (15\(\mu\)m). The spread in \(z\) of the beam-spot is about 5cm and the position of the PV will be determined event-by-event using the fitted tracks. In principle, I should do this in my study; however it is non-trivial because of the need to take care with the non-prompt tracks. Therefore I have avoided the issue and assumed that the \(z\) value of the PV can be determined with an error which is smaller than the \(\sigma(z_{0})\) of any single track in an event (this is not obvious) and I have chosen to ignore this contribution to the error.9

Footnote 9: If I increase the error on \(\sigma(z_{beam\to ppt})\) to 100\(\mu\)m, the Rejection in \(r\)-\(z\) falls by a factor 3, yet in 3-D, it falls by only 15%.

For these studies, I have considered the tagging in \(r\)-\(\phi\), \(r\)-\(z\) and in 3-D using the quantities \(d_{\parallel}/\sigma(d_{\parallel})\), \(z_{0}/\sigma(z_{0})\) and \(L/\sigma(L)\) respectively. Cuts similar to those for \(r\)-\(\phi\) analysis have been applied to the other two analyses. In addition, I have required that corresponding estimate of the transverse decay length (\(L_{\phi}\), \(L_{z}\) or \(L\)) should be less than 2cm, and a track is only used in 3-D provided it is used in both the \(r\)-\(\phi\) and \(r\)-\(z\) projection. In addition, for this study I have estimated the jet-axis by smearing the true MC jet-axis by \(\pm 1\,^{\circ}\) in \(\phi\) and \(\theta\) separately.

Figure 25 shows distributions of \(d_{\parallel}\), \(\cos\lambda\cdot z_{0}\), \(L\) and \(prob(\chi^{2}_{axis})\) for \(b\)-dughters and prompt particles in a \(b\)-jet. \(d_{\parallel}\) and \(\cos\lambda\cdot z_{0}\) for the \(b\)-dughters are similar since they are dominated by the \(b\)-lifetime. They

are different for the prompt particles because of the different resolutions. The probability distribution for \(b\)-daughters shows a spike at zero which arises from PR problems, V\({}^{0}\) daughters, and most significantly, the sequential nature of the \(b\)-decays. The cut on \(prob(\chi^{2}_{\pi\pi})\) improves the Efficiency-Rejection curve.

Figures 26 and 27 show the comparison between tagging in the different projections. While tagging in \(r\)-\(z\) is significantly worse than that in \(r\)-\(\phi\), the combination in 3-D is advantageous. Of course this is dependent on the assumption that \(z_{b\,em\,-\,sp\,d}\) can be determined with a negligible error.

#### 3.9.2 Improved \(r\)-\(z\) performance

Two scenarios have been considered for improving the \(r\)-\(z\) performance: a) analog readout of 'bricked' pixels could yield resolutions as good as \(\sigma_{z}=23\mu\)m\({}^{11}\)[5] and b) increased stereo angle for SCT of \(\alpha=1\,5^{\circ}\) (260mrad). The modified stereo angle was proposed by the Atlas referee and corresponds to the angle favoured by D0 for their forward wheels. Improving \(\sigma_{z}(pixel)\) must represent an improvement to the tagging capability, whereas it is less clear whether increasing \(\alpha\) will necessarily help since it will increase the number of ghosts in the stereo layers by approximately 260/40 = 6.5. The first part of Table 2 shows the resolution \(\sigma(z_{\rm b})\) at \(\eta=0\). The MS term is of the order of 85/\(p_{T}\), however the expression \(\sigma(z_{\rm b})=A\oplus B/p_{T}\) is something of a simplification, as was explained in INDET-NO-91 [6]. Note: in all of this work, it is assumed that the \(r\phi\) and \(z\) resolutions of the pixel detectors are constant and do not depend on the angle of incidence of a charged track. This will not be true in practice due to charge sharing.

Figures 28 and 29 show the results of making the changes indicated above in the Barrel. From the pixel change, the \(r\)-\(\phi\) and \(r\)-\(z\) tagging capabilities become quite similar. This is not the case for changing the stereo angle: to obtain comparable \(r\)-\(\phi\) and \(r\)-\(z\) performance in measuring just the IP's (\(d_{\rm b}\) and \(\cos\lambda\cdot z_{\rm b}\), respectively) without modifying the pixels requires going to 90\({}^{\circ}\) stereo, ie. crossed-strips. In this case, the ghost problem becomes severe. It is worth noting that increasing \(\alpha\) to 15\({}^{\circ}\) slows the PR by a factor of 2.

Values for \(b\)-jet Efficiency at a _prompt_jet Rejection of 100 are also shown in Table 2. In looking at the Efficiencies, it is worth bearing in mind that there is an offset in the Efficiency before the Rejection begins to rise, and so it is best to compare the Efficiencies tabulated with a value around 92%. Interpreting the results is somewhat confused because when the change to the pixels and to the stereo angle are both made, results appear to be worse (in particular, for the \(r\)-\(\phi\) tagging). My guess is that these anomalies are highlighting inadequacies in the PR algorithm.

\begin{table}
\begin{tabular}{|l|c c c c|} \hline \multicolumn{5}{|c|}{Scenarios} \\ \hline \(\sigma_{z}\,(piz)\) (\(\mu\) m) & 87 & **23** & 87 & **23** \\ \(\alpha\) (mrad) & 40 & 40 & **260** & **260** \\ \hline \hline \(\sigma(z_{\rm b})\) (\(\mu\)m) at \(\eta=0\) and for large \(p_{T}\). (For comparison, \(\sigma(d_{\rm b})=17\mu\) m.) & \multicolumn{1}{c|}{} \\ \hline \multicolumn{5}{|c|}{87} & 30 & 64 \\ \hline \hline \(b\)-jet Efficiency (\%) for _prompt_jet-jet Rejection of 100 for \(|\eta|\leq 0.9\) (\(1.9\leq|\eta|\leq 2.5\)). & \multicolumn{1}{c|}{} \\ \hline \(r\)-\(\phi\) & 71 (64) & 72 (63) & 71 (64) & 67 (61) \\ \(r\)-\(z\) & 49 (55) & 67 (64) & 53 (54) & 67 (62) \\ \(3\)-D & 74 (67) & 76 (68) & 72 (66) & 74 (67) \\ \hline \end{tabular}
\end{table}
Table 2: Consequences of different scenarios aimed at improving the \(r\)-\(z\)\(b\)-tagging capabilities.

These studies are important to help identify the benefits of particular detector designs. While my analysis is weakened by concerns over the suitability of the PR algorithm and algorithms starting with the pixels may provide more sensitive tests of the pixel designs, these \(b\)-tagging studies would suggest:

1. There is little to be gained from modifying the stereo angle to values of the order of \(15^{o}\). In deed, the engineering is likely to be significantly more complicated (in the Barrel).
2. The gains which can be derived from improving the \(z\) resolution of the pixels probably do not justify the increased expenditure on more sophisticated electronics.

## 4 Conclusions

This work casts light on a number of important topics associated with the performance of the Atlas ID, in particular those associated with \(b\)-tagging. Nevertheless, it is important to firm up these studies with analyses performed using the full simulation. In particular, studies of tagging in 3-D and the consequences for the parameters investigated in section 3.9 deserve more investigation.11 The use of more sophisticated tagging algorithms is part of on-going work by other collaborators.

Footnote 11: | do not have time to do more in the next few months.

While \(H\to gg\) might provide a more natural background process to \(H\to b\overline{b}\), the Rejection which can be achieved is limited by the physics of the fragmentation of a gluon jet. Therefore, it may not prove to be a sensitive test of the design of the ID and may overlook requirements posed by other physics channels. (On the other hand, we cannot afford to over-design the ID.) _Prompl_-jets or light-quark jets may provide a more useful test - the latter need the identification of V\({}^{\circ}\)'s to be well under control. Also, the _probabilistic_ method to \(b\)-tagging may not provide the best Rejection, but may yield more information about the performance since it highlights the regions of good and poor PR.

## References

* [1]'\(B\)-tagging using the Atlas Inner Detector', S. Haywood, INDET-NO-092.
* [2]'Present status of \(b\)-tagging studies in Atlas', I. Gavrilenko, S. Haywood, A. Clark, D. Froidevaux, L. Rossi, INDET-NO-115.
* [3]'A Precise Measurement of \(\Gamma_{Z\to b\overline{b}}/\Gamma_{Z\to hadrons}\)', Phys. Lett. B313 (1993) p535.
* [4]'Cost Ceiling, Staging and Descoping Studies', The Atlas Collaboration, CERN/LHCC/95-68.
* [5]Suggestion by Sherwood Parker at Berkeley SCT meeting, March 1995.
* [6]'Impact Parameter Resolution in the presence of Multiple-Scattering', S. Haywood, INDET-NO-091.