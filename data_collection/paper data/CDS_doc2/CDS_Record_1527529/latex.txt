[

###### Abstract

This note documents the performance assumptions used for the preparation of the ATLAS inputs to the 2012 European Strategy for Particle Physics. In particular, it lists the parametrization functions describing the object selection efficiencies and resolutions for an upgraded detector observing proton-proton collisions at a High Luminosity LHC.

The ATLAS Collaboration]ATLAS-PHYS-PUB-2013-004

[1.5cm]

[1cm]

March 13, 2013

Revision: April 3, 2014

[1cm]

**Performance assumptions for an upgraded ATLAS detector at a High-Luminosity LHC**

The ATLAS Collaboration

[1cm]

_Typographical errors were found in the formulas describing the energy resolutions for electrons and photons in Section 3 and for taus in Section 6. The analysis results are unchanged._

[1cm]

(c) Copyright 2014 CERN for the benefit of the ATLAS Collaboration.

 Reproduction of this article or parts of it is allowed as specified in the CC-BY-3.0 license.

###### Contents

* 1 Introduction
* 2 Trigger
	* 2.1 Introduction
	* 2.2 Rate Scaling
		* 2.2.1 Multi-object scaling
		* 2.2.2 Missing Energy Scaling
	* 2.3 Rate Estimates and Plausible Thresholds
* 3 Electrons
	* 3.1 Selection efficiencies
		* 3.1.1 Performance assessment
		* 3.1.2 Smearing functions
	* 3.2 Energy resolution
* 4 Photons
	* 4.1 Parametrization of the identification performance
	* 4.2 Fake photons and photon identification
* 5 Muons
* 6 Taus
	* 6.1 Selection of tau decaying leptonically
	* 6.2 Performance parametrization for hadronically decaying tau
* 7 Jets
	* 7.1 Jet energy resolution
	* 7.2 Jet threshold
	* 7.3 Jet reconstruction efficiency
* 8 Missing transverse energy
* 9 b-tagging
	* 9.1 \(b\)-tagging performance parametrization
	* 9.2 Performance in presence of pile-up
* 10 Conclusion

Introduction

The ATLAS Collaboration carried out physics studies as input to the Update of the European Strategy for Particle Physics [1, 2]. These compare the results that can be achieved with the integrated luminosity of about 300 fb\({}^{-1}\) delivered in Phase-I, up to the year 2020, with those from additional running with a High Luminosity Large Hadron Collider (HL-LHC), giving a total integrated luminosity of 3000 fb\({}^{-1}\) after ten additional years running in Phase-II.

In the absence of a full simulation of the upgraded Phase-II detector, these studies rely on a fast simulation, based on parametrizations of the trigger and detector response to objects such as leptons, jets, in particular b-jets, and missing transverse energy, \(E_{\mathrm{T}}^{\mathrm{miss}}\). Functions to describe the resolution, and the reconstruction and trigger efficiencies were defined by extrapolations from the existing data sample, and Monte Carlo simulations that include up to an average number, \(\mu\), of 69 proton-proton interactions in the same bunch crossing (pileup). The nominal luminosity for HL-LHC is \(5\times 10^{34}\) cm\({}^{-2}\) s\({}^{-1}\), corresponding to \(\mu=140\). An upper limit on the possible instantaneous luminosity of \(7\times 10^{34}\) cm\({}^{-2}\) s\({}^{-1}\), corresponding to \(\mu=200\) is also considered. The Phase-II detector is designed to retain the performance of the present detector for many aspects, and this is also taken into account.

The parametrizations described in this note are designed to give a realistic or even slightly pessimistic description of the performance of a future ATLAS detector in order to give an achievable estimate of the physics case of the HL-LHC.

## 2 Trigger

### Introduction

The trigger threshold and efficiency assumptions used for the sensitivity studies in the European Strategy reports are based on the Phase-II baseline design documented in the Phase-II Letter of Intent. The current Level-1 system is replaced by a two stage hardware Level-0/Level-1 (L0/L1) design. After a L0 accept, the data would remain in the detector front-end electronics except for a small subset in the regions of interest defined by the L0 system that would be transferred to the L1 system for further processing. Because full events are not transferred out of the tracking systems at the full L0 rate, this configuration enables a L0 trigger rate much larger than the current L1-only system, which allows for trigger thresholds comparable to the current system despite a significant increase in rate.

The software-based high-level trigger (HLT) is not yet designed. It is assumed that if events can be accepted by the L0/L1 system, an HLT requirement to select the relevant events can be defined by moving a sufficient part of the offline selection online. The reported thresholds are therefore offline thresholds that one would use with the corresponding L0/L1 selection.

In the L0/L1 design, the L0 system is essentially the same as the Phase-I L1 system described in the Phase-I Letter of Intent [3]. The key improvements for this system are:

1. A finer granularity EM calorimeter trigger replacing the current \(0.1\times 0.1\) in \(\Delta\eta\times\Delta\phi\) trigger towers with super-cells sums that are \(0.1\times 0.1\) in \(\Delta\eta\times\Delta\phi\) for the preshower and third layer of the electromagnetic calorimeter, and \(0.025\times 0.1\) in for the first and second layers,
2. A new small wheel of the muon system, and
3. A topological capability.

In the Phase-II L1, the full calorimeter granularity and track trigger information are available.

In the two-level design, the nominal acceptance rates are 500 kHz for L0 and 200 kHz for L1. The L1 rate limit is due to legacy muon chamber monitored drift tube electronics that will not be replaced. The L0 rate could be increased if motivated by a physics requirement.

### Rate Scaling

The rates presented here are extrapolated from current 8 TeV data with scaling factors applied to account for improved rejection of the trigger. The extrapolation accounts for the difference in energy, luminosity, and pile-up.

For calorimeter L0, a scaling factor of 3 is used to account for the Phase-I improvements in the EM identification due to the finer granularity. A scaling factor of 3 is used to account for the new muon small wheel which will be installed for Phase-I. No improvement is assumed for the L0 tau lepton identification; however this is probably pessimistic. For electrons and muons an estimated factor of 5 background suppression for requiring a hard track is used at L1. The fact that scaling factors are the same for electrons and muons is coincidental. For the photon rate, the suppression factor due to using the full granularity in the Phase-II L1Calo system is assumed to give a suppression factor of 3, half the suppression achieved by the current L2.

For the energy scaling, an increase by a factor of 2.3 is used to scale between 8 TeV and 14 TeV based on the relative parton luminosities. This value is motivated by the scaling of gluon-gluon collisions at approximately 30-40 GeV which are assumed to dominate the rates at L0.

#### 2.2.1 Multi-object scaling

With the exception of \(E_{\mathrm{T}}^{\mathrm{miss}}\), the only pile-up dependence that is included is the effect of concidences of independent collisions on the rate of multi-object triggers. For a simple, single-feature trigger, the rate scales linearly with \(\mu\) and the instantenous luminosity. For a multi-object trigger with \(N_{\mathrm{obj}}\) features the rate scales approximately with \(\mu^{N_{\mathrm{obj}}}\).

For the current di-object triggers, the coincidences turn out to be a small effect. E.g. for L1_EM10VH (an electromagnetic calorimeter isolated energy deposit corresponding to \(p_{\mathrm{T}}>10\) GeV), the \(10^{34}\,cm^{-2}s^{-1}\) 8 TeV rate is 190 kHz giving a coincidence rate of approximately 1 kHz compared to the \(\approx 8\) kHz rate of L1_2EM10VH (two electromagnetic calorimeter isolated energy deposits corresponding to \(p_{\mathrm{T}}>10\) GeV) due to a single hard interaction. For the HL-LHC configuration including the hardware improvements, this becomes 29 kHz from coincidences and 14 kHz from single hard interactions. Low thresholds move from being dominated by single hard interactions to being dominated by coincidences.

#### 2.2.2 Missing Energy Scaling

The \(E_{\mathrm{T}}^{\mathrm{miss}}\) rates and thresholds are modeled using an analytic model of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) resolution in each coordinate (\(x\)_y_) as scaling with the square-root of \(\mu\), so that the rate is proportional to \(e^{-E_{\mathrm{thresh}}^{2}/2\mu\sigma_{1}^{2}}\). Using a resolution of \(\sigma_{1}=2.5\) GeV based on an extrapolation of the current L2 FE \(E_{\mathrm{T}}^{\mathrm{miss}}\) performance as a proxy for the performance of the upgrade digital readout based L0 MET, we find a rate of 4 kHz at L0 for a 150 GeV missing energy trigger, with little expected gain from an L1 system.

### Rate Estimates and Plausible Thresholds

The resulting rate estimates for plausible thresholds for single and multi-object triggers are shown in Tables 1 and 2 for the present L1 trigger extrapolated to a luminosity of \(1\times 10^{34}\)\(cm^{-2}s^{-1}\) at 8 TeV, and for the Phase-II L0 and L1 triggers at high luminosity and 14 TeV. The corresponding offline thresholds that would be applied and the efficiencies of the online selection are given in Table 3. It is assumed that adding a track requirement at L1, together with the upgraded L1Calo, will still allow high efficiencies to be preserved.

At the luminosity considered for HL-LHC, the current di-\(\tau\) trigger 2TAU111_TAU15 (one isolated \(\tau\) with \(p_{\mathrm{T}}\) above 11 GeV and one \(\tau\) above 15 GeV) is not plausible. Otherwise, most of the current triggerchains presently used by ATLAS at the LHC can be conserved at HL-LHC luminosities by raising their thresholds by only a couple of GeV.

\begin{table}
\begin{tabular}{l l|r r r} \hline  & & 8 TeV \(10^{34}\)\(cm^{-2}s^{-1}\) & 14 TeV \(7.10^{34}\)\(cm^{-2}s^{-1}\) \\ Object & L0/L1 threshold & L1 rate & L0 rate & L1 rate \\ \hline Muons & MU20 + ID track & 7 kHz & 42 kHz & 8 kHz \\ Electrons & EM18VH + ID track & 23 kHz & 130 kHz & 25 kHz \\ Tau & TAU40 + ID track & 7 kHz & 110 kHz & 14 kHz \\ Photon & EM30 + L1Calo & 9 kHz & 48 kHz & 16 kHz \\ Missing Energy & XE150 & - & \(\approx 5\) kHz & \(\approx 5\) kHz \\ \hline \end{tabular}
\end{table}
Table 1: Single object rates for the current L1 trigger and 8 TeV operation extrapolated to \(1\times 10^{34}\)\(cm^{-2}s^{-1}\) luminosity and to 14 TeV and \(7\times 10^{34}\)\(cm^{-2}s^{-1}\) luminosity with L0 hardware improvements, an L1 track trigger and for photons only a high granularity L1 calorimeter trigger. The number appearing in the trigger names indicates the \(p_{\mathrm{T}}\) threshold in GeV. For electrons, the additional VH indicates a veto on hadronic energy behind the electromagnetic cluster.

\begin{table}
\begin{tabular}{|l|l|r r|r r|r r|} \hline  & L0/L1 & \multicolumn{2}{c|}{L1 rate 7 TeV} & \multicolumn{2}{c|}{L0 rate 14 TeV} & \multicolumn{2}{c|}{L1 rate 14 TeV} \\  & & \multicolumn{2}{c|}{\(10^{34}\)\(cm^{-2}s^{-1}\)} & \multicolumn{2}{c|}{\(7.10^{34}\)\(cm^{-2}s^{-1}\)} & \multicolumn{2}{c|}{\(7.10^{34}\)\(cm^{-2}s^{-1}\)} \\ Object & threshold (GeV) & total rate & coincidence & non-coinc. & coincidence & non-coinc. \\ \hline \(\mu\mu\) & \(p_{\rm T}>10\) & 960 Hz & 77 Hz & 1700 Hz & 2000 Hz & 67 Hz \\ \(ee\) & \(p_{\rm T}>10\) & 7800 Hz & 1100 Hz & 14000 Hz & 29000 Hz & 540 Hz \\ \(e\mu\) & \(p_{\rm T}^{e}>10\), \(p_{\rm T}^{\mu}>6\) & 2600 Hz & 1100 Hz & 4500 Hz & 29000 Hz & 180 Hz \\ \(\gamma\gamma\) & \(p_{\rm T}>10\) & 7800 Hz & 1100 Hz & 14000 Hz & 29000 Hz & 5000 Hz \\ \hline \(e\tau\) & \(p_{\rm T}^{\tau}>11\), \(p_{\rm T}^{e}>14\) & 8 kHz & \(<1\) kHz & 43 kHz & 45 kHz & 1 kHz \\ \(\mu\tau\) & \(p_{\rm T}^{\tau}>11\), \(p_{\rm T}^{\mu}>10\) & 2 kHz & \(<1\) kHz & 11 kHz & 39 kHz & \(<\)1 kHz \\ \(\tau\tau\) & \(p_{\rm T}>15\) & 3 kHz & \(<1\) kHz & 53 kHz & 95 kHz & 1 kHz \\ \(\tau\tau\) & \(p_{\rm T}>20\) & 7 kHz & \(<1\) kHz & 130 kHz & 25 kHz & 2 kHz \\ \(\tau\tau\) & \(p_{\rm T}^{1}>11\), \(p_{\rm T}^{2}>15\) & 17 kHz & 1 kHz & 280 kHz & 290 kHz & 4 kHz \\ Jets\(\dagger\) & \(4\times p_{\rm T}>20\) & 400 Hz & - & 6 kHz & \({\cal O}(100)\) kHz & 6 kHz \\ \hline \multicolumn{7}{l}{\(\dagger\) The 4-jets coincidence rate has been estimated by assuming that all collisions with a jet trigger object above \(p_{\rm T}=20\) GeV are actually dijets events producing two jets objects above \(p_{\rm T}=20\) GeV at the trigger level.} \\ \multicolumn{7}{l}{This certainly over-estimates the number of coincidences, but sets a rough scale.} \\ \end{tabular}
\end{table}
Table 2: Multi-object rates for the current L1 trigger and 8 TeV operation extrapolated to \(1\times 10^{34}\)\(cm^{-2}s^{-1}\) luminosity and to 14 TeV and \(7\times 10^{34}\)\(cm^{-2}s^{-1}\) luminosity with L0 hardware improvements, an L1 track trigger and for photons only a high granularity L1 calorimeter trigger. The rates are divided into coincidence (from different proton-proton interactions in the same bunch crossing) and non-coincidence. For the 14 TeV L1 rates, it is a assumed that vertex reconstruction removes all the coincidences except for photons.

\begin{table}
\begin{tabular}{|l|l|l|l|l|} \hline Object & L1 threshold & Offline & Eta Cov. & Efficiency \(\approx\epsilon_{\rm cur}\times\epsilon_{\rm tk}\) \\ \hline Single object & & & & \\ Electrons & EM18VH + ID track & 25 GeV & \(|\eta|<2.5\) & \(93\%\times 95\%=88\%\) \\ Muons & MU20 + ID track & 25 GeV & \(|\eta|<2.4\) & \(67\%\times 95\%=64\%\)\(|\eta|<1.0\) \\  & & & & \(91\%\times 95\%=86\%\)\(|\eta|>1.0\) \\ Tau & TAU40 + track-iso & 60 GeV & \(|\eta|<2.5\) & 80\% \\ Photon & L0\_EM30 + L1calo & 60 GeV & \(|\eta|<2.5\) & \(\approx 100\%\) \\ MET & XE150 & 180 GeV & - & \(\approx 100\%\) \\ \hline Multi-object & & & & \\ \(ee\),\(e\mu\),\(\mu\mu\) & - & 15 GeV & \(\eta\) from & product of \(\epsilon_{e}\) and \(\epsilon_{\mu}\) \\  & & & above & \(e:93\%\times 95\%=88\%\) \\  & & & & \(\mu,|\eta|<1.0:77\%\times 95\%=73\%\) \\  & & & & \(\mu,|\eta|>1.0:95\%\times 95\%=90\%\) \\ \(e\tau\), \(\mu\tau\) & & \(e/\mu:\)15, \(\tau:\)25 GeV & \(\eta\) as above & product of single obj \(\epsilon_{e}\), \(\epsilon_{\mu}\), and \(\epsilon_{\tau}\) \\ \(\tau\tau\) & 2TAU15I + ID tracks & 40 GeV & \(|\eta|<2.5\) & 64\% \\ Diphotons & 2EM10VH & 15 GeV & \(|\eta|<2.5\) & \(\approx 100\%\) \\
4 70 GeV Jets & & 70 GeV & \(|\eta|<3.2\) & \(\approx 100\%\) \\ \hline \end{tabular}
\end{table}
Table 3: Offline trigger thresholds with estimated efficiencies and \(\eta\) coverage. The effective efficiencies are the product of the efficiency of the present system and a predicted efficiency for the track requirement.

Electrons

### Selection efficiencies

The increase of instantaneous luminosity directly affects the electron selection performance via:

* higher pile-up rates forcing the trigger thresholds to be raised
* difficulties in distinguishing jets faking electrons from real leptons in the busy detector environment.

So far the ATLAS Collaboration has based its electron selection strategy on a cut-based approach, while multivariate techniques for particle identification (MVA PID) are being commissioned which should give improved performance. The assumption made here is that the current 2012 performance will be maintained, i.e. that the rejection/efficiency power decrease because of pile-up can be compensated by a better use of the detector inputs and, in particular, the change from a cut-based approach to MVA PID. The current 2012 performance is therefore parametrised, to be used as a smearing function applied to generator-level Monte Carlo samples.

#### 3.1.1 Performance assessment

The performance of the electron reconstruction and identification cuts used during the 2012 data-taking [4] was assessed using fully-reconstructed Monte Carlo samples including a realistic level of pile-up for the 2012 sample. Two sets of identification cuts, "loose" and "tight", were investigated.

A \(\mathrm{PowHeg}\)\(Z\to ee\) sample is used to compute the probability of finding a reconstructed electron passing the full sets of cuts matching (using a \(|\Delta R|<0.1\) criterion) each truth electron. These probabilities are binned in pseudorapidity \(\eta\) and transverse momentum \(p_{\mathrm{T}}\).

Similarly, a \(\mathrm{Pythia8}\) di-jet sample is used to compute equivalent probabilities for truth jets above \(p_{\mathrm{T}}^{\mathrm{truthjet}}=5\) GeV. In this case, only truth jets far from truth high-\(p_{\mathrm{T}}\) electrons are considered.

As for the probabilities (efficiencies) for truth electrons, the efficiencies for truth jets are parametrized using their truth energy. However, the energy of the corresponding reconstructed electron candidate (fake) is significantly different from the truth jet energy; for this reason, for both loose and tight selections, in addition to computing the efficiencies, parametrizations are provided of the relative difference \((E_{\mathrm{fake}}-E_{\mathrm{truthjet}})/E_{\mathrm{truthjet}}\) as a function of \(p_{\mathrm{T}}^{\mathrm{truthjet}}\).

#### 3.1.2 Smearing functions

Functional parametrizations reflecting the "truth electron \(\to\) identified electron probability" and the "truth jet \(\to\) fake electron probability" as a function of the initial truth electron/jet \(p_{\mathrm{T}}\) are derived from the previous procedure. These are the following:

* Truth electron Loose efficiency : \(\epsilon(p_{\mathrm{T}})=0.97-0.103\times\exp\left(1-\frac{p_{\mathrm{T}}}{15}\right)\),
* Truth electron Tight efficiency : \(\epsilon(p_{\mathrm{T}})=0.85-0.191\times\exp\left(1-\frac{p_{\mathrm{T}}}{20}\right)\),
* Truth jet Loose efficiency : \(\epsilon(p_{\mathrm{T}})=0.11\times\exp\left(-0.033\times p_{\mathrm{T}}\right)\),
* Truth jet Tight efficiency : \(\epsilon(p_{\mathrm{T}})=0.0048\times\exp\left(-0.035\times p_{\mathrm{T}}\right)\),where all \(p_{\rm T}\) are expressed in GeV.

As mentioned above, calibrated as an electron, the reconstructed energy/\(p_{\rm T}\) of a jet can be significantly different from the truth jet energy. Since the latter is the only quantity accessible when running over "generator level" events, it is important to take this effect into account. The \((E_{\rm fake}-E_{\rm truthjet})/E_{\rm truthjet}\) relative difference for truth jets passing the loose or tight requirements is parametrized as a function of \(p_{\rm T}\) and is roughly \((E_{\rm fake}-E_{\rm truthjet})/E_{\rm truthjet}=-60\%\) for values of the transverse momentum in the range from 10 to 200 GeV.

The difference between the two sets of cuts is small (most of the effect comes from "invisible" energy and mis-calibration) but two different sets of energy/\(p_{\rm T}\) rescaling weights have been taken into account using Gaussian functions having for each bin of \(p_{\rm T}^{\rm truthjet}\) the average relative difference as central value and its RMS as width.

### Energy resolution

When using "generator level" electrons, as in most of the European Strategy studies, it is important to account for detector effects by smearing their energies and momenta by the expected resolution. The functions, which are based on current detector performance, and not expected to be affected significantly by large pileup conditions, are the following:

* \(\sigma({\rm GeV})=0.3\oplus 0.10\times\sqrt{E({\rm GeV})}\oplus 0.010\times E ({\rm GeV})\) for \(|\eta|<1.4\),
* \(\sigma({\rm GeV})=0.3\oplus 0.15\times\sqrt{E({\rm GeV})}\oplus 0.015\times E ({\rm GeV})\) for \(1.4<|\eta|<2.47\).

## 4 Photons

### Parametrization of the identification performance

The parametrization of the photon identification performance was made under the same assumptions as for the electron parametrization: performance of photon identification at higher luminosities with an upgraded ATLAS detector will be roughly the same as photon identification performance on lower instantaneous luminosities with the current ATLAS detector.

The parametrization of the photon identification efficiency for truth photons was obtained using fully simulated MC samples of di-photon decay of a Standard Model Higgs with 120 GeV mass, produced at 14 TeV with an average pileup of \(\mu=\)46. Similarly to the electron parametrization, the photon identification efficiency with respect to the \(p_{\rm T}\) of the truth photon (\(\sigma(p_{\rm T})\)) was parametrized with the function in equation 1.

\[\epsilon(p_{\rm T})=0.83-0.26\times\exp\left(1-\frac{p_{\rm T}}{24.8{\rm GeV}}\right) \tag{1}\]

The parametrized identification efficiency is in very good agreement (\(\lesssim 5\%\)) with the reconstructed identification efficiency for truth photons with \(p_{\rm T}\)>40 GeV while it slightly overestimates (\(\lesssim 10\%\)) the efficiency for truth photons with \(p_{\rm T}<40\) GeV.

It has also been checked that the di-photon invariant mass with smeared truth photon variables and with reconstructed photon variables are in good agreement.

### Fake photons and photon identification

The efficiency of the identification and isolation requirements on fake photons is used, for example, to estimate the rejection of single and di-jet background to the Higgs di-photon decay in the HL-LHC studies.

In this analysis, a fake photon is defined as a truth particle which is neither a truth photon nor a truth electron, matching a reconstructed photon candidate. The fake photon is required to be located in the detector acceptance region \(|\eta|<2.35\) and outside the transition regions 1.37\(<|\eta|<\)1.52. It is considered a fake candidate if it passes the tight photon identification and an additional isolation cut on the sum of transverse energy in a cone of radius \(\Delta R=4\), which must be less than 4 GeV after subtracting pile-up induced energy.

Samples of fully simulated QCD di-jet events with \(\sqrt{s}\) of 14 TeV at \(\mu=8,46\) and 69 were used as sources of fake photons. The efficiency of photon identification and isolation requirement on fake photons was found to be 0.1%.

In addition, the ratio of the reconstructed and truth \(p_{\mathrm{T}}\) of the fake photon was studied using truth jets in MC di-jet samples at the same values as the efficiency. For low \(p_{\mathrm{T}}\) fakes, this ratio was found to be the same whether using all reconstructed photon fakes or only those passing the tight identification. Because of lack of statistics in the background samples, this ratio is computed for all reconstructed photons matching a fake with fake reconstructed \(p_{\mathrm{T}}>30\) GeV. The distributions of those ratios are roughly Gaussian. The energy of fake photons from events with higher pile-up is a lower fraction of the total truth energy of the fake than those on lower pile-up events.

Table 4 shows the Gaussian fitted mean and width values used to parametrize the reconstructed photon \(p_{\mathrm{T}}\) from a truth jet.

As a sanity check, the parametrization of truth photons and fake photons was implemented on di-photon, photon-jet and jet-jet samples generated at \(\sqrt{s}\)=14 TeV at high luminosity and combined them with their corresponding production cross sections to produce the invariant mass distribution between 100 GeV and 160 GeV shown in figure 1 for an integrated luminosity of 3000 fb\({}^{-1}\). The histograms are stacked and show similar fractions of di-photon, photon-jet and jet-jet in the background to those measured in 2012 data with data-driven methods at 8 TeV [5].

\begin{table}
\begin{tabular}{|c|c|c|} \hline \(\mu\) & mean & \(\sigma\) \\ \hline
8 \(<\mu<\)46 & 0.78 & 0.143 \\
46 \(<\mu<\)69 & 0.6072 & 0.1709 \\ \(\mu>69\) & 0.552 & 0.173 \\ \hline \end{tabular}
\end{table}
Table 4: Mean and width of a Gaussian function fitted to the ratio of reconstructed fake photon \(p_{\mathrm{T}}\) and truth \(p_{\mathrm{T}}\) of the fake.

Figure 1: The distribution of irreducible di-photon and reducible gamma-jet and jet-jet backgrounds from MC truth generated events, after smearing photons and fakes with functions parameters described in the text.

Muons

Muons with absolute pseudorapidity below 2.5 and with \(p_{\mathrm{T}}>7\) GeV have been considered. The associated reconstruction efficiency is 97%.

Separate momentum resolution functions are derived for the Inner Detector (ID) and Muon Spectrometer (MS) resolution functions, based on the current ID performance which is not expected to be affected by pileup. These are then merged into a "Combined" resolution function.

\[\sigma_{\mathrm{ID}} = p_{\mathrm{T}}\times\sqrt{a1^{2}+(a2\times p_{\mathrm{T}})^{2}},\] \[\sigma_{\mathrm{MS}} = p_{\mathrm{T}}\times\sqrt{\left(\frac{b0}{p_{\mathrm{T}}} \right)^{2}+b1^{2}+(b2\times p_{\mathrm{T}})^{2}},\] \[\sigma_{\mathrm{CB}} = \frac{\sigma_{\mathrm{ID}}\times\sigma_{\mathrm{MS}}}{\sqrt{ \sigma_{\mathrm{ID}}^{2}+\sigma_{\mathrm{MS}}^{2}}}\]

where \(p_{\mathrm{T}}\) is the truth muon transverse momentum (in GeV) and the different factors in these formulas are listed in Table 5.

## 6 Taus

A tau lepton can decay either hadronically (branching fraction \(\sim 65\%\)) or leptonically (branching fraction \(\sim 35\%\)). The following section describes the parametrization of the tau selection performance for these two different cases.

### Selection of tau decaying leptonically

Experimentally, when a tau decays leptonically, i.e. to a charged lepton and neutrinos, it is identified by the detection of a charged lepton + \(E_{\mathrm{T}}^{\mathrm{miss}}\). For leptonic tau decays, a single lepton trigger and a \(E_{\mathrm{T}}^{\mathrm{miss}}\) cut are used.

The performance of the lepton selection in tau decays is based on the parametrization described in the previous sections. The \(E_{\mathrm{T}}^{\mathrm{miss}}\) performance is discussed in section 8.

### Performance parametrization for hadronically decaying tau

The parametrization of the hadronic tau decay identification performance is made under the assumption that the performance as observed in Monte Carlo samples tuned to reflect the 2012 data can be maintained up to higher pileup values, once new jet calibration weights and appropriate pileup noise term values are determined for higher luminosity scenarios. Indeed, from the 2011 run to the 2012 run, pileup increased by a factor 4, and the performance was demonstrated to remain stable once these two issues were taken into account and a slight optimization of the inputs used for tau identification was made. Similarly the resolution has remained rather unchanged, once appropriate pileup subtraction and noise levels at cluster level are used.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|} \hline  & a1 & a2 & b0 & b1 & b2 \\ \hline \(|\eta|<1.05\) & 0.01607 & 0.000307 & 0.24 & 0.02676 & 0.00012 \\ \hline \(|\eta|>1.05\) & 0.03000 & 0.000387 & 0.00 & 0.03880 & 0.00016 \\ \hline \end{tabular}
\end{table}
Table 5: Parameters used in the muon momentum smearing functions.

The parametrization of the tau identification efficiency is obtained using fully simulated \(Z\to\tau\tau\) decays with 2012 data-taking conditions, combined with higher \(p_{\rm T}\) physics samples like Higgs and \(Z^{\prime}\) signal samples.

A "medium level" tau selection was applied to candidates with \(p_{\rm T}>20\) GeV and \(|\eta|<2.3\). The efficiency is 55% for a 1 track tau candidate matched to a true 1 prong tau, and 50% for 2 or 3 track tau candidates matched to true 3 prong taus. To obtain the efficiency for 3 track candidates only, one should multiply 50% by 70%, giving 35% for 3 track candidates matched to 3 prong true taus.

These efficiencies include the identification efficiency, once a real hadronic tau reconstructed with 1 or 2 or 3 tracks, is identified, multiplied by the reconstruction efficiency for a real 1 or 3 prong tau to be reconstructed. The fraction of QCD jets accepted with one or three tracks and passing the medium level tau identification cuts is 5% and 2% respectively.

Figures 2(a) and 2(b) show the tau identification efficiency for true taus in fully simulated MC events. The parametrised estimates are assumed to be slightly lower than shown in the figures, to give a more conservative extrapolation to high \(\mu\).

The function used to smear the tau energy is the following:

* \(\sigma/E({\rm GeV})=0.03\oplus\left(0.62/\sqrt{E({\rm GeV})}\right)\) for 1 prong
* \(\sigma/E({\rm GeV})=0.03\oplus\left(0.76/\sqrt{E({\rm GeV})}\right)\) for 3 prong

Note that distinguishing the hadronically decaying taus with only truth information was challenging, due to the lack of charged hadron information regarding the number of tracks in the tau decay. In addition, to emulate different fractions of 1/3 track taus, a Monte Carlo method was used to produce distribution with 77% 1 track and 23% 3 tracks.

## 7 Jets

The very high pile-up environment expected at the HL-LHC introduces several challenges to jet reconstruction and calibration, and affects the jet performance in several ways. Additional pile-up interactions add an energy offset (of the order of 1 GeV per additional vertex for jets reconstructed with the anti-\(k_{t}\) algorithm with distance parameter \(R=0.6\)) and make the jet performance dependent on the pile-up varying conditions. A more severe effect arises from the event-by-event fluctuations of the pile-up contributions to jets. Such fluctuations degradate the jet energy resolution, specifically the "noise" term of the absolute jet energy resolution which is \(p_{\rm T}\)-independent as it is not related to the hard jet fragmentation but to the effect of pile-up contributions to jets. Pile-up fluctuations also lead to the presence of pile-up (fake) jets,

Figure 2: Probabilities for a true tau (a) 1 prong (b) multi prong (2 or 3 tracks reconstructed) to pass the Loose/medium/tight identification.

which consists of random combinations of soft particles from multiple vertices that fluctuate above the minimum jet \(p_{\rm T}\) threshold as well as hard jets originating from additional pile-up interactions.

High luminosity conditions may also severely affect the inputs to jet finding (topological clusters [6]) and the cluster level hadronic calibration procedure used in the Run 1 ATLAS jet reconstruction. The optimization of input objects for jet finding is outside the scope of this document and it is assumed that topological clustering (after re-optimization) will continue to be reliable up to \(\mu=140\).

The strategy used to estimate the jet performance at high luminosity consists of extrapolating the jet performance obtained in Pythia dijet Monte Carlo event samples [7] generated with varying pile-up conditions. Dijet events were generated with an average number of additional minimum bias interactions (\(\mu\)) of 0, 10, 20, 30 and 40. In all cases, dedicated pile-up offset and jet energy scale corrections [6] were derived and applied to each dataset to ensure jets were properly calibrated enabling meaningful comparisons and extrapolations of performance.

We estimated three figures of merit for jet performance at \(\mu=\)140: the jet energy resolution, the minimum jet \(p_{\rm T}\) threshold, and the jet reconstruction efficiency. The later two measures depend on the fake rate of pile-up jets, and on the efficiency of a pile-up jet suppression algorithm based on tracking and vertexing information.

In all cases, jets were reconstructed using the anti-\(k_{t}\) algorithm [8] with distance parameter \(R\)=0.4.

### Jet energy resolution

The fractional jet energy resolution is described by three parameters: noise (N), stochastic (S), and constant (C) terms, as shown in equation 2.

\[\frac{\sigma_{p_{T}}}{p_{T}}=\sqrt{\frac{N^{2}}{p_{T}^{2}}+\frac{S^{2}}{p_{T}} +C^{2}} \tag{2}\]

Pile-up affects the noise term of the jet energy resolution through the effect of event-by-event fluctuations within jets. Such term has a \(1/p_{\rm T}\) dependence in the fractional jet energy resolution (or a constant term in the absolute energy resolution \(\sigma(p_{\rm T})\)) meaning that it is more important at low \(p_{\rm T}\). The stochastic and constant terms of the jet energy resolution are expected to be independent of the pile-up noise and driven by jet fragmentation and detector and physics effects.

The fractional jet energy resolution was measured for each of the \(\mu=0-40\) datasets, keeping the constant term fixed to the corresponding constant term for the \(\mu=0\) (no pile-up) sample. This is important because N, S, and C in equation 2 are correlated, and the constant term does not depend on pile-up contributions. The parameter S was found to be independent of \(\mu\), as expected. The dependence of N on \(\mu\), shown in Figure 3, was parametrized in four different jet pseudo-rapidity (\(\eta\)) bins with a linear function of the form:

\[N=a(\eta)+b(\eta)\mu \tag{3}\]

The four \(\eta\) bins were chosen to contain jets in different calorimeter regions (barrel (\(|\eta|<0.8\)), extended barrel (\(0.8<|\eta|<1.2\)), endcap (\(1.2<|\eta|<3.2\) ), forward (\(3.2<|\eta|<4.9\))). Table 6(a) shows the values of the parameters \(a\) and \(b\) in each \(\eta\) region.

The above estimates correspond to the use of an average offset pile-up subtraction. A more sophisticated, event-by-event, pile-up subtraction based on the jet areas method [9] allows a significantly improvement on the jet energy resolution, as shown in Table 6(b). This latter is the one considered for the studies for the European Strategy.

### Jet \(p_{\rm T}\) threshold

The minimum jet \(p_{\rm T}\) threshold was estimated based upon the rate of pile-up jets (after offset and jet energy scale calibration). Pile-up jets are defined as reconstructed jets unmatched to truth-level particle jets. The average number of fake pile-up jets above a given jet \(p_{\rm T}\) (and normalized by the \(\eta\) acceptance in each of the four \(\eta\) regions as \(N=N_{\eta\rm-range}.(4.5/\eta\) - range)) was determined in samples with \(\mu\)=20 and \(\mu\)=40. Figure 4 shows these values and the power law fit (\(\sim\mu^{2}\)) which was used to extrapolate to \(\langle\mu\rangle\)=140. The jet \(p_{\rm T}\) threshold was then determined such that there is less than 1 additional jet (on average) in 10% or 1% of the events (target of most of the studies, in particular the Higgs ones, for the European Strategy inputs).

Table 7(a) summarizes the jet thresholds corresponding to a 10% and 1% fake rate. For jets fully contained within the tracker acceptance (\(|\eta|<2.1\)), the same estimates using a track-jet confirmation pile-up suppression algorithm are indicated in parentheses.

### Jet reconstruction efficiency

The efficiency of the track-confirmation algorithm has been estimated by extrapolating its performance in samples with \(\mu\)=20-40 and in different \(p_{\rm T}\) bins. Table 7(b) shows the expected jet reconstruction efficiency at \(\mu\)=140.

\begin{table}

\end{table}
Table 6: Noise (parametrized by a and b), stochastic and constant terms of the jet energy resolution using (a) an average pile-up subtraction correction, (b) an event-by-event pile-up correction based on the jet areas.

Figure 3: Noise term obtained from the fits to the resolution as a function of \(\mu\) shown for \(|\eta|<0.8\) in the EM+JES anti-\(k_{t}\). A linear fit was performed to allow to extrapolate to \(\mu=140\).

## 8 Missing transverse energy

The two components of the true \(E_{\mathrm{T}}^{\mathrm{miss}}\), calculated from the sum of true stable interacting particles in the detector acceptance, were smeared by the resolution.

The parametrization of the resolution of the two \(E_{\mathrm{T}}^{\mathrm{miss}}\) components is based on the formula:

\[\sigma(E_{\mathrm{x}}^{\mathrm{miss}},E_{\mathrm{y}}^{\mathrm{miss}})=a\cdot \sqrt{\sum E_{\mathrm{T}}}.\]

The \(E_{\mathrm{T}}^{\mathrm{miss}}\) is calculated as the sum of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) of the hard scattering event and \(\mu\cdot 20\) GeV, where \(\mu\) is the average number of pileup interactions considered, and 20 GeV is the \(\sum E_{\mathrm{T}}\) associated to a generic minimum bias interaction (estimated using data and MC events).

The coefficient "a" has been studied in Ref. [10] and its dependence from the pile-up has been found to be:

\[a=0.40+0.09\cdot\sqrt{\mu}.\]

In the European Strategy studies, the true \(E_{\mathrm{T}}^{\mathrm{miss}}\) has thus been smeared by a Monte Carlo method selecting from Gaussian distributions centred on zero and of width:

\[\sigma(E_{\mathrm{x,y}}^{\mathrm{miss}})[\mathrm{GeV}]=(0.40+0.09\times\sqrt{ \mu})\times\sqrt{\sum E_{\mathrm{T}}[\mathrm{GeV}]+\mu\times 20}\]

\begin{table}

\end{table}
Table 7: (a) Jet \(p_{\mathrm{T}}\) threshold for a 10% and 1% fake rate (b) Expected jet reconstruction efficiency of the track-confirmation algorithm at \(\mu=140\).

Figure 4: Mean pile-up jet multiplicity extrapolation as a function of \(\mu\) for each \(p_{\mathrm{T}}\) threshold. The fit is a power law proportional to \(\mu^{2}\).

where \(\mu\) is the considered average number of interactions per bunch-crossing.

## 9 b-tagging

### \(b\)-tagging performance parametrization

The \(b\)-tagging performance of the multivariate tagger MV1[11] has been parametrized as a function of \(\eta\) and \(p_{\rm T}\) for each jet flavour. The parametrization has been computed using a simulated sample of \(tt\) events having at least one of the two W bosons decaying leptonically. The working point chosen for MV1 corresponds to an average \(b\)-jet efficiency of 70%. Figure 5 shows the behavior of the \(b\)-jet, \(c\)-jet and light-flavour jet efficiencies in the \(p_{\rm T}\)-\(\eta\) plane.

### Performance in presence of pile-up

Several effects can contribute to the degradation of \(b\)-tagging performance in the presence of pile-up, e.g. primary vertex misidentification, contamination of tracks from pile-up and an increase of fake tracks due to the dense environment. The overall degradation of the \(b\)-tagging performance has been studied for the tagger IP3D+SV1 at a working point corresponding to 70% efficiency. It is assumed that the degradation observed for the IP3D+SV1 tagger also applies to the MV1 tagger; the Phase-II tracking detector should ameliorate this deterioration, but for the present physics studies this has not been taken into account. For this study truth jets have been used to avoid biases from pile-up jets which, having a low probability of being \(b\)-tagged, could artificially increase the light jet rejection. Figure 6 (left) shows the behavior of the light jet rejection as a function of the \(b\)-tagging efficiency for three different simulated \(tt\) samples generated with 0, 20, 40 average interactions (\(\mu\)) per bunch crossing respectively.

The extrapolation to higher \(\mu\) value has been deduced by parametrizing the light jet rejection degradation with the function \(Ae^{-B\mu}+C\) (Fig. 6 (right)), using the values of the light jet rejection for a b-tagging efficiency of 70% at \(\mu\)=0, \(\mu\)=20 and \(\mu\)=40, normalized to the light jet rejection at \(\mu\)=0. The degradation factor between \(\mu\)=6, corresponding to the average in 2011, and \(\mu\)=140 turned out to be \(1.9\pm 0.4\). The uncertainty on this factor has been evaluated by redoing the fit with all the points shifted up and down by one sigma in a correlated way. The \(b\)-tagging performance at \(\mu\)=140 is obtained by scaling the \(p_{\rm T}-\eta\) parameterizations of the \(b\)-tagging efficiency for \(b\)-, \(c\)- and light-flavour jets with the scale factor \(1.9\pm 0.4\); an optimistic and pessimistic scenario was defined by varying the 1.9 up and down by the 0.4 uncertainty.

Figure 5: \(b\)-jet, \(c\)-jet and light jet \(b\)-tagging efficiencies as function of \(p_{\rm T}\) and \(|\eta|\) in a sample of \(tt\) events.

## 10 Conclusion

The parameterizations of the performance of a future ATLAS detector at the HL-LHC are based on experience of the present detector, including Monte Carlo simulations with pile-up of up to an average 69 events per proton-proton bunch crossing. It is assumed that the proposed upgrades to the detector and trigger will allow single-object triggers to be used with similar thresholds to those used in the 2012 data taking. The degradation in the performance for objects such as jets, \(b\)-tagged jets and \(E_{\mathrm{T}}^{\mathrm{miss}}\) with increasing \(\mu\) has been estimated, without taking into account that this may be compensated by detector upgrades and/or more sophisticated algorithms using more information. Other objects such as electrons and muons, are assumed to have a similar identification efficiency and resolution to the present performance. These parametrizations represent realistic to pessimistic assumptions, and as such form a firm basis for the initial studies of the physics case for HL-LHC, as input by ATLAS to the update of the European Strategy for Particle Physics [1, 2].

Figure 6: Left: light jet rejection as a function of the \(b\)-jet efficiency of IP3D+SV1 for three different sample with \(\mu\)=0, \(\mu\)=20, \(\mu\)=40. Right: light jet rejection as a function of \(\mu\), normalized to the value at \(\mu\)=0, of IP3D+SV1 at a \(b\)-jet efficiency of 70%.

## References

* [1] ATLAS Collaboration, _Physics at a High-Luminosity LHC with ATLAS_, Tech. Rep. ATL-PHYS-PUB-2012-001, CERN, Geneva, Aug, 2012.
* [2] ATLAS Collaboration, _Physics at a High-Luminosity LHC with ATLAS (Update)_, Tech. Rep. ATL-PHYS-PUB-2012-004, CERN, Geneva, Oct, 2012.
* [3] ATLAS Collaboration, _Letter of Intent for the Phase-I Upgrade of the ATLAS Experiment_, Tech. Rep. CERN-LHCC-2011-012. LHCC-1-020, CERN, Geneva, Nov, 2011.
* [4] ATLAS Collaboration, _Electron efficiency measurements in early 2012 data_, Tech. Rep. ATL-COM-PHYS-2012-783, CERN, Geneva, Jun, 2012.
* [5] ATLAS Collaboration, _Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC_, Phys.Lett. **B716** (2012) 1-29, arXiv:1207.7214 [hep-ex].
* [6] ATLAS Collaboration, _Jet energy measurement with the ATLAS detector in proton-proton collisions at \(\sqrt{s}\) = 7 TeV_, arXiv:1112.6426 [hep-ex].
* [7] T. Sjostrand, S. Mrenna, and P. Z. Skands, _PYTHIA 6.4 Physics and Manual_, JHEP **05** (2006) 026, arXiv:0603175 [hep-ph].
* [8] M. Cacciari, G. P. Salam, and G. Soyez, _The anti-\(k_{t}\) jet clustering algorithm_, JHEP **04** (2008) 063, arXiv:0802.1189 [hep-ph].
* [9] M. Cacciari and G. P. Salam, _Pileup subtraction using jet areas_, Phys.Lett. **B659** (2008) 119-126, arXiv:0707.1378 [hep-ph].
* [10] ATLAS Collaboration, _Performance of Missing Transverse Momentum Reconstruction in ATLAS with 2011 Proton-Proton Collisions at \(sqrt{s}\) = 7 TeV_, Tech. Rep. ATLAS-CONF-2012-101, CERN, Geneva, Jul, 2012.
* [11] ATLAS Collaboration, _b-jet tagging calibration on \(c\)-jets containing \(D^{*+}\) mesons_, Tech. Rep. ATLAS-CONF-2012-039, CERN, Geneva, Mar, 2012.