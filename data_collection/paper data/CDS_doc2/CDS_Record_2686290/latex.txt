RECAST framework reinterpretation of an ATLAS Dark Matter Search constraining a model of a dark Higgs boson decaying to two \(b\)-quarks

The ATLAS Collaboration

###### Abstract

The reinterpretation of a search for dark matter produced in association with a Higgs boson decaying to \(b\)-quarks performed with RECAST, a software framework designed to facilitate the reinterpretation of existing searches for new physics, is presented. Reinterpretation using RECAST is enabled through the sustainable preservation of the original data analysis as re-executable declarative workflows using modern cloud technologies and integrated with the wider CERN Analysis Preservation efforts. The reinterpretation targets a model predicting dark matter production in association with a hypothetical dark Higgs boson decaying into \(b\)-quarks where the mass of the dark Higgs boson \(m_{s}\) is a free parameter, necessitating a faithful reinterpretation of the analysis. The dataset has an integrated luminosity of \(79.8\,\mathrm{fb}^{-1}\) and was recorded with the ATLAS detector at the Large Hadron Collider at a centre-of-mass energy of \(\sqrt{s}=13\,\mathrm{TeV}\). Constraints on the parameter space of the dark Higgs model for a fixed choice of dark matter mass \(m_{\chi}=200\,\mathrm{GeV}\) exclude model configurations with a mediator mass up to \(3.2\,\mathrm{TeV}\).

ATLAS -PHYS-PUB-2019-032 2019-032 2019-032

## 1 Introduction

The Large Hadron Collider (LHC) [1] provides a unique window on physics Beyond the Standard Model (BSM) at the energy frontier. Several testable extensions of the Standard Model (SM) have been proposed over the last decades, motivated by deficiencies of the SM, such as the hierarchy problem [2] and the nature of dark matter (DM) [3, 4] and dark energy [5]. Theories for new physics often predict, in addition to the SM particle content, the existence of additional particles that can be searched for at high energy colliders.

Conventionally, only a few models for new physics are considered in the interpretation of the results of experimental searches, although the searches are often sensitive to a broad range of models and the number of models that can be considered for interpretation is growing with time. These searches for new physics represent a significant investment of time and resources of experimental physicists. In the light of a steadily increasing sensitivity to a growing number of BSM scenarios, a powerful reinterpretation framework is needed to reach the full potential of the results released with the ATLAS experiment in a sustainable way.

This note presents the use of RECAST (Request Efficiency Computation for Alternative Signal Theories) [6] applied to an ATLAS search for new physics. RECAST is a framework designed to reuse estimates of backgrounds, systematic uncertainties and observations in the data from the original search to test alternative signal hypotheses. The analysis preservation strategies employed by RECAST have also more widely influenced the overall CERN Analysis Preservation efforts [7].

The power of RECAST is demonstrated by reinterpreting a search for DM particles produced in proton-proton collisions at the LHC. A possible DM candidate is a massive, stable and electrically neutral particle with gravitational interactions and whose non-gravitational interactions with SM particles are weak. Many models can accommodate the observed relic abundance of DM and predict production rates of DM particles at the scale of electroweak symmetry breaking that could be detected at the LHC. Conventionally, only a few simplified models with distinct detector signatures are taken into account when interpreting the results of a search [8, 9]. The plethora of models predicting similar signatures makes collider-based DM searches a prime candidate for RECAST.

In this note, we present the reinterpretation of a search for DM particles produced in association with a Higgs boson decaying into \(b\)-quarks [10] for a yet unexplored model which predicts production of DM with a hypothetical dark Higgs boson [11]. The existing search was performed with \(79.8\,\mathrm{fb}^{-1}\) of proton-proton collision data at a centre of mass energy of \(13\,\mathrm{TeV}\) recorded with the ATLAS detector during Run 2.

The primary observables of this search are the missing transverse momentum \(E_{\mathrm{T}}^{\mathrm{miss}}\) and the jet mass of either a jet with large radius parameter \(R=1.0\) or two jets with small radius parameter \(R=0.4\), ranging from \(50\,\mathrm{GeV}\) to \(280\,\mathrm{GeV}\). This allows the re-interpretation of the analysis for non-SM Higgs bosons with hypothetical mass \(m_{s}\), which is predicted by the dark Higgs model, probing the accessible parameter space \(50\,\mathrm{GeV}\leq m_{s}\leq 150\,\mathrm{GeV}\). The upper limit is set by the dark Higgs branching ratio to \(b\)-quarks, as discussed in Section 4.

This note is organised as follows. Section 2 provides a discussion of the RECAST framework, while Section 3 discusses the specific implementation of Ref. [10] in RECAST. Section 4 presents the Dark Higgs model that is investigated by this reinterpretation. The results are presented in Section 5, and conclusions are drawn in Section 6.

## 2 Recast

Originally proposed in Ref. [6], RECAST is a software framework that aims to provide the computational infrastructure and user interface necessary to efficiently organise systematic reinterpretations of analyses performed in high energy physics experiments. This in turn enables a more comprehensive assessment of the viability of a wide range of theories of fundamental particles and their interactions.

To motivate reinterpretation, it is useful to consider the human and computational resources required to create an analysis that is optimised with respect to a specific model. A search analysis typically attempts to identify a phase space region \(\Omega\) that is efficient for the signal model while rejecting as much SM background as possible. The definition of such a region through construction of suitable observables and development of the event selection constitutes a large fraction of the analysis work. This region must be examined not only for reconstructed data from an experiment, but also for both SM and BSM physics processes, which must be simulated, reconstructed and passed through the event selection to estimate their expected contribution to the total event rate, including their response to systematic variations of the simulation or reconstruction algorithms. All three inputs then form the basis for a statistical inference procedure. The full analysis workflow is shown in Figure 1 (a).

From the above, it is clear that it is unfeasible to construct optimised analyses for the wide range of physics models discussed in the context of the LHC. However, the region \(\Omega\) of an analysis, optimised for its original model, may still offer good sensitivity to other models as long as the model signatures are sufficiently similar. This forms the basis of the reinterpretation approach, which RECAST is designed to facilitate. The resources required for a reinterpretation are far fewer than those necessary for the construction of a new analysis. As the event selection of an analysis is fixed, the background estimates and observed data distributions for the corresponding phase space region \(\Omega\) do not change in the context of a reinterpretation, and only the distributions due to a new signal model must be derived. Once the new signal contribution has been derived, an adapted statistical analysis, which combines the newly derived signal estimate with the archived data measurement and background estimate, determines whether the model is viable in light of the data observed in the analysis. At a high level of abstraction, a reinterpretable analysis provides an _interface_ which takes the model and its parameters as inputs and yields an inference result. The interface must be general enough that it does not depend on the implementation details of the analysis.

Reinterpretation through the original analysis, compared to e.g. approximate third-party implementations, provides a more accurate reinterpretation result. It allows a faithful selection of events in addition to a more accurate statistical interpretation of the result in the context of a new model. With this in mind, one important point to consider is the possibility of the new signal having an appreciable yield across multiple regions of the analysis beyond the intended signal region in the original analysis. This may affect the background estimates of the analysis and in turn, compromise the validity of the reinterpretation or influence the quoted sensitivity from it. Reinterpretation with the full analysis enables a complete evaluation of the magnitude of such signal contributions and even the ability to incorporate them consistently through a joint statistical analysis, thereby taking them into account properly. However, it is important to note that prior to performing such a statistical interpretation, the extent of the signal contamination should be inspected carefully to evaluate its effect on the final result.

From a computational point of view, a reinterpretation requires the simulation of events according to the new model followed by the execution of the analysis chain on this newly produced input as shown in Figure 1 (b). The former consists of event sampling using a Monte Carlo generator, the derivation ofdetector effects using a Geant 4 [12]-based simulation as well as event reconstruction [13]. The latter requires the re-execution of the analysis specific event selection and statistical analysis.

The generation of a new signal in the format required by the user analysis (i.e. derived analysis data) may be performed using standard tools implemented within the experiments and thus is fairly independent of the analysis team. In contrast, the ability to execute event processing and inference stages are a priori only available to the original authors of the analysis. In previous reinterpretation efforts such as the Run-1 pMSSM reinterpretation [14], execution of the reinterpretation required the involvement of the original authors of the analyses, which in general may not be sustainable. Systematic reinterpretation hinges on preserving analyses in a way that allows new collaborators to re-execute them independently. This is achieved in RECAST through the use of declarative specifications and modern cloud computing technologies. A catalogue of preserved analyses that can be re-executed for a new signal model may then provide the basis of a streamlined reinterpretation program within the high energy physics community [15].

### Analysis Preservation using Containerised Workflows

For the purposes of this note, we denote an _analysis_ to be the portion of LHC data processing which is not handled by centrally provided infrastructure. Thus, for ATLAS analyses to be implemented in RECAST, an analysis begins with the processing of events provided by the collaboration in the _derived Analysis Object Data_ (DxAOD) format [16]. The analysis typically consists of a number of interdependent processing steps that culminate with a statistical inference. This analysis "workflow" can be efficiently modelled as a _directed acyclic graph_ (DAG), in which nodes represent individual processing steps and edges denote step dependencies. This workflow must be preserved so that it can be re-executed on new inputs. This means that the software for the individual processing steps must be archived, the correct usage of the software must be preserved in parameterised job templates, and the workflow graph structure must be preserved.

#### 2.1.1 Software Preservation

To be reusable, the software of an analysis must be preserved in a manner that is portable and deployable on a wide range of computing infrastructure. This is primarily achieved through software archival in the form of Linux container images [17]. In contrast to virtual machines, no hardware virtualisation is performed; instead, containerised processes share a single host kernel. Container images provide the full _root filesystem_, i.e. all software dependencies, necessary for the execution of a given process, which only interacts with the host through system calls to the Linux kernel. The kernel system call interface is very stable, such that containers provide a suitable balance between isolation and efficient deployment on diverse infrastructure. In recent years, popularised through Docker [18], containers have played an important role in the adoption of cloud computing [19].

#### 2.1.2 Workflow Preservation

The preservation of the workflow graph structure and the correct usage of the software is achieved through the use of a _workflow description language_. A number of such languages exist: in this case, yadage[20] was used. yadage is integrated into the developing CERN infrastructure for Analysis Preservation and Analysis Re-execution, the CERN Analysis Preservation Portal [21] and Reusable Analysis Platform (REANA) [22]. Within yadage, job templates and the workflow structure are captured as YAML documents, a format chosen for its simplicity and ubiquity. The parameterised job templates specify the commands required to execute the user analysis code for performing specific tasks, such as physics object calibration, event selection, estimation of systematic uncertainties, and statistical inference. The workflow orchestrates the individual steps of the analysis by specifying invididual steps using templates and their input parameters.

[MISSING_PAGE_EMPTY:6]

Preserving the Search for Dark Matter Produced in Association with a Higgs Boson decaying to \(b\)-quarks

The basis of the reinterpretation presented in this note is the analysis of \(79.8\,\mathrm{fb}^{-1}\) of \(pp\) collision data collected at a centre-of-mass energy of \(\sqrt{s}=13\,\mathrm{TeV}\) recorded by the ATLAS detector, made public as a preliminary result in Ref. [10]. The analysis targets scenarios of DM produced in association with a Higgs boson \(H\) decaying to \(b\)-quarks. The event selection and statistical inference of the original analysis have been preserved for RECAST and are reused in this reinterpretation. Details about the analysis are provided in Ref. [10]. This section provides a summary and details the required analysis preservation.

The final state is characterised by large missing transverse momentum \(E_{\mathrm{T}}^{\mathrm{miss}}\) from a pair of DM particles and by a system of jets originating from the Higgs boson decay products. The jets are reconstructed from clusters of calorimeter cells [23] using the anti-\(k_{t}\) jet clustering algorithm [24] and the presence of b-hadrons in the jets, referred to as b-tagging, is inferred using multivariate techniques[25, 26]. To match the targeted final state topology, a pair of \(b\)-tagged small-radius jets with radius parameter \(R=0.4\) (resolved topology) is required. Alternatively, in the case of boosted Higgs bosons (merged topology) a single large-radius jet with \(R=1.0\) is required which contains two \(b\)-tagged subjets and has been trimmed using the procedure described in Refs. [27] and [28]. These subjets, called variable radius (VR) track jets, are reconstructed from tracking information with a radius parameter that decreases as the subjet \(p_{\mathrm{T}}\) increases to improve reconstruction efficiency [29, 30].

Events satisfying the selection requirements are divided into a signal region (SR) targeting the merged topology with \(E_{\mathrm{T}}^{\mathrm{miss}}>500\,\mathrm{GeV}\) and SRs targeting the resolved topology with \(E_{\mathrm{T}}^{\mathrm{miss}}<500\,\mathrm{GeV}\), defined by the \(E_{\mathrm{T}}^{\mathrm{miss}}\) bins shown in Table 1. In addition, the statistical analysis is based on either the jet mass of the large-radius jet \(m_{J}\) (merged topology) or the invariant mass of the two leading \(b\)-tagged small-radius jets \(m_{jj}\) (resolved topology).

While the background estimation and the data of the analysis are preserved and re-used for the reinterpretation, the same event selection documented in Ref. [10] is required to select simulated signal events for the statistical interpretation. This is achieved by using the preserved stack of analysis software that is described in the following.

### Software preservation

To facilitate the creation of container images for analysis software, ATLAS provides suitable container _base images_, which hold the ATLAS analysis software release as well as its dependencies [31]. The analysis release provides the necessary software libraries to read the ATLAS data files and implement calibrations of the selected physics objects and thus is a key dependency of the user analysis code. When using the base images it is easier to reproduce a typical analysis. The software used in the present analysis is organised across two separate software repositories; thus, two container images are produced.

\begin{table}
\begin{tabular}{l c c c} \hline signal region & resolved & merged \\ \hline \(E_{\mathrm{T}}^{\mathrm{miss}}\) bin [GeV] & [150, 200) & [200, 350) & [350, 500) & 500 or more \\ \hline \end{tabular}
\end{table}
Table 1: \(E_{\mathrm{T}}^{\mathrm{miss}}\) bins used to define the signal regions targeting resolved and merged event topologies.

The first repository holds the code for the event selection and calibration of the event objects. It provides a command line interface to extract histograms from centrally-produced derived xAOD data files.

The second repository handles the statistical analysis based on the extracted histograms using HistFactory[32] and RooFit[33]. It uses the ROOT[34] installation provided as part of the analysis release.

For both repositories, the image building is integrated into the continuous integration configuration provided by the version control application GitLab[35] such that the software is preserved automatically each time the source code is modified. The continuous integration infrastructure furthermore allows for a consistent preservation of multiple versions of the analysis software, such as those corresponding to preliminary and final publications.

The technical implementation of the Dockerfiles used to capture the code of the repositories is provided in Appendix A.1.

### Preserving processing steps

Preserving the software environments of an analysis is not sufficient to re-execute it. Additionally, a description is needed that documents the correct usage of, e.g., the command line programs provided within these environments. As reinterpretation is concerned not only with _reproducing_ a given result but also _reusing_ the software to derive new results, this description must also be _parameterised_ to allow the specification of new input data. Thus, the individual processing steps of the analysis - such as executing the event selection or running the statistical analysis - must not be static reproductions of the commands executed to obtain the original result, but rather be job templates with parameters that are referenced in the workflow specification. Given a new input, the templates can then be translated to produce concrete job descriptions.

A job description needs to provide three basic pieces of information:

* A description of the processing step, often in the form of a command line string.
* The software environment, i.e. the container image to be used for the above process.
* An identification of the relevant outputs, i.e. the result of the processing. This is needed in order to assemble multiple such steps into a workflow as described below.

The corresponding yadage YAML documents are reproduced in Appendix A.2.

#### 3.2.1 Event Selection

The first processing step handles calibration of physics objects, event selection, and systematic variation of latent parameters in the simulation to account for systematic uncertainties. This step is performed within the event processing container images described above. The main input to the processing step are derived AOD data[36]. The simulated events provided for a new signal model do not have the correct mixture of events with varying number of additional proton-proton interactions (pile-up). In order to retain the correct pile-up distribution, the events are reweighted, for which an additional input file is necessary. Finally, the processing step also performs a weighting of the events according to the observed integrated luminosity, which is also provided as an external input.

As the data-taking conditions during the years 2015-2016 and the year 2017 differ, predominantly in the different pile-up profiles of the data, two separate simulated event samples are produced and need to be processed separately.

The output of the processing step is a set of histograms that capture the expected number of events within the respective signal regions of the analysis. The processing step also produces the inputs required to estimate the uncertainty in the expected event rates under systematic variations.

#### 3.2.2 Merging

In order to obtain a full estimate of the signal discriminant within the phase space regions defined by the analysis, the results of the individual event processings must be merged. This step is straightforwardly achieved through ROOT's file merging script hadd[37]. The output is a histogram file with the same structure as the inputs. As a computing environment, the ROOT installation of the analysis release is used.

#### 3.2.3 Statistical Analysis

Given a merged input, the workflow is completed by a statistical inference stage using a modified inference step. The analysis uses a binned likelihood constructed using the HistFactory[32] family of likelihoods by an internally developed toolkit named WSMaker1. In order to build the new likelihood, the (archived) background and data inputs as well as the newly derived signal estimates are needed. As the signal yields are the result of a simulation-based estimate, the exact value is uncertain. The likelihood structure accounts for this through the addition of a nuisance parameter and a corresponding constraint term. In the reinterpretation setting, the likelihood structure is kept invariant, but the size of the uncertainty may vary. The job script therefore receives the signal uncertainty as an input, which must be re-derived externally through truth-based analysis for the new signal hypothesis, as described in Section 4.3. As no excess has been observed in the data, upper limits on the signal cross-section normalised to the theoretical expectation are derived from the constructed likelihood.

Footnote 1: The name references ROOT’s statistical model serialization format: the RooFit workspace (WS).

### Workflow preservation

With the individual processing steps preserved, a parametrised workflow can be formulated that captures the full analysis chain from centrally produced data to statistical inference results. The workflow consists of four individual steps, as described above: event selection for the two datasets followed by merging and statistical analysis. Similar to the step templates, the full workflow is specified in yadage by a YAML document. The workflow references the step templates and declares the dependencies between the individual steps. The yadage workflow definition for the present analysis is shown Appendix A.3 and visualised in Figure 2.

The entire workflow was validated with signal samples from the original analysis. The upper limit on the production cross-section of the \(Z^{\prime}\)-2HDM signal model that was used for the original interpretation of the result can be reproduced using the RECAST workflow.

[MISSING_PAGE_EMPTY:10]

## 4 Dark Higgs signal model

The signal used for reinterpretation in this note is a dark Higgs model [11]. It predicts a signature with missing transverse momentum \(E_{\rm T}^{\rm miss}\) due to DM pair production and SM particles from the decay of an additional Higgs boson \(s\) in the DM sector via a small mixing with the SM Higgs boson \(H\). This model predicts a signature similar to the \(\rm H\to b\overline{b}+E_{\rm T}^{\rm miss}\) signature explored in Ref. [10] with the difference of the mass of the Higgs candidate, which is a free model parameter. From a theory point of view, the model is motivated by the need to generate the masses of particles in the DM sector and the possibility to relax DM relic abundance constraints [38] by opening up a new annihilation channel, giving rise to predictions of DM signals at the LHC in accordance with the observed relic abundance and constraints from \(Z^{\prime}\) resonance searches (see for example Ref. [9]).

### Theoretical framework

The dark Higgs model explains the mass generation mechanism for DM particles by postulating a Higgs mechanism in the DM sector. This also generates a new state: the dark Higgs boson \(s\). The presence of an additional mediator, such as a \(Z^{\prime}\) boson, that produces DM sector states, allows the model to be probed at colliders via processes illustrated in Figure 3.

The model postulates a Majorana fermion \(\chi\) as a DM particle. Its mass is obtained from the vacuum expectation value \(w\) of a complex Higgs field \(S\). A new \(U(1)^{\prime}\) gauge group is postulated. The associated \(Z^{\prime}\) boson mass is generated by spontaneous symmetry breaking since \(S\) carries a charge \(q_{s}\) under the new gauge group. The symmetry breaking gives rise to the dark Higgs boson that is defined as \(s=\sqrt{2}S-w\). The DM particle \(\chi\) axially couples to the \(Z^{\prime}\) boson2. The interaction Lagrangian is:

Footnote 2: A similar, although less simple model can be designed considering a Dirac fermion as DM particle, allowing DM to have in addition a vectorial coupling to the \(Z^{\prime}\) boson, which is independent of all other parameters [11].

\[\mathcal{L}_{\chi}=-\frac{1}{2}g_{\chi}Z^{\prime\mu}\overline{\chi}\gamma^{5} \gamma_{\mu}\chi-g_{\chi}\frac{m_{\chi}}{m_{Z^{\prime}}}s\overline{\chi}\chi +2g_{\chi}Z^{\prime\mu}Z^{\prime}_{\mu}(g_{\chi}s^{2}+m_{Z^{\prime}}s). \tag{1}\]

The DM sector is coupled to the SM by postulating vector couplings of the \(Z^{\prime}\) to quarks (\(q\)) by gauging baryon number. The interaction Lagrangian is

\[\mathcal{L}_{q-Z^{\prime}}=-g_{q}Z^{\prime\mu}\overline{q}\gamma_{\mu}q. \tag{2}\]

The independent parameters of the model are: \(m_{\chi}\), the mass of the DM particle; \(m_{Z^{\prime}}\), the mass of the Z' boson; \(m_{s}\), the mass of the dark Higgs boson; \(g_{\chi}=g^{\prime}q_{\chi}\), the DM coupling, where \(g^{\prime}\) is the \(U(1)^{\prime}\) gauge

Figure 3: Processes leading to a dark Higgs signal with \(E_{\rm T}^{\rm miss}\) due to DM particles \(\chi\) and decay of the dark Higgs to \(b\)-quarks.

coupling and \(q_{\chi}=q_{S}/2\) is the charge of the DM particle, as required by gauge invariance; and \(g_{q}\), the coupling of the \(Z^{\prime}\) boson to quarks. Axial-vector couplings of the \(Z^{\prime}\) boson to quarks and its couplings to leptons are neglected for simplicity.

For the relic abundance to be largely set by the process \(\chi\chi\to ss\), \(s\) is required to be the lightest state in the dark sector. A non-zero mixing between the dark Higgs boson and the SM Higgs boson with mixing angle \(\theta\) ensures that the dark Higgs boson is unstable and can decay into SM states promptly.

The branching ratio of the dark Higgs boson decay, computed with MadGraph5_aMC@NLO 2.6.6 [39], is shown in Figure 4. For \(m_{s}<160\,\mathrm{GeV}\), the decay to \(b\)-quarks dominates until the \(W^{+}W^{-}\), \(ZZ\) and \(HH\) decays become relevant at their respective kinematic threshold, limiting the sensitivity of this analysis and allowing the model to be probed with other signatures.

In a large fraction of the \(pp\to Z^{\prime}\to\chi\chi s\) three-body decays, the resulting dark Higgs boson is soft, with the pair of DM particles oriented back-to-back. Still, the probability of producing a dark Higgs boson with sufficiently large momentum to pass the \(E_{\mathrm{T}}^{\mathrm{miss}}\)-based selection requirements is sizable: \(\sim 35\%\) of all events satisfy \(E_{\mathrm{T}}^{\mathrm{miss}}>150\,\mathrm{GeV}\) and \(\sim 2\%\) satisfy \(E_{\mathrm{T}}^{\mathrm{miss}}>500\,\mathrm{GeV}\) for a signal model with \(m_{Z^{\prime}}=1\,\mathrm{TeV}\) and \(m_{s}=90\,\mathrm{GeV}\).

The resulting signature of \(E_{\mathrm{T}}^{\mathrm{miss}}\) and either a single large-\(R\) jet containing two \(b\)-subjets or two \(b\)-jets achieves much better sensitivity compared to common \(X+E_{\mathrm{T}}^{\mathrm{miss}}\) searches, where the \(X\) state originates from initial state radiation, even if the cross-section for the process \(pp\to Z^{\prime}\to\chi\chi s\) is smaller than the one for \(pp\to Z^{\prime}+X\to\chi\chi+X\)[11].

### Generation of simulated signal events

Guided by a sensitivity estimate based on generic limits on the visible \(\mathrm{H}\to\mathrm{b}\overline{\mathrm{b}}+E_{\mathrm{T}}^{\mathrm{miss}}\) cross-section [40], the parameter space of the model is scanned for \(m_{Z^{\prime}}\) from 0.5 to 3.5\(\,\mathrm{TeV}\) in steps of 0.5\(\,\mathrm{TeV}\) and for

Figure 4: Branching ratio of the dark Higgs boson \(s\) decay to \(b\)-quarks, \(W^{+}W^{-}\), \(ZZ\) and \(HH\). The branching ratio was computed using MadGraph5_aMC@NLO 2.6.6 at leading order.

from 50 to 150 GeV in steps of 20 GeV. The upper limit on \(m_{s}\) is set by the branching ratio of \(s\to b\bar{b}\). The other parameters of the model are fixed to \(m_{\chi}=200\) GeV, \(g_{q}=0.25\), \(g_{\chi}=1\), and \(\theta=0.01\). The value of \(\theta\) follows the choice in Ref. [11], \(g_{q}\) and \(g_{\chi}\) follow conventional coupling choices for vector mediator simplified model searches at the LHC [8; 9].

The signal samples are produced by calculating the hard process cross section matrix elements at leading order (LO) with MadGraph5_aMC@NLO 2.6.2 interfaced with Pythia 8.230 [41] for the modelling of the parton shower and underlying event, allowing for up to one additional jet in the event. To remove overlap between the matrix element and the parton shower, the CKKW-L merging procedure [42; 43] is applied. This setup is referred to as MadGraph5_aMC@NLO +Pythia CKKW-L. The matching scale in the CKKW-L procedure is set to 40 GeV.

The NNPDF3.0 PDF set with \(\alpha_{S}=0.13\)[44] and the A14 set of tuned parameters (tune) [45] of Pythia are used. Renormalisation and factorisation scales are set to the MadGraph5_aMC@NLO default values, based on a clustering of the event. The EvtGenv1.2.0 program [46] is used for properties of the bottom and charm hadron decays.

The generation of the simulated event samples includes the effect of multiple \(pp\) interactions per bunch crossing, as well as the effect on the detector response due to interactions from bunch crossings before or after the one containing the hard interaction. This is achieved by overlaying inelastic pp collision events simulated using Pythia 8, with the A3 tune [47] and the NNPDF2.3LO PDF set [48]. Simulated events are corrected using per-event weights to describe the distribution of the average number of interactions per bunch crossing as observed in data. All simulated event samples are processed with the Geant 4 [12] based ATLAS detector simulation [49].

### Systematic uncertainties

The normalisation and shapes of the signal estimate are affected by systematic uncertainties due to experimental and theoretical sources. Sources of experimental uncertainty are accounted for as in Ref. [10]. Due to the similar signature of the dark Higgs model and the original \(Z^{\prime}\)-2HDM signal model, no additional systematic uncertainties due to experimental sources need to be taken into account.

Systematic uncertainties due to theoretical predictions are estimated by varying the model parameters and comparing the event yield after applying the analysis selection at generator level. Renormalisation and factorisation scale uncertainties and variations of parton distribution functions (PDFs) are considered in the four \(E_{\mathrm{T}}^{\mathrm{miss}}\) bins of of the analysis. The uncertainty on pdfs in individual \(E_{\mathrm{T}}^{\mathrm{miss}}\) bins ranges from 3.8 % to 16.4 %, while scale uncertainties range from 6.2 % to 22.2 %.

## 5 Results

This section presents the results from using the archived data and background model in combination with the new dark Higgs signal hypothesis for a profile likelihood fit to the jet mass of the Higgs boson candidate in the RECAST framework.

Figures 5 and 6 show the observed data and simulated background data scaled to the result of a conditional background-only fit with an overlay of the expected signal distribution scaled to an arbitrary cross-section

chosen for visualisation. The background-only fit is, as expected, identical to the original result shown in in Ref. [10]; the only difference is the exchanged signal which is overlayed.

The distributions of the invariant mass of the dark Higgs boson candidates are shown in Figure 6 on top of the background distributions. A representative dark Higgs signal model with dark Higgs mass \(m_{s}=90\,\mathrm{GeV}\) scaled to an arbitrary cross-section for visualisation is overlayed. The resonant structure of the dark Higgs signal in the two-body decay is clearly visible as a mass peak.

Figure 5: \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution for the resolved and merged signal regions combined. The upper panel shows a comparison of data to the SM expectation before (dashed lines) and after the fit (solid histograms) with no signal included. The lower inlet displays the ratio of data to SM expectations after the background-only fit, with its systematic uncertainty considering correlations between individual contributions indicated by the hatched band. The expected signal from a representative dark Higgs model scaled to the cross-section predicted by theory of \(356\,\mathrm{fb}\) is also shown (long-dashed line).

Figure 6: Distributions of the invariant mass of the dark Higgs boson candidates \(m_{jj},m_{J}\) with two \(b\)-tagged jets on top of the background distributions in the signal region (SR) for the four \(E_{\rm T}^{\rm miss}\) categories that are used as inputs to the fit. The comparison of data to the SM expectation is shown before (dashed lines) and after the fit (solid histograms). The lower inlet displays the ratio of data to SM expectation after the fit with no signal considered, with its systematic uncertainty considering correlations between individual contributions indicated by the hatched band. The expected signal from a representative dark Higgs model with dark Higgs mass \(m_{s}=90\,\)GeV is also shown (long-dashed line). It is scaled to a cross-section of \(356\,\)fb (\(178\,\)fb) for the distributions showing the resolved (merged) SR, which corresponds to signal strengths of \(\mu=1(0.5)\) compared to the prediction from theory.

Since no deviation from the SM prediction was observed in the original search, the parameter space of the dark Higgs model can be constrained by setting exclusion limits.

Figure 7 shows the observed and expected limits on the production cross-section for the dark Higgs model. The sensitivity of the reinterpretation extends to production cross-sections as low as \(\mathcal{O}(2\,\mathrm{fb})\).

Figure 8 shows the expected and observed upper limits on the signal strength \(\mu\) at 95% CL\({}_{s}\). If the upper limit is below one, the specific model configuration is excluded at 95% CL\({}_{s}\).

Figure 8: Upper 95% CL\({}_{s}\) exclusion limits on the signal strength \(\mu\) for the dark Higgs model with parameters \(m_{\chi}=200\,\mathrm{GeV}\), \(g_{q}=0.25\), \(g_{\chi}=1.0\) and different values of \(m_{Z^{\prime}}\) and \(m_{s}\). The left figure shows the expected exclusion limits while the right figure shows the observed exclusion limits.

Figure 7: Observed and expected limits on the production cross-section \(\times\) branching ratio (\(s\to b\bar{b}\)) for the dark Higgs model with parameters \(m_{\chi}=200\,\mathrm{GeV}\), \(g_{q}=0.25\), and \(g_{\chi}=1.0\). The solid (dashed) line shows the observed (expected) limit for different values of \(m_{Z^{\prime}}\) and \(m_{s}\).

The exclusion contours based on expected and observed limits are shown in Figure 9. The region to the left of the black line indicating the observed 95% \(\text{CL}_{s}\) exclusion contour is excluded. The exclusion contours are overlayed with a pink dotted line indicating the model configurations for which the expected relic density is consistent with the PLANCK [50] measurement (i.e. \(\Omega h^{2}=0.114\), computed with MadDM [51], a numerical tool to compute the DM relic abundance). The predicted relic abundance in the dark Higgs model strongly depends on the coupling parameters and DM mass, allowing various choices of the parameters to satisfy the constraints set by PLANCK. These are satisfied in the region on the left of the curve in the benchmark scenario discussed here, thus highlighting the complementarity of collider and astrophysical measurements.

The expected exclusion of the dark Higgs model parameter space extends up to \(m_{Z^{\prime}}\) of \(2.3\,\text{TeV}\) for \(m_{s}=50\,\text{GeV}\) and up to \(2.8\,\text{TeV}\) for \(m_{s}=70\), \(90\), and \(110\,\text{GeV}\). The analysis is less sensitive to small dark Higgs masses because of the larger SM background for lower \(m_{jj},m_{J}\). The decrease in expected sensitivity for \(m_{s}>130\,\text{GeV}\) results from the diminishing branching ratio of the \(s\to b\overline{b}\) process, as discussed in Section 4.1.

The observed exclusion of the dark Higgs model parameter space extends up to \(m_{Z^{\prime}}=3.2\,\text{TeV}\) for \(m_{s}=90\,\text{GeV}\). The exclusion for model configurations with large \(m_{Z^{\prime}}\) is almost entirely driven by the event selection targeting the merged topology. As it can be noted in Figure 6 there is an under-fluctuation in data for \(70\,\text{GeV}<m_{J}<110\,\text{GeV}\), resulting in stronger observed limits than expected.

Figure 9: Exclusion contour for the dark Higgs model with parameters \(m_{\chi}=200\,\text{GeV}\), \(g_{q}=0.25\), \(g_{\chi}=1.0\) and different values of \(m_{Z^{\prime}}\) and \(m_{s}\). The solid (dashed) line shows the observed (expected) limit, while the green and yellow bands indicate the \(\pm 1\sigma\) and \(\pm 2\sigma\) uncertainty on the expected limit, respectively. The pink dotted curve corresponds to the set of points for which the expected relic density is consistent with the PLANCK [50] measurement (i.e. \(\Omega h^{2}=0.114\)), as computed with MadDM [51]. The region on the right of the curve corresponds to a predicted relic abundance which is higher than these measurements.

## 6 Conclusion

In this note, a reinterpretation of an existing DM search in ATLAS proton-proton collisions has been presented. The original analysis targeted models predicting DM produced in association with a Higgs boson and studied collision events with \(b\)-jets and missing transverse momentum. This final state signature also matches an alternative model of new physics in which the \(b\)-jets originate from a dark sector Higgs decay. The existing analysis is reinterpreted using the infrastructure provided by the RECAST framework. Here, the original analysis code has been preserved using Linux container images and the workflow specification was captured using the workflow language \(\mathsf{yadage}\). Both formats are supported by the CERN Analysis Preservation Portal. Constraints on the parameter space of the dark Higgs model for a fixed choice of \(m_{\chi}=200\,\mathrm{GeV}\), \(g_{q}=0.25\), and \(g_{\chi}=1.0\) are set in an efficient and thus sustainable way, excluding mediator masses up to \(m_{Z^{\prime}}=3.2\,\mathrm{TeV}\).

## Appendix A Appendix

### Dockerfiles for analysis software preservation

The software used to perform the analysis can be captured by building a container image. The XAMPPmonoH software for the event selection was captured using the Dockerfile provided below. The resulting docker container builds upon a centrally provided container by the ATLAS collaboration which contains the analysis software release AthAnalysis. The analysis code is copied and compiled against the analysis release.

```
FROMatlas/athanalysis:21.2.31 ADD./xampp/XAMPPmonoH WORKDIR/xampp/buildRUNsource-/release_setup.sh&& \ udochown-Ratlas/xampp&& \  cmake../XAMPPmonoH&& \  make-j4
```

The Dockerfile which has been used to preserve the analysis is reproduced below. Similarly to the event selection software the analysis code for the statistical evaluation WSMaker is copied and compiled against the analysis release AnalysisBase.

```
FROMatlas/analysisbase:21.2.39 ADD./code WORKDIR/code RUNsodchown-Ratlas/code&& \ source/home/atlas/release_setup.sh&& \  cd/code&& \  source/code/setup.shMonOH&& \  gmake -j5&& \  echo "time stamp:$(date)">/code/build.stamp&& \  git rev-parse --shortHEAD>/code/build.revision
```

### Job templates for analysis processing step preservation

The fully preserved job template for running the event selection is reproduced below. It specifies commands to authenticate for accessing the input data, setting up the analysis code, applying the event selection and variations of distributions to account for sources of systematic uncertainty, and finally to copy the resulting output to be available for the merging step.

``` selection_stage: process: process_type:interpolated-script-cmd script:| source/recast_auth/getkrb.sh source/home/atlas/release_setup.sh cd/xampp/XAMPPmonoH if[-z"(PRWFILE_mc16)"];then echo "Usinginternal/central PRWfiles" else echo 'Setting paths to pile-up reweighting files:'echo(PRWFFILE_mc16)  exportPRWFFILE=(PRWFFILE_mc16)  fi  sourceXAMPPmonoH/recast/recast_run.sh{input_mc16}{dsid}{datayears}[lumi]{xsecpb}{nevents}  sudocp/xampp/XAMPPmonoH/output_recast/recast_signal_WSMaker.root{outputfile}  publisher:  publisher_type: interpolated-pub  publish:  selected_signal_mc16:{outputfile}'  glob: true  environment:  environment_type: docker-encapsulated  image: gitlab-registry.cern.ch/atlas-mpp-xampp/xamppmonoh  imagetag: recast_monosbb  resources:  - GRIDProxy ```
The output file merging step job template follows in a straight forward fashion. It specifies to merge the output of the two selection steps using the ROOT application **hadd**.
``` merging_stage:  process:  process_type: interpolated-script-cmd  script: |  source/home/atlas/release_setup.sh  hadd recast_signal_WSMaker.root{selected_signal_mc16a}{selected_signal_mc16d}  publisher:  publisher_type: interpolated-pub  publish:  selected_signal:{outputfile}'  glob: true  environment:  environment_type: docker-encapsulated  image: gitlab-registry.cern.ch/atlas-mpp-xampp/xamppmonoh  imagetag: recast_monosbb ```
The job template for the statistical interpretation step is reproduced below. It specifies commands to authenticate for retrieving the archived data and background estimation, combine those with the output of the merging step, create RooFit workspaces and use those to compute expected and observed limits on the signal strenght.
``` fitting_stage:  process:  process_type: interpolated-script-cmd  interpreter: bash script: |  source/recast_auth/getkrb.sh  source/home/atlas/release_setup.sh  exportFITINPUT=limit_backgrounds)  exportISIMAL_GLIPFOT=[limit_inputs_signal]  exportINPUT=ERSION=catConfigs/monoH_700_0121ep_151617_recast.configrepInputVersion|sed's/.'//g'  cp(signaltheoryuncertainty)/code/sigsys_signal.txt  cat/code/sigsys_signal.txt  sed-i |'s/SignalSystemRECASTfalse/SignalSystemRECASTtrue/g' { /code/configs/monoH_700_0121ep_151617_recast.conf  source setup.sh MonoH  source/code/recast_setup.sh  MakeWorkspaceconfigs/monoH_700_0121ep_151617_recast.conf recast  echo'Expectedlimit'  pythonscripts/getlimit.pySIMPUTVERSION.recast 1 ```echo 'observed limit'  python scripts/getLimit.py SINPUTVERSION.recast 0 1  sudw mv root-files/ (outputdir)  publisher:  publisher_type: interpolated-pub  publish:  outputs: '(outputdir)'  environment:  environment_type: docker-encapsulated  image: gitlab-registry.cern.ch/monohbb/2017/fitting  imagetag: recast_CONF2018  resources: - GRIDproxy

### Workflow specification

The specification of the reinterpretation workflow of the analysis is provided below. It specifies the order of the individual steps using the dependency keyword, referring to the definition of the parameterised templates in the steps.yml file and details their respective inputs and parameters using the parameters section.

```
stages: -name:selection_stage_mc16a dependencies: [init] scheduler:  scheduler:  scheduler_type: singlestep-stage parameters:  input_mc16: {step: init, output: input_mc16a}  outputfile: '(workdir)/recast_signal_WSMaker_mc16a.root'  persistentdir: '(workdir)'  dsid: {step: init, output: dsid}  datayears: data1516  nevents: {step: init, output: nevents}  xsecpb: {step: init, output: xsecpb}  lumi: {step: init, output: lumi_mc16a}  PRMFFILE_mc16: {step: init, output: PRMFFILE_mc16a}  step: {step:'steps.yml#/selection_stage'} - name: selection_stage_mc16d dependencies: [init] scheduler:  scheduler_type: singlestep-stage parameters:  input_mc16: {step: init, output: input_mc16d}  outputfile: '(workdir)/recast_signal_WSMaker_mc16d.root'  persistentdir: '(workdir)'  datayears: data17  dsid: {step: init, output: dsd}  newts: {step: init, output: nevents}  xsecpb: {step: init, output: xsecpb}  lumi: {step: init, output: lumi_mc16d}  PRMFFILE_mc16: {step: init, output: PRMFFILE_mc16d}  step: {step:'steps.yml#/selection_stage'} - name: merging_stage dependencies: [selection_stage_mc16a, selection_stage_mc16d]  scheduler:  scheduler_type: singlestep-stage parameters:  selected_signal_mc16a: {step: selection_stage_mc16a, output: selected_signal_mc16b}  selected_signal_mc16d: {step: selection_stage_mc16d, output: selected_signal_mc16b}  outputfile: '(workdir)/recast_signal_WSMaker.root'  persistentdir: '(workdir)'
```
step: {**sf:**'steps.yml#/merging_stage'} - name: fitting_stage dependencies: [merging_stage] scheduler: scheduler_type: singlestep-stage parameters: limit_backgrounds: {**step:** init, **output:** limit_backgrounds} - signaltheoryuncertainty: {**step:** init, **output:** signaltheoryuncertainty} - limit_inputs_signal: {**step:** merging_stage, **output:** selected_signal} - persistentdir: '{**workdir}' outputdir: '{**workdir}' step: {**sf:**'steps.yml#/fitting_stage'} ```

## References

* [1] L. Evans and P. Bryant, _LHC Machine_, JINST **3** (2008) S08001 (cit. on p. 2).
* [2] S. Dimopoulos and H. Georgi, _Softly broken supersymmetry and SU(5)_, Nuclear Physics B **193** (1981) 150, issn: 0550-3213, url: [http://www.sciencedirect.com/science/article/pii/0550321381905228](http://www.sciencedirect.com/science/article/pii/0550321381905228) (cit. on p. 2).
* [3] G. Bertone, D. Hooper and J. Silk, _Particle dark matter: evidence, candidates and constraints_, Physics Reports **405** (2005) 279, issn: 0370-1573, url: [http://www.sciencedirect.com/science/article/pii/S0370157304003515](http://www.sciencedirect.com/science/article/pii/S0370157304003515) (cit. on p. 2).
* [4] J. L. Feng, _Dark Matter Candidates from Particle Physics and Methods of Detection_, Annual Review of Astronomy and Astrophysics **48** (2010) 495, eprint: [https://doi.org/10.1146/annurev-astro-082708-101659](https://doi.org/10.1146/annurev-astro-082708-101659), url: [https://doi.org/10.1146/annurev-astro-082708-101659](https://doi.org/10.1146/annurev-astro-082708-101659) (cit. on p. 2).
* [5] M. Kunz and D. Sapone, _Dark Energy versus Modified Gravity_, Phys. Rev. Lett. **98** (12 2007) 121301, url: [https://link.aps.org/doi/10.1103/PhysRevLett.98.121301](https://link.aps.org/doi/10.1103/PhysRevLett.98.121301) (cit. on p. 2).
* [6] K. Cranmer and I. Yavin, _RECAST: Extending the Impact of Existing Analyses_, (2010), arXiv: 1010.2506, url: [http://dx.doi.org/10.1007/JHEP04](http://dx.doi.org/10.1007/JHEP04)(2011)038 (cit. on pp. 2, 3).
* [7] X. Chen et al., _Open is not enough_, Nature Physics **15** (2019) 113, issn: 1745-2481, url: [https://doi.org/10.1038/s41567-018-0342-2](https://doi.org/10.1038/s41567-018-0342-2) (cit. on p. 2).
* [8] A. Boveia et al., _Recommendations on presenting LHC searches for missing transverse energy signals using simplified \(s\)-channel models of dark matter_, (2016), arXiv: 1603.04156, url: [http://arxiv.org/abs/1603.04156](http://arxiv.org/abs/1603.04156) (cit. on pp. 2, 13).
* [9] ATLAS Collaboration, _Constraints on mediator-based dark matter and scalar dark energy models using \(\sqrt{s}=13\) TeV pp collision data collected by the ATLAS detector_, Journal of High Energy Physics **2019** (2019) 142, issn: 1029-8479, url: [https://doi.org/10.1007/JHEP05](https://doi.org/10.1007/JHEP05)(2019)142 (cit. on pp. 2, 11, 13).
* [10] ATLAS Collaboration, _Search for Dark Matter Produced in Association with a Higgs Boson decaying to \(b\bar{b}\) at \(\sqrt{s}=13\) TeV with the ATLAS Detector using \(79.8\,fb^{-1}\) of proton-proton collision data_, ATLAS-CONF-2018-039, 2018, url: [https://cds.cern.ch/record/2632344](https://cds.cern.ch/record/2632344) (cit. on pp. 2, 7, 11, 13, 14).
* [11] M. Duerr et al., _Hunting the dark Higgs_, JHEP **04** (2017) 143, arXiv: 1701.08780 [hep-ph] (cit. on pp. 2, 11-13).