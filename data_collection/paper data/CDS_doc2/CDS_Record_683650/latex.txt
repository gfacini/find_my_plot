**ATLAS Internal Note**

**DAQ-NO-100**

**22 May 1998**

EUROPEAN ORGANIZATION FOR NUCLEAR RESEARCH

ATLAS TDAQ XXXX

DRAFT

2 April 1996

**F/E DAQ Discussion Group**

**Summary Document and Work Plan**

G. Ambrosini, A. Bogaerts, P. Farthouat, R. Ferrari, D. Francis, R. Jones, A. Lacourt

L. Mapelli, R. McLaren, G. Mornacchi, C. Parkman, G. Polesello, J. Petersen, Y. Perrin,

L. Tremblet, V. Vercesi, J. Vermeulen

## 1 Introduction

This document summarises the results of the F/E DAQ discussion group, set up in November 1995 by the ATLAS DAQ community, in the context of the development of a functional vertical slice of the ATLAS DAQ architecture, also referred to as "prototype -1" (minus 1) [2]. The scope of the prototype -1 is to demonstrate the validity of the overall architecture, without necessarily reaching the final LHC performance.

The discussion group focused on two areas of the ATLAS DAQ architecture:

* the read-out crate and
* the DAQ aspects of the Level-3 sub-farm (not including the development of algorithms and related analysis programs).

The discussion group was setup to define a work plan aimed at pre-design analysis and selection of major system components.

The document is structured in two parts.

* The first contains:

1. a general overview of the problem, its definition, boundaries, break-down into major elements and overall target performance figures,

2. a description of system components: their scope, requirements, functionalities and candidate technologies.
* The second, based on the results of the first part, defines a list of evaluation activities and studies.

## Part I Systems Overview and Components

### 2 Problem Overview

#### Prototype -1 Systems

In the context of the prototype -1 (sketched in Figure 1) we have addressed the DAQ aspects of the read-out crate and level-3 sub-farm systems.

The read-out crate and the sub-farm DAQ are intended as modular DAQ units. Their functions include all aspects associated with the main flow of the data and those issues associated with monitoring and control.

The two systems have common features and differ only with respect to their data flow requirements.

* In the read-out crate, small event fragments are concurrently read-out and buffered at a high rate. Fragments from the same bunch crossing are collected and sent to the level-3.
* In the sub-farm, full events are received at a low rate, buffered and subsequently distributed to analysis and/or calibration programs.

The sub-farm DAQ has been initially included as a topic for this discussion group due to the common view of DAQ issues with the read-out crate.

Figure 1 Prototype -1The relationships between the other DAQ prototype sub-systems, the read-out crate and the sub-farm are sketched in Figure 2.

The read-out crate exchanges data and control information with the detector, trigger, back-end (B/E) software and event builder1 sub-systems. The sub-farm DAQ supports the flow of the data from the event builder to the analysis software and the mass storage, and interfaces to the back-end software.

Footnote 1: With the term Event Builder we intend, in this document, the combination of both the switch and the associated data flow management.

Reference to the description of the ATLAS prototype -1 [2] should be made to have an overall view of the scope and objectives of the DAQ prototype -1. The read-out crate will be the vehicle for a number of DAQ studies, for example:

* Evaluation of single system elements, e.g. a ROB module.
* Studies of data flow problems related to level-3 as well as data flow issues, inside the read-out crate, related to level-2.
* Understanding of the interaction and the integration with other sub-systems, such as the trigger and the event builder.
* Understanding of the integration issues with the B/E system.
* Evaluation of processor elements and related real-time environments.
* The read-out crate as a modular element in prototype -1 applications.
* The read-out crate as a modular element in test beam applications.

### Boundaries

The relationships with other prototype-1 sub-systems have been sketched in section 2.1, here we state the boundaries and limits in scope of the read-out crate and the sub-farm DAQ.

Figure 2: Read-Out crate (2a) and sub-farm (2b) context diagrams

* The integration with the event builder is realised via a Level-3 interface (in the read-out crate) and the switch to farm interface (in the sub-farm). A generic interface to the memory management and control aspects in the two components will be proposed, with the objective of remaining neutral with respect to the technology used for event building.
* The integration with the trigger sub-system is done via the trigger module element (described in section 3.4), whose specifications is outside of the scope of this group.
* The study and the development of level-3 algorithms and analysis programs are out of the scope of this discussion.
* The integration with the back-end software will be done in the local DAQ (LDAQ) element (see section 3.5), so as to maintain modularity and flexibility in the overall prototype.

The level-2 is not directly addressed by the prototype -1, nevertheless interfaces are foreseen for the purpose of both providing a data rate reduction mechanism and of testing the effects of level-2-functionality on the prototype -1 performance (e.g. ROI selection mechanisms, effects of high rate transfer requests on ROB memory). The ROB, as far as level-2 issues in the read-out crate are concerned, is a memory and a data server. This is analogous, although with different data flow requirements, to the function provided to level-3. Modularity will help the integration of level-2, level-3 and DAQ.

### Constraints

The following assumptions constraint the options for various system components.

1. Commercially available hardware and software components will be used for the prototype, unless no component available on the market can satisfy the functional requirements.
2. PCI is assumed as the I/O integration bus at the component level in the read-out crate. PCI performance studies and understanding are nevertheless required to assess the feasibility of such an assumption.
3. VME is assumed as the integration bus among components in the read-out crate.
4. Points 2 and 3 are relevant to the sub-farm only for an organisation based on single board computer modules.
5. Flexibility to study variations on the base architecture takes priority over "special" solutions aimed at higher performance. Performance must nevertheless stay at a level meaningful for the studies (see Table 1 on page 1).

### System break-down and areas for evaluation

On the basis of the ATLAS DAQ architecture [1] the read-out crate and sub-farm have been decomposed into their major elements. Elements that require study and evaluation work prior to the design phase have been defined.

From the input into the read-out crate and following the flow of data towards the farm, the ATLAS TP DAQ architecture singles out the following system elements:

* The read-out link: it provides the means to move event fragments from a detector ROD into a read-out buffer (ROB) in a read-out crate.
* The RoB: it is responsible for reading and buffering data from the read-out link and distributing those same data to the level-3 interface, the level-2 system and to the local DAQ for monitoring purposes.
* The data collection element: it provides the means to move the event fragments, belonging to the same event, from all or some ROBs into the memory of the L3IF.
* The trigger module: it distributes both the TTC and the ROI information to other elements in the read-out crate.
* The local DAQ (LDAQ): it supports, in the read-out crate and in the sub-farm, the high level DAQ functions other than the main data flow.
* The level-3 interface (L3IF) or link (L3LNK): it gathers, via the data collection element, event fragments from the ROBs, buffers them and provides a "crate-fragment" to the event builder. A less sophisticated version of a L3IF could be envisaged without an internal buffer, such that event

Figure 3: ATLAS T/DAQ TP architecturefragments are retrieved from the ROB buffers and sent directly to the switch. Since this is a design issue which needs further study (not only with respect to cost and performance but also to function), we consider in this document the more general case which includes a buffering function.
* The switch to farm interface (SFI): it receives full events from the event builder, buffers them and distributes full events to the sub-farm processors via the sub-farm data distribution system.
* The sub-farm data distribution system: it is the mean to distribute full events in the sub-farm.

The read-out link, RoB, data collection, L3IF, SFI and the sub-farm data distribution system are the components supporting the flow of the data through the DAQ system. Given their prominence, preliminary studies need to be performed in order to aid the selection of technical options and the design.

For the elements at the interface between sub-systems, i.e. the ROB, L3IF and SFI, common issues have been factored out to give a generic component called the I/O Module. At different levels in the DAQ the I/O Module supports the main flow of the data through input and output channels, it buffers events, it provides secondary data flow for monitoring purposes, it connects to the LDAQ. While the three instances have different performance requirements, they share a core of functionality which can be exploited at the evaluation and design stages.

Processors and their operating system environments are a pervasive component in the systems. The constraints of PCI and VME as integration busses still leaves a wide spectrum of options which can be reduced by a priori selection of the processor architecture. As regards operating systems today's proven choice, in the short time scale, is UNIX systems with real-time features.

It is considered worthwhile to study the possibility of using PCs running WindowsNT and in which areas it would be best suited (Local DAQ CPU, sub-farm, etc.). Two complementary issues should be investigated: the use of PCs with WindowsNT and WindowsNT on a broader hardware platform i.e. VME based processor boards.

An overview of the required features of the local DAQ is considered necessary insofar as it will help guide the selection process of processor elements and operating systems.

While the specification of the trigger module is outside of the scope of this group, its features do influence the control, synchronisation and data transfer aspects in the read-out crate. Also its first implementation is expected to be based on elements analogous to those indicated for the I/O modules. A definition of its features and its expected performance represents additional input to the evaluation process.

The format of the data, both in terms of how data fragments are structured and of the information contained in them, also deserves thought at this point as it influences control and checking actions during the flow of the data.

Modelling also deserves consideration at the pre-design stage: model building helps focusing on the essential features, pruning of architectural and organisational options may be achieved through modelling results. Performance models are also a useful tool at the post-implementation phase to help understanding e.g. bottlenecks.

### Performance targets

Within the scope of the present document the following table summarises the performance requirements.

For the input into the ROB, it may not be possible to achieve, on the prototype -1 timescale, the final ATLAS performance requirements and to maintain at the same time the suitable degree of flexibility needed for functional studies at the crate level. On the other end single components, such as the R/O link, may be studied in isolation for maximum performance.

The final ATLAS throughput requirements for the flow of data inside the crate and into the sub-farm are considered an achievable objective using current technology. The initial evaluation studies will tell if the same is true in terms of the rate. The operation of the trigger module is a latency oriented problem, an assessment of the achievable performance will be determined by the evaluation work.

The amount of event data made available for monitoring is an external requirement which has to be agreed with the detector groups. Nevertheless one should aim at the maximum achievable without interfering (i.e. creating dead time) with the main data flow. The issue of "sample all" like monitoring, i.e. all events have to be analysed before being removed from buffers, while dealt within the sub-farm remains open, to be addressed during the design phase, at the level of the read-out crate (see also section 3.5).

## 3 Read-out crate and Sub-farm DAQ

### Read-out link

#### 3.1.1 Role

The read-out link will provide the means to move event fragments from the detector front/end (ROD) to the ROB.

\begin{table}
\begin{tabular}{l l l} \hline
**Application Area** & **Final ATLAS DAQ** & **Prototype -1** \\ R/O Link & 100 MB/s bandwidth & Max Achievable \\ ROB Input & 1-2 KB @ 100 KHz & Max Achievable \\ ROB Output (L3) & 1-2KB @ few KHz & same \\ Data Collection (L3) & 10-15 * 1-2KB @ few KHz & same \\ Trigger Distribution & Few tens of bytes at 100KHz & Max Achievable \\ SFI Input & 1-2 MB @ 10-50 HZ & same \\ Monitoring (ROB) & & Max compatible with main data flow \\ Monitoring (Crate) & & Max compatible with main data flow \\ Monitoring (sub-farm) & & Max compatible with main data flow \\ \end{tabular}
\end{table}
Table 1: Performance requirements

#### 3.1.2 Features

Adhere to front-end requirements rules 5.1, 5.2 and 5.3 [3].

It must be a standard, have adequate bandwidth (Table 1 on page 1), flow control and the possibility to send commands which are distinguishable from Data. An in-situ test mode is also required to allow the links to be tested after they have been installed in the DAQ system. Software and hardware link reset is also required. The link interface should conform to the PMC standard to allow different implementations of the R-O link to be evaluated. Electrical or optical versions of the R-O Links may also be useful for some of the Front-End Links.

#### 3.1.3 Selection criteria

In addition to meeting user requirements the R-O links must have a low bit error rate, reliable connectors, low cost and preferably be built and supported by industry. Test equipment and programs must be supplied. Design aids should include emulation modules which allow designers of RODs and ROBs to test their logic without a complete R-O link in place.

#### 3.1.4 Environment for evaluation

In stand alone tests we will measure the performance by transferring data from one memory to another and the bit error rate by using VMEbus based PMC carriers to transmit data to link, receive the data and check for errors. In the prototype -1 environment at the transmitting end we could use a PCI to R-O link or a modified Slate to generate data. At the receiving end of the R-O link we could use a PCI to R-O link interface or the RHBNC LVL2 buffer.

#### 3.1.5 Options

S-link, G-link, HS.

### 3.2 I/O Module

#### 3.2.1 Role

Provide the mechanisms for the: receiving of event fragments on an input channel, buffering the event fragments in memory and transferring of event fragments (or sub-event fragments) from memory to output channels.

Specific instances of the I/O module are the: Read-out Buffer, the Level 3 and Level 2 link modules and the Switch to Farm Interface.

#### 3.2.2 Requirements

Based on an event ID and an event size1, buffer incoming event fragments in free memory. Maintain a data structure which maps the event ID to memory location and subsequently use this data structureto perform event management and service I/O requests with other F/E DAQ modules based on the event ID. Data transfers will be required between an I/O module and:

* the LDAQ for monitoring,
* other I/O modules, e.g. RoB to level-3 link module,
* the trigger module.

In addition to the above, the I/O module will be required to communicate with the LDAQ for the purposes of control (e.g. run control).

Any data corruption should be flagged in an event fragment header and counted locally. It should also be foreseen to time-out during I/O and subsequently flag a time-out condition in partially received/ transmitted data. Fatal errors1 should be reported and recovery actions performed based on external actions.

Footnote 1: This information should be contained in an event fragment header, see section 3.9.

Flexibility is required to allow different schemes (pull or push) and technologies to be implemented. In addition it may be necessary to perform preprocessing within the module e.g. append additional data to the event or re-format the event.

#### 3.2.3 Functionality

To fulfil the user requirements the I/O module functionality should be addressed at three levels: hardware, event management and memory management, see Figure 4 on page 4.

#### Hardware

The hardware upon which the I/O module is based should provide the means for moving data to(from) memory from(to) I/O channels based on stimuli: external events (e.g. interrupts) or internal events (e.g. signals).

The timescale for the realization of prototype -1 and the functionality requirements dictate that the hardware should be based on:

Figure 4: The I/O module functional layers

* Intelligent 6U VME boards,
* the PCI bus as I/O integration bus,
* Support two or more mezzanine cards,
* incorporate a block mover.

Thus VME, P2 technologies and mezzanine cards will provide the physical I/O paths.

Possible options are the: RIO2 8061 from CES and the PPC604/353 from Radstone.

Event management

The event management should manage event fragments. It should provide the co-ordinated access to event fragments arising from multiple I/O requests.

Memory management

The memory management is distinct from the event management and any memory management schemes that may be provided by an operating system. It must manage free memory and service requests for blocks of free memory. Any memory management mechanism (algorithms and data structures) should exploit any regularities in the request stream with the aim of minimising fragmentation and time cost.

#### 3.2.4 Issues

The I/O module, as specified above, requires: concurrent data transfers with low overheads, efficient buffer management and event handling.

To achieve the best I/O performance and maintain flexibility, intelligent interfaces should be studied. These interfaces should be able to perform I/O to or from a buffer without the intervention of the main CPU on the I/O module. The interface should take the form of a mezzanine card which may or may not include some buffering.

The integration bus, PCI, plays an important role and its performance must be understood. In particular its performance with respect to concurrent data transfers, programming overheads and any additional overheads introduced by an operating system.

Current buffer and event management schemes should be evaluated. Based on the evaluation, new methods need to be developed. In particular distributed methods and multi-processor issues. The latter is of particular interest when considering intelligent interfaces. The frequency of memory allocation requests and the length of memory blocks requested are parameters that should be exploited by a memory allocation scheme, these parameters need to be evaluated.

### Data Collection in the read-out crate

#### 3.3.1 Role

The data collection component transfers a sequence of fragments, belonging to the same event, from ROB memories to a memory in the L3 interface module.

#### 3.3.2 Features

Given the role and the performance required (a sequence, 10-15, of small transfers, 1-2 KB each) by the data collection component, as specified in Table 1 on page 1:

* The required bandwidth, of the order of a few tens of MB/sec., can be satisfied today by a system such as VME64.
* The setup time for the transfers should be minimised, so as to optimise the latency of the overall transfer.

The commercial availability of the selected options is also required.

#### 3.3.3 Options

There are three categories of options which may fulfil the required features of the data collection component:

1. Chained block transfers on VME, where the level-3 interface pulls event fragments from ROB's memories into its memory. The chained DMA available with standard BMA interfaces is the simplest mean to achieve data collection, it will be the first option to be evaluated. The development of other chained transfers over VME, such as the slave terminated block transfer, which are still being defined, should be monitored and followed up as potential candidates.
2. PCI based solutions, such as the compact PCI technology, which bridge VME to PCI although not yet commercially available are interesting candidates. This set of technologies should be monitored in view of their evaluation.
3. There are a few other technologies, in particular SCI, FDL, and some P2 systems, such as the Raceway system, which are potentially applicable to the problem of data collection. Their feasibility to the data collection problem needs to be assessed.

#### 3.3.4 Issues

VME as the medium for data collection: performance study of chained DMA and effect of sharing the VME bandwidth with other data transfers (e.g. LDAQ, trigger module), monitor the development of other chained transfers on VME.

Emerging PCI based solutions should be monitored and the applicability of SCI, FDL and Raceway to data collection should be assessed.

The use of a pull or push, with respect to the L3IF module, approach to the data collection should be investigated with the objective of optimising the transfer while minimising the effects on the ROB performance.

### Trigger Module

#### 3.4.1 Role

The trigger module receives information from the level-1 trigger system and distributes this information to appropriate components in the read-out crate. It may be also used, in read-out crate studies, to emulate level-2 functionality.

#### 3.4.2 Features

The trigger module receives:

1. Level-1 identifier, trigger type and BCID from the Level-1 (central trigger processor) via the TTC system.
2. The ROI information, event read (to start a read from ROB's) and event clear commands (to free a buffer in a ROB). The medium to transport such information is, as yet, undefined.

The above information may be distributed (depending on the specific read-out crate implementation) to ROB's, interface modules and to the LDAQ, the latter for monitoring purposes. In general:

* The TTC information may be broadcast to the ROB's for ROB input checking and is stored and made available to the LDAQ for monitoring purposes
* The ROI information is translated and used to find which ROBs in the crate are involved in the event read command.
* The event clear command has to be logically broadcast to all the ROB's in the crate

#### 3.4.3 Suggested Implementation

How ROI information will reach the crate is, as yet, unknown; hence the need to remain as open as possible in the implementation.VME can be used to transmit data from the trigger module to the ROB, the L3IF and the LDAQ.

An intelligent PMC carrier could be used: with one PMC to interface to the TTC system and the second PMC to interface to the ROI distribution system. ROI decoding may be performed by the on board processor, which could also be used for testing purposes.

The complete trigger module can be easily emulated by a VME processor board before the real one becomes available.

#### 3.4.4 Issues

The data rate in the read-out crate, even at the maximum expected trigger rate of 100 KHz, is of the order of a few MB/sec; this makes, from the bandwidth point of view, VME a feasible solution as the in-crate distribution medium.

The performance of the trigger module, however, is mainly latency limited, as it has to perform two inputs, some small amount of calculation and one output within (a few) 10(s of) microseconds. Studies to address the feasibility of fulfilling this requirement are therefore needed.

Synchronisation in the read-out crate (for example between ROB read-out and fragment collection by the level-3 interface) also ought to be studied.

### Local DAQ

#### 3.5.1 Role

The local DAQ is the system component which provides, at the level of a modular DAQ unit (read-out crate or sub-farm), all the high level DAQ functions with the exception of the main data flow. In particular: control, error handling and reporting, support for event (fragment) monitoring and integration with the back-end software. Its functions encompass several elements in a sub-system (e.g. main data flow elements, workstations, etc.) but they are physically located in processors communicating via data and control links with the other components. The coherent operation of a system including several LDAQs is an integration issue to be considered at the design stage.

In the context of the present study the required features of the local DAQ provide constraints and requirements to other system elements such as I/O module, operating systems and processor hardware evaluations.

#### 3.5.2 Features

High level DAQ functions:

1. Sub-system (crate or sub-farm) control: this requires the capability of sending (asynchronous) commands to sub-system elements, receiving command acknowledges and notification of state change from the elements.
2. Initialisation of sub-systems and single elements.
3. Sub-system or element testing.
4. Error notification and handling.
5. Event (fragment) monitoring component with the following characteristics: - Data are pulled from buffers in I/O modules (ROB fragments, crate fragments, full events) - Data are categorised, in terms of types (ROB fragments, crate fragments, fragments prior to/after level-2/3 processing, specially tagged events, etc.) and maintained in a "data base". - Data is distributed to requesting monitoring tasks which may run on the LDAQ supporting processor(s) or on other machines.

- "Sample all" (i.e. all the events must be monitored) support at the LDAQ level is an issue which will be studied at the time of design. While this feature will be provided in the sub-farm by the data distribution system, it has to be understood whether it will also be required at the level of the read-out crate. In this latter case a more complicated interplay between the LDAQ and L3IT components will be required (and may only be achieved with a reduction of the overall rate).

- The same event monitoring component applies to both read-out crate and sub-farm.

The LDAQ provides the interface point to other DAQ software components where necessary (for example the relaxed real-time constraints of the sub-farm may allow running the back-end software directly in the sub-farm):

1. The back-end system, in particular the global control and error reporting components.
2. The data bases to retrieve configuration descriptions and parameters.
3. Status information and statistics gathered from the sub-system elements will be provided to back-end components such as the status display.

#### 3.5.3 Possible Options

While it is expected that common software modules, such as the monitoring component, will be shared between the read-out crate and sub-farm LDAQ's, the supporting hardware, that is processors and the connection between the LDAQ and the sub-system, will vary.

VME single board computers are the proven option for the read-out crate LDAQ processor. Inter-processor communication features such as fifos or mailboxes may provide the control and message path. VME is also a candidate for the data transfer path for event fragment monitoring.

In the sub-farm the LDAQ functions may be supported by a processor participating in the sub-farm or by some external computer sharing the data distribution system with the sub-farm.

The possibility of using PC hardware, with a VME interface in the case of the read-out crate, to support the LDAQ functions is a potentially valuable solution which should be investigated.

#### 3.5.4 Issues

A data link connecting all sub-system elements to the LDAQ processors is required. According to the current figures shown in Table 1 on page 1, the bandwidth required may be larger than the one needed by the level-3 interface. The possibility of sharing the same medium used for data collection has therefore to be carefully assessed.

The control path connecting all sub-system elements to the LDAQ processors requires low bandwidth (a few bytes per message at the rate of a few Hz), low latency (it might even require deterministic behaviour) and minimal impact on the I/O module performance. VME based features such as fifo message queues and mailboxes are potential candidates to be evaluated.

The LDAQ constraints the choice of the operating system in several areas: it has to be capable of running back-end software, it needs soft real-time features (in particular as regards to multi-tasking, priority scheduling and inter-process communication) and may need multi-processor support to distribute an LDAQ over several CPUs.

### Processor elements and operating system environment

#### 3.6.1 Role

Processors with their O/S environment support DAQ functions in different parts of the system: at the level of the I/O modules, the LDAQ, the sub-farm. While the whole contents of this section apply to the read-out crate area, it is recognised that in the case of the sub-farm, due to its relaxed real-time constraints and its wider spectrum of organisation options, the processor elements may not need to fully comply with the requirements.

#### 3.6.2 Features and Requirements

The processor elements will support PCI as an I/O bus and will interface to VME. A mechanism for inter-processor communications, such as fifos and mailboxes, is required to support the messaging aspects related to control and error reporting.

While operating system uniformity (a common O/S everywhere) is a desirable objective, we recognise that it may not be a practical one, due for example to the wide performance and functional requirement differences between applications. In the case of the ROB the feasibility of not using an operating system at all has to be evaluated. A common interface is likely to be the aim which we can best achieve. A sub-set of this common interface, dealing with the critical I/O (interrupt and device programming) aspects of I/O modules, may need to be implemented directly on top of the hardware.

As regards a common operating system interface, the current trend points to (real-time) UNIX systems with POSIX support. The research nature of the DAQ prototype -1 effort makes the use of PC's and/or PC-like O/S such as WindowsNT an appealing option to evaluate (see section 3.8).

Operating system requirements are categorised below on the basis of the application:

1. The operating system environment, at least in the case of the LDAQ and the sub-farm, will have to be compatible, capable of integrating with and/or running back-end (such data bases, run control) and monitoring/analysis software. Portability and compatibility issues between different (flavours of) operating systems and availability of commercial products (possibly selected elsewhere in the prototype -1) are the major issues to be understood.
2. Real-time requirements at the LDAQ level where we have less emphasis on highly efficient I/O and interrupt handling but more requirement for real-time features such as those provided by POSIX 1003.4.
* Software interface to I/O devices (such as VME, PCI devices, etc.): the possibility of using standard drivers, normally delivered with the device interface, should be evaluated in view of the software simplification involved.
* Multi-tasking issues: scheduler capabilities, threads, synchronisation tools (semaphores), inter-process communications features (messages, shared memory, etc.)
* Multi-processor system support. Performance can be scaled by redistributing the LDAQ functions over more processors, either in a symmetric multi-processor system (SMP) or by having a number of individual computers/boards co-operating together.
3. Real-time requirements at the I/O module level, where highly efficient I/O and interrupt handling are the main issues.

* The software interface to I/O devices is a key issue to performance. The use of standard drivers is probably to be ruled out for a direct access (via some sort of low overhead library) to I/O resources and interrupts. The operating system should therefore provide the capability of writing user level device drivers.
* Effect of an operating system on performance: the overhead of running a multi-tasking operating system must be assessed in terms of the time cost involved in e.g. scheduling, synchronisation and inter-process communication.
* Multi-processing at the level of an I/O module is a twofold issue. On the one hand an I/O module may consist of several processors, each assigned one or more I/O functions, co-operating to provide the I/O module functionality. On the other hand several components in the crate (e.g. the pair L3IF and ROB, or LDAQ and L3IF, etc.) may need to co-operate together, synchronisation features at this level are required.
* The selected operating system is required to be ROMable.

#### 3.6.3 Selection Criteria

The aim of the studies are both to discriminate among options to select the one(s) which best fit the applications and to provide useful information (e.g. in terms of performance of various features) for optimal design and implementation.

The choice for the pair processor and operating system will be made on the basis of: price, adherence to required features and performance considerations.

#### 3.6.4 Options

PowerPC VME based boards are considered the best candidates as processors and I/O modules in the read-out crate. Examples are the CES RTPC and RIO II, Radstone PowerPC card, Motorola MVME card. Operating system candidates are (depending on the platform of choice) for example LynxOS and PowerUnix.

The feasibility study for the use of PC hardware and the WindowsNT operating system is the subject of a separate evaluation (see section 3.8).

#### 3.6.5 Issues

Software portability across prototype-1 subsystems and availability of commercial products.

I/O device programming: where standard device drivers can be used, possibility of direct (user level) programming of devices (user drivers).

Feasibility of using an operating system at the I/O module level: the cost of operating system features such as interrupt handling (latency and dispatching), the scheduler, synchronisation and inter-process communication have to be assessed.

Multi-Processor aspects.

### Sub-Farm Data distribution

#### 3.7.1 Role

The level-3 sub-farm must cope with two basic requirements. It should provide enough CPU power to build the high-level information which will allow the level-3 to take the decision of accepting or rejecting the event, and it should moreover insure the appropriate bandwidth for the permanent data recording.

A further requirement stems from the fact that the global data flux which will flow through the level-3 sub-farm will involve data of different trigger types, including events, calibration, monitoring and even alignment data. In this context, it is important to carefully study the data distribution model, in order to avoid the creation of bottlenecks due to lack of resources and to allow at the same time for the maximum flexibility. Unfortunately, at the current stage, not much information is available on the output storage itself, which will depend on the ATLAS data model and its hardware implementation. It is therefore difficult to estimate the constraints coming from this side, but it is also reasonable to say that this will not be an issue for the prototype -1 implementation.

#### 3.7.2 Calibration and monitoring issues

In a vertical slice of the proposed ATLAS DAQ model [1], data coming via the SFI will be distributed to several processors, working in parallel. The activity in this level-3 sub-farm needs to be monitored at the input and at the output stages, to check the performances of the level-3 algorithms and the integrity of the data themselves (DAQ monitoring). This is one of the tasks of the level-3 supervisor. In addition, hooks must be provided at both stages to allow the monitoring, on request, of the sub-detector activity. This will be typically done using programs different from the main filtering one. Those programs will be under the control of the sub-detector groups, and will have the tendency of being updated more often. In this respect, it is envisageable to avoid keeping these programs on the same set of processors which takes part in the main filtering activity, but to run them on different processors (workstations) which could be geographically located elsewhere.

For the calibration data, of course input is needed from the sub-detector groups in order to fully understand the impact of this data flow in terms of size, frequency and computational speed. However, already at this stage, we can assume that several types of calibration data should be foreseen, where high level calculations could be needed on subsets of the total number of events, or where large samples of specific data will be processed through dedicated programs. In general, it is likely that for the calibration data a more complicate interplay between producers and consumers will be needed, if compared to the standard filtering activity. We should consider, for example, the possibility of dynamically assigning processors of the sub-farm, to perform calibration tasks during normal data taking. This option, while increasing the flexibility of the system and reducing the need for run restarting, might impose additional constraints on the event builder, which should be capable of routing tagged events to a specific sub-farm.

Given the complexity of the ATLAS detector, it is also possible that some sub-detectors will need alignment data to be processed in the level-3 farm, to allow for a quick response about the geometrical integrity of the data taken.

#### 3.7.3 Sub-farm organisations

Needless to say, the unknowns related to the evolution of the computer technologies forbids any decision on a detailed implementation scheme to be taken now. At this stage we should concentrate on the choice of the sub-farm organisation better suited to the requirements of the sub-farm system. An essential input to this work is a careful monitoring of the status and the trends of the computer market. We plan to evaluate the following possibilities:

* SMP (Symmetric Multi Processor), where processors are sitting on an internal bus at high speed (for example, 960 Mbytes/s peak transfer rate for the HP K series) and all see the (shared) core memory where events are buffered into (and out from). This solution could simplify the whole farm architecture, acting as a (partial) replacement of the SFI and the buffer manager at the price of having a closer interaction with the Operating System. In this respect, also the output stream operations could be easier to handle.
* Processors (cards) sitting in racks could be considered as a variant of the above. It is possible that those processors might be clusterized, and the choice might depend on the cost effectiveness of the solution.
* Processors (workstation or PC) in network, which will likely form a cluster for each sub-farm. The event mapping in this case could be done via message passing or shared memory, depending on several requirements, like latency, O/S characteristics, complexity of the farm activities. The global synchronization is certainly an issue in this context, as it is the choice of the network protocol to be used (FDD/FCC/ATM).
* MPP (Massive Parallel Processing) is clearly one of the most important (and currently most expensive) market trends. Practically all the computer firms are selling already now some implementation of this architecture. Even if it might be premature, if seen as the implementation for the prototype -1 project, the study of MPP is mandatory in the more general ATLAS context. It should be mentioned that at CERN NA48 is already writing and processing data on the CS-2 machine based on 100 MHz Hyper Sparc processors, and SP2, based on RS6000, will be the CERNVM replacement. It should be also remembered that CRAY T3D is using DEC Alpha 21064 chips (and plan to use the new 21164) and that HP has recently acquired Convex and is currently selling the SPP1200/XA, a 128 nodes machine based on the PA7200 chip.

#### 3.7.4 Issues

The organisation of a level-3 benchmark suite based on a realistic ATLAS analysis program (e.g. ATRECON).

The definition of a suitable skeleton data distribution software which can be implemented on different farm organisations (e.g. SMP and workstation cluster).

The evaluation of sub-farm organisations based on the two above items.

### PC and WindowsNT

#### 3.8.1 Rationale

Given the strong research flavour of the prototype -1 effort and the relatively long time scale for the final ATLAS DAQ system, it is considered important to explore the applicability of personal computer (PC) hardware and related system software to DAQ problems. Success would allow DAQ applications to exploit a market of cheap hardware and a huge base of software products.

While the general use of PCs for HEP applications, such as offline farms, is already a subject which is being studied, we aim at studying the integration and performance issues in suitable DAQ applications. The LDAQ processor and the sub-farm are the two most promising, at this moment in time, areas where PC hardware can be assessed.

WindowsNT, unlike Windows 3.x, is a modern and well-designed operating system with a long term support commitment by Microsoft. Given that WindowsNT is a multi-platform operating system, i.e. in addition to Intel-based ix86 processors it also runs on RISC processors such as MIPS, PowerPC and Alpha, its applicability to DAQ problems is an issue worth study in itself; with for instance PowerPC VME boards as supporting hardware. The proprietary nature of WindowsNT is, in our open environment, not an advantage.

#### 3.8.2 WNT Features

WindowsNT is a general-purpose operating system which although not designed for real-time (in particular is not romable nor capable of running diskless at the moment) offers a number of important features.

* It is a pre-emptive multitasking and multi-threaded operating system. The basic scheduling items are threads and not processes. As a consequence the context switch overhead is expected to be relatively low.
* It offers symmetric multi processor (SMP) support: the system allocates a number of threads to a number of CPUs in a transparent way and in particular interrupts can be handled by idle processors.
* It supports 32 priority levels which can be assigned to specific threads. 16 of these levels are fixed priorities reserved for real-time classes of applications.
* An application process can lock itself into memory
* It supports asynchronous I/O (VMS style notifications)

#### 3.8.3 Issues

The main issue related to the use of PCs is the one of integration with other part of the DAQ, in particular in the read-out crate.

WindowsNT will have to meet the requirements of the LDAQ component and its associated timing constraints as described in section 3.5,for the LDAQ features, and section 3.6 for details of the major requirements for the operating system. Tasks which traditionally are supported by a real-time operating system, such as OS9 in the past and real-time UNIX systems today.

It is therefore necessary to answer the following questions:

* I/O integration issue: how feasible, economically, technically and performance wise, is the integration with the rest of the sub-system (e.g. the read-out crate)?
* Real-time issue: is WindowsNT suitable to real-time operations?
* Code (and human) portability issue: how feasible it is to have UNIX-like (which for some time will remain the main platform) and WindowsNT based processors to co-exist in the same system?

### Event Format

From the general point of view the data format should fulfil the following requirements:

1. The format should reflect the hierarchical structure of the DAQ.
2. It should provide the same organisational view of the event at any point in the system.
3. It should allow insertion of non-detector data along the path from ROB to mass storage, such as information and data produced by triggers.

The level of compatibility with and efficient conversion to the format used by the offline software have to be understood.

With respect to the ROD to ROB transfer:

1. To simplify the read-out and buffer management in the ROB, the transfer size should be communicated first by the ROD, i.e. it should be part of the header. The consequences of a transmission error affecting the word containing the size should be understood.
2. A few more fields are also required in the header produced by the ROD: event identifier, BCID, event type (e.g. calibration data, etc.)

## Part II Work Plan

### 4 Introduction

Based on the system components and their relevant issues identified in part I, we list the study and evaluation activities we consider necessary to select elements and complement the design phase. The activities are organised in terms of the system components of part I, in turn grouped by major function.

While this document presents the generic outline of the activities, the detailed planning will be developed, as the first step of an activity, by the people involved.

Since most if these activities are preliminary to or complement the design phase, their indicative time scale will range, depending on the particular activity, between the beginning and the end of the summer.

### 5 Main Data Flow

#### Read-Out Link

The required function of the Read-out link calls for the study of components which would allow us to design a link capable of sustaining 100MB/sec. These components will be studied in a well defined environment which would allow both electrical and optical version of a Read-Out link to be evaluated.

The links will be interfaced to the PMC to allow integration into the functional vertical slice of the ATLAS DAQ architecture. In addition some components developed in the context of the LVL2 trigger (e.g. SLATE) may be adapted to emulate data.

Test equipment for the links must also be studied.

#### I/O Module

The basis for the I/O module is a PowerPC based VME board with at least two PMC slots; the RIO2 8061 from CES and the Radstone PPC604/353 are the most suitable options today.

To respond to the main required features of an I/O module: low latency I/O transfers, buffer management and efficient "event" handling, we indicate a list of studies to evaluate hardware solutions and to understand the influence of software on hardware performance. Such studies will be performed on the board options defined above.

Generic board studies:

* PCI performance measurements and understanding for single source transfers, concurrent transfers from multiple sources as well as possible interference between transfers.
* Hardware layer: performance of basic hardware resources (such as BMA, VME, interrupts, fifos, mailboxes)
* LDAQ connection (control, error, event monitoring): the feasibility of using VME should be studied. This also includes the evaluation of VME based resources (such as fifos or mailboxes) for control and error handling purposes.

I/O layer:

* Performance of specific PMC/PCI interfaces such as those for the fiber channel or the ATM links.

Intelligent interface:

* Assess the need for a PCI/PMC board (typically for the ROB) including an input (or output) interface, buffer and a processor. Understand the implications, in particular in terms of synchronisation and data access (both within and from outside the ROB). Analyse the problem from the point of view of the currently planned options for the development of an intelligent interface: respectively the proposals from the UK-Level2 community, Saclay and Nikhef.
* Investigate the possibility of using e.g. the Midas product as an "hardware model" of an intelligent interface. Build, if feasible, such an hardware model on the basis of the results of the previous point.

Software oriented studies.

* Understanding and performance evaluation of the basic I/O software layer (PCI programming, interrupt handling, etc.), with the objective of minimising the overhead related to I/O and event handling.
* Study the effect of an operating system on I/O module performance: for example on interrupt latency, task dispatching, context switching, synchronisation, etc.
* Study the buffer management problem: choose and implement a buffer management technique, measure the timing of elementary buffer management operations (e.g. get buffer), understand the issues related to co-operating multiple processors. This latter both for processors co-operating on the same board (e.g. in the case of an intelligent input interface on the ROB) and in the case where processors on different boards (e.g. ROB and L3IF) need access to the same buffer space.

Modelling activities.

* Produce models at the component level both for the generic I/O module and for specific instances (e.g. ROB). The objectives include: problem understanding, modelling of alternative implementations unavailable in hardware, performance model of implementation. Parametrisation will be provided by the results of the studies listed above.

### Data Collection

The major issue identified in relation with the data collection component is that of minimising the setup time and protocol handling for a sequence of small transfers.

Study the simplest option: the use of a chained DMA on VME. Evaluate the achievable performance, understand the interference with other VME transfers (e.g. LDAQ monitoring) and the effect on the ROB performance.

Monitor other emerging VME based techniques for data collection: slave terminated transfers on VME, compact PCI, etc.

Assess the possibility of using another, non VME based, technology such as FDL, SCI or a P2 system (e.g. Raceway).

Understand the need and suitability of using a protocol, such as MPI, for data collection.

Study the push and pull, with respect to the L3IF module, data transfer options and investigate the need for models.

### Trigger module

The proposed implementation of the trigger module is based as the I/O module on a two PMC PowerPC board. Therefore several of the studies listed in section 5.2 apply to the trigger module as well.

The required function of the trigger module imposes the study of the problem of an "I/O module" performing 2 short inputs, from two different PMCs, some small calculation followed by its output on VME within (a few) 10(s of) microseconds.

Study the issue of synchronisation within the read-out crate, in particular with respect to the transfers between ROBs and the L3/L2 link modules.

### Event Format

Define read-out requirements for event format.

Investigate general requirements for event format in collaboration with ATLAS offline; given the likely use on different processor architectures, the implications of the little/big-endian byte ordering conventions should also be studied.

## 6 Processors and O/S

Processor boards: evaluate hardware features, such as the performance of the VME interface and the PCI sub-system. Current options are: the RTPC 8067 from CES, the PPC604/353 from Radstone, the MVME 1604 from Motorola for LDAQ supporting processors, the RIO2 8061 from CES and theRadstone PPC604/353 as I/O module supporting board. The evolution of the market for PowerPC based VME boards should nevertheless be monitored.

Operating system: define a set of micro-benchmarks to evaluate functionality and performance of individual O/S features, this should also include a benchmark suite for POSIX features.

Port a full DAQ system for the most interesting candidates (to be determined on the basis of the single feature evaluation process), to fully evaluate board and O/S features in a real-life context.

## 7 WNT and PC

The local-DAQ application provides the initial context for the evaluation of PC's and WindowsNT. Future candidate applications could be the SFI or the sub-farm processor element.

PC issues: study the integration of a PC with the rest of the read-out crate. This in particular includes the interface between PCI and VME or some other bus (e.g. a VICbus). A survey of available options should precede the selection of suitable candidates for which an I/O performance study will be done. A proper I/O study environment should be defined for this latter purpose.

Generic WindowsNT: along with a general evaluation of the WindowsNT features and programming interface, some emphasis should be given to a study of the POSIX interface, to I/O integration, e.g. VME drivers, and the related performance (interrupt latency, task dispatching, context switching, I/O transfers). A relevant subset of the real-time benchmarks defined by the local-DAQ application should also be run. Port of the Spider DAQ to WindowsNT (either on a PC or on a VME board); this will provide both a real-life test and an environment to study portability issues between UNIX and WindowsNT.

WindowsNT on VME board: make a survey of available ports, in particular and availability and the features of the ports for the PPC604/353.and the MVME1604 should be investigated.

## 8 Sub-Farm

It is suggested to continue the work planned by G. Fumagalli [4] and use the ATLAS reconstruction program ATRECON as a tool for evaluation. In this context, one could generate Monte Carlo events and implement this program (may be preceded by some trigger simulator like ATRIG) as a consumer of a suitable data distribution system on the different farm organisations. The degree of complexity can then be increased by adding the issues on calibration and monitoring to this schema. This work will allow studying issues such as processor farm organisation in particular with respect to the impact of offline algorithms.

## 9 References

[1] ATLAS technical proposal. CERN/LHCC/94-43

[2] ATLAS LVL3/DAQ meeting 28-11-95. T/DAQ TR-327

[3] ATLAS T/DAQ Steering Group, Trigger and DAQ interfaces with Front-End Systems - Requirements Document

(Version 1.0). [http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/FRONTEND/freq.ps](http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/FRONTEND/freq.ps)

[4] G. Fumagalli et al., Performance Increase in Level-3 Trigger Software, RD13 Note 161. [http://rd13doc.cer.ch/public/doc/Note161/Note161.html](http://rd13doc.cer.ch/public/doc/Note161/Note161.html).