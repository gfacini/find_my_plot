**The Atlas Software Process**

S. M. Fisher, _RAL_, K. Bos, _NIKHEF_, R. Candlin, _Edinburgh University_

14th June 1996

###### Abstract

We propose a model, the Atlas Software Process, to describe the way in which software development for the Atlas experiment should be organised. The Atlas Software Process defines the sequence of activities that take place during the lifetime of the project, the roles that must be filled in order to implement the Atlas Software Process and the deliverables that will be produced in the course of software development. We propose an evolutionary model, with short cycles of development followed by a review of what has been produced, so that management gets rapid feedback on the progress of the project. A particular concern is to ensure that good quality and easily maintainable code is produced. For this reason, considerable emphasis has been placed on the necessity for documentation and coding standards.

This document exists both as html and as postscript.

$Date: 1996/06/07 16:54:38 $

$Revision: 1.11 $

###### Contents

* 1 Introduction
	* 1.1 What is a "software process"?
	* 1.2 Relevance of a software process to Atlas
* 2 Overview of the Atlas Software Process
	* 2.1 Evolutionary development and incremental delivery
	* 2.2 Deliverables
	* 2.3 Walk-throughs
	* 2.4 Reviews
	* 2.5 Inspections
	* 2.6 Problems with input deliverables
	* 2.7 The project diary
	* 2.8 Choosing between designs
	* 2.9 Quality Assurance
* 3 Preparation
	* 3.1 Gather key user requirements
	* 3.2 Construct support team
	* 3.3 Perform exploratory studies
	* 3.4 Construct test plan
	* 3.5 Construct configuration management plan
	* 3.6 Plan training
	* 3.7 Select development tools
	* 3.8 Appoint domain architects
* 4 Prepare for Cycle
	* 4.1 Analyse overall problem domain and existing solutions, prepare high level Use Cases, and revise User Requirements
	* 4.2 Define architecture
	* 4.3 Define objectives for this cycle and sketch out future cycles
* 5 Design software to meet objectives
	* 5.1 Analyse problem domain and existing solutions
	* 5.2 Prepare Use Cases
	* 5.3 Define architecture for the domain
	* 5.4 Produce dynamic models
	* 5.5 Write user documentation
	* 5.6 Define test plan for this domain
* 6 Write Code
* 7 Integrate code and test
* 8 Evaluate new version of system
	* 8.1 Inspect new system
	* 8.2 Customise and refine this ASP definition
* 9 Go round again
* A Reviews
* A.1 Procedures common to all reviews
* A.2 Review of a User Requirements Document
* A.3 Format of an reviewer's report
* B Roles
* C Summary of deliverables
* C.1 Principal reviewed deliverables
* C.1.1 User requirements
* C.1.2 Use Case descriptions
* C.1.3 Architecture
* C.1.4 Cycle plan
* C.1.5 Nested state charts
* C.1.6 Detailed design
* C.1.7 Code
* C.2 Reviewed deliverables with undefined format *
C.2.1 Exploratory studies document C.2.2 Development tools document C.2.3 Problem domain and existing solution analysis C.2.4 Training plan C.2.5 User documentation C.2.6 Configuration management plan C.2.7 Test plan C.2.8 ASP documents C.3 Deliverables which are not reviewed C.3.1 Reviewer's reports C.3.2 Consolidated log of reviewers' reports C.3.3 Producers' response to reviewers' reports C.3.4 Moderators summary of review discussion C.3.5 Test results C.3.6 System report C.3.7 Metrics report C.3.8 Roles list D Rules to be followed by all documents (DOC-RULES) D.1 Document Structure D.2 Notations D.3 References E Rules for user requirements (URD-RULES) E.1 Document Structure E.2 Relevance to the Application E.3 Consistency E.4 Quality Requirements for System Properties F Glossary

[MISSING_PAGE_EMPTY:4]

Introduction

### What is a "software process"?

A software process defines the way in which the development of a software system is to take place and specifies standards for all deliverables of the system.

Software projects of any size are notoriously difficult to control, and it has been recognised for some time that their success will depend on the way in which they are managed. It is useful to characterise the sophistication of management procedures in terms of Humphrey's _software process maturity framework[1]_. This distinguishes five maturity levels for the process of software development in a given organisation.

1. Initial: few procedures are defined and success depends on the initiative of individuals.
2. Repeatable: basic management processes are in place for planning and tracking a project. A similar, subsequent project could be carried out with the same degree of success by using the same management procedures.
3. Defined: the management processes and form of the deliverables are defined, documented and standardised throughout the organisation.
4. Managed: quantitative measurements are collected of the progress of the project and of the quality of the system deliverables. These are used to control the evolution of the project.
5. Optimising: the process itself is adapted to improve project control and the quality of the deliverables.

The benefits to be expected of moving up the maturity level scale are better project control and better quality software. All members of the development team know what they have to do, and can see how their contributions fit in with overall goals. Problems are picked up and tackled earlier. Better estimates can be made of time-scales and resource requirements.

### Relevance of a software process to Atlas

Improved project control and good quality products seem like a good idea in any environment, but we have to see if software process models designed for industrial organisations are also appropriate for a large particle physics collaboration. It is clear that there will be overheads in defining and running the software process. Individual developers may find it troublesome to have to conform to standards for design and code, and may resent some of the quality control procedures. We have to be convinced that there is a very high probability that having a defined software process will bring overall benefits!

Atlas is widely distributed geographically with a great many groups at different institutes and it is expected that few groups will have more than two or three people working on the software. In this paper, we suggest a software process model which we think is appropriate for Atlas. It is based on the idea of _evolutionary development_[2], with rather short development cycles. We shall therefore get very rapid feedback on the way in which the project is progressing and be able to update estimates of time-scales and resource requirements as we go along. Since the Atlas software will be developing continually, and will have a lifetime of many years, we hope that the time period will be long enough to pay back the extra costs due to new quality assurance procedures. The process defines a number of deliverables. The existence of these clearly defined deliverables allows individuals and small groups to be effective, without needing day to day interaction with other developers. Though other particle physics projects may choose not to adopt our proposed process exactly, they should be able to benefit from the generally better understanding of the dynamics of software development that we shall have, by then, achieved.

It is proposed that we should aim at maturity level 4, with procedures in place for quantitative process management and quality management of deliverables. We shall thus have mechanisms in place for moving to level 5 (optimisation of the software process itself) after the project has been running for a year or two.1 During the first years of the project, it should be possible to make increasingly better estimates of the resources (both time and people) required to implement software modules. But probably the main benefit would be obtained after the first complete version of the software had been released, when standardised, good quality software should lead to much lower maintenance costs.

Footnote 1: When we refer to levels 4 and 5, we simply mean _managed_ and _optimising_ processes respectively without implying anything about the very formal SEI definitions.

## 2 Overview of the Atlas Software Process

The Atlas Software Process (ASP) is built on ideas in _Towards a Software Development Process for Atlas_[3]. It is rather lighter than the PSS-05[4] of the European Space Agency. There is an initial preparation stage, as shown2 on figure 1.

The main tasks to be performed in this preparation stage, which is launched by the project manager, are collecting key user requirements and finding people to act as chief architect, as domain team architects and to fill the roles in the support team.

Development goes in cycles as explained in section 2.1 The core team starts the first cycle by modelling and identifying domains, and defining the overall system architecture.

After this, the objectives for the first cycle are defined and then agreed with the domain teams who then go into their first cycle. The domain teams, each led by its domain architect, design the software to meet the objectives, and write and test the code. Integration tests are then carried out and a new version of the system offered for inspection. If it is accepted then they agree upon the objectives for the next cycle and go round again.

A supportive management is essential to provide inspiration, to obtain money, people, office space and infrastructure, and to ensure that the work is properly focused.

The following subsections explain about cycles, deliverables and how they are walked through, reviewed and inspected and mentions the project diary, how to choose from competing designs and a paragraph on quality assurance.

### Evolutionary development and incremental delivery

We will develop in a number of cycles of _fixed duration_ (probably 8 working weeks3, i.e. 5 cycles a year) where each cycle will build on previous work to deliver a new working system of _production quality_. The system will be made up of a set of defined deliverables as discussed in Section 2.2 below. At the end of each cycle any problems should be apparent, so helping us to manage the risks. The software system will be built and delivered incrementally with milestones measuring completion of demonstrable software, not designs.

Footnote 3: This time may be too short if many people are not working 100% of their time on Atlas software development. The time will be adjusted in the light of experience.

The software process is the design process and the management process. The management process will be formal, cyclic and evolutionary, whereas the development process, within each cycle, will be informal and with the minimum of constraints upon the developers[5]. Note that the core team as defined here perform both management and development roles.

Cycles will be planned to allow progress within each domain as early as possible so that we will get a complete system at an early stage.

Figure 1: DFD showing the Atlas Software Process

### Deliverables

The deliverables serve not only to allow development teams to define their interfaces cleanly but also to act as internal documentation which is especially useful when someone is new to a domain, and to allow the monitoring of the progress of the project. The definition of the deliverable in conjunction with the definition of the review process is meant to ensure that all deliverables are useful. Of course the deliverables of code and user documentation are the most important. The deliverables are all listed in Appendix C. As far as possible the deliverables are defined in terms of the notation of the Unified method[6].

### Walk-throughs

The walk though is an entirely informal developer driven activity where he4 explains some part of the software to his colleagues and seeks comments. It has no deliverables.

Footnote 4: Throughout this document ‘he’ is used as a gender-less pronoun

### Reviews

Almost all deliverables should be reviewed. A review is more formal than a walk-through and has some deliverables. The ASP relies upon processing input documents to produce some output documents. The review is to ensure that a reasonable job has been made of that transformation. The review process is described in Appendix A.

### Inspections

The inspection is a formal procedure for accepting or rejecting a piece of work. It will be done on a sampling basis to ensure that standards are maintained.

### Problems with input deliverables

When designing one deliverable to be consistent with its inputs, a developer my consider that the input document is clearly wrong, and he may well be the author of that document, however he is not allowed to go back and change it. Rather he should suggest the change for the next cycle5, and if he feels that the agreed input document is unworkable, ignore it, with a justification in the output deliverable which can then be considered by the reviewers. The reason for this is that the input document has been approved and it would slow things down too much to have to go back.

### The project diary

A project diary will be used to record all important decisions along with an explanation. There will be diaries for the core team, for the support team and for each domain team. These will be electronic and will not be classed as deliverables nor will they be inspected nor reviewed.

### Choosing between designs

It may happen that members of the core team or of a domain team produce more than one competing design, and the leader of the team feels unable to select one. In this case, when a team cannot decide internally which design to choose then the matter must be handled by a specially convened committee. This committee will be appointed by the collaboration in consultation with the project manager. It will consider the results of a review, where each design has been reviewed by a team which includes members responsible for the competing designs. The committee may decide to allow multiple designs to be pursued (perhaps just for a limited period) or decide which design should be chosen. Such a procedure is likely to promote bad feeling, and cause delay, and so should be avoided where at all possible.

### Quality Assurance

The whole point of the ASP is to improve the quality of the software, however we need some way of measuring the quality of the produced software, and ensuring that software is of the desired standard.

One such mechanism has already been mentioned, the review of all deliverables, and the inspection of a sample of them. This should ensure that the software system corresponds to the user requirements. However it does not guarantee that it works well.

We have several rules sets for deliverables as appendices to this document, these are based on various authors and on our own prejudices. We hope to identify useful software metrics that might give an indication of quality. For example, classes with a very large number of methods, or very long methods, might be more bug-prone and more difficult to maintain. The "metrics collector" has the responsibility for collecting the data and providing feedback, both to the developers and to those maintaining the ASP rule sets.

## 3 Preparation

A core team is started with the _project manager_ who is responsible for planning, reporting and coordination and his _chief architect_ who is responsible for making the thing work. The chief architect should be able to have an overview of the whole problem area, be able to partition it in domains andknow about the relationships between them. The collaboration should find people to fill these two key roles. All roles are defined more fully in Appendix B.

The preparation work, which is summarised in figure 2 is normally only done once during a project6. However some of the deliverables are sure to need updating later. The figure indicates that domain architects are chosen after the first set of key user requirements have been collected. In reality some domains will be recognised very early, along with a domain architect and he can be expected to take a lead in collecting requirements for that domain.

Footnote 6: This work has already started within Atlas, however many of the deliverables are missing.

### Gather key user requirements

A list of key user requirements should be collected; more will be added later. This work is coordinated by the project manager and the chief architect and carried out by domain specialists--who might well become domain architects-and user requirements monitors who are nominated by the collaboration. The requirements fall into three groups:

**Functional requirements**: This should define the boundaries of the system to be built: what goes in and what comes out but not the internals. The job is not to build a model but just to list, probably in simple English, what the system must do, what it should do, and what it would be nice for it to do. Requirements must be precise.
**System properties**: These are the properties which the resulting system should have, for example performance numbers. There will be a set of numbers associated with each property according to the necessity of the property.
**Constraints**: These are distinct from the functional requirements. Often hardware or software constraints are imposed for example the Atlas decision to standardise on Unix.

The user requirements should be structured into domains with one set of top level requirements which are not part of any one domain7.

Footnote 7: Further substructure of requirements within a domain may be useful.

Deliverables are:

* User requirements (C.1.1)8Figure 2: DFD showing the preparation stage

### Construct support team

People are needed to fill various roles. In some cases more than one person will be needed, and in others a person may fulfil more than one role. The roles are defined more fully in Appendix B. The _documentation coordinator_ is responsible that all documentation is available and up to date; the _repository manager_ is responsible for all the files and their organisation; the _metrics collector_ is responsible for defining a useful set of metrics and making sure that the information is collected; the _toolsmith_ is responsible for making sure that we have the tools we need; the _software process monitor_ ensures that standards are being followed and the _chief tester_ oversees testing. The project manager is responsible for finding people to fill these roles.

Deliverables are:

* Roles list (C.3.8)

### Perform exploratory studies

Prototype work might be carried out in critical areas. The work is organised by the chief architect but carried out by domain specialists.

Deliverables are:

* Exploratory studies document(C.2.1)

### Construct test plan

This activity is led by the chief tester in consultation with the chief architect. The global test plan must allow system testing, testing of individual domains and testing of classes to be carried out.

Deliverables are:

* Global test plan(C.2.7)

### Construct configuration management plan

The repository manager is responsible for the overall structure of the repository and he must produce a plan at this stage explaining how it will all work. He should take note of the requirements of the chief architect.

Deliverables are:

* Configuration management plan(C.2.6)

### Plan training

The project manager should construct a plan to provide sufficient trained people to fill all jobs for the project. Some retraining during the project will be required as will training for new people as they come in. Deliverables are:

* Training plan. (C.2.4)

### Select development tools

Tools must be selected and notation agreed upon. Items to be selected include: methods, CASE tools, languages and configuration management tools. This work is led by the toolsmith, taking note of the requirements of the chief architect. Deliverables are:

* Development tools document. (C.2.2)

### Appoint domain architects

A domain team will be responsible for a domain and will be led by its architect. The team may choose to build throw-away prototypes from time to time to try things out. The team is responsible for the complete life-cycle of that domain (specification, design, code and testing). The domain team architects are ex-officio members of the core team and should be appointed by the project manager in consultation with the chief architect and the collaboration. Some domains will be fairly obvious from the start, others will emerge after trying to do the domain decomposition, and some might appear (or disappear) several cycles down the line. This means that here too we can expect to see domains and therefore domain architects added and removed as necessary.

The full set of roles of the ASP are summarised in figure 3 and defined more fully in Appendix B.

## 4 Prepare for Cycle

This is the beginning of each cycle of development and is shown in figure 4. On the second and subsequent cycles the amount of work to be done will be much smaller because the overall architecture will be known and it should only need refining. The steps must be carried out in sufficient detail to be able to break the problem up and to get the architecture right but not to go inside any domain. The next step "Design software to meet objectives" described in section 5 has many similar processes but is carried out by several domain teams, each working in its own domain.

Figure 3: Class Diagram showing the roles in the ASP

Figure 4: DFD showing the preparation for a cycle

Analyse overall problem domain and existing solutions, prepare high level Use Cases, and revise User Requirements

We have to understand the problem domain. Much of this work will already have been done, for example the descriptions of detectors. Existing solutions should also be analysed.

Analysis is strongly coupled to requirements capture and it is likely that the two activities may benefit from each other and should be worked on in parallel. It is unlikely that this analysis would be revised after the first cycle.

Problem domain analysis and analysis of existing solutions are not necessarily object oriented as much of the world doesn't behave like objects sending messages to each other.

Use Cases are examples, recorded as simple text, of what a system might do. It is not practical to write down all Use Cases but rather to concentrate on the most illustrative or critical ones.

With this analysis and the Use Cases the user requirements can be revised. The user requirements monitors should ensure that the requirements as written down correspond to what is needed. Formal approval of the requirements is by the collaboration. During a cycle new requirements might be introduced for consideration at the next cycle.

Deliverables are:

* Overall problem domain and existing solution analysis (C.2.3)
* Use Case descriptions (C.1.2)
* User requirements (C.1.1)

### Define architecture

The purpose of this is to define domains by defining their interfaces. Domains are represented by category diagrams and the classes which are visible outside the domain are identified in category interface diagrams. These classes only show the interface, and not any private data. For each Use Case an Object Message Diagram and/or a Message Trace Diagram should be drawn9.

Footnote 9: It may turn out that there are services offered by classes which are not required by any use case. It is not yet clear whether it is better to add to the set of Use Cases or to accept this situation.

Deliverables are:

* Overall architecture (C.1.3)

### Define objectives for this cycle and sketch out future cycles

The core team plan objectives for the first cycle and sketches out the next cycle. The domain architect of each domain should make estimates of what can be done in this and the next cycle for their own teams. Because future cycles are sketched there is an opportunity for re-estimating. The requirements should be checked to ensure that the functionality will be provided by the desired cycle.

Deliverables are:

* Cycle plan (C.1.4)

## 5 Design software to meet objectives

This part of the process is shown in figure 5, and is very similar to the first part of the preparation for the first cycle except that in that case the aim was to define the domains and their interfaces, but here the aim is to design the software within a single domain. Each domain team works to develop software for its own domain.

### Analyse problem domain and existing solutions

This time the analysis is confined to the single domain but otherwise the process and the deliverables are as explained in section 4.1.

Deliverables are:

* Problem domain and existing solution analysis (C.2.3)

### Prepare Use Cases

Use cases should be prepared as examples of system behaviour within the domain. Often the use cases will be showing details of high level use cases defined in section 4.1.

Deliverables are:

* Use Case descriptions within the domain (C.1.2)

### Define architecture for the domain

The major architectural decisions will have been taken in section 4.2. Categories are identified within a domain. If it appears that these categories are better treated as separate domains then they should be split by the core team at the next development cycle10. If the domain is split into categoriesFigure 5: DFD showing the design process of a domain team

then category diagrams, category interface diagrams and OMDs should be drawn for each category.

In addition for each category (maybe just one) the workings of that category must be shown by class diagrams and OMDs.

Implementation of object linkage must be considered. Normally sub/supertype relationships will be implemented by inheritance, but this need not be the case. Other questions to be decided for each association are: to use value or reference, to have exclusive or shared possession, and for the association to be dependent or independent. In addition a choice has to be made for a deletion strategy and how to implement associations (especially for the symmetric cases).11

Footnote 11: It is highly desirable for a CASE tool to be able to omit a lot of this detail when desired.

The deliverables are:

* Architecture for the domain (C.1.3)
* Detailed design of all classes in the domain (C.1.6)

### Produce dynamic models

At this stage those classes which do not always behave the same way when they receive a message, because they carry state information, are described by means of state diagrams.

Deliverables are:

* State diagrams (C.1.5)

### Write user documentation

Deliverables are:

* User documentation for the domain (C.2.5)

### Define test plan for this domain

Deliverables are:

* Test plan for the domain (C.2.7)

## 6 Write Code

All services specified in diagrams and text must now be implemented in code, which should include preconditions and postconditions.12 Deliverables are:* full class implementations--i.e. code (C.1.7)

## 7 Integrate code and test

Deliverables are:

* test results from single domain (C.3.5)
* test results from integration tests (C.3.5)

## 8 Evaluate new version of system

The new version of the system must be examined to see if it is better than the old system, and improvements to the ASP should be considered. The domain teams should have worked together in integration testing so that the whole system works.

### Inspect new system

The core team ensures that the new system is inspected before going into production. This inspection ensures that all the correct deliverables have been produced. As far as possible checks should be mechanised--self testing classes, coding rule checkers and regression testing. It is not practical to examine all the deliverables in detail, but a random selection should be inspected manually by the core team to ensure that standards are maintained. In rare cases it may turn out that the new system is not of acceptable quality. The core team would generally still accept it as a new base line to work from, but must take steps to avoid it happening again. In particular submitting deliverables which do not pass the mechanical tests is clearly unacceptable. Deliverables are:

* Core teams report on the new system (C.3.6)
* Metrics for this cycle (C.3.7)

### Customise and refine this ASP definition

This document describing the software process will be treated like any other document of the process. It will be regarded as part of the process and not part of some larger organisation, so that two projects within the same organisation may have a slightly different software process. Naturally those concerned for the process within a project will watch the evolution of the processes of other projects--which is easy if they are documented. To assist further in this process of _Software Process Improvement_, we hope that someone from outside Atlas will monitor our progress and our process and compare it with those of other projects. Deliverables are:* Revised version of this document (C.2.8)

## 9 Go round again

One day the software will be finished. If this day has not yet arrived then

goback to Section 4 to prepare for the next cycle. In this next and all subsequent cycles the work ofthe core team should be moving from high level

design to management of the domain activities. Many revisited processes

will require little or no work. However, as all deliverables are part of the

process because they are considered to be useful, some care will be needed

to ensure that the less frequently modified deliverables are maintained.

Reviews

The review procedure is described here. The first subsection defines a procedure which is common to all reviews and it is followed by subsections defining special procedures for certain kinds of review.

### Procedures common to all reviews

Reviews are carried out by peers, so the core team will review its own work and the development teams their own work--though it would be useful to have at least one person from a different team in each review to ensure consistency of approach. The person whose work is being reviewed is known as the producer; there may be more than one. The reviewers reports go to the moderator who produces the consolidated log of all issues brought up by the reviewers and so ensures the anonymity of the review process. The process is based on a scheme by Humphrey[7] but modified so that it can be carried out by e-mail and WWW. Note that when reviewing some documents, for example the User Requirements, there will be no input document--but there will always be evaluation criteria. Some criteria will apply to all documents and some will be particular to that kind of document. The units of work to be reviewed must be small but may contain more than one deliverable.

Note that the producers are responsible for a skiing for a review--because they should know when the time is right. Deliverables which have not been reviewed or which have changed substantially since the last review are unlikely to be considered to be of sufficient quality if they are offered to the core team to be part of a new version of the system. It is therefore in the interests of the domain architects to make sure that reviews are taken seriously. Rejection of deliverables holds everyone up--so it is in everyones interests, in particular the project manager, to make sure that reviews are carried out.

**Appoint moderator**: The producers ask for a review. A moderator is appointed who is acceptable to the producers.
**Appoint reviewers**: The moderator in consultation with the producers appoints 3 to 6 reviewers.
**Initiate Review**: The producers ensure that the work to be reviewed is checked into the code management system and tagged, along with any special instructions for the reviewers. The moderator checks that the work is of some minimum standard to avoid wasting the time of the reviewers and that any input document have been reviewed then makes all this information available on the web and informs the reviewers that they can get started and tells them by when they should have their logs ready. When making the rough quality check the moderator checks that:- both the general rules and the specific rules for that document type and
* that the text is readable and reasonably free of grammatical or spelling errors.
* where a document has standard sections, there is a relevant entry for each section. Sections which do not need to be filled for the particular application should be marked "not applicable", rather than being left empty.

In some cases different reviewers will be asked to look for different things.

**Reviewers prepare logs**: Each reviewer will return a report by the agreed date. The format of this report is described below in Appendix A.3. Reviewers should review the whole work for failures to conform with the rules. These rules are both the general rules which apply to all documents as listed in Appendix D and those peculiar to the type of document being reviewed. In some cases it may be a matter of opinion whether a rule is violated or not, but a reviewer should err on the side of identifying too many issues. It is the responsibility of the moderator to persuade the reviewers to keep to time.
**Moderator consolidates reviewers reports and sends to producers**: When all logs have been received he will find which issues have been reported on multiple occasions and record this information in a file which can be processed to produce a consolidated log. This consolidated log does not contain the name of the reviewers responsible for identifying an issue.
**Producers respond to consolidated log**: The producers, after reading the consolidated log, produce a report describing the action taken for each issue which was not simply accepted. The responses from the producers will indicate that either the producers did not accept the reported issue and so it will need discussing or that even though he accepts it, he has some comment to make. If the producers consider that two issues in the consolidated log are really the same and have not been consolidated as they should, this should be reported to the moderator who will generate a new consolidated log as it is desirable that reports are consolidated in a consistent manner such that statistics derived from the reports are meaningful.
**Discussion to resolve remaining issues**: Discussions should all be by e-mail, which should be sent to the producers, the reviewers and the moderator, or if it is preferred just to the moderator. After an issue is resolved the moderator will edit the log, to reflect the agreement which has been reached. Changes to the rules might also be proposed.
**Moderator completes process**: When all issues are resolved, the moderator will then make all reports available along with statistics on the review.
**Deliverables** are:

* Reviewers' reports (not public) (C.3.1)
* Consolidated log of reviewers' reports (C.3.2)
* Producers' response to reviewers' reports(C.3.3)
* Moderators summary of review discussion (C.3.4)

### Review of a User Requirements Document

Since the URD is the first document to be reviewed, we do not in general have a formally reviewed source document against which it can be checked. The checking of the URD will therefore have to be done against any technical documents that may already exist and the reviewers general knowledge of the application.

If this URD refers to a subsystem, it must be compatible with the requirements defined in its parent URD.

The following is in addition to what was described in Appendix A

**Initiate Review**: Some extra documents should be made available to the reviewers:

* Any URD upon which this URD depends must also be checked in to the code management system.
* Exploratory studies documents (see Appendix C.2.1) and any other relevant reports.
**The moderator will also check before starting the review that:**

* Requirements look complete for at least a coherent subsystem of the proposed system. The test is to see whether enough has been defined to start designing this part of the system after the review.

### Format of an reviewer's report

The term "issue report" refers to a report on a single issue. The entire set of issue reports is referred to as the "reviewers report". This should start with a preamble of the form13:

Footnote 13: Many of the details here depend upon what is practical with the tool support we have. This is both tool support for producing the deliverables - e.g. how easy is it to produce line numbers in the text and node and line numbers in a diagram and the tool support for the review process itself

Work 2 Review 1 Reviewer 4 Time 9.5

The work and review identifiers and each reviewer's identifier will be allocated by the moderator. The "Time" field is there for reviewers to include the time that they have taken to study the input documents and complete the report. This time should not include that spent in background reading. Reviewers should expect an average checking rate of about a page an hour. A block of issues reports for a deliverable should start with a line such as:

Deliverable Track.cxx

Where "Deliverable" is a keyword and is followed by the identifier of the deliverable (provided by the moderator to identify each deliverable within the work being reviewed).

Reviewers should present their issue reports in the form error_id rule_tag position comment

The four fields are:

Error_id is an integer identifying a particular issue report14 and does not have to run consecutively (a reviewer may scan the document several times, finding more issues each time, or may in fact decide to erase a issue report). However, to make sure that you don't use the same number twice, you will probably find it best to assign your error numbers consecutively just before you submit your error log.

Rule_tag for the rule that you think has been violated. If a section of a deliverable (i.e. a single UR or line of code) violates several rules, record each issue separately. You should use your judgement in reporting multiple issues.

**Position Record the line number on the document where the issue occurred, if this can be clearly determined. If the issue is more widespread (perhaps because a whole paragraph is incorrect or badly written) record the the start and finish of the section of text with a "-" between. If it is hard to pin down a position, perhaps because the issue is that something is missing, insert a "-" character in the position field. The format for issues in figures has yet to be defined. Comment This should provide a short explanation of why the issue has been reported here. It should be full enough to be helpful to the author, but not verbose--in particular, avoiding advice to the author on how to correct it (the reviewer can do that later on, if asked for help).**

An example log is given below, where for purposes of illustration we imagine that two URDs are reviewed together as a single unit of work.

Work 2 Review 1 Reviewer 4 Time 9.5

Deliverable URD-xxx

1 DOC-03 22-44 this section deals with 3 unrelated topics: hardware requirements, precision and reliability

2 DOC-09 61 closing bracket missing in expression (a+b/2

3 DOC-10 70-85 this notation needs to be defined

4 DOC-12 95 where is this work described?

5 URD-13 103 no source has been given for this UR

6 URD-10 147 unnecessary constraint to require particular file names Deliverable URD-yyy

7 URD-24 156 no units have been defined for processing rate

8 URD-19 171 UR26 is not compatible with UR14* 9 URD-23 192 '{good'' is not quantified
* the URD does not specify what statistics are required
Roles

**project manager**: is responsible for the success of the project. He should interact with the users to make sure that the system really does what they want. He must attract the right people to the core team and to the support team--both of which he leads. He must make estimates of required resources, often based on information from the domain architects. He must support the software process.
**chief**: **architect**: is responsible for the overall design and making the project a success technically. He should to be able to oversee the whole problem area, be able to partition it in domains and know about the relationships between them. The architects overall vision continues throughout all phases of the project.
**domain**: **architect**: acts both asarchitect and team leader for the domain. As the leader of a domain team he is also a member of the core team. He represents his domain in the core team and makes sure that what the core team expects from his domain is reasonable-- both in terms of functionality and time scales, taking into account the available manpower. He must be able to overlook the domain, be able to identify any categories and know about the relationships between them.
**user**: **requirements**: **monitor**: talks with the users to make sure that their requirements are really understood. He takes a very strong interest in the UR document and notes if the software deviates significantly from these URs either by not providing what is needed or by providing what has not been requested.
**repository manager**: is responsible for the overall structure of the repository. He authorises changes to the master repository and controls people's access to the repository. He coordinates new releases. He discusses with the toolsmith to make sure that he has the necessary tools.
**documentation coordinator**: ensures that all documentation is up to date and available on WWW. This includes user documentation, code, this document and all other deliverables. For each domain he collects documents from the domain documentation editor and provides the top level structure. In the case of user documentation he should edit the supplied documents, if necessary, and return them to the domain documentation editor as well as integrating them into the user documentation system. He is responsible for the coherency of the user documentation. He is not expected to write much but to make sure that all the necessary user documentation exists and that it reads properly.

He must also ensure that this part of the web structure is suitable for the users and has information that is easy to find.
**domain documentation editor collects documentation for the development team and makes it available to the documentation coordinator. This documentation includes the user documentation. To avoid having too many styles of user documentation the development team documentation librarians should either write the user documentation or should at least edit it.**
**metrics collector is responsible for defining useful metrics, making sure (with the toolsmith) that it is feasible to collect them. He should ensure that they are collected and analyse them and make the results available. He should also correlate them with changes to the ASP.**
**toolsmith**: is responsible for making sure that the necessary tools are available, and that they work together. The need for some specific tools is identified in this ASP document. The actual choices will be listed in the technical choices document. Some other tools may be needed which are not identified in the ASP--the toolsmith might provide these as well.
**software process monitor**: ensures that standards are being followed--deliverables and processes. He ensures that the ASP is modified to suit actual needs and takes note of the effect upon metrics. He will look over some of the project documentation, and talk to people to ensure that the ASP evolves in the best way. If he identifies problems these will be discussed with the project manager and the chief architect--who should approve changes to the ASP. He also ensures that the reviews and inspections are being carried out correctly.
**chief tester takes overall responsibility for the quality of the testing. He does not do the testing but ensures that the test plans are acceptable and that they are carried out. He works with the toolsmith to ensure that the necessary tools are available for regression and other testing. He devises a global test plan which will allow system testing, testing of individual domains and testing of classes to be carried out. He will provide some assistance to new moderators.**
**programmer**: produces design, code and documents.

Summary of deliverables

### Principal reviewed deliverables

These are the key deliverables--they all have a set of rules to assist reviewers in judging their quality. Some of these rules relate to the appearance of the document, and others to its meaning. Tools must be provided to help in producing these documents. Rules to be followed by all documents are collected in Appendix D and subsequent appendices have the rules for individual document types.

Each deliverable is generated, according to the ASP, from earlier deliverables. The obvious exception is the user requirements document which depends upon nothing.

#### c.1.1 User requirements

This document, which is described more fully in Appendix E, is made up of the three main sections, an introduction, then more detail, and finally a detailed list of requirements. The second and third sections are broken down to cover functional requirements, system properties and constraints.

This document is reviewed and inspected by reviewers drawn from the user community and the developers. The users are there because it is essential that the document describes their requirements, and the developers are there to ensure that the requirements are reasonable.

#### c.1.2 Use Case descriptions

Each use case should be short (maximum of 'a side of A4') of textual description.

#### c.1.3 Architecture

Category diagrams, Category interface diagrams, an Object Message Diagram (OMD) and/or Message Trace Diagram (MTD) for each Use Case studied along with supporting text.

#### c.1.4 Cycle plan

The plan should list:

* Number of man hours to perform tasks
* Date by which a task will be complete, with assumptions of how many people are working on the task.
* Expected date of major milestones* Annotated class and other diagrams showing which functionality will be implemented for each domain for this cycle.15 Footnote 15: This might actually show which functionality is not expected to be completed by the end of the cycle.
* Supporting text

#### c.1.5 Nested state charts

Follow the unified notation.

#### c.1.6 Detailed design

Class diagrams for all classes which highlight the allocation of classes to categories and interface definitions for all classes.

#### c.1.7 Code

To be defined.

### Reviewed deliverables with undefined format

#### c.2.1 Exploratory studies document

This is a catch-all type of deliverable to record at least a reference to any relevant work.

#### c.2.2 Development tools document

List of choices and reasons for those choices.

#### c.2.3 Problem domain and existing solution analysis

This will be mostly descriptions of detectors and of algorithms. It will mostly be references to other work.

#### c.2.4 Training plan

To Be Written

#### c.2.5 User documentation

To Be Written

#### c.2.6 Configuration management plan

This will cover all aspects of maintaining code and other documents, e.g.

* Library structures and access control
* Backups
* Updating
* Naming conventions (including version and revision numbers)
* Code and configuration management tools, and how they will be used

This plan must be established early on by the repository manager.

#### c.2.7 Test plan

To Be Written

#### c.2.8 ASP documents

This and related documents.

### Deliverables which are not reviewed

#### c.3.1 Reviewer's reports

To ensure the anonymity of the review, these reports are not made public. It contains a list of reports on issues which the reviewer has noticed. The format is described in Appendix A.3

#### c.3.2 Consolidated log of reviewers' reports

These are produced by the moderator from the reviewers' reports. It does not contain the name of the reviewers responsible for identifying an issue.

#### c.3.3 Producers' response to reviewers' reports

This is the response to the consolidated log.

#### c.3.4 Moderators summary of review discussion

A summary of a discussion (e-mail or otherwise). It should be a well structured document rather than slavishly following the actual discussion.

#### c.3.5 Test results

To Be Written

#### c.3.6 System report

To Be Written

#### c.3.7 Metrics report

To Be Written

#### c.3.8 Roles list

List of roles and the people filling those roles for the entire software development.

Rules to be followed by all documents (DOCRULES)

$Date: 1996/05/03 14:19:00 $ Revision: 1.3 $

These rules apply to all documents, irrespective of other rules that may apply to documents of specialised type, such as class diagrams or code.

### Document Structure

**DOC-01.**: The following information shall appear at the head of the document

* a unique document identifier
* the version number
* the names of the author(s)
* the date the document was produced
* the review status of the document.
**DOC-02.**: The content of each section or further subdivision of the document shall be accurately described by its title or subtitle.
**DOC-03.**: Each section or further subdivision of the text shall deal with a unified topic.
**DOC-04.**: In a hierarchy of textual subdivisions, each set of subdivisions at a lower level shall expand in greater detail the material which is treated at the next higher level.

### Notations

**DOC-05.**: The textual part of the document shall be written in English.
**DOC-06.**: All words shall be correctly spelled.
**DOC-07.**: All continuous text shall be grammatically correct.
**DOC-08.**: Mathematical notations shall have their usual meaning.
**DOC-09.**: Mathematical expressions shall be correct in their context.
**DOC-10.**: Any symbolic notations other than well-known mathematical notations, including diagrammatic notations, shall be defined either in this document, or in a named document referenced from this document.

**DOC-11.**: All words that are used with specialised meaning shall be defined in this document, or in a named document referenced from this document.

### References

**DOC-12.**: A reference shall be given for any technical data which is not contained in this document.

## Appendix E Rules for user requirements (URD-RULES)

$Date: 1996/05/03 14:19:04 $

$Revision: 1.3 $

In the following text, a "general" requirement is one that is expressed in Section 1 or 2 of the URD document, whereas a "specific" requirement is one that appears in a single statement identified by a unique tag in Section 3. These latter requirements are referred to here as URs. The structure of the URD is based on that in PSS-05-0, but there are some differences, which are marked in the text below. Rules which correspond to mandatory practice in PSS-05-0 are marked with their PSS-05-0 tag. The word "software" has been replaced generally by "system" and, in some cases, rules have been slightly reworded. It should be noted that not all PSS-05-0 mandatory practices have been kept.

### Document Structure

**URD-01.**: The URD shall conform to the rules in DOC-RULES (Appendix D).
**URD-02.**: The URD shall be divided into sections and subsections according to the following format.

 1 Introduction  1.1 Purpose of the document  1.2 Scope of the system  1.3 Definitions, acronyms and abbreviations  1.4 References  1.5 Overview of the document  2 General Description  2.1 Product perspective  2.2 Overview of functional requirements  (These correspond to PSS-05-0 ''general capabilities'', but  are restricted here to binary, present/not present,  functions.)  2.3 Overview of system properties  (These are quantified properties which describe the overall  quality that the system has to attain)  2.4 Overview of constraints  2.5 User characteristics  2.6 Operational environment  2.7 Assumptions and dependencies  3 Specific Requirements  3.1 Functional Requirements  3.2 System properties3.3 Constraints
* **URD-03.**: References shall be given for all external documents which support the request for a user capability.
* **URD-04.**: Requirements which depend on oneanother shall be cross-referenced.

### Relevance to the Application

* **URD-05.**: The URD shall provide a general description of what the user expects the system to do (PSS-05-0 UR12).
* **URD-06.**: The URD shall describe the operations the user wants to perform with the system (PSS-05-0 UR14).
* **URD-07.**: The URD shall define all the constraints that the user wishes to impose on the system (PSS-05-0 UR15).
* **URD-08.**: The URD shall describe the external interfaces to the system, or reference the documents where they are defined (PSS-05-0 UR16, reworded).
* **URD-09.**: All requirements, explicitly or implicitly arising from descriptions of the application in user sources, shall be listed in the URD.
* **URD-10.**: The URD shall contain no requirement that does not correspond to a need expressed by the users.
* **URD-11.**: The URD describing requirements for a subsystem shall be compatible with the URD for the parent system.
* **URD-12.**: All requirements shall be realistic.
* **URD-13.**: The source of each UR shall be stated (PSS-05-0 UR05).
* **URD-14.**: A priority number shall be assigned to every UR to indicate the development cycle by which that UR should be satisfied.

### Consistency

* **URD-15.**: Each specific requirement shall be identified by a unique identifier (PSS-05-0 UR02).
* **URD-16.**: Each UR shall be verifiable (PSS-05-0 UR06).
* **URD-17.**: Each UR shall appear in a separate statement.
* **URD-18.**: Each UR shall be internally unambiguous.
* **URD-19.**: URs shall be consistent with each other.

**URD-20.**: Each specific UR shall be consistent with the general requirements.
**URD-21.**: No UR shall be explicitly or implicitly duplicated.
**URD-22.**: If a UR contains a TBD ("to be defined"), it shall be stated clearly exactly what aspect of the system still has to be defined.

### Quality Requirements for System Properties

**URD-23.**: Each UR describing the quality requirements of a system property shall provide a quantified specification of quality.
**URD-24.**: The units shall be defined for every quantified property.
**URD-25.**: The values specified for each quantified property shall be attainable.
**URD-26.**: Each UR describing a quantified property shall define an estimate of the times at which the given sequence of values (which may be a single value) are to be achieved.

Glossary

**ASP Atlas Software Process** (as defined in this document)

**Architect is a software designer who is concerned with the structure of the software.
**Category**: a coherent set of coupled classes. See domain.
**Category**: Diagram a class diagram showing just categories.
**Category**: Interface Diagram a class diagram showing just those classes which act as the interface for one domain.
**Configuration management**: is the management of the various versions of files that contribute towards a working system.
**Deliverable**: is an output of the ASP, which must be produced at a stated stage of the process.
**Domain**: a domain is a special kind of category, which is at the top level and will be developed by one domain team. The concept of the domain is necessary to clarify the responsibilities of the domain teams.16
**Evoluti**: to nary development emphasises building some kind of simple working system very early on then adding bits it. This means that work on design and coding can start very early, and that there is no need to aim for perfection in the user requirements at the beginning as everything will evolve. This fits in well with the notion of incremental delivery.
**Increment**: **delivery**: means delivering a software system in stages. When coupled with evolutionary development this implies that each increment will tend to be spread over the whole system, rather than delivering by domain.
**Inspection**: is more formal than a review as it is the procedure to accept a new piece of work.
**MTD**: Message Trace Diagram.
**Message Trace Diagram**: looks a bit like a timing diagram and has vertical lines for objects and horizontal lines for messages between them. See Object Message Diagram.
**OMD**: Object Message Diagram.

**Object Message Diagram shows objects as nodes of a graph and the messages between them as arcs. See Message Trace Diagram.**
**Review is similar to a walk-through but with deliverables and more formality.**
**Use Case is a short statement describing an operation that the user might wish to carry out on the system.**
**Walk-t**: **through is an informal developer driven procedure where the developer explains what something is supposed to do, or how it does it and where he expects to receive immediate feedback. The developer should do a little preparation, but preparation is** _not_ **required by those to whom he presents his work.**
**Work in the context of a review, is a set of deliverables which is reviewed as a coherent unit.**

## References

* [1] Watts S. Humphrey, _A Discipline for Software Engineering_, Addison-Wesley, 1995.
* [2] Tom Gilb, _Principles of Software Engineering Management_, Addison-Wesley, 1988.
* [3] S.M.Fisher and K.Bos _Towards a Software Development Process for Atlas17_, Atlas Software Note 23
Footnote 17: [http://moose.cem.ch/moose/documents/swp/swp.html](http://moose.cem.ch/moose/documents/swp/swp.html)
* [4] European Space Agency _ESA Software Engineering Standards, Issue 2_, ESA Publications Division, 1991.
* [5] Steve Cook and John Daniels _Designing object systems: object-oriented modelling with Syntropy_, Prentice Hall, 1994, ISBN 0-13-203860-9
* [6] Grady Booch and James Rumbaugh _Unified Method for object-oriented development: documentation set--version 0.8_ Rational Software Corporation (product_info@rational.com), 1995
* [7] Watts S. Humphrey, _Managing the Software Process_, Addison-Wesley