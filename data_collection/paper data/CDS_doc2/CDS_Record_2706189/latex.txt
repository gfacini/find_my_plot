[MISSING_PAGE_EMPTY:1]

## 1 Introduction

Jets are collimated sprays of particles resulting from high-energy quarks and gluons. Due to the ubiquity of quarks and gluons at the Large Hadron Collider (LHC), jets are used by nearly every physics analysis. In particular, there is a growing importance in studying the substructure of jets to identify boosted, hadronically decaying massive particles [1; 2]. Large-radius (large-\(R\)) jets are used to capture the decay products of boosted massive particles. Detector effects modify the reconstructed energy and mass of such jets, which must be corrected for in order to improve the precision of both Standard Model (SM) measurements and searches for physics beyond the SM.

The calibration scheme for large-\(R\) jets in ATLAS is documented in Ref. [3]. First, a jet energy scale (JES) correction is derived from Monte Carlo (MC) simulations by fitting the reconstructed jet energy response \(E^{\rm reco}/E^{\rm true}\) in bins of truth jet energy, and then applying numerical inversion to reconstructed jet energies. The same technique is used for the calibration of small-radius (small-\(R\)) jets as well [4; 5; 6]. For large-\(R\) jets, the second step is a jet mass scale (JMS) correction. The JMS correction is derived by fitting the reconstructed jet mass response \(m^{\rm reco}/m^{\rm true}\) across different bins of truth jet energy and truth jet mass and applying numerical inversion. One challenge with this sequential JES+JMS approach is that the jet energy response depends on the jet mass, and therefore the jet energy may be biased across bins of jet mass.

Previous ATLAS calibrations for small-\(R\) jets proposed a solution to similar non-closure issues with a residual correction called the Global Sequential Calibration (GSC) [7]. The idea of the GSC is to apply a series of sequential calibrations after the JES correction to mitigate the residual dependence of the JES on various jet features. The GSC concept was recently extended to a simultaneous correction for auxiliary features using neural networks (NN) with a method called Generalized Numerical Inversion (GenNI) [8]. In addition to taking into account correlations between various features and the response, GenNI is also unbinned (readily allowing for more features) and provides one calibration instead of many residual calibration steps.

The goal of this note is to extend the GenNI framework to large-\(R\) jets. The main difference between large-\(R\) jets and small-\(R\) jets is that for the former, one of the residual features (the jet mass) must also be calibrated. Therefore, this note demonstrates how to simultaneously calibrate the jet energy and the jet mass so that both calibrations use the jet energy and the jet mass. This framework may be extended to include other auxiliary features in the future.

In addition to extending the GenNI framework to large-\(R\) jets, this note also proposes a solution to one of the challenges with neural network-based calibrations. Typically, regression tasks use the mean squared error (MSE) loss function when optimizing the network parameters. The MSE learns to approximate the mean of the response distribution, while the traditional calibration procedure focuses on the mode by fitting Gaussian functions to the core of the response distributions. When there are long and asymmetric tails, the mean and mode of the response distribution are not the same. Recently, the CMS collaboration investigated the use of the Huber loss [9] to reduce the sensitivity to outliers [10]. The Huber loss is a combination of the MSE and the mean absolute error and interpolates between the response mean and median. The median is less sensitive to outliers than the mean, but is still biased by the asymmetry of the response distribution. Therefore, this note instead uses the Leaky Gaussian Kernel (LGK) loss function [11] that is designed to directly learn the mode.

This note is organized as follows. A review of GenNI and the difference between MSE and LGK losses are provided in Section 2. The results for simultaneous energy and mass calibration of large-\(R\) jets at the ATLAS detector are presented in Section 3. The note closes with conclusions in Section 4.

## 2 Methods

Numerical inversion proceeds in two steps. For a given quantity \(x\), such as energy or mass, the central tendency of the reconstructed value of \(x\), \(x^{\rm reco}\), is parameterized as a function of the true value of \(x\), \(x^{\rm true}\). The traditional statistic of central tendency is the mode, usually approximated by fitting a truncated Gaussian function. As a second step, the parameterized function is inverted and the calibrated \(x^{\rm reco}\) is simply the inverted function evaluated at \(x^{\rm reco}\). Further mathematical details of numerical inversion can be found in Ref. [12].

As with classical numerical inversion, GenNI also proceeds in two steps, each with a dedicated NN [8]. The first NN \(L\) is trained to regress the response of the detector to some truth physical observable. Symbolically, this can be expressed as \(L:(x^{\rm true},\theta)\mapsto x^{\rm reco}\), where \(\theta\) is some auxiliary measurement(s) useful for calibration which is not itself calibrated. The NN \(L\) is trained using a set of \((x^{\rm true},\theta,x^{\rm reco})\)'s where the first two values are used to predict the last value as the output. The function \(L\) itself represents some central tendency of \(x^{\rm reco}\) given \((x^{\rm true},\theta)\) (in this note, this will be the mode). Then, the second NN \(C:(L(x^{\rm true},\theta),\theta)\mapsto x^{\rm true}\) learns to invert the function from the first network. Since \(L\) is a deterministic function, the distribution of \(x^{\rm true}\) given \(L(x^{\rm true},\theta)\) and \(\theta\) is a Dirac \(\delta\) distribution at the true value as long as the response learned by \(L\) is monotonic as a function of \(x^{\rm true}\). Then, the training of \(C\) is trivial as there is no spread or local minima in the training set distribution1. The calibrated value of \(x^{\rm reco}\) is then given by \(C(x^{\rm reco},\theta)\).

Footnote 1: Cases where \(\theta\) is strongly correlated with \(x^{\rm true}\) must be considered with caution because \(C\) can then successfully learn to regress \(x^{\rm true}\) purely based on \(\theta\) without actually inverting the learned response function \(L\). However, this is not a concern within the scope of this note.

This note extends the GenNI formalism to simultaneously calibrate multiple features: the energy \(E\) and the mass \(m\). As before, there are two steps. The first step is a NN \(L:(E^{\rm true},m^{\rm true})\mapsto(E^{\rm reco},m^{\rm reco})\). The second step then learns \(C:L(E^{\rm true},m^{\rm true})\mapsto(E^{\rm true},m^{\rm true})\). The calibrated values of \(E^{\rm reco}\) and \(m^{\rm reco}\) are then given by \(C(E^{\rm reco},m^{\rm reco})\). The detector pseudorapidity, \(\eta_{\rm det}\), is used as an auxiliary variable in each of the above NNs2.

Footnote 2: ATLAS uses a right-handed coordinate system with its origin at the nominal interaction point (IP) in the center of the detector and the \(z\)-axis along the beam pipe. The \(x\)-axis points from the IP to the center of the LHC ring, and the \(y\)-axis points upward. Cylindrical coordinates (\(r\), \(\phi\)) are used in the transverse plane, \(\phi\) being the azimuthal angle around the beam pipe. The pseudorapidity is defined in terms of the polar angle as \(\eta=-\ln\tan({\rm polar\ angle}/2)\). Pseudorapidity defined with the geometric center of the detector (instead of the estimated IP) as the origin is called the detector pseudorapidity, \(\eta_{\rm det}\). The distance \(\Delta R\) is defined by \((\Delta R)^{2}=(\Delta\eta)^{2}+(\Delta\phi)^{2}\).

The choice of loss function when training \(L\) determines what statistic of the response distribution is parameterized by GenNI. A common loss function is the MSE, which asymptotically learns the mean of the response distribution. Instead of the MSE, this note considers the LGK loss function [11]:

\[\text{Loss}(x^{\rm target},x^{\rm pred})=-\exp\left(-\frac{(x^{\rm target}-x^ {\rm pred})^{2}}{2\alpha}\right)+\beta|x^{\rm target}-x^{\rm pred}|,\]

where \(x^{\rm target}\) is the target value given by the training set, \(x^{\rm pred}\) is the NN-predicted value, and \(\alpha\) and \(\beta\) are hyper-parameters to be tuned. In the limit that \(\alpha\to 0\) for fixed \(\beta\), the LGK loss formally learns the mode, but is highly sensitive to the limited statistics of the training set. The mean absolute error term is added to ensure that the gradient of the loss function does not vanish for large \(|x^{\rm target}-x^{\rm pred}|\).

For the simultaneous calibration of \(E\) and \(m\), the total loss is simply the sum of the individual LGK loss functions:

\[\text{Loss}(E^{\text{target}},m^{\text{target}};E^{\text{pred}},m^{\text{pred}})= \text{Loss}(E^{\text{target}},E^{\text{pred}})+\text{Loss}(m^{\text{target}},m^{ \text{pred}}). \tag{1}\]

The energies and masses are independently and inclusively standardized using the global mean and standard deviation. The NNs are trained with Keras [13] and TensorFlow [14] using the Adam [15] optimization algorithm. The \(L\) network uses two hidden layers, with 60 and 40 nodes, respectively. All intermediate layers use the Leaky ReLU [16] activation function. The loss used is the total loss as defined in Eq. (1), with the values of \(\alpha\) and \(\beta\) set to \(10^{-3}\) after a coarse hyperparameter scan. The \(C\) network uses two hidden layers, with 60 and 2 hidden nodes, respectively. As the \(C\) network is learning a generally single-valued function, the loss function is not as important and thus the MSE is used.

To accommodate differences in the performance of the calibration across multiple features, care must be taken to ensure that the learning of one feature calibration does not adversely impact that of another. A split network architecture is used when training \(L\) to address this problem. The first intermediate layer with 60 nodes has access to all the input parameters, and the successive layers are split in two such that the back propagation gradient flow from the output nodes flows through one half of the split intermediate layers. In this way, while gradient descent tries to minimize the overall loss across all the parameters being calibrated, gradients from each parameter output node have access to a unique portion of the training network (half of the intermediate layer with 40 nodes), ensuring that each parameter is calibrated sufficiently well without being affected by the calibration of a different parameter.

In order to ensure that the networks learn the response equally well all across the phase space, the energy and mass spectra of the training set are flattened by assigning weights of \(1/p(E^{\text{true}},m^{\text{true}})\). During the evaluation of results, the physical spectra of jets are used.

## 3 Results

Studies documented in this section are performed using a variety of MC simulated samples. The nominal set is simulated using Pythia 8.230 [17; 18; 19] with the NNPDF2.3 leading order (LO) [20] set of parton distribution functions (PDF's) and the A14 set of tuned parameters [21]. Additional sets are simulated using Sherpa 2.2.5 [22] with the CT14NNLO PDF set [23] and the default cluster hadronization model and Herwig 7.1.3 [24; 25; 26] with the MMHT2014NLO PDF set [27] and the default angularly-ordered parton shower with the default cluster hadronization [28]. Additional details of these samples may be found in Ref. [29].

All simulated events have been reconstructed using a full simulation of the ATLAS detector [30] implemented in Geant 4[31], which describes the interactions of particles with the detector and the subsequent digitization of analog signals. The effect of multiple \(pp\) interactions in the same and neighbouring bunch crossings is modelled in these samples by overlaying minimum-bias \(pp\) collisions, which are generated using Pythia 8 with the A3 set of tuned parameters [32] and NNPDF2.3 LO PDF set.

Reconstructed jets are formed from topologically connected, locally calibrated and noise-suppressed topological clusters (topoclusters) of calorimeter cells [33] using the FastJet [34] implementation of the anti-\(k_{t}\) jet algorithm [35] with radius parameter \(R=1.0\). The angular coordinates of the topoclustersare corrected to point to the nominal IP instead of the geometric center of the detector. Truth jets are formed from detector-stable simulated particles (\(c\tau>10\) mm), excluding muons and neutrinos. All jets are trimmed [36] using \(R=0.2\) subjets and a momentum fraction of 5%. Reconstructed jets are geometrically matched to truth jets using the \(\Delta R\) distance metric; all jets with a reconstructed-truth match are considered. Reconstructed jets with truth matched energy \(>200\) GeV and mass \(>60\) GeV, and the detector pseudorapidity \(0.0\leq|\eta^{\rm det}|\leq 0.5\) are considered for this study.

The standard and GenNI methods are evaluated using the same metrics. In particular, in a given bin of truth \(x\) (\(x=E\) or \(m\)), the distribution of the response \(x^{\rm reco}/x^{\rm truth}\) is fit with a Gaussian function over a restricted range around the peak of the distribution defined by the distribution mean and standard deviation. Assuming that the core of the response is Gaussian, the mean of the Gaussian fit gives an unbiased and stable (i.e. independent of non-Gaussian tails) estimator of the mode of the response within the given bin of truth \(x\). This is used to characterize the response within the bin.

Variations of the GenNI calibration are presented in Fig. 1. There is an overall bias in the response when using MSE compared with the LGK, which is due to the difference between the mean and mode of the response distribution. Furthermore, the residual mass dependence of the energy response is nearly eliminated by using a simultaneous approach over a sequential approach, as demonstrated by the better closure in the bottom right plot of Fig. 1.

Figure 1: A comparison of the mode of the energy response as a function of truth energy for GenNI trained using MSE (top) and LGK (bottom) in bins of the truth jet mass using the nominal Pythia 8 MC sample. The left plots use GenNI sequentially while the right plots use it simultaneously for mass and energy. Data points off the scale of the plots may have their error bars within the visualization range, which may appear as marker-less vertical lines on the plot.

A comparison of the standard sequential calibration and the simultaneous GenNI-based calibration is presented in Fig. 2. Raw jet energy and mass responses for the nominal Pythia 8 sample are shown in the top row. Prior to any jet calibration, both the energy and mass response show a strong energy-dependence. Due to the local topocluster energy calibration, the overall energy scale is within 10% of unity across the entire energy range. The overall scale is significantly improved after the standard sequential calibration, although there is a significant non-closure (i.e., response different from 1) at low energy and a 1-2% spread over the range of truth jet masses. The GenNI approach nearly eliminates the residual mass dependence of the energy response and also improves the response at low energy. The energy and mass resolutions (not shown) are nearly the same between the the traditional simultaneous and GenNI.

The results presented in Figs. 1 and 2 use the Pythia 8 MC simulation for both training and evaluating the calibration. Figs. 3 and 4 show the performance of the calibration trained on Pythia, but tested with Sherpa and Herwig, respectively. The figures show the difference in the responses of Pythia and Sherpa/Herwig respectively. The overall calibration closure for the alternative MC simulations is typically no worse for GenNI compared with the standard method. These results suggest that the systematic uncertainties from the NN-based method will be no larger than the standard method and that the gains in closure are robust.

Figure 2: Energy (left) and mass (right) responses for the nominal Pythia 8 MC sample. From top to bottom, each row represents: no calibration, current ATLAS JES+JMS, and GenNI calibration with LGK loss. Data points off the scale of the plots may have their error bars within the visualization range, which may appear as marker-less vertical lines on the plot. Note that the top row has a different \(y\)-axis scale than the lower two rows.

Figure 3: Energy (left) and mass (right) responses for the Sherpa MC sample. The top row shows the uncalibrated energy and mass responses of the Sherpa MC sample. The middle and the bottom rows show the response differences between Pythia and Sherpa (Pythia 8 – Sherpa 2) samples after current ATLAS JES+JMS and GenNI calibration with LGK loss, respectively. Data points off the scale of the plots may have their error bars within the visualization range, which may appear as marker-less vertical lines on the plot.

## 5 Conclusions

Figure 4: Energy (left) and mass (right) responses for the Herwig MC sample. The top row shows the uncalibrated energy and mass responses of the Herwig MC sample. The middle and the bottom rows show the response differences between Pythia and Herwig (Pythia 8 – Herwig 7) samples after current ATLAS JES+JMS and GenNI calibration with LGK loss, respectively. Data points off the scale of the plots may have their error bars within the visualization range, which may appear as marker-less vertical lines on the plot.

## 4 Conclusion

The work reported in this note has extended the GenNI framework for NN-based jet calibrations to simultaneously calibrate multiple input features. Additionally, the calibration directly uses the mode of the response instead of the mean using a new loss function. As an illustration of this method, the energy and mass of large-radius jets have been calibrated. The residual mass-dependence of the energy calibration is reduced and is robust to variations in the MC simulation. These techniques will be useful tools for further calibration developments in jet reconstruction and beyond. In particular, additional features can be easily added to this streamlined approach where only two neural networks are required instead of many sequential steps.

## References

* [1] R. Kogler et al., _Jet Substructure at the Large Hadron Collider: Experimental Review_, Rev. Mod. Phys. **91** (2019) 045003, arXiv: 1803.06991 [hep-ex] (cit. on p. 2).
* [2] A. J. Larkoski, I. Moult, and B. Nachman, _Jet Substructure at the Large Hadron Collider: A Review of Recent Advances in Theory and Machine Learning_, Physics Reports (in press) (2019), arXiv: 1709.04464 [hep-ph] (cit. on p. 2).
* [3] ATLAS Collaboration, _In situ calibration of large-radius jet energy and mass in \(13\) TeV proton-proton collisions with the ATLAS detector_, Eur. Phys. J. C **79** (2019) 135, arXiv: 1807.09477 [hep-ex] (cit. on p. 2).
* [4] ATLAS Collaboration, _Jet energy scale measurements and their systematic uncertainties in proton-proton collisions at \(\sqrt{s}=13\) TeV with the ATLAS detector_, Phys. Rev. D **96** (2017) 072002, arXiv: 1703.09665 [hep-ex] (cit. on p. 2).
* [5] ATLAS Collaboration, _Jet energy measurement and its systematic uncertainty in proton-proton collisions at \(\sqrt{s}=7\) TeV with the ATLAS detector_, Eur. Phys. J. C **75** (2015) 17, arXiv: 1406.0076 [hep-ex] (cit. on p. 2).
* [6] ATLAS Collaboration, _Determination of jet calibration and energy resolution in proton-proton collisions at \(\sqrt{s}\) = 8 TeV using the ATLAS detector_, (2019), arXiv: 1910.04482 [hep-ex] (cit. on p. 2).
* [7] ATLAS Collaboration, _Jet global sequential corrections with the ATLAS detector in proton-proton collisions at \(\sqrt{s}=8\) TeV_, ATLAS-CONF-2015-002, 2015, url: [https://cds.cern.ch/record/2001682](https://cds.cern.ch/record/2001682) (cit. on p. 2).
* [8] ATLAS Collaboration, _Generalized Numerical Inversion: A Neutral Network Approach to Jet Calibration_, ATL-PHYS-PUB-2018-013, 2018, url: [https://cds.cern.ch/record/2630972](https://cds.cern.ch/record/2630972) (cit. on pp. 2, 3).
* [9] P. J. Huber, _Robust Estimation of a Location Parameter_, Ann. Math. Statist. **35** (1964) 73, url: [https://doi.org/10.1214/aoms/1177703732](https://doi.org/10.1214/aoms/1177703732) (cit. on p. 2).
* [10]_A deep neural network for simultaneous estimation of \(b\) quark energy and resolution_, CMS-PAS-HIG-18-027 (2019), url: [https://cds.cern.ch/record/2690804](https://cds.cern.ch/record/2690804) (cit. on p. 2).
* [11] S. Cheong, A. Cukierman, B. Nachman, M. Saffari, and A. Schwartzman, _Parametrizing the Detector Response with Neural Networks_, JINST (in press) (2019), arXiv: 1910.03773 [physics.data-an] (cit. on pp. 2, 3).