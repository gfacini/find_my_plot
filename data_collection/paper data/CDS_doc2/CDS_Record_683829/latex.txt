**A Minimum Set of Measurements**

**to be Made on the Application Testbeds.**

**Version 1.0**

**The ATLAS Second Level Trigger Community**

**29\({}^{\rm th}\) March 1999**

## 1 Background

We summarise the measurements to be carried out on the ATLAS second level trigger application testbeds. The measurements described form a minimal set with some optional extensions. It is assumed that there will be interest in making additional measurements. The note represents our current view, which will most likely evolve as understanding of the testbeds increases. It will therefore be updated as time goes on.

In preparing this summary, input has been taken from many different sources spanning multiple drafts of this document. Several meetings have been devoted to discussions of its content. We believe it represents a community consensus.

## 2 Goals of the Application Testbed Programme

The prime motivation for the work to be carried out is to provide input to the Technical Proposal. Specific goals may be listed as follows:

1. The overall goal of the application testbed programme is to establish "proof of existence" for a prototype ATLAS second level trigger on a significant sized system, with all components, interfaces and software present. All other goals stated here contribute to this one through the making of measurements that will drive the modelling of the full system.
2. Trigger algorithms with simulated event data will be run on the testbed to give realistic time delays and CPU usage, and to demonstrate correct delivery of data. To validate the measurements it will be necessary to demonstrate that the algorithms are functioning correctly, so some algorithm performance data will be extracted for comparison with full trigger performance measurements. However, the testbed will not be used as a platform for algorithm development and trigger performance studies.

3. Priority should be given to investigating single farm systems based on Ethernet and ATM interconnects running either the Linux or Windows NT Operating System. Initially mono-processor nodes will be used. However, multi-processor nodes should be incorporated once an understanding of their operation and performance under the various Operating Systems has been gained.
4. Measure the performance of "node" components, ROB, processor and supervisor running the standard Level 2 Reference Software [1]. Comparisons will be made of the performance achieved with the Reference Software with that achieved by existing, alternate software [2].
5. Test, evaluate and optimise the Reference Software. Check that any dependence of the performance on system size parameters is acceptable; i.e. check that O(N) effects are not catastrophic for a full sized system (see later). This will involve, as far as possible, running components as if they were in the final full size system. Whilst the overall design of the Reference Software has been developed with efficiency in mind, local small-scale optimisation has not been included. Such optimisation should be carried out as measurements proceed.
6. Measure network performance in terms of latency and achieved throughput as a function of offered load. For most purposes random traffic will suffice. However, traffic more representative of the second level trigger and event building will also be used. These measurements should be made as a function of network size.
7. Provide inputs for modelling a full size second level trigger. Achieve confidence in the modelling procedure by repeated calibration and cross checks on various size testbed systems and configurations.
8. Map the complete full size Level 2 software process onto the available processors. This requires many processes to be mapped on to each processor and will most likely be done on very large systems outside of CERN. Current thoughts suggest that this test would be most useful to check that the Reference Software functionality scales to a large configuration, rather than to measure performance aspects.
9. Investigate data corruption rates and possible recovery strategies.

## 3 Making Measurements

The Reference Software allows different configurations of the system to be implemented i.e. different numbers of ROBs and processors. Hooks for timing are not yet in place and should be discussed in the light of the required measurements.

A common clock to support global system timing will be developed [3] and other work is being carried out to synchronise local PC clocks [4]. Strong encouragement is given to testing out both schemes.

## 4 Level 2 - DAQ/EF considerations

Part of the load on the ROBs comes from providing the event data for accepted events to the Event Builder. To obtain realistic results it is thus important that measurements of system and ROB performance should allow for the extra load. In addition, it would be useful to know whether a single network can handle both the Level 2 and DAQ/EF traffic.

## 5 Errors, data loss and error recovery strategy

As we will have significant size systems available, we should begin to make quantitative measurements about data loss (e.g. dropped cells or packets) and data corruption. The aim is to identify generic errors and define our strategy for fault detection and our recovery procedures. This will require careful study of the various error conditions that can occur in the final ATLAS components and in the overall system. It must be noted that errors could be associated to today's hardware and therefore not be generic. A lower emphasis should be put on these.

## 6 Suggested Measurement Philosophy

* Components will be initially measured and characterised in a simple way, treating them as black boxes. For example, networks will be measured in terms of data throughput and latency distributions, as a function of applied load and data traffic patterns; processors will be specified in terms of event throughput and latency distributions, as a function of load, for typical events; ROBs will be able to provide data, at a certain rate, with a certain latency, depending on load. The testbed will be configured to measure one type of component at a time.
* The results from the measurements of individual components will provide the characteristics for constructing component models and allow calibration of such models.

* Emulated components (such as the ROB complex and supervisor) can be improved to give more realistic behaviour based on the measurements from real components.
* The testbed should be set up as a large "vertical slice" and its system performance measured in terms of overall throughput and latency, as a function of load, for various size systems.
* Modelling will be used to produce results for the different size vertical slices, which can be compared with measurements made on the testbed. This gives a further calibration check and tune up. Within the limitations of the available testbed hardware, the reliability of the models for reproducing the scaling behaviour of the system will thus be checked.
* Modelling will be extended to predict the performance of a full size ATLAS Level 2 system that should be presented in the Technical Proposal. Given the size of the full system it is important that the tools used should be efficient and allow for adequate statistics to be obtained in a reasonable time.
* Further detailed checks should be made on the Reference Software, both for individual components and the overall system, to optimise the code and to identify and eliminate any unwanted O(N) effects that limit scalability. For example, as the system size N increases the event throughput should increase linearly with N and the event latency should remain constant or scale linearly with the number of sources participating in data collection. If the system does not scale linearly it should be at least acceptable up to the maximum required throughput.
* Many measurements depend on "Simulated data" or "realistic data". Studies of the exact content of this data, the production method and the mapping of the data onto the available hardware will be done.

## 8 Measurement Procedure and Parameters

Measurements to be made are presented in Table 1. We have indicated the source of information for each table entry. Most measurements will provide data for calibrating and cross checking the modelling. It is understood that not all measurements will be done on all platforms due to limited time and manpower.

The FPGA and HPCN studies functionally span several rows in the table. However, for clarity they have been allocated rows of their own.

Some overlap between rows exists due to the, sometimes, arbitrary boundaries that have been chosen and the multiple contributors for the table. This is especially relevant for the RoI Collection and Processor rows.

The ROB complex, in the context of these tests, is most probably a PC with one or more ROB cards (ROBIN for example).

[MISSING_PAGE_EMPTY:8]

[MISSING_PAGE_EMPTY:9]

## 9 Conclusions

We have tried in this note to pull together some of the current ideas about the application testbed. In particular, we have attempted to formulate an agreed possible set of configurations, procedures and measurements. We expect other more detailed characterisations and measurements of components to be made outside the framework of the ATB, in parallel studies within the ATLAS Trigger DAQ programme.

## 10 References, supporting documents and notes

References:

[1] Reference Software:

[http://www.cern.ch/Atlas/project/LVL2testbed/www/](http://www.cern.ch/Atlas/project/LVL2testbed/www/)

[2] Alternate Software from Saclay:

[http://www-dapnia.cea.fr/Phys/Sei/](http://www-dapnia.cea.fr/Phys/Sei/)

[3] Global Clock by Lorne Levinson:

[http://home.cern.ch/](http://home.cern.ch/)\(\sim\)levinson/PPT_spec.html

[4] Clock synchronisation:

F. Saka et al., "Synchronisation and Global Clock for the Ethernet testbeds",

being written.

Supporting documents and notes:

D. Calvet et al., "Emulation of the Sequential Option of the ATLAS Trigger and Event Builder using the ATM Network of the RCNP Institute",

ATLAS DAQ Note 98-130.

D. Calvet et al., "An ATM based Demonstrator System for the Sequential Option of the ATLAS Trigger", ATLAS DAQ Note 98-104.

M. Boosten et al., "Fine-Grain Parallel Processing on Commodity Platforms", Proceedings of WoTUG-22: Architectures, Languages and Techniques, edited by B.M. Cook. IOS Press, 1999.

F. Saka et al., "Ethernet Communications measurements", being written.