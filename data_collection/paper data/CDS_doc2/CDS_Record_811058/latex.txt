[MISSING_PAGE_FAIL:1]

After Tier-0 processing, the events are streamed to optimize access. The raw data is shared between the Tier-1 facilities for later processing each of N Tier-1 holds 2/N of the ESD on disk. It is intended to reprocess new data with new calibrations after 1-2 months. At the year-end, all of the cumulative Raw data will be reprocessed with improved algorithms and calibrations.

The Tier-1 cloud also allows scheduled physics group access to the full ESD sample, producing collections and Derived Physics Data (\(\sim\)100 full ESD reads per year). These are distributed to Tier-2 facilities for user analysis.

Small Raw and ESD samples are also copied to the Tier-2s to allow the user to develop algorithms and calibrations. The Tier-2s (and local group clusters) are the primary centres for chaotic user analysis. The Tier-2 also have a very important role in the production of the simulated data, which is then shipped to the Tier-1s for storage.

As well as the external user community (\(\sim\)600-700 users) there will be a significant number of \(\sim\)100 CERN-based users and users wishing to make use of a super-Tier-2 facility. This does not carry the usual simulation load, but keeps larger than usual samples of data on disk and has occasional access to the data retained in the Castor mass store.

### Deployment, Tests and Data Challenges

The ATLAS Computing Model is currently under test in a world-wide data challenge. This has three phases, all of which test the integration with the Grid deployments (the Challenge is exclusively Grid-based), bottlenecks and the effectiveness and accuracy of the information systems:

* The production phase, testing the production system, real-world network and middleware response and generating data for the later phases
* The Tier-0/Tier-1 phase, emulating the first-pass data processing and data transfer to the Tier-1 centers.
* The data analysis phase, testing the event model, data distribution and chaotic access patterns.

The exercise also tests the human and organizational aspects of the model: the required production team size, roles and distribution; the role of local managers; the interactions with other users and other scheduling policies; and the role of mutual-coverage between sites in operations.

Figure 3: ATLAS Data Challenge 2 production uses facilities worldwide using the LHC Computing Grid deployment (pictured), Grid3 and NorduGrid.

Figure 2: A schematic summarising the expected data flows and processing capacities in the ATLAS Computing model.

### Network

The demands on the network between the Tier-0 and Tier-1s are well established at \(\sim\)3Gbps (\(\sim\)75MBps raw rate for ATLAS), with similar capacities required between the Tier-1s. (Additional loads from traffic direct from the online system are being evaluated.) Future work will determine the network requirements between the Tier-2s and the other Tiers; initial work suggests 622Mbps would be sufficient, but 1 Gbps would be more effective. Dedicated tests of the network performance and data transfer tools are planned, with specific investigations of light path connectivity.

### Conclusions and Plans

A document for external review describing the Computing Model will be produced by the end of the year, and the full ATLAS Computing TDR is due in Summer 2005. The model described above encapsulates the steady-state system for ATLAS computing. The overall resource requirements for the first year are shown in Table 2. At the start of data-taking, there will be particular pressures on the system, and more recourse will be had to the Raw and ESD data formats. Current work is focused on the best way to address this transient need within the resource limits set by the steady-state model, and without disrupting the final system.

## References

* [1] ATLAS Computing Technical Proposal, CERN LHCC/96-43 (1996).
* [2] Report of the Steering Group of the LHC Computing Review. CERN/LHCC/2001-4
* [3] Principles of cost sharing for the ATLAS Offline Computing Requirements, September 2002..

\begin{table}
\begin{tabular}{|l|l|l|l|l|} \hline  & CERN & All T1 & All T2 & Total \\ \hline
**Tape (PB)** & **4.3** & **6.0** & **0.0** & **10.3** \\ \hline
**Disk (PB)** & **0.7** & **9.2** & **5.4** & **15.3** \\ \hline
**CPU (MSI2k)** & **5.6** & **16.6** & **5.7** & **27.9** \\ \hline \end{tabular}
\end{table}
Table 2: The required overall capacity in the various Tiers for one year of data-taking. Approximately 10 Tier-1s and 30-40 Tier-2s are expected.