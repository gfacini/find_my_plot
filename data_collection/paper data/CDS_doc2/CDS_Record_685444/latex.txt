[MISSING_PAGE_EMPTY:1]

Introduction

In this note we consider the production of Higgs via vector boson fusion (VBF). An early analysis performed at the parton level with the decay \(H\to W^{+}W^{-}\) indicated that this process could be the most powerful discovery mode in the range of the Higgs mass, \(M_{H}\), \(115<M_{H}<200\) GeV [1]. Our group has developed an improved cut analysis in this mass range based on [2, 3] and presented in [4]. In this paper we present a neural network based analysis with an identical Monte Carlo data set, similar pre-selection, and similar set of variables in order to investigate the power of multivariate algorithms.

The mass of the SM Higgs is bounded at 95% Confidence Level to be greater than 114.4 GeV from direct searches at LEP [5] and less than 193 GeV from LEP electroweak fits [6]. In this mass range the observation of the Higgs boson at the LHC is most challenging and sophisticated algorithms will certainly be used. It is our feeling that, while it is still an early stage for ATLAS, it is, nonetheless, imperative that we start developing and understanding these more sophisticated approaches. Neural networks have been used within High Energy Physics for nearly 15 years in a variety of settings, and our group has accumulated 12 years of experience with these algorithms in the context of Higgs searches [7, 8, 9]. This note marks an extension of that experience and an evaluation of those techniques in contemporary Higgs searches at the LHC.

## 2 Higgs Production via Vector Boson Fusion

The Higgs at the LHC is produced predominantly via gluon-gluon fusion. For Higgs masses such that \(M_{H}>100\) GeV the second dominant process is Vector Boson Fusion. The lowest order Feynman diagram of the production of Higgs via VBF is depicted in Figure 1. The decay channel chosen is \(H\to W^{+}W^{-}\to e^{\pm}\mu^{\mp}\nu\overline{\nu},\;e^{+}e^{-}\,\nu\overline {\nu},\;\mu^{+}\mu^{-}\nu\overline{\nu}^{1}\). Depending on \(M_{H}\), one or both of the decaying \(W\)'s may be off-shell.

Figure 1: Tree-level diagram of Vector Boson Fusion with \(H\to W^{+}W^{-}\to l^{+}l^{-}\nu\overline{\nu}\)

In addition to discovery, identification of the Higgs produced via VBF provides a privileged tool to study a number of properties of the Higgs boson due to the relatively small theoretical uncertainties in the VBF matrix element calculation [10]. In particular, the absolute normalization of the _HWW_ coupling is accessible via the extraction of the \(H\to W^{+}W^{-}\) partial width in VBF.

The basic signature of this process is:

* Two oppositely charged leptons (a muon and an electron, two electrons or two muons) with large transverse momentum, \(P_{Tl}\).
* Two energetic jets in the forward detectors (tagging jets).
* Large missing transverse momentum, \(\not{p}_{T}\).

Figure 1 illustrates the complexity of the final state for which we search. Angular correlations between the \(W\) decay products impact all variables derived from leptons and the missing transverse momentum. These relationships simultaneously make the analysis challenging and provide the handles with which to reject background. Furthermore, these relationships invite multivariate techniques capable of exploiting correlations among a number of variables. To accurately assess the sensitivity of the ATLAS detector to the detection of Higgs produced via VBF, we must examine the impact of multivariate algorithms. In this initial study, we have chosen neural networks as a benchmark multivariate algorithm.

## 3 Motivation for Neural Networks

Neural Networks have a long history and their generality has given them utility in a broad range of fields. For our purposes, however, it is best to consider a neural network as nothing more than a function from a \(d\)-dimensional input space to a \(1\)-dimensional output space parametrized by some weights \(\alpha_{i}-\mathrm{viz}\). \(f(x;\alpha):\mathbb{R}^{d}\to\mathbb{R}\). There are a number of theorems which describe the impact of the internal structure of a neural network on the class of functions to which it belongs. The most significant result is known as Kolmogorov's Superposition Theorem [11, 12, 13] which states:

**Theorem 1** (Kolmogorov's Superposition Theorem): _For each \(d\geq 2\) there exist continuous functions \(\phi_{q}:[0,1]\to\mathbb{R},\ q=0,\ldots,2d\) and constants \(\lambda_{p}\in\mathbb{R},\ p=1,\ldots,d\) such that the following holds true: for each continuous function \(F:[0,1]^{d}\to\mathbb{R}\) there exists a continuous function \(g:[0,1]\to\mathbb{R}\) such that_

\[F(x_{1},\ldots,x_{d})=\sum_{q=0}^{2d}g\left(\sum_{p=1}^{d}\lambda_{p}\phi_{q}( x_{p})\right).\]Note, \(\phi_{q}\) and \(\lambda_{p}\) are independent of the represented function \(F\).

Kolmogorov's paper, published in 1957, did not refer to neural networks directly; instead, it was in response to Hilbert's \(1\,3^{\text{th}}\) problem [14]. Exactly 30 years later Hecht-Nielsen noticed the application to the theory of neural networks [15, 16]: each continuous function \(F:[0,1]^{n}\to\mathbb{R}\) can be implemented by a feed-forward neural network with continuous activation functions \(t:[0,1]\to\mathbb{R}\).

For our purposes, the function we wish to represent is one which sends events from signal processes to a target value of 1 and events from background processes to a target value of 0. While we do not have any representation of this function, we do have Monte Carlo generated events which we can consider as samples of this function. In order to apply a function to an event we must choose some kinematic and particle identification variables to represent each event. The choice of these variables, specified in Section 6, is a practical matter guided by several considerations:

* Is the variable trustworthy? Is the Monte Carlo simulation prone to theoretical uncertainties? Are the relevant aspects of the detector simulation well-modeled?
* Does the variable discriminate between signal and background?
* Is the variable strongly correlated with other variables already included? If so, are the correlations well modeled?

We do not expect, nor do we desire, a function which exactly categorizes our signal and background training sample -a behavior referred to as _overtraining_. We know that there are regions of phase space for which either a signal or background event could occur. In such regions we do not wish for our neural network to fluctuate wildly between 0 and 1 to accommodate the training points. Instead, we desire an approximate solution which smoothly varies and has good generalization properties. Neural networks which use the so-called "sigmoid function" \(t(x)=1/(1+e^{-x})\) are known to posses good generalization properties and are the most common type for our application [17].

What Kolmogorov and Hecht-Nielsen did not specify was how to find the weights \(\alpha_{i}\) given a function \(F\) we wish to represent. Excluding neuroscience, the bulk of the literature focuses on so-called "learning algorithms" which attempt to find the optimal weights. The most widely used class of learning algorithm is called backpropagation, and is essentially a gradient-descent algorithm which aims to reduce an error function with respect to the network's weights [17, 18, 19, 20, 21]. This issue will be taken up in detail in Section 6.

Because of their generalization properties, Neural Networks have become quite common within High Energy Physics - below we cite a few examples. They first appeared in the literature for their use in triggering and other online applications [22, 23, 24]. Quickly they were used for jet and track finding in an offline environment [7, 25, 26]. In addition to b-tagging, they were specifically used for Higgs searches in the early 19 90s [8, 9, 27, 28].

Generation of Signal and Background Processes

The generation of the signal and background processes has been documented in [4], and is reproduced here for completeness. The cross-section for the VBF process has been obtained with the matrix element calculation provided within PYTHIA 6.1[29]. The Higgs branching ratio to two \(W\)'s, have been calculated with the package HDECAY[30].

A sizable contribution from the production of Higgs via gluon-gluon fusion is expected to appear. This note is concerned with the feasibility of the observation of a Higgs signal with a dedicated event selection. Therefore, the contribution from Higgs production via gluon-gluon fusion is considered as a signal processes. The production of this process has been modeled with PYTHIA 6.1.

A number of background are relevant to the channels under consideration:

* \(t\overline{t}\) production. This process yields the largest contribution to the background due to its large cross-section at the LHC and the large branching ratio \({\cal B}(t\to Wb)\). In this process, a pair of \(W\)'s and a pair of \(b\)-jets are produced. The matrix element calculation in PYTHIA 6.1 has been used to simulate this process.
* QCD 2\(W^{+}W^{-}\)+jets production. This is due to QCD emissions to the production of \(W^{+}W^{-}\). The matrix element calculation in PYTHIA 6.1 has been used to simulate this process. Footnote 2: QCD refers to processes mediated by the exchange of quarks and gluons.
* EW 3\(W^{+}W^{-}\)+jets production. These processes come from \(W^{+}W^{-}\) bremsstrahlung in quark-quark, \(q-q\), or quark-antiquark, \(q-\overline{q}\), scattering via \(t\)-channel EW boson exchange. Despite its small cross-section compared with QCD \(W^{+}W^{-}\)+jets production, the contribution of this process is the second largest in the analysis. Like the VBF signal process, the \(W^{+}W^{-}\) production produces little activity in the central area of the detector. A generator from the MadCUP 4 project has been used for this process [31]. Footnote 3: EW refers to processes mediated by the exchange of electro-weak bosons.
* QCD \(Z\)+jets production. The production of \(Z\)+jets is dominated by QCD corrections to the Drell-Yan process \(q\overline{q}\to Z,\gamma^{*}\to ee,\mu\mu,\tau\tau\). The leptonic decay of \(\tau\)'s is a source of electrons and muons and neutrinos that may be misidentified as \(W\)'s. This is relevant to the \(e\mu\) channel. The production of \(Z\)+jets with \(Z\to ee,\mu\mu\) is most relevant for the \(ee\) and \(\mu\mu\) channels. The \(2\to 2\) matrix elements 5 in PYTHIA 6.1 have been used here. Footnote 4: MadCUP stands for **Madison** Collection of **User** Processes.
* EW \(Z\)+2 jets. The matrix elements for the \(Zjj\) production via charged-current exchange and the \(W^{+}W^{-}\)-fusion have been been used. A generator from the MadCUP project has been used for this process.

For the calculation of the total cross-sections in \(pp\) collisions the CTEQ5L structure function parameterization was used [32]. The impact of initial state radiation (ISR), final state radiation (FSR), fragmentation and hadronization have been simulated with PYTHIA 6.1. In the case of the production of \(\tau^{+}\tau^{-}\) the spin correlations have been modeled with the help of the TAUOLA package [33]. In order to simulate the response of the ATLAS detector the ATLFAST 6 package version 00-02-21 has been used [34].

Footnote 6: The definition of \(p_{T}\) has been slightly modified (see Appendix A), which has a small impact on the final results

## 5 Preselection

A number of processes considered have a significantly larger cross-section compared to that of the VBF signal process. In order to save disk space, some preselection cuts have been applied at the generator level for a number of processes. Once the kinematics of the event have been produced by PYTHIA 6.1 the ATLFAST package is called. The detector reconstruction produced by ATLFAST is corrected and the following preselection is applied:

* Two oppositely charged leptons found in the pseudorapidity range 7\(|\eta|<2.5\) with \(P_{Te}>10,\mathrm{GeV}\) and \(P_{T\mu}>5,\mathrm{GeV}\), where \(P_{Te}\) and \(P_{T\mu}\) are the transverse momentum of the electrons and muons, respectively. Footnote 7: Pseudorapidity, \(\eta\), is defined as \(\eta=-\log(\tan\theta/2)\).
* Two tagging jets in opposite hemispheres, with transverse momentum greater than \(1\,\mathrm{GeV}\) and with at least two units of pseudorapidity difference between them.
* Both leptons should lie between the tagging jets (in pseudorapidity).

The processes whose generation includes these preselection cuts are: Higgs production via gluon-gluon fusion, \(t\overline{t}\) and QCD \(Z+\)jets production.

The efficiency of tagging forward jets in the ATLAS detector with a full GEANT simulation in the presence of pileup has been studied [35]. A correction has been calculated to account for detector effects that are not sufficiently well modeled in the ATLFAST simulation. The size of the correction remains within 10% in the pseudorapidity range \(|\eta|<4.5\). However, the correction is very large for larger pseudorapidities.

The remaining preselection described below is based on the so-called ATLAS reference selection (cf. [2, 3, 36]) and the low-mass optimized analysis our group presented in [4]. Before the application of the selection cuts specified below, a b-jet veto is applied in the range \(|\eta|<2.5\). The efficiency assumed for tagging \(b\)-jets in this range of pseudorapidity is 60%8. This requirement is effective in suppressing the \(t\overline{t}\) background. Additionally, we use the definition of tagging jets presented in [1]: the two highest \(P_{T}\) jets. The cuts applied in the preselection are:

Footnote 8: This corresponds to NSET = 5 when calling the routine ATLFBJE of the package ATLFAST-B.

* Two isolated, oppositely charged leptons (one electron and one muon, two electrons, or two muons) in the central detector region \(|\eta_{l}|<\,2.5\) with \(P_{T}\) requirements for electrons \(P_{Te}>15\) GeV and for muons \(P_{T\mu}>\,10\) GeV.
* Two tagging jets, with transverse momentum, \(P_{Tj_{1}}>20\) GeV, separated in pseudorapidity by \(|\eta_{j_{1}}-\eta_{j_{2}}|>3.0\). The pseudorapidities of the charged leptons are required to lie between the pseudorapidities of the tagging jets. Requirements on \(P_{T}\) and the rapidity gap have been loosened from the cut analysis.
* Charged lepton angular cuts. For the separation in polar angle between leptons it is required that \(\cos\Delta\theta_{ll}>0.2\). Additionally, we require that \(M_{ll}<100\) GeV for the \(e\mu\) channel, and \(M_{ll}<75\) GeV for the \(ee\) and \(\mu\mu\) channels. Finally, we require that the maximum transverse momentum of the leptons, \(P_{Tl}^{max}<120\) GeV. Many of the lepton angular cuts from the cut analysis have been removed.
* Tau rejection. A set of cuts was proposed to suppress the production of \(\tau^{+}\tau^{-}\) pairs from the decay of the \(Z\) boson [37]. The momentum of the \(\tau\)'s, to a good approximation, may be considered in the lab frame as being collinear with the momentum of the decaying \(\tau\)'s. Variables \(x_{\tau 1}\) and \(x_{\tau 2}\) are defined as the energy fraction of the decaying \(\tau\)'s carried by the leptons. By using the conservation of transverse momentum, \(\sum_{i=1,2}P_{T\tau i}=\sum_{i=1,2}P_{Tli}+\not{p}_{T}\). The variables \(x_{\tau 1}\) and \(x_{\tau 2}\) are the solution of two linear equations. The mass of the \(\tau^{+}\tau^{-}\) pair, \(M_{\tau\tau}\), may be computed as \(M_{\tau\tau}=M_{ll}/\sqrt{x_{\tau 1}x_{\tau 2}}\). In the case of the \(\tau^{+}\tau^{-}\) production from decays of real \(Z\)'s the values of \(x_{\tau 1}\) and \(x_{\tau 2}\) lie between 0 and 1, and \(M_{\tau\tau}\) displays a resonant structure. In the case of leptons of the decay of \(W^{+}W^{-}\) pairs, this approximation fails. Events in which \(x_{\tau 1}>0\), \(x_{\tau 2}>0\) and \(|M_{\tau\tau}-M_{Z}|<25\) GeV are rejected.
* Invariant mass of the two tagging jets, \(M_{jj}\), \(M_{jj}<5000\) GeV. The lower-bound of the cut analysis has been removed.
* Sum of the transverse momentum vectors of tagging jets, charged leptons and missing momentum. Unlike the cut analysis, this cut was not applied. We have preserved this label to remain consistent with the cut analysis.
* Jet veto. No additional jets (apart from the tagging jets) with transverse momentum greater than 20 GeV are allowed in the pseudorapidity range \(|\eta|<3.2\).
* Minimum lepton-neutrino transverse mass, \(M_{T}^{ll\nu\overline{\nu}}\). This variable is defined as: \[M_{T}^{ll\nu\overline{\nu}}=\sqrt{2P_{T}^{ll}\not{p}_{T}\cdot(1-\cos\Delta \phi)}\] (1)where \(P_{T}^{ll}\) corresponds to the modulus of the transverse momentum of the two leptons and \(\Delta\phi\) corresponds to the angle in the transverse plane between the vector of the two leptons and the missing transverse momentum. A cut is made \(M_{T}^{ll\nu\overline{\nu}}>30\;\mathrm{GeV}\). This is done primarily to suppress further \(\tau^{+}\tau-\) pairs via Drell-Yan production. In the analysis of the \(ee\) and \(\mu\mu\) channels it is required that \(\not{p}_{T}>30\,\mathrm{GeV}\).

* The transverse mass is defined as \[M_{T}=\sqrt{\left(E_{T}^{ll}+E_{T}^{\nu\overline{\nu}}\right)^{2}-\left(\overrightarrow {P}_{T}^{ll}+\overrightarrow{\not{p}}_{T}^{\overline{\jmath}}\right)^{2}}\] (2) where \[E_{T}^{ll}=\sqrt{\left(\overrightarrow{P}_{T}^{ll}\right)^{2}+M_{ll}^{2}} \hskip 28.452756pt\mathrm{and}\hskip 28.452756ptE_{T}^{\nu\overline{\nu}}= \sqrt{\left(\overrightarrow{\not{p}}_{T}^{\overline{\jmath}}\right)^{2}+M_{ ll}^{2}}.\] (3) The reconstructed transverse mass in the event is supposed to lie in the mass window, \(50<M_{T}<200\;\mathrm{GeV}\). This window has been loosened from the cut analysis.

## 6 Neural Network Training

The training of the neural network requires the choice of input variables, neural network architecture, and a learning algorithm together with the corresponding parameters. The subsequent sections describe each of these choices in detail. In addition to these basic requirements, our field has additional _de facto_ requirements. The most notable of these _de facto_ requirements is that the network be validated, or tested, on a sample independent from the training sample. In our case, each channel's sample of events was split in half. When the training samples are presented to the neural network, the effective cross-sections and jet tagging efficiencies are taken into account.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Process & \(e\mu\;\sigma_{\mathrm{eff}}\) (fb) & \(ee\;\sigma_{\mathrm{eff}}\) (fb) & \(\mu\mu\;\sigma_{\mathrm{eff}}\) (fb) \\ \hline \hline \(t\overline{t}\) & 12.79 & 4.75 & 5.22 \\ \hline EW \(WW+\mathrm{jets}\) & 1.05 & 0.39 & 0.50 \\ \hline QCD \(WW+\mathrm{jets}\) & 1.56 & 0.52 & 0.61 \\ \hline EW \(Z+\mathrm{jets}\) & 0.12 & 0.04 & 0.07 \\ \hline QCD \(\gamma^{*}/Z+\mathrm{jets}\) & 5.40 & 2.22 & 2.70 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Effective cross-section by channel for each background processes after preselection.

### Input Variables

As stressed in Section 3 the choice of input variables is a practical matter guided by several considerations. It would not be justified to use variables which rely heavily on a detailed detector description when one is only using a fast simulation. Our approach in this note is to present an analysis that parallels the cut analysis presented in [4]. Thus, we restrict ourselves to kinematic variables which were used or can be derived from the variables used in the cut analysis.

* the pseudo-rapidity difference between the two leptons,
* the azimuthal angle difference between the two leptons,
* the invariant mass of the two leptons,
* the pseudo-rapidity difference between the two tagging jets,
* the azimuthal angle difference between the two tagging jets,
* the invariant mass of the two tagging jets,
* the transverse mass.

### Network Architecture

The internal structure of a neural network, often referred to as its _architecture_, has impact both on the generalization properties of the neural network and the time required

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Process & Mass & VBF \(\sigma_{\,\mathrm{eff}}\) (fb) & gluon-gluon \(\sigma_{\,\mathrm{eff}}\) (fb) \\ \hline \hline \(e\mu\) & 115 & 0.58 & 0.15 \\ \hline \(e\mu\) & 120 & 1.05 & 0.26 \\ \hline \(e\mu\) & 125 & 1.78 & 0.48 \\ \hline \(e\mu\) & 130 & 2.53 & 0.64 \\ \hline \hline \(ee\) & 115 & 0.19 & 0.04 \\ \hline \(ee\) & 120 & 0.36 & 0.08 \\ \hline \(ee\) & 125 & 0.64 & 0.16 \\ \hline \(ee\) & 130 & 0.94 & 0.22 \\ \hline \hline \(\mu\mu\) & 115 & 0.32 & 0.07 \\ \hline \(\mu\mu\) & 120 & 0.57 & 0.13 \\ \hline \(\mu\mu\) & 125 & 0.95 & 0.23 \\ \hline \(\mu\mu\) & 130 & 1.34 & 0.31 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Effective cross-section for each of the signal processes after preselection.

to train it. Kolmogorov's Theorem suggests the network needs at least twice as many "hidden" nodes as input variables, but does not provide an upper-bound. In practice, the optimal architecture is determined empirically from the performance on an independent testing sample after a limited training time. After trying a number of network architectures we found that both the 7-10-10-1 structure (illustrated in Figure 2) and and 7-10-3-1 structure worked well. Table 3 shows the architectures used for each mass and channel.

### Learning Algorithm

After thorough testing of the options provided in and SNNS[38] we found backpropagation with momentum9 to be an efficient algorithm - in agreement with our groups previous experience [7, 8, 9]. In an independent analysis, we used MLPfit[39] with learning method 6: the BFGS method. The choice parameters for the Broyden - Fletcher - Goldfarb - Shanno (BFGS) method [40] were a reset frequency of 400 and a \(\tau\) value for the line search of 1.2. The two methods agreed within the expected and variability of different training runs. The results from the MLP analysis are reported below.

Footnote 9: A learning parameter \(\eta=0.01\) and momentum term \(\mu=0.01\) were used

Figure 2: A schematic representation of the 7-10-10-1 neural network architecture used in this study.

## 7 Neural Network Testing

Figure 3 illustrates the discriminating power of a neural network output trained for the \(e\mu\) channel with a Higgs mass between \(115-130\) GeV. In each case, the signal is concentrated near 1, while the background peaks near 0.

To gain some insight into what the neural network is doing we investigate which regions of phase space are being mapped to a Neural Network Output (NNO) close to 1 and which are being mapped to an NNO close to 0. Figure 4 shows the distribution of the seven input variables for the background-like region \(0<NNO<0.1\) and the signal like region \(0.85<NNO<0.95\) for VBF, \(t\bar{t}\), and EW \(WWjj\). Many of the features of the cut analysis are manifest in Figure 4:

* The background-like region is most strongly characterized by \(M_{jj}\lesssim 600\) GeV
* The signal-like region is characterized by \(M_{T}\lesssim M_{H}+30\) GeV.
* The signal-like region is characterized by \(M_{ll}\lesssim 60\) GeV.
* The signal-like region is characterized by \(\Delta\phi_{ll}\lesssim 1.5\) and \(\Delta\eta_{ll}<\Delta R_{ll}\lesssim 1.6\).

In contrast to the cut analysis, we see additional discriminating power in \(\Delta\eta_{jj}\). Furthermore, a number of signal events which do not satisfy the \(\Delta\phi_{ll}\) cut are still classified as signal-like \(-\,\)an indication of correlations being exploited by the neural network.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Process & Mass & Architecture & CUT & \(\sigma_{P}\) \\ \hline \hline \(e\mu\) & 115 & 7-10-10-1 & 0.89 & 2.30 \\ \hline \(e\mu\) & 120 & 7-10-3-1 & 0.87 & 3.71 \\ \hline \(e\mu\) & 125 & 7-10-10-1 & 0.86 & 5.73 \\ \hline \(e\mu\) & 130 & 7-10-10-1 & 0.83 & 7.12 \\ \hline \hline \(ee\) & 115 & 7-10-3-1 & 0.87 & 0.94 \\ \hline \(ee\) & 120 & 7-10-3-1 & 0.79 & 1.72 \\ \hline \(ee\) & 125 & 7-10-3-1 & 0.77 & 2.79 \\ \hline \(ee\) & 130 & 7-10-3-1 & 0.76 & 3.62 \\ \hline \hline \(\mu\mu\) & 115 & 7-10-3-1 & 0.85 & 1.36 \\ \hline \(\mu\mu\) & 120 & 7-10-3-1 & 0.83 & 2.36 \\ \hline \(\mu\mu\) & 125 & 7-10-3-1 & 0.82 & 3.66 \\ \hline \(\mu\mu\) & 130 & 7-10-3-1 & 0.79 & 4.77 \\ \hline \hline \end{tabular}
\end{table}
Table 3: For each channel and mass, the architecture is abbreviated \(i-j-k-l\) where \(i\) is the number of input nodes, \(j\) and \(k\) are the number of nodes in the first and second hidden layer, and \(k\) is the number of output neurons. The optimal cut on Neural Network Output and the corresponding significance for each channel and each mass is reported for an integrated luminosity of \(30\) fb\({}^{-1}\)Figure 3: The neural network output for signal (solid line) and background (dashed line) for \(M_{H}=115-130\) GeV with \(H\ \to\ W^{+}W^{-}\ \to\ e^{\pm}\ \mu^{\mp}\ \not{p}_{T}\). Notice that the signal is concentrated near 1 and the background is concentrated near 0.

Figure 4: The distribution of the input variables for VBF, \(t\bar{t}\), and EW \(WWjj\) events with a neural network output (NNO) in the background-like region \(0<NNO<0.1\) (left) and signal-like region \(0.85<NNO<0.95\) (right). These plots are meant to illustrate which regions of phase space the neural network is associating with background and signal. The neural network was trained for \(M_{H}=120\) GeV with \(H\ \rightarrow\ W^{+}W^{-}\ \rightarrow\ e^{\pm}\ \mu^{\mp}\not\!p_{T}\).

Optimizing the Cut on Neural Network Output

Once we have trained our neural network to discriminate between signal and background, we must choose a cut on the neural network output to select events considered as signal. This optimization is no different than the optimization of a cut on any other variable. In some contexts the purity may be the most appropriate measure of performance, but for a search we wish to optimize the sensitivity to signal. While the heuristic \(s/\sqrt{b}\) is very convenient, we know that it is not very accurate for moderate numbers of events. This inaccuracy manifests itself in two ways. First, for a given number of signal, \(s\), and background, \(b\), the significance is systematically too large when calculated with the heuristic. Secondly, the heuristic and the correct Poisson calculation produce qualitatively different optimal cut values. Table 3 indicates the optimal cut values obtained from using a sensitivity calculation based on Poisson statistics for each of the channels with \(30\) fb\({}^{-1}\) of data.

## 9 Calculating Significance with the Likelihood Ratio

The neural network can be considered as creating a single, powerful discriminating variable from seven input variables. For events which satisfy the cut on the neural network output, there is additional information which is not used in the Poisson significance calculation. To take advantage of this information, confidence level calculations based on the likelihood ratio have been developed and applied to Higgs Searches [5, 41]. These techniques have been applied to this analysis for both the cut-based analysis and this neural network analysis [42].

As was mentioned in [42], when one uses a discriminating variable in the likelihood ratio calculation, it is beneficial to loosen the cuts on that variable. In principle, additional phase space (independent of its purity) will only improve the significance of the analysis when computed with a discriminant variable. In practice, the entirety of the significance comes from a well defined region and the technical challenges in parametrizing the pdf's in a substantially larger region are not worth the effort. As a rule of thumb, the largest region of the discriminant variable for which the pdf can be reliably parametrized should be used.

## 10 Results and Discovery Potential

Table 5 summarizes the significance of the neural network based analyses in the Higgs mass range \(115-130\) GeV for the \(e\mu\), \(ee\), and \(\mu\mu\) channels with \(30\) fb\({}^{-1}\) of data. Combined significance values are obtained from likelihood ratio techniques described in [42]. For comparison, Table 4 shows the corresponding results for the low-mass optimized cut analysis presented in [4]. The neural network based analysis without the use of

discriminating variables achieves a 20-40% improvement over the cut-based analysis. This is due to the exploitation of correlations between the variables (recall, the same variables are used in both analyses). Furthermore, the use of discriminating variables in the confidence level calculation improves the significance by about an additional 15%.

## 11 Stability of the Neural Network with respect to Cuts

We now turn briefly to the stability of the neural networks with respect to theoretical uncertainties in the Monte Carlo. We have considered two different parton shower models and two different matrix elements for the \(t\overline{t}\) background. The identical neural network and cut analysis were used to estimate the effective cross-section for the different \(t\overline{t}\) background samples10. In contrast to theoretical uncertainties in the leading order cross-sections, the parton shower uncertainties do not necessarily apply equally to the cut and neural network analysis.

Footnote 10: The 20 % increase in the \(t\overline{t}\) cross-section used in Reference [4] to account for finite width effects has been neglected in this section.

In order to estimate the sensitivity to the parton shower model, we have used PYTHIA and HERWIG interfaced to an external matrix element calculation provided by the MadCUP project [31]. The use of a common external matrix element sample

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline \(M_{H}\) (GeV) & Channel & \(s\) & \(b\) & \(s/\sqrt{b}\) & \(\sigma_{P}\) & \(\sigma_{L}\) \\ \hline
115 & e \(\mu\) & 7.54 & 14.80 & 1.96 & 1.67 & 2.04 \\  & e e & 2.63 & 10.53 & 0.81 & 0.64 & 0.87 \\  & \(\mu\)\(\mu\) & 4.09 & 11.99 & 1.18 & 1.02 & 1.31 \\  & Combined & 14.26 & 37.33 & 2.43 & 2.27 & 2.57 \\ \hline
120 & e \(\mu\) & 13.28 & 16.23 & 3.30 & 2.79 & 3.35 \\  & e e & 4.80 & 10.94 & 1.45 & 1.34 & 1.49 \\  & \(\mu\)\(\mu\) & 7.12 & 12.49 & 2.01 & 1.63 & 2.10 \\  & Combined & 25.19 & 39.67 & 4.13 & 3.74 & 4.23 \\ \hline
125 & e \(\mu\) & 22.11 & 17.33 & 5.31 & 4.41 & 4.91 \\  & e e & 8.23 & 11.87 & 2.39 & 2.07 & 2.38 \\  & \(\mu\)\(\mu\) & 11.69 & 13.80 & 3.15 & 2.63 & 3.16 \\  & Combined & 42.03 & 43.00 & 6.62 & 5.77 & 6.31 \\ \hline
130 & e \(\mu\) & 30.62 & 18.64 & 7.09 & 5.78 & 6.28 \\  & e e & 11.69 & 12.41 & 3.32 & 2.84 & 3.19 \\  & \(\mu\)\(\mu\) & 16.23 & 14.56 & 4.25 & 3.68 & 4.10 \\  & Combined & 58.54 & 45.61 & 8.91 & 7.48 & 8.16 \\ \hline \end{tabular}
\end{table}
Table 4: Comparison of significance calculations obtained with the optimized event selection with 30 fb\({}^{-1}\) of data for different channels and Higgs masses [4].

allows for the isolation of the systematic uncertainty due to the parton shower model. Table 6 indicates the effective cross-sections, after the succesive cuts documented in [4], for both samples. After all cuts, we see that the effective cross-section of the sample processed with HERWIG is only 61 \(\pm\) 9% of the sample processed with PYTHIA. Table 7 shows the corresponding situation for the neural network analysis. After pre-selection cuts, the ratio of effective cross-sections is 86%. After placing a cut on the neural network output, at the value optimized on the training sample (see Table 3), the ratio is 68 \(\pm\) 7%.

Both the cut and neural network analysis are sensitive to the parton shower model, implying a theoretical uncertainty in the effective cross-section at the level of 35% for the regions of phase space selected in the analyses. In Table 6 it can be seen that the cuts which are most affected by the parton shower model are the forward tagging, jet mass, and central jet veto. The improvement of the neural network analysis relative to the cut analysis seems to be fairly stable.

The \(t\overline{t}\) training sample produced with PYTHIA's internal process does not correctly model the angular correlations between the top anti-top decay products. However, the MadCUP matrix element approach does include all spin correlations in the top decay and subsequent W decays. We have compared PYTHIA's internal \(t\overline{t}\) sample to the sample produced with the MadCUP matrix element interfaced to PYTHIA's parton

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline \(M_{H}\) (GeV) & Channel & \(s\) & \(b\) & \(s/\sqrt{b}\) & \(\sigma_{P}\) & \(\sigma_{L}\) \\ \hline
115 & e \(\mu\) & 9.83 & 14.96 & 2.54 & 2.30 & 2.61 \\  & e e & 3.69 & 12.25 & 1.05 & 0.94 & 1.18 \\  & \(\mu\)\(\mu\) & 6.31 & 19.37 & 1.43 & 1.36 & 1.56 \\  & Combined & 19.82 & 46.58 & 3.10 & 2.87 & 3.26 \\ \hline
120 & e \(\mu\) & 17.70 & 17.17 & 4.27 & 3.71 & 4.33 \\  & e e & 8.09 & 18.74 & 1.87 & 1.72 & 2.05 \\  & \(\mu\)\(\mu\) & 11.04 & 17.83 & 2.62 & 2.36 & 2.84 \\  & Combined & 36.84 & 53.74 & 5.35 & 4.78 & 5.58 \\ \hline
125 & e \(\mu\) & 30.07 & 18.82 & 6.93 & 5.73 & 6.85 \\  & e e & 14.20 & 21.51 & 3.06 & 2.79 & 3.19 \\  & \(\mu\)\(\mu\) & 17.71 & 18.05 & 4.17 & 3.66 & 4.34 \\  & Combined & 61.98 & 58.38 & 8.65 & 7.72 & 8.72 \\ \hline
130 & e \(\mu\) & 43.74 & 24.66 & 8.81 & 7.12 & 8.38 \\  & e e & 19.83 & 21.75 & 4.25 & 3.62 & 4.33 \\  & \(\mu\)\(\mu\) & 25.92 & 21.21 & 5.63 & 4.77 & 5.66 \\  & Combined & 89.52 & 67.59 & 11.29 & 9.47 & 11.0 \\ \hline \end{tabular}
\end{table}
Table 5: Comparison of significance calculations obtained with the neural network analysis with 30 fb\({}^{-1}\) of data for different channels and Higgs masses.

shower.11 Effects from the choice of factorization scale and PYTHIA's treatment of external processes are thus convoluted with the sensitivity to these angular correlations. In Table 8 it can be seen that the cuts with the most significant variations between PYTHIA's internal and the MadCUP matrix element are cuts **c**, **f**, and **i**. Cuts **c** and **i** are based on the angular correlations between the leptons, thus are sensitive to the correlations between the top decay products. Cut **f** is sensitive to QCD radiation and thus the choice of factorization scale. Table 9 indicates that by going from PYTHIA's internal \(t\bar{t}\) calculation to the MadCUP matrix element, the effective cross section drops 35% for the optimized cut analysis and 36% for the neural network analysis. Again the improvement of the neural network with respect to the cut analysis is quite stable.

Footnote 11: It is somewhat misleading to refer to the differences between the two samples as “theoretical uncertainty”, since we are quite sure that the spin correlations should be present.

The use of the neural network output as a discriminating variable is contingent upon the stability in its shape. Figure 5 illustriates the stability of the neural network output for the three different \(t\bar{t}\) background samples considered. While the effective cross-sections for the three samples differ significantly, the shape of the neural network output appears to be quite stable.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Cut & HERWIG (fb) & PYTHIA (fb) & HERWIG/PYTHIA & \(\epsilon_{H}/\epsilon_{P}\) \\ \hline \hline a & 1132 \(\pm\) 1.8 & 1268 \(\pm\) 0.8 & 89\% \(\pm\) 0\% & \\ \hline b & 33.44 \(\pm\) 0.307 & 39.83 \(\pm\) 0.146 & 84\% \(\pm\) 1\% & 94\% \\ \hline c & 3.394 \(\pm\) 0.097 & 4.487 \(\pm\) 0.049 & 75\% \(\pm\) 2\% & 90\% \\ \hline d & 2.914 \(\pm\) 0.090 & 3.806 \(\pm\) 0.045 & 76\% \(\pm\) 2\% & 101\% \\ \hline e & 1.489 \(\pm\) 0.064 & 2.189 \(\pm\) 0.034 & 68\% \(\pm\) 3\% & 89\% \\ \hline f & 0.333 \(\pm\) 0.030 & 0.538 \(\pm\) 0.017 & 61\% \(\pm\) 6\% & 91\% \\ \hline g & 0.171 \(\pm\) 0.022 & 0.263 \(\pm\) 0.012 & 65\% \(\pm\) 8\% & 105\% \\ \hline h & 0.137 \(\pm\) 0.019 & 0.227 \(\pm\) 0.011 & 60\% \(\pm\) 9\% & 93\% \\ \hline i & 0.124 \(\pm\) 0.018 & 0.201 \(\pm\) 0.010 & 61\% \(\pm\) 9\% & 101\% \\ \hline \end{tabular}
\end{table}
Table 6: Effective cross-sections after succesive cuts for \(t\bar{t}\) events simulated with the MadCUP external matrix element interfaced to HERWIG and PYTHIA’s parton shower and hadronization routines. The cuts correspond to the optimized cut analysis for VBF \(H\to\ WW\ \to\ e\mu\not{p}_{T}\) for \(M_{H}=130\) GeV. The collumn labeled \(\epsilon_{H}/\epsilon_{P}\) represents the ratio of the efficiency of each cut for the HERWIG sample to the efficiency for the PYTHIA sample.

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Cut & HERWIG (fb) & PYTHIA (fb) & HERWIG/PYTHIA & \(\epsilon_{H}/\epsilon_{P}\) \\ \hline \hline Lepton Acceptance & 11 32 \(\pm\) 1.8 & 1268 \(\pm\) 0.8 & 89\% \(\pm\) 0\% & \\ \hline cut \(NNO>0.83\) & 0.192 \(\pm\) 0.021 & 0.280 \(\pm\) 0.011 & 68\% \(\pm\) 7\% & 77\% \\ \hline \end{tabular}
\end{table}
Table 7: Same as Table 6 for the Neural Network analysis.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Cut & MadCUP (fb) & PYTHIA (fb) & MadCUP/PYTHIA & \(\epsilon_{M}/\epsilon_{P}\) \\ \hline \hline b & 39.83 \(\pm\) 0.146 & 40.86 \(\pm\) 0.049 & 97\% \(\pm\) 0\% & \\ \hline c & 4.487 \(\pm\) 0.049 & 6.224 \(\pm\) 0.019 & 72\% \(\pm\) 1\% & 74 \% \\ \hline d & 3.806 \(\pm\) 0.045 & 5.948 \(\pm\) 0.019 & 64\% \(\pm\) 1\% & 89 \% \\ \hline e & 2.189 \(\pm\) 0.034 & 2.926 \(\pm\) 0.013 & 75\% \(\pm\) 1\% & 117\% \\ \hline f & 0.538 \(\pm\) 0.017 & 0.912 \(\pm\) 0.007 & 59\% \(\pm\) 2\% & 79 \% \\ \hline g & 0.263 \(\pm\) 0.012 & 0.481 \(\pm\) 0.005 & 55\% \(\pm\) 2\% & 93 \% \\ \hline h & 0.227 \(\pm\) 0.011 & 0.463 \(\pm\) 0.005 & 49\% \(\pm\) 2\% & 90 \% \\ \hline i & 0.201 \(\pm\) 0.010 & 0.310 \(\pm\) 0.004 & 65\% \(\pm\) 3\% & 1 32\% \\ \hline \end{tabular}
\end{table}
Table 8: Same as Table 6 for \(t\overline{t}\) events simulated with PYTHIA’s internal \(t\overline{t}\) simulation and MadCUP external matrix element interfaced to PYTHIA’s parton shower and hadronization routines. Cut **a** is excluded because the generator-level cuts are different for the two samples.

\begin{table}
\begin{tabular}{|l|l|l|c|} \hline \(M_{H}=130\) & MadCUP (fb) & PYTHIA (fb) & MadCUP/PYTHIA \\ \hline \hline Optimized Cut & 0.201 \(\pm\) 0.010 & 0.310 \(\pm\) 0.004 & 65 \(\pm\) 3\% \\ \hline NN & 0.280 \(\pm\) 0.011 & 0.436 \(\pm\) 0.005 & 64 \(\pm\) 3\% \\ \hline \end{tabular}
\end{table}
Table 9: Effective cross-sections for \(t\overline{t}\) events simulated with PYTHIA’s internal \(t\overline{t}\) simulation and MadCUP external matrix element interfaced to PYTHIA’s parton shower and hadronization routines.

Figure 5: Neural Network output distribution for three different samples of \(t\bar{t}\) background samples.

Conclusions

In conclusion, we have implemented a neural network based analysis for the Vector Boson Fusion process with \(H\to W^{+}W^{-}\to l^{+}l^{-}\nu\overline{\nu}\). The variables used within the neural network are the same as those used in the cut analysis; however, the neural network shows a strong improvement in the significance. This is because the complicated final state of the process gives rise to correlations among the variables. Multivariate algorithms are able to exploit these correlations leading to 25-35% improvements in the combined significance across the mass range 115-130 GeV. We have considered parton shower and matrix element theoretical uncertainties, and the improvement is fairly stable.

## 13 Acknowledgments

We would like to thank Karina Loureiro and Boris Levchenko for their contributions to the training of the Neural Networks used in this note.

Another difference between this analysis and the analysis of the previous section is that here, no modifications were applied to the missing \(p_{T}\). ATLFAST-B applies energy corrections to the jets but does not adjust the missing \(p_{T}\). Thus, the missing \(p_{T}\) plus the total transverse momentum of all recalibrated jets and leptons in the event does not add up to \(\vec{0}\) (unclustered cells and clusters not classified as jets still remain in the event). The analysis of the previous section applied a modification to the missing \(p_{T}\) that forced the total transverse momentum in the event to be \(\vec{0}\). For the analysis of this section, we do not apply this modification. The impact of these changes is small and can be estimated by comparing Table 1 with Table 10, Table 2 with Table 11, and Table 5 with Table 13.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Process & \(e\mu\)\(\sigma_{\rm eff}\) (fb) & \(ee\)\(\sigma_{\rm eff}\) (fb) & \(\mu\mu\)\(\sigma_{\rm eff}\) (fb) \\ \hline \hline \(t\bar{t}\) & 11.4 & 4.20 & 4.89 \\ \hline EW \(WW+{\rm jets}\) & 1.08 & 0.40 & 0.51 \\ \hline QCD \(WW+{\rm jets}\) & 1.65 & 0.52 & 0.63 \\ \hline EW \(Z+{\rm jets}\) & 0.11 & 0.07 & 0.13 \\ \hline QCD \(\gamma^{*}/Z+{\rm jets}\) & 2.65 & 1.77 & 2.75 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Same as Table 1 for the parallel analysis.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Process & Mass & VBF \(\sigma_{\rm eff}\) (fb) & gluon-gluon \(\sigma_{\rm eff}\) (fb) \\ \hline \hline \(e\mu\) & 115 & 0.60 & 0.15 \\ \hline \(e\mu\) & 120 & 1.08 & 0.28 \\ \hline \(e\mu\) & 125 & 1.75 & 0.46 \\ \hline \(e\mu\) & 130 & 2.61 & 0.67 \\ \hline \hline \(ee\) & 115 & 0.20 & 0.04 \\ \hline \(ee\) & 120 & 0.37 & 0.09 \\ \hline \(ee\) & 125 & 0.62 & 0.16 \\ \hline \(ee\) & 130 & 0.96 & 0.23 \\ \hline \hline \(\mu\mu\) & 115 & 0.32 & 0.08 \\ \hline \(\mu\mu\) & 120 & 0.58 & 0.14 \\ \hline \(\mu\mu\) & 125 & 0.93 & 0.23 \\ \hline \(\mu\mu\) & 130 & 1.43 & 0.33 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Same as 2 for the parallel analysis.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Mass & \(e\mu\) & \(ee\) & \(\mu\mu\) \\ \hline \hline
115 & 0.89 & 0.81 & 0.83 \\ \hline
120 & 0.87 & 0.83 & 0.85 \\ \hline
125 & 0.83 & 0.77 & 0.80 \\ \hline
130 & 0.82 & 0.76 & 0.74 \\ \hline \end{tabular}
\end{table}
Table 1: Same as Table 5 for parallel analysis.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline \(M_{H}\) (GeV) & Channel & \(s\) & \(b\) & \(s/\sqrt{b}\) & \(\sigma_{P}\) & \(\sigma_{L}\) \\ \hline \hline
115 & e \(\mu\) & 9.26 & 13.49 & 2.52 & 2.28 & 2.80 \\  & e e & 4.53 & 20.17 & 1.01 & 0.97 & 1.18 \\  & \(\mu\)\(\mu\) & 6.85 & 22.84 & 1.43 & 1.37 & 1.75 \\  & Combined & 20.64 & 56.50 & 3.06 & 2.84 & 3.51 \\ \hline
120 & e \(\mu\) & 16.98 & 15.69 & 4.29 & 3.74 & 4.46 \\  & e e & 7.43 & 16.27 & 1.84 & 1.72 & 2.08 \\  & \(\mu\)\(\mu\) & 10.25 & 16.52 & 2.52 & 2.29 & 2.81 \\  & Combined & 34.66 & 48.48 & 5.30 & 4.72 & 5.67 \\ \hline
125 & e \(\mu\) & 29.25 & 20.44 & 6.47 & 5.46 & 6.53 \\  & e e & 1 3.93 & 22.74 & 2.92 & 2.68 & 3.12 \\  & \(\mu\)\(\mu\) & 1 8.62 & 26.06 & 3.65 & 3.31 & 3.97 \\  & Combined & 61.80 & 69.24 & 7.82 & 6.91 & 8.25 \\ \hline
130 & e \(\mu\) & 4 8.02 & 28.76 & 8.95 & 7.04 & 8.87 \\  & e e & 20.91 & 24.98 & 4.18 & 3.71 & 4.36 \\  & \(\mu\)\(\mu\) & 28.60 & 32.08 & 5.05 & 4.49 & 5.58 \\  & Combined & 97.53 & 85.82 & 11.09 & 9.42 & 11.4 \\ \hline \end{tabular}
\end{table}
Table 2: Cut values on Neural Network output for the parallel analysis.

[MISSING_PAGE_EMPTY:24]

Figure 8: The improvement in the combined significance for VBF \(H\to WW\) as a function of the Higgs mass, \(M_{H}\). Each of the three improved analyses are compared to the cut-based analysis with number counting (Poisson statistics). The black (solid) line shows the improvement due to the use of the likelihood ratio with the transverse mass as a discriminant variable. The red (dashed) line shows the improvement due to the use of a neural network for event selection. The green (dotted) line shows the improvement due to the use of the likelihood ratio with the neural network output as a discriminant variable.

## References

* [1] D. Rainwater and D. Zeppenfeld, Phys. Rev. **D60** (1999) 113004; N. Kauer et al., Phys. Lett. **B503** (2001) 113.
* [2] C. Buttar, R. Harper, K. Jakobs, Weak Boson Fusion \(H\to WW^{(\star)}\to l^{+}l^{-}P_{T}^{m\,ss}\) as a Search Mode for an Intermediate Mass SM Higgs Boson at ATLAS, ATLAS Internal Note ATL-PHYS-2002-033.
* [3] The Higgs Working Group, D. Cavalli et al., Summary Report, hep-ph/0203056 (2002).
* [4] K. Cranmer, P. McNamara, B. Mellado, W. Quayle, Sau Lan Wu, Search for Higgs Bosons Decay \(H\to W^{+}W^{-}\to l^{+}l^{-}/\!\!\!\!/_{T}\) for \(115<M_{H}<130\) GeV Using Vector Boson Fusion, ATLAS internal note ATL-PHYS-2003-002 (2002).
* [5] LEP Higgs Working Group, Search for the Standard Model Higgs Boson at LEP, LHWG Note/2002-01 (2002).
* [6] LEP Electroweak Working Group, A Combination of Preliminary Electroweak Measurements and Constraints on the Standard Model, CERN-EP/2002-091 and hep-ex/0212036.
* [7] L. Bellantoni, J.S. Conway, J.E. Jacobsen, Y. Pan, and Sau Lan Wu, Nucl. Instrum. Meth. **A310** (1991) 618-622.
* [8] D. Decamp et. al., Phys. Lett. **B265** (1991) 475-486.
* [9] D. Buskulicet al., Phys. Lett. **B313** (1993) 299-311.
* [10] D. Zeppenfeld et al., Phys. Rev. **D62** (2000) 013009.
* [11] A.N. Kolmogorov, Dokl. Akad. Nauk USSR **114** (1957) 953-956 [translated in: American Mathematical Society Translations **28** (1963) 55-59].
* [12] G.G. Lorentz, Approximation of Functions (Athena Series, Selected Topics in Mathematics. Holt, Rinehardt and Winston, Inc., New York, 1966).
* [13] D. Sprecher, Transactions American Mathematical Society **115(3)** (1965) 340-355.
* [14] D. Hilbert, Nachrichten der Koniglichen Gesellshaft der Wissenschaften zu Gottingen (1900) 253-297.
* [15] R Hecht-Nielsen, Proceedings IEEE International Conference On Neural Networks **Volume II** (1987) 11-13.
* [16] R Hecht-Nielsen, Neurocomputing (Addison-Wesley, Reading, 1990).

* [17] P.J. Werbos, The Roots of Backpropagation (John Wiley & Sons., New York, 1974).
* [18] H. Robbins and S. Monroe, Annals of Mathematical Statistics **22** (1951).
* [19] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, Parallel Distributed Processing Explorations in the Microstructure of Cognition (The MIT Press, Cambridge, 1986).
* [20] B.T. Polyak, Z. Vycisl. Mat. i Mat. Fiz. **4** (1964) 1-17.
* [21] B.T. Polyak, Introduction to Optimization (Optimization Software, Inc., New York, 1987).
* [22] Denby and H. Bruce, Applications of neural networks and cellular automata in experimental high-energy physics, In *Trieste 1988, Proceedings, The impact of digital microelectronics and microprocessors on particle physics* 150-153..
* [23] C. Barter. et. al., NEURAL NETWORKS, D0, AND THE SSC, Presented at the Workshop on Triggering and Data Acquisition for Experiments at the Supercollider, Toronto, Canada, January 1989.
* [24] L. Lonnblad C. Peterson and T. Rognvaldsson, Phys. Rev. Lett. **65** (1990) 1321-1324.
* [25] T. D. Gottschalk and R. Nolty, Identification of physics processes using neural network classifiers, CALT-68-1680.
* [26] B. Humpert, Comput. Phys. Commun. **56** (1990) 299-311.
* [27] P. Chiappetta, P. Colangelo, P. De Felice, G. Nardulli and G. Pasquariello, Phys. Lett. **B322** (1994) 219-223.
* [28] T. Maggipinto et. al., Phys. Lett. **B409** (1997) 517-522.
* [29] T. Sjostrand, High-Energy Physics Event Generation with PYTHIA 5.7 and JETSET 7.4, Comp. Phys. Comm. **82** (1994) 74; T. Sjostrand et al., High-Energy-Physics Event Generation with PYTHIA 6.1, Comp. Phys. Comm. **135** (2001) 238.
* [30] A. Djouadi, J. Kalinowski and M. Spira, HDECAY: a Program for Higgs Boson Decays in the Standard Model and its Supersymmetric Extention, Comp. Phys. Comm. **108** (1998) 56.
* [31] D. Zeppenfeld et al., The home page of the MadCUP Project, [http://pheno.physics.wisc.edu/Software/MadCUP/](http://pheno.physics.wisc.edu/Software/MadCUP/).
* [32] H. L. Lai et al., Eur. Phys. J. **C12** (2000) 375.

* [33] S. Jadach et al., The tau decay library TAUOLA, version 2.4, Comp. Phys. Comm. **76** (1993) 361.
* [34] E. Richter-Was, D. Froidevaux and L. Poggioli, ATLFAST2.0 a Fast Simulation Package for ATLAS, ATLAS Internal Note ATL-PHYS-98-131.
* [35] V. Cavasinni, D. Costanzo, I. Vivarelli, Forward Tagging and Jet Veto Studies for Higgs Events Produced via Vector Boson Fusion, ATLAS communication ATL-COM-CAL-2002-003 (2002).
* [36] K. Jakobs, Status of the Scientific Note of VBF, Presetation given at the Higgs working group meeting on 29/05/02.
* [37] R. K. Ellis et al., Nucl. Phys. **B297** (1988) 221.
* [38] The homepage of the Stuttgart Neural Network Simulator (SNNS), [http://www-ra.informatik.uni-tuebingen.de/SNNS](http://www-ra.informatik.uni-tuebingen.de/SNNS).
* [39] The homepage of MLPfit, [http://schwind.home.cern.ch/schwind/MLPfit.html](http://schwind.home.cern.ch/schwind/MLPfit.html).
* [40] R. Fletcher, Practical Methods of Optimization, second eddition (Wiley, New York, 1987).
* [41] H. Hu, J. Nielsen, Analytic Confidence Level Calculations Using the Likelihood Ratio and Fourier Transform, "Workshop on Confidence Limits", Eds. F. James, L. Lyons and Y. Perrin, CERN 2000-005 (2000), p. 109.
* [42] K. Cranmer, P. McNamara, B. Mellado, W. Quayle, Sau Lan Wu, Confidence Level Calculations for \(H\to W^{+}W^{-}\to l^{+}l^{-}/\!\!\!\!/_{T}\) for \(115<M_{H}<130\) GeV Using Vector Boson Fusion, ATLAS communication ATL-COM-PHYS-2002-049 (2002).