# The Supervisor of the Atlas Detector Control System

H.J. Burckhart, J. Cook, F. Varela, B. Varnai, CERN, Geneva, Switzerland

V. Filimonov, V. Khomoutnikov, Y. Ryabov, PNPI, St.Petersburg, Russia

###### Abstract

The design and the implementation of the supervisor system of the ATLAS Detector Controls are described. The interaction with the Data Acquisition system, which is needed for the coherent operation of the experiment, and in particular the use of Finite State Machine tools, is discussed.

## 1 Introduction

ATLAS is a general-purpose High Energy Physics (HEP) experiment, scheduled to start data taking in 2007 at the proton collider LHC at CERN, Geneva, Switzerland. Its size and complexity is unprecedented in HEP, in both technical terms - the detector elements are distributed over a cylindrical volume of 25m diameter and 45m length - and organisationally - 1500 people of 150 institutes in 35 countries contribute. Equally outstanding are its complexity and segmentation: e.g. the Pixel sub-detector alone already accounts for 140 million electronic channels. These new features lead to new requirements on the Detector Control System (DCS).

The very long timescale of more than three decades for the phases R&D, construction, and exploitation requires the control system to be able to evolve from small, very flexible systems for prototyping up to the final size for operation of the experiment as a whole. The detailed requirements on the individual components and their repercussions on the design and implementation will be discussed in the chapters following.

## 2 Architecture

The ATLAS detector is hierarchically organized in a tree-like structure into sub-detectors, sub-systems, etc. where each element has a certain level of operational independence. This must be reflected in the design and implementation of the DCS.

The traditional separation of the control system into several Front-End (FE) systems and into a common Back-End (BE) has been followed. The former are placed close to the detector and provide signal conditioning, digitisation and, to some extent, data reduction. The latter performs processing, visualization, and storage of this data and its main function is the overall supervision of the detector.

## 3 Front-End Systems

The FE is the responsibility of the sub-detector groups and the selection of the equipment is mainly determined by the detector hardware. The FE is the responsibility of the sub-detector groups and the selection of the equipment is mainly determined by the detector hardware. However, a set of common solutions for the interface to the BE has been adopted. In particular, most sub-detectors have selected the client-server mechanism OPC [1].

Many of the FE devices have to operate in the cavern of the experiment, where special environmental conditions prevail. The controls equipment is exposed to ionizing radiation of about 1 Gray per year and a flux of 3*10\({}^{10}\) neutrons per year. In addition there is a strong magnetic field of up to 1.5 Tesla present. Therefore all equipment to be deployed in this hostile environment is subject to a stringent selection and qualification procedure.

There are essentially two categories of FE devices: commercial systems like power supplies, electronics crates, etc. and purpose-built equipment like for the cooling or for the supervision of an individual detector element. For the control of the second class a flexible I/O system, the Embedded Local Monitor Board (ELMB) has been developed in order to achieve as much homogeneity as possible and therefore to save development effort and ease maintenance.

The ELMB features 64 analogue input channels of 16-bit resolution, 24 digital input/output lines and a serial bus to connect further I/O devices. More details can be found in [2]. The ELMB can either be integrated into more complex devices such as high or low voltage systems for their control or it can work stand-alone with sensors and actuators directly connected to it. It performs local data processing and communicates via a CAN field bus network with the BE. As the ELMB tolerates the harsh environment mentioned above and can hence be placed in the experimental cavern, it reduces enormously the size and complexity of cabling. In ATLAS the order of 5000 ELMBs will be used, which corresponds to more than 400.000 channels.

## 4 Back End System

The BE analyzes, presents, and stores the data, provides the interface for the operator, and also executes automated supervisory functions as described below. It is organised in 3 functional layers as shown in Fig. 1.

The top-most level comprises the Global Control Stations (GCS), which provide the tools for the overall operation of the detector, such as an operator interface for commands and for data visualization, a status and alarm system and an Information Server (DCS_IS) for data exchange with systems external to the DCS. Also, as shown in Fig. 1, Web connections are handled at this level.

The next layer down consists of the Sub-detector Control Station (SCS), one for each of the 9 sub-detectors and one for the Common Infrastructure Control (CIC). The SCS provides full stand-alone controls capability, including a human interface, and in particular all commands from the different sources pass through this point for validation. It synchronizes the supervision of all subsystems below it and combines their status into an overall sub-detector status. Procedures without operator interaction can be executed as well at this level i.e. to take an "automatic" corrective action in order to keep the equipment in safe operating conditions, using all relevant information from other BE stations and from external systems, which are transmitted by the DCS_IS.

The lowest level consists of the Local Control Stations (LCS), which read the data from the FE and supervise subsystems or parts of a sub-detector. They can be organized either according to the topological segmentation of a sub-detector as shown for the Tile calorimeter or they can follow functional criteria like it is the case for the Liquid Argon (LAr) calorimeter. Additional levels of LCS can be introduced if required by the sub-systems. The LCS perform more complex calculations like detailed calibrations and may store the results in a database. They can also send commands to the FE hardware and have in this way the capability to autonomously supervise a specific subsystem, including closed-loop control. Logging of commands, errors and data is possible at all three levels of the BE.

The implementation of all the BE system is based on the commercial software package PVSS-II [3], which runs on PC under either the Windows or the Linux operating system. The communication between the different PVSS-II stations is entirely handled by the package over a LAN. This software product has been chosen within the Joint COntrols Project (JCOP), which provides additional components and support to all four LHC experiments for building their DCS as described in another contribution to this conference [4].

## 5 Connection DCS - DAQ

The ATLAS Data Acquisition system (DAQ) [5] reads the so-called "physics event data", the response of the detector to a proton-proton interaction. The Online Software component, which provides the synchronisation and the control within the DAQ, interacts with DCS as shown in Fig. 2 in order to enable the coherent operation of the experiment as a whole.

The elements involved in the communication with the DCS are the DAQ Information System (DAQ_IS), the DAQ Message Reporting System (DAQ_MRS) and the Run Control (RC).

A software package, called DAQ-DCS Connection (DDC) has been implemented, which consists of three parts. Data are exchanged asynchronously in both directions between the DAQ_IS and all levels of the DCS BE. The data describes the present status of the detector, of the DAQ, and of other systems. Typical examples are a high voltage value of a chamber, the present DAQ run number, or the actual luminosity of the LHC, respectively. The data may be used by DCS or by DAQ as conditions to allow actions.

The second function of DDC is the transfer of messages from all levels of the DCS BE to the DAQ_MRS. These can simply contain information for the DAQ operator, but they can also signal operational problems of the detector, like a high voltage trip, in order to automatically suspend physics data taking.

The third function of DDC is the transmission of commands from DAQ to DCS. Also in the DAQ the detector is hierarchically modelled as Fig. 2 shows with the RC tree and its different controllers. For each sub-detector one controller is dedicated to DCS and sends commands to the relevant SCS. These commands essentially trigger the execution of pre-defined procedures in DCS with the possibility of also transmitting parameters. The result of the command execution is sent back to DAQ. A typical example is a command to ramp up the high voltage system of a sub-detector in order to

Figure 1: Hierarchical organisation of the BE.

Figure 2: Interaction of DCS with DAQ

set it ready for data taking. Both DAQ and DCS will be operated as Finite State Machines (FSM). Details about the implementation on the DCS side will be given in the next chapter.

## 6 Operation as Fsm

A sub-detector is the main autonomous entity for DCS. It can be operated in three ways: either stand-alone from the SCS, by the DAQ as described in the previous chapter, or by the GCS. Different operational modes exist, like physics data taking, calibration or debugging. DAQ defines so-called partitions, into which sub-detectors or parts of them are placed in order to operate them together in one of these modes. DCS has to dynamically follow this partitioning and to group the parts of the detector accordingly.

In order to provide this functionality, the different sub-detectors and systems will be logically represented in the BE by means of control units. These are logical entities characterized by well-defined states and transitions between them.

Figure 3 shows a generic state model for a sub-detector. There are several states defined and transitions between them can be triggered by appropriate commands. Asynchronous transitions are possible as well, e.g. most malfunctions will cause to transit to the "Error" state.

The control units are organized in a tree-like structure to reproduce the hierarchical organization of the ATLAS detector as shown in Fig. 4. Each control unit has the capability to report its own state or to pass commands to other control units in the hierarchy. The data flow will only be vertical, commands flowing downwards and status and alarms being transferred upwards in the hierarchy. A command may trigger a state change at a lower level in the hierarchy, which in turn may cause then states to change higher up. Failure of FE equipment, which is placed at the bottom of the hierarchy, will cause an error condition, which will be transmitted to the higher levels according to the rules defined with the FSM tool.

An example is given in Fig. 4, where the device T1 of the cooling for the segment EBA of the Tile calorimeter signals an overheating, and the error is signalled up to the top. In addition at the level EBA a command is generated to switch off the appropriate high voltage unit.

An important aspect of the FSM approach is the possibility to distribute the different control units amongst the various PC shown in Fig. 1. In order to enable the flexible operational schema presented above, the SCS has been chosen to be the central place for validating commands and where the mastership for a sub-detector is assigned, either to DAQ, or to the GCS, or stand-alone.

## 7 Summary

The supervisor part of the control system has been modelled according to the hierarchical structure of the detector. The commercial software package PVSS-II allows such a distributed system to be built and provides the necessary communication. The interaction with DAQ, required for coherent operation of the experiment, is achieved via the dedicated software package DDC. Both the DAQ and DCS operate as Finite State Machines. This approach facilitates the description of the logical connection between the different levels and will provide the necessary flexibility to set up the operational procedures of the experiment.

## References

* [1] OLE for Process Control, [http://www.opcfoundation.org/](http://www.opcfoundation.org/)
* [2] H. Boterenbrood and B. Hallgren, "The development of the Embedded Local Monitor Board (ELMB)", Proceedings of 9th Workshop on Electronics for LHC Experiments, Amsterdam, The Netherlands, Sept 2003
* [3] PVSS-II, [http://www.pvss.com](http://www.pvss.com)
* [4] Wayne Salter on behalf of the Joint COntrols Project, "Status of the LHC Experiments' control systems", ICALEPCS'03, Gyeongju, Korea, October 2003
* [5] The ATLAS TDAQ Collaboration, "ATLAS High-Level Trigger Data Acquisition and Controls Technical Design Report", CERN/LHCC/2003-022, ATLAS TDR 016, June 2003.

Figure 4: State model of the ATLAS detector

Figure 3: Example of a state model of a sub-detector.