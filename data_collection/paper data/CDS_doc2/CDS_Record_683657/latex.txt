**ATLAS Internal Note**

**DAQ-NO-107**

**18 June 1998**

J.Bystricky(1), R.W.Dobinson(2), S.Haas(3), D.Hubbard(1), B.Thooris(1)

(1) DAPNIA/CEA Saclay

(2) CERN

(3) University of Liverpool

This note presents the results of the Architecture C emulation studies performed on the MACRAME testbed [1]. Both Level 2 and DAQ/EF emulations have been performed. Input parameters and emulation results are presented here for each of the two studies.

## 1 Testbed Configuration

The MACRAME testbed [1] is a modular setup with three components: traffic generating nodes, timing nodes, a network composed of DS links and C104 32x32 cross bar switches. The testbed used for the measurements presented here consisted of 512 traffic nodes connected via network in a Clos topology. Two timing nodes were added on separate nodes to measure the single packet latency across the network.

During the initialization procedure, each of the traffic nodes was loaded with a set of traffic descriptors. These descriptors tell the node when to send how much data to whom. The traffic nodes do not respond to the incoming messages except to keep track of how much data was received. Similarly, they also keep information about much data they transmit.

The Architecture C emulation was performed using the maximum DS-link speed of 100 Mbits/s. Each network node port drives a bi-directional DS-link. The maximum bandwidth for application data at this maximum speed is 2 x 9.5 Mbytes/s per link.

## 2 Level 2 Emulation

### Input Traffic Patterns

The Level 2 traffic patterns have been produced based on the trigger menus in DAQ-NO-54 [2] and the hardware and software parameters in DAQ- NO-55 [3]. The algorithm sequences use the full low-luminosity LVL2 trigger menu, including the missing-Et recalculation (using data from all of the calorimeter towers) and the b-jet vertex tags; this trigger menu was labelled "LVL3" in DAQ- NO-54 [2] and modelled as the "second option" for LVL2 in DAQ- NO-55 [3].

This menu generates 75 different trigger sequences, corresponding to the LVL1 muon trigger and the three LVL1 calorimeter trigger types: electron/gamma/tau triggers, jet triggers and missing-Et triggers. The overview of the LVL1 trigger is given in Figure 1, with rates corresponding to the design trigger rate of 34,330 Hz. The LVL2 algorithm sequences are shown in Figs. 2,3 and 4, respectively, for the muon triggers, the electron/gamma/tau triggers, and the jet triggers; the event rate and the number of Rols to be processed are given for each of the processing steps.

A program has been written (in FORTRAN 90) to generate time-tagged data files to be used on the MACRAME testbed. The program uses the parameters defined in DAQ- NO-55 [3] to determine the traffic patterns for each algorithm sequence.

For each algorithm, the following parameters are needed: the average time for pre-processing in the ROBs or RSIs and the average time for processing in the LVL2 processors, the number of ROBs per RSI and the number of RSI per RoI, and the data volume per RoI and per RSI.

Seven specific algorithms are used in the simulation: muon, calorimeter (e/gamma/tau),tracker, full_scan, jet, b_jet and missing Et. The muon algorithm is a stand-alone algorithm based on data from the muon trigger chambers plus the muon precision chambers. The e/gamma/tau algorithm uses the fine-grain ECAL dat in a 0.4x0.4 Rol. The tracker algorithm uses silicon data (pixel plus SCT) and TRT data to find tracks in a Rol. The full_scan algorithm searches from tracks in the full TRT tracking volume. The jet algorithm uses 0.1x0.1 calorimeter energy sums in a 0.8x0.8 Rol. The b-jet tag uses silicon data (pixels plus SCT) in a jet Rol (0.8x0.8). The missing-Et algorithm uses energy sums from all 0.1x0.1 calorimeter towers.

The method employed to prepare the time-stamped data files was the following :

Select an event at time t0 and assign a processor for its analysis.

Select the trigger type and the algorithm sequence.

Select \(\eta\) and \(\phi\) for each non-overlapping Rol.

Determine which ROBs and RSIs contain data for each Rol.

Determine the messages that must be sent and their delays ( DAQ-NO-55).

Order the messages by port and by time stamp.

Select a time interval with a maximum of 6552 messages for any port.

Exclude events from the initial turn-on period.

The t0 for each successive event is chosen using a Poisson distribution.

The algorithm sequence for each event is determined by a random number in the interval [0, 34330], where the maximum number is the total LVL1 trigger rate (in Hz). The random number corresponds to one of the algorithm sequences in Fig. 1.

Each algorithm sequence corresponds to a certain number of processing steps (maximum 4).Two different algorithms are processed at the same rate in some of the processing steps. The maximum number of algorithms in any of the algorithm sequences is 6. The number of Rols to be processed for each algorithm is determined by the single random number used to select the algorithm sequence.

The RSIs required for a given algorithm are chosen using random numbers. The total number of RSIs for each subdetector are given in the following list:

\begin{tabular}{l l} MUON & : & 32 RSIs \\ CALO & : & 64 RSIs ( containing ECAL and HCAL data) \\ TRT & : & 64 RSIs \\ SCT & : & 32 RSIs \\ \end{tabular}

The ports on the MACRAME switching network are assigned as follows:

\begin{tabular}{l l l} Supervisor & 16 ports & ports \# 1 \textgreater{} 16 \\ Processors & 240 ports & ports \# 17 \textgreater{} 256 \\ Muon RSIs & 32 ports & ports \# 257 \textgreater{} 288 \\ Calo RSIs & 64 ports & ports \# 289 \textgreater{} 352 \\ TRT RSIs & 64 ports & ports \# 353 \textgreater{} 416 \\ SCT RSIs & 32 ports & ports \# 417 \textgreater{} 448 \\ \({}^{*}\)unused \({}^{*}\) & 64 ports & ports \# 419 \textgreater{} 512 \\ \end{tabular}

Random numbers select \(\eta\) and \(\phi\) for each of the Rols. The specific RSIs for the different algorithms are determined from the \(\eta\), \(\phi\) position using the following simplified model: (n is the chosen RSI number)

\begin{tabular}{l l l} Muon & : & 1 & RSI MUON \\ Calo & : & 2 & RSI CALO \\ Tracker & : & 2 & RSI TRT \\  & : & 1 & RSI SCT \\ Full\_scan & : & 64 & RSI CALO \\ Jet & : & 6 & RSI CALO \\ B\_jet & : & 4 & RSI SCT \\ Missing Et & : & 64 & RSI CALO \\ \end{tabular}

For each event, the following messages are generated and time-stamped:

Message from the supervisor to the SFI assigning the processor

Loop over all algorithms in the algorithm sequence:

Loop over all RoIs for the algorithm:

Loop over all RSIs in the RoI:

Message from SFI to RSI requesting data

Message from RSI to SFI transmitting data

Message from the SFI to the supervisor with the final decision

A file is created for each active port. The file contains a list of instructions for all messages originating at this port number. Each instruction contains the time of the message, the destination port number and the volume of transmitted data. All of the files are synchronised in time and each file contains 6552 instructions, completed by null instructions as needed.

Figure 1

Figure 2:

Figure 4:

### Measurements and Results

The total network throughput for LVL2 traffic, as a function of the LVL1 event rate, is shown in Fig. 5. The total throughput is linear up to 75 kHz. Data loss is evident at higher event rates.

The data rate for each of the input and output ports is shown in Fig. 6, at 75 kHz, just before data loss becomes evident. Short data requests are transmitted by the processors at a low average data rate of about 0.5 MB/s and received by the RSIs at a similarly low data rate. Event data is transmitted by the RSIs at a much higher data rate and received by the processors at an average rate of about 2.5MBs. The total data rate transmitted by the processors is equal to the total data rate received by the RSIs, and vice versa, except for the very small messages exchanged between the supervisor ports and the processors. Each node on the network is a dual port, so transmission and reception use different data links.

The highest data rate on any network link in Fig.6 is the 6.5 MB/s rate observed on the calorimeter output ports. The DS-link ports used in the MACRAME testbed have a nominal rate of 100 Mbits/s and a maximum useful data rate of about 10 MB/s. In the present study, obvious data loss occurred when the data requested exceeded about 65% of this maximum rate.

The latency distributions for data requests sent from the processors to the RSIs is shown in Fig.7. The latency from the RSIs to the processors is shown in Fig.8. The data size was 100 Bytes for the data request messages sent to the RSIs. The data volume varied from 300 Bytes to 16 kBytes for the data transmitted from the RSIs to the processors, with up to 64 RSIs responding to a single data request. The minimum latency was about 13 \(\upmu\)s for the data requests and 60 \(\upmu\)s for the data transmission. At the 50 kHz event rate, about 0.01% of the data requests were delayed beyond 40 \(\upmu\)s, and about 0.01% of the data transmissions were delayed beyond 1 ms.

In the LVL2 study described above, nodes of the same type (processors, RSIs) were grouped on the network as shown in Fig.6. An additional study was performed with a uniform distribution of each of the 6 different types of nodes across the network. This was intended to explore the effect of the grouping on the network performance. The network throughput for this distributed network traffic is shown in Fig.9. The total throughput at high event rates is slightly higher than that achieved with the grouped traffic, but some data still can't be sent for event rates above 75 or 80 kHz.

Figure 5Figure 6:

Figure 7:

Figure 9:

## 3 DAQ/EF study

### Input Assumptions

The single-switch, single-farm configuration proposed in Architecture C is very close to the DAQ/EF hardware configuration. It is quite possible that both LVL2 and DAQ/EF traffic can pass by the same switching network.

Emulation studies of the full event building required for the DAQ/EF have been performed on the same MACRAME testbed as was used for the LVL2 studies. The ROB/RSI mapping described for the LVL2 studies contains 192 RSI sources; each source transmits 4 KBytes of data to the EF processors. The data transfer used a 'bare-shifted' transfer mode, in which the transmission from successive sources is delayed by 500 \(\upmu\)s. Full event transmission required about 100ms.

For this emulation study, 112 nodes were assigned to the DAQ/EF processors (leaving 128 nodes for LVL2 processors). The timing of the incident events was taken from a Poisson distribution, as for the LVL2 studies.

### Measurements and results

The total network throughput for the DAQ/EF traffic is shown in Fig.10 as a function of the input event rate (the LVL2 accept rate). The total throughput is linear up to about 840 Hz. Beyond this limit, all additional data is lost.

The transmission rate from the RSIs and the corresponding reception rate for the processors are shown in Fig.11 for an event rate of 750 Hz. The integrated transmission rate is equal to the integrated reception rate ( note the off-set on the vertical scale). The highest data rate is the 5.1 MB/s rate input to the processors, since they are less numerous than the RSIs. This rate extrapolates to about 5.7 MB/s, or 57% of the maximum link speed, at 840 Hz, the maximum event rate without evident data traffic congestion.

Figure 10:

## 4 Conclusion

These emulation studies have shown that LVL2 traffic patterns generated for Architecture C and the full event building required for DAQ/EF can be handled successfully by a switching network in a Clos configuration. Maximum event rates were limited to 60% or less for each of the nodes on the MACFAME switching network. Converting these results to an ATM switching network with 155 Mbits/s dual ports (15 MB/s for application data), we would expect performance limits of 112 kHz for LVL2 traffic and 1260 Hz for DAQ/EF traffic, compatible with the requirements in the ATLAS Technical Proposal.

Nonetheless, a larger switching network would be required for stable operation at the design trigger rates. Current design studies based on a 1024-port ATM switching network provide the possibility of mixing LVL2 and DAQ/EF data on the same network while retaining a security factor sufficiently large to ensure robust performance [4]. Work is continuing to optimize the system design.

Further studies will be necessary to evaluate the performance of specific hardware - present and future. The MACFAME input buffers are extremely small (32 bytes). ATM switches with much larger input buffers, such as the FORE switches used in Demo C and in the Pilot Project, may function without traffic congestion at data rates closer to the ultimate link speed. Furthermore, higher-rate switching networks now coming onto the market could eliminate any difficulties for the LVL2 traffic (since the input rate will not exceed 100 kHz) and allow higher rates into DAQ/EF (higher LVL2 accept rates).

## References

* [1] "Realisation of a 1000 node high speed packet switching network" Presented at ICS-NET'95, St Petersburg, Russia.
* [2] J.Bystricky, et al. "ATLAS Trigger Menus at Luminosity 10\({}^{\prime}\)33/cm2/s" ATLAS internal note DAQ-NO-54, 17 June 1996
* [3] J.Bystricky, et al. "A Model for Sequential Processing in the ATLAS LVL2/LVL3 Trigger" ATLAS internal note DAQ-NO-55, 27 June 1996
* [4] A.Amadon, et al. "Architecture C Optimization from Paper Models" ATLAS Internal DAQ Note, in preparation.