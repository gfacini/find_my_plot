Offline Alignment & Calibration of the Inner Detector

Working document - Version 1.2 (23/12/99)

Stephen Haywood

with contributions from

Simon Peeters, Steve Snow and Monika Wielers

###### Abstract

In this note, we examine the possibility of understanding the ID alignment and calibration sufficiently well to allow a 25 MeV measurement of \(\mathrm{M_{W}}\) (from one channel) - see Chapter 16 of the Physics TDR [1]. In particular, this requires understanding the momentum scale at the 0.02% level.

The requirements for the alignment, magnetic field and material of the ID are considered. Questions are posed and results from some studies are summarised. Even if the goal of improving \(\mathrm{M_{W}}\) should prove impossible, the methods discussed will be necessary to achieve the best possible understanding of the ID performance.

This is a **working document**: as information becomes available and methods are developed, the document should be updated (this may lead to a certain repetition or untidiness in the layout of the information). Corrections, improvements and updates are very welcome. It is clear that there are very many questions which can be asked. What is set out here is intended to be a starting point. There are very many uncertainties in what is described in this note and it is far from clear how things will really turn out - only time will tell. To a large extent, if the nature of the imperfections affecting the ID were known, then they would be easy to correct. It is what is unknown which causes the real uncertainties.

The latest version of this document can be found from the Web by following the links:

\[\mathrm{ATLAS}\rightarrow\mathrm{ID}\rightarrow\mathrm{Performance}\rightarrow \mathrm{Alignment}\]

**Revision History + Major Corrections**

**v1.2****Autumn 99**: Small correction concerning FSI.
**v1.1****Autumn 99**: Small updates after presentation at IDWG on 7/12/99 and comments from David Rousseau.
**v1.0****Autumn 99**: Update after Physics TDR [1] (some pieces of text are copied directly) and more work on Stability.

_The limits on \(\Delta R\) have become tighter: \(\Delta R\) less than \(\sim\)10 \(\upmu\)m._
**v0.1 Spring 99**: Precision in R-z. Stability.
**v0.0 Summer 98**: First draft for discussion.

**Work To Do for This Report**

1. Simple simulations to understand requirements in R-z - see Section 2.1.2.2.
2. Write-up _Dilution Factors_ - somewhat academic.
3. More work on TRT.

###### Contents

* 1 Introduction
* 2 Alignment
	* 2.1 Discussion
		* 2.1.1 General Considerations
		* 2.1.2 Requirements
			* 2.1.2.1 In the Bending Plane
			* 2.1.2.2 In the R-z Plane
			* 2.1.2.3 Summary
		* 2.1.3 Practical Issues
		* 2.1.4 Local Alignment with Overlaps
		* 2.1.5 Local Alignment using the Complete Detector
			* 2.1.5.1 Obtaining a Solution
		* 2.1.6 Sagitta Corrections
			* 2.1.6.1 Simple Estimates
		* 2.1.7 Radial Distortions
	* 2.2 Questions
		* 2.2.1 Specifications
		* 2.2.2 Hardware Alignment
		* 2.2.3 Local Alignment with Overlaps
		* 2.2.4 Local Alignment using the Complete Detector
	* 2.3 Studies
		* 2.3.1 Local Alignment
		* 2.3.2 Conclusion of Studies
* 3 Stability
	* 3.1 Discussion
		* 3.1.1 Introduction
		* 3.1.2 Requirements
		* 3.1.3 Different Types of Time-dependent Distortions
			* 3.1.3.1 Vibrations
			* 3.1.3.2 Local Distortions
			* 3.1.3.3 Global Distortions
		* 3.1.4 Measuring Time-dependent Distortions.
		* 3.1.4 Local Distortions
			* 3.1.4.2 Global Distortions
* 4 B-field and Momentum Scale
	* 4.1 Discussion
		* 4.1.1 Magnetic Field
		* 4.1.2 Direct Measurements of the Field
		* 4.1.3 Calibration with Z Mass
		* 4.1.4 Calibration with J/\(\psi\) Mass
	* 4.2 Questions.
		* 4.2.1 Magnetic Field

[MISSING_PAGE_EMPTY:4]

## 1 Introduction

At the end of 1997, it was believed that the precision which was required for \(\rm p_{T}\) resolution would be -0.1% [2] - this was considered "ambitious"! However, at the Grenoble 1998 ATLAS Physics Workshop, Fabiola Gianotti proposed that we could make a useful measurement of the W mass if we could **control the p and E scales measured in the ID and EM Calo respectively at the 0.02% level**[3]. You can think of your own adjective for such a requirement! The requirements were based on an extrapolation of what might be possible and appropriate from measurements of the W mass from CDF [4]. Analysis of the W mass will be a low luminosity study.

A more complete set of requirements is:

1. Control alignment of ID to \(\sim\)1 \(\rm\upmu\)m in x-y plane.
2. Understand the B-field to better than 0.02%.
3. Understand the material in the ID (in particular the Pixels and SCT) to \(\sim\)1% of its value.
4. Understand the \(\rm p_{T}\) resolution to 1%.

There are many questions associated with these requirements - they will be addressed in the subsequent sections. Further, the list should not be considered to be complete, but rather indicative.

To some extent, all uncertainties can be "mopped up" in an overall calibration using the Z mass. However, it is usually the case that to really understand and control systematics, it is necessary to understand the individual components of the systematics with the highest possible accuracy and only at the end mop up any remaining uncertainties.

The requirements listed above will be difficult enough to meet on their own. However, undoubtedly things will be made significantly more complicated because of couplings between the different measurements. In order to make progress with the studies, such complications will be ignored.

CDF has been used as a comparison for ATLAS. ATLAS is better than CDF in so far as:

* It has far higher statistics with which to understand systematics, e.g. 6M Z\(\rightarrow\)ll and 60M W\(\rightarrow\)lv after cuts per year at 10\({}^{33}\) cm\({}^{-2}\)s-1, compared with 3.5K and 35K respectively from CDF Run I. In comparison with CDF, it might be hoped that many of the systematics may be understood by a factor of \(\rm\sqrt{10^{3}}\) better at ATLAS. This includes all physics systematics (such as the W \(\rm p_{T}\) spectrum) which are controlled by studying real data.
* Better resolutions, e.g. \(\rm\sigma(1/p_{T})\sim 0.4\times 10^{-3}\) GeV-1 compared to 0.8\(\times 10^{-3}\) GeV-1 at CDF.
* Somewhat higher energy particles.
* A large fraction of the tracking will be provided by solid-state devices (although the TRT is very important for the \(\rm p_{T}\) measurement).
* ATLAS has pixels, which have 3D readout.

ATLAS suffers from:

* A significantly more complicated tracker. The SVX is much more compact than the Pixels and SCT.

* Pile-up of minimum bias events (although for an \(\mathrm{M}_{\mathrm{w}}\) measurement at low luminosity, the number of superimposed events will be 2 or 3).
* all of which degrade resolution, while the brem can introduce systematic shifts.
* A significantly non-uniform B-field.
* The SCT has binary readout rather than analogue.

While CDF have achieved an impressive understanding of their detector, it has taken ~10 years... ATLAS should be trying to achieve a much better understanding on the time-scale of the low-luminosity running.

The requirements advocated in [3] are five times more than "ambitious" (see opening sentence). Nevertheless, the approach which will be adopted for this programme of work is:

1. Check the assumptions and omissions.
2. Understand the requirements.
3. Identify any "killer" arguments.
4. Study parts of the problem with an attitude of trying to succeed. It is all too easy to see difficulties with meeting the requirements - but in many cases, there are ways around the problems.

To achieve the desired understanding will take many man-years. It is important to start thinking about the requirements, problems and solutions now so that we will be in good shape when data taking starts in 2005. There are plenty of things which people can begin to think about and understand better even now (see Section 8.2). However, as a word of warning, it can be expected that as we start to build detectors in the next 2-3 years, we will become aware of many unforeseen types of distortion. Although it is good to start thinking seriously now about how the alignment code may be constructed, it may be worth proceeding carefully, rather than rushing to write an all-singing, all-dancing program. It is clearly worthwhile to have a "mock-data challenge" where one attempts to exercise an alignment procedure. However, there is a danger that if one simulates expected systematics and then tries to determine the distortions using fitted tracks, then one will inevitably recover the original distortions. In real life it is the _unexpected_ systematics which tend to limit the understanding.

To achieve the ultimate precision will require the use of all possible "handles" and while the use of real tracks will supersede any measurements made by hardware, the hardware measurements will provide very useful starting points to enable the alignment to converge rapidly and they will provide valuable constraints, especially for parameters which are difficult to measure with tracks.

## 2 Alignment

### Discussion

#### 2.1.1 General Considerations

In this note, a distinction is made between **local** effects which in principle should be corrected at the level of individual components, such as modules or straws, and **global** effects which are extended in space and can be corrected at the level of complete units, such as barrels, subdetectors or even the complete ID.

Ideally, one would like to have straight line tracks passing through the ID for alignment purposes. Unfortunately there is no source of sufficiently many really high-\(\mathrm{p_{T}}\) tracks. Since a priori, the \(\mathrm{p_{T}}\) of a given track is not known, alignment by shifting modules so as to reduce the means of residuals can only correct certain distortions. Single charged tracks alone can be used to produce continuity through out the detector, but cannot remove global **translations**, **rotations** or, most importantly, **sagitta or scale corrections**.

Global translations and rotations can only be corrected by reference to external systems - in fact, such distortions are only of relevance to measurements with external systems, i.e. the calorimeters and Muon Spectrometer. Sagitta distortions cause shifts in \(\mathrm{Q/p_{T}}\) and can be corrected by reference to particles of opposite charge - this is discussed more in Section 2.1.6.

The handle on alignment comes from the **residual distributions**. The residual of a track in a detector is the difference between the measured hit position and the extrapolated track fit. Unfortunately, one cannot use the true position where the track crosses the detector, since in real life, this is not known. Undoubtedly, the most important effects which need to be corrected are in the x-y projection.

In what follows, greater emphasis is placed on alignment with the **Precision Tracker**, consisting of the SCT and Pixels, rather than the TRT. This is not very satisfactory since it is well known that the TRT contributes very significantly (more so than the Pixels or SCT) to the \(\mathrm{p_{T}}\) determination - see Figure 4.9 of [5]. Clearly more thought must go into the TRT alignment. Nevertheless, a well aligned Precision Tracker will be a valuable tool for understanding the TRT.

#### 2.1.2 Dilution

In general, if a module of the Precision Tracker can be considered to be a well understood unit, then it is a rigid body with 6 Degrees of Freedom (DoF). Each module makes 2D measurements. The measurements provided by many tracks in principle give many constraints, however, it is necessary to group these measurements from separate track residuals into a reduced number of characteristic quantities - the most obvious being the mean. It is impossible to determine 6 DoF from two means, and so additional quantities (such as the means in different regions of the module or moments of the residuals over the spatial extent of the module) must be formed. When complete equations are written to solve for all DoF's of the module, it can be seen that the errors on a given constant are larger than would be naively found by trying to determine that constant alone. For example, in the studies which follow in Section 2.3.1, only the mean averaged over complete a complete module is considered in order to understand the precision which can be obtained for a simple R\(\phi\) shift of the module. The accuracy is overestimated due tocorrelations arising from other DoF's. I refer to this as **dilution** and define the dilution factor D as the increase in the variance on a fitted parameter (in particular, the R\(\phi\) shift) arising from allowing for all DoF compared to the naive assumptions. This factor corresponds to the increase in statistics required to achieve a given precision. These dilution factors turn out to be quite delicate to estimate and cannot be guessed naively. They are typically between 1 and 2 and are discussed in more detail in Appendix 1. However, what really matters is the errors on reconstructed measurements and fitted track parameters, and any dilution of precision of the alignment constants due to correlations will tend to cancel when correcting measured coordinates.

This is hard to follow, so let me illustrate. If one attempts to use the R\(\phi\) residual distribution to estimate the R\(\phi\) shifts to a module, the alignment constant which is derived may be wrong since the actual cause of the shift of the mean of the residual distribution may have originated from a displacement in some other DoF (for example a rotation about the z-axis of the module). Hence the location of the module in space will not be as well understood as the errors would lead one to believe. However, since the corrections are designed to correct the residual distributions, the net effect on the corrected hit positions will be that they are pretty much correct. Discrepancies will appear if one investigates the residual distribution as a function of position across the module - the fact that something is wrong should be indicated by the fact that the residual distribution is not a perfect Gaussian (unfortunately, there will be other things, like the discrete nature of the readout which also confuse this).

#### 2.1.2 Requirements

After all residual effects have been'mopped up' using global calibration, in particular the Z mass constraint, the effect of alignment systematics will be to smear W transverse mass distributions, but the mean position of the peak should be correct. Hence it is not obvious that superb alignment is necessary. However, unless all possible contributions to systematics are well understood, then differences between W's and Z's (number of leptons, \(\mathrm{p_{T}}\) and angular distributions) could lead to systematics which are hard to reduce.

##### 2.1.2.1 In the Bending Plane

The current specifications are 7 and 12 \(\mathrm{\SIUnitSymbolMicro m}\) for the Pixels and SCT respectively - these come from the desire that the momentum resolution (and other track parameter resolutions) should not be degraded by more than 20% (see Table 9-5 of [5]).

If the alignment were sufficiently good that the local systematics on 1/\(\mathrm{p_{T}}\) were less than 0.02%, then it would require that the alignment would be good to O(0.1) \(\mathrm{\SIUnitSymbolMicro m}\)1. This seems completely unreasonable.

Footnote 1: For example, a 0.1 \(\mathrm{\SIUnitSymbolMicro m}\) error on R\(\phi\) measurements from super-layers in the Precision Tracker and 5 times this for the TRT yields a 0.02% systematic at \(\mathrm{p_{T}}\) = 50 GeV.

Since this number seems so ridiculous, it is worth re-thinking. It is probably not necessary to have this precision for every single track, but it would probably be satisfactory to attain this averaged over a limited angular region. For example, if this was averaged over \(\sim\)100 modules and the misalignments were fairly random, then one might hope that the mean for this region would be correct to 1 \(\mathrm{\SIUnitSymbolMicro m}\)/\(\mathrm{\SIUnitSymbolMicro 100}\) = 0.1 \(\mathrm{\SIUnitSymbolMicro m}\). The assumption of a random distribution is very questionable, since that is precisely what systematic effects tend not to have. Nevertheless, the effect of correlations may not matter too much (what is most important is relative effects) and the correlations may be removed by more global approaches.

In some trivial sense, any resolution (for example 1 \(\mu\)m) can be achieved by the averaging of residual distributions containing very many tracks: the means are determined to \(\sigma/\sqrt[]{\mathrm{N}}\) (where \(\sigma\) is the width of the residual distribution). However, what type of _average_ is satisfactory is far from clear, in other words: over what intervals should one average. Since one ought to try to understand systematics as a function of many variables, for example

* time: fill-to-fill or within a fill
* p\({}_{\mathrm{T}}\), \(\eta\), \(\phi\)
* local position within module

\(\mathrm{N}\) will not be infinite. Further, there will be many complicated effects which will come in at the \(\sim\)1 \(\mu\)m level. While some of these may be capable of being parametrised separately, it will be difficult to understand the alignment better than a factor of ten below these effects.

Lastly, it goes without saying that 1 \(\mu\)m really is very small on the scale of the ID. 1 \(\mu\)m is a hundredth of the width of a human hair!

#### 2.1.2 In the R-z Plane

Z Mass Calibration

The ultimate calibration will come from the use of the Z resonance. Effectively, this will be used to understand the measurement of p\({}_{\mathrm{T}}\) since it is p\({}_{\mathrm{T}}\) which enters the formula for the transverse mass, used to estimate M\({}_{\mathrm{W}}\) But since M\({}^{2}\) = 2 p\({}^{+}\)p- (1-cos\(\Delta\omega\)), to understand p\({}_{\mathrm{T}}\) = p sin\(\theta\) will require that the polar angle is measured sufficiently well. The statistical error on \(\theta\) is of the order of 1 mrad (see [1]) and is dominated by the Pixels - this will lead to a statistical error O(0.1)% on the p\({}_{\mathrm{T}}\) scale. To obtain a systematic error which is a factor of 10 less would require an understanding of the position of the pixel modules to a 10% of their intrinsic resolution. Since the Pixels provide \(\sigma_{z}\) \(\sim\) 70 \(\mu\)m (see [15]), this would necessitate understanding the position of the pixel modules to \(\sim\)7 \(\mu\)m in z. This seems quite tough and maybe the effects of averaging random displacements would soften this requirement so that it does not have to be met for all positions and times. Further, since this requirement comes from the Z mass calibration, which will necessitate data accumulated over a period of the order of a month or more, it may be sufficient to obtain this precision after averaging over the same time-scale.

Some of the systematics associated with the difference between using the mass for calibration with the Z, and using the transverse mass for the W mass measurement would be reduced by the use of the transverse mass for both [7]. However, in this case, only those Z decays at large M\({}_{\mathrm{T}}\) contribute significantly to the calibration, thus reducing the statistical power of the Z sample.

This would benefit from some simple simulation to check this out a bit more carefully.

Angular Width

A displacement \(\Delta\) at the edge of a detector, normal to the surface, will be mainly radial, but also will have an azimuthal component (ignore any tilts to start with). If the half width of the detector at radius R is h, then the azimuthal displacement at the edges relative to the centre is h/R \(\times\Delta=\pi/\)N \(\times\Delta\) (where N is the number of modules in a ring). In the Barrel, N varies from 18 (B-layer) to 56 (outer SCT or Pixel), and tends to be \(>\)30 in the End-cap wheels. Taking N \(\sim\) 30, we require \(\Delta\leq\) 10 \(\upmu\)m. The effect of tilts will increase the effect on one side and decrease it on the other side.

#### 2.1.2.3 Summary

Plausible estimates of the requirements for the alignment are:

* a factor of \(\sim\)10 better than indicated in [5];
* a factor of \(\sim\)2 and \(\sim\)10 better than indicated in [5] for the Pixels and SCT respectively.
* a factor of \(\sim\)2 better than indicated in [5].

#### 2.1.3 Practical Issues

There are many practical issues which will make alignment very difficult. Nevertheless, by taking care to understand all issues in some detail, it ought to be possible to allow for most of these effects by **parametrising** the effects and **factorising** the alignment problem so that the effects of some features can be "deconvoluted". This process will necessitate the use of all available information, including that from hardware alignment (FSI), surveying1 and lab measurements of individual components. For example, if there are global expansions of the detector due to thermal effects, these could be scaled to determine the motion at a given place of the ID, and such effects could be considered separately from local displacements of individual components.

Footnote 1: Surveys will be made using the X-ray measurement system on the surface. When the ID is moved into ATLAS, the alignment will inevitably change. Nevertheless, the local alignment of one module to another will probably be much less affected than global distortions of the structure.

The list which follows is undoubtedly not exhaustive, but should be illustrative. The emphasis is on the SCT, but some of the considerations will also apply to the TRT. A crucial issue is that of **stability** - see Section 3.

Global Distortions of Structures

During running, the effects due to thermal expansion are expected to be small due to the low CTE of the support structures and the goal of isothermal running. Nevertheless, some expansions cannot be ruled out. If sufficiently large, these maybe detected by the FSI system, otherwise they should be relatively easy to determine as a function of time during a fill using tracks which can be averaged over larger regions of the ID. It may be possible to parametrise such effects as a function of measured temperatures or luminosity.

There will also be gravitational effects whereby complete assemblies will be distorted. In addition, in the case of the TRT, individual wires will sag in the barrel by as much as 15 \(\upmu\)m (see Section 12.4.3.2 of [6]).

Module Assembly Errors

It would be nice to be able to treat SCT modules as perfect, well-aligned components which can be used as basic building blocks. Each module consists of four silicon wafers. Clearly it would be nice not to have to align the individual wafers separately. Further, there will be a finite number of jigs and should these have systematic effects, the imperfections can be parametrised for sets of modules. The silicon wafers will be aligned correctly to 2-3 um in R\(\phi\). Each module will be measured with a precision of \(<\) 1 um on each side; the relative alignment of one side to another should be good to 4 um. With the possibility of using these measurements, it seems feasible to use the SCT modules as well defined sub-units.

One major qualification of this is that most measurements will be made at room temperature, and as modules are cooled, so they will distort. Nevertheless, this can be studied and parametrised at some level.

#### Thermal Effects in Modules

As for the support structures, the thermal effects for the modules should be minimised. However there may be effects whereby modules will distort: expand, twist and go banana-shaped. By measuring detectors in the lab as a function of heat-load, for example with ESPI, it should be possible to understand distortions and parametrise them as a function of measured temperature.

Recent (Spring 99) tests of cooling a module by 30\({}^{\circ}\)C from room temperature caused distortions at the ends of -16 um. This module did not have a base-board - it is not clear whether a real module will be more or less rigid.

#### Placement Errors

See Section 2.1.4.

#### Vibrations

Significant effort is going in to reducing vibrations. Nevertheless, other experiments have seen vibrations at the level of a few microns. The most likely consequence of vibrations is a small degradation of the resolution, but they should not lead to systematic effects.

Recent (Autumn 99) studies [8] on the SCT end-caps (which are more prone to vibrations than the other parts of the ID) have indicated longitudinal vibrations up to -60 um, and \(<\)2 um transverse - these are much less than the module resolutions of -500 um and 16 um respectively.

#### Silicon Wafer Construction

The features on the silicon wafers will be positioned to precisions generally better than 1 um. In particular the active width of the SCT detectors will be correctly described by 768\(\times\)80 um. In practice, care should be taken to avoid edge effects from the outer strips when reconstructing hits. There may be small relative misalignments at the micron level between the implants in the silicon and the metal layers which include the readout strips.

#### Location of Charge Deposition in Silicon

While mechanical surveys of detectors may indicate that they are at some location, the actual location of charge deposition and the subsequent location of reconstructed coordinates is a separate issue. It is the latter which is of greatest significance to track reconstruction. Reconstructed coordinates will depend on

* can be understood at some level by careful modelling. Angular effects can be investigated in test-beam.
* will necessitate a modest knowledge of the local B-field.
- this is important for the reasons discussed in Section 2.1.7. It also induces R\(\phi\) displacements by projection due to non-normal angles of incidence. It has been observed (for example in Delphi) that the effective charge deposition is not located at the mid-plane of the detector because of the details of charge transportation. * The SCT strip detectors are p-on-n, and ought to be operated fully depleted. However, after irradiation, the detectors undergo inversion. Should radiation damage prevent the fully-depleted operation of the detectors, the effective depth will not be in the centre of the wafer. * The pixel detectors are n-on-n and can be operated partially depleted.

##### Beam-spot Variations

It is anticipated that the beam-spot size will be \(\sigma_{\mathrm{x}}=\sigma_{\mathrm{y}}=15\)\(\mathrm{\SIUnitSymbolMicro m}\). This is compatible with the resolutions of the silicon detectors. However, from fill to fill, it is likely that the beam-spot will move around by \(\sim\)100\(\mathrm{\SIUnitSymbolMicro m}\) (in LEP, comparable jumps were observed typically a few times per fill corresponding to the loss of RF power-units). It should be no problem to monitor this motion: for example, for a single B-physics event, the beam spot can be determined to \(\sim\)30\(\mathrm{\SIUnitSymbolMicro m}\) and hence the mean position can readily be followed at the 1\(\mathrm{\SIUnitSymbolMicro m}\) level. (Will there be BOM's as at LEP?)

#### Local Alignment with Overlaps

Overlaps will enable local alignment of modules within units (barrels or wheels). Currently (Spring 99) there is some debate as to how large these overlaps should be in the SCT. The size of the overlaps must allow for edge-effects when reconstructing measurements (which may necessitate ignoring edge strips) as well as placement tolerances. The tolerances (half-width of a square distribution) on the overlaps are guessed as being 100-200\(\mathrm{\SIUnitSymbolMicro m}\) in the SCT Barrel and 40\(\mathrm{\SIUnitSymbolMicro m}\) in the SCT Wheels. Overlaps are also planned in the Pixels. The R\(\phi\) overlaps will enable alignment of rings of modules in \(\phi\); the R/z overlaps will correct the relative alignment of rings in \(\phi\), and in the Pixels, provide the R/z alignment shifts.

The advantage of the overlaps is that there is very little extrapolation error and hence the effects of multiple scattering are small. The appropriate quantity to consider is the **difference of the residuals** between the two overlapping detectors. The uncertainty due to extrapolation from other layers cancels to first order. The width of the distribution should be close to \(\sqrt[]{2}\) times the intrinsic resolution (22\(\mathrm{\SIUnitSymbolMicro m}\), for SCT). The measurement of the residuals around a barrel effectively 'pins' the modules at the edges. The method is unable to identify which module is displaced (i.e. global \(\phi\)-rotations of the complete barrels/wheels) and is insensitive to most global distortions (e.g. elliptical or 'pear-shaped' deformations) in x-y in the barrel. The corresponding algebra is considered in Appendix 0.3. Despite the fact that there will be global distortions to be removed and some DoF's of single modules will not be well constrained, locally \(\phi\) measurements will probably be correct to the level of the naive expectations without the need for dilution factors. If the complete SCT modules can be considered to be well understood, there is the possibility of a factor of 2 improvement in the variances coming from the use of the stereo measurements.

A valuable constraint in using the overlaps comes from the ability to "complete the circle" in each ring. If this constraint is lost then it will be analogous to performing the alignment of a simple line of detectors. Problems will occur from dead modules or inefficiencies which may be more likely for edge diodes. There is the possibility to lose complete ladders of modules in the barrels which would certainly weaken this approach.

#### Local Alignment using the Complete Detector

Using tracks crossing several barrels/wheels, the complete ID can be internally aligned. The advantage is that the acceptance corresponds to the complete detector rather than just the overlaps. The disadvantage is that extrapolation between layers is more uncertain and will be sensitive to multiple-scattering.

Because nearby tracks will cross different modules, since the modules in different SCT barrels are not arranged in "towers", constraints can be obtained between adjacent modules by connection to those in different layers. The tracks can be considered to loosely "pin" the modules with a "web" of connecting measurements - just like the FSI networks. The statement about towers is not completely true: due to the periodicity of 8 in the SCT, the connections made by stiff tracks will be limited to sectors of 45\({}^{\circ}\).

This approach is _local_ in so far as corrections are made by aligning individual modules. Nevertheless, it should be able to remove many of the larger scale _global_ distortions. While radial distortions (e.g. elliptical distortions) would seem hard to remove, the finite width of modules should provide some handle on this.

Similar remarks as for the overlaps can be made about dilution factors and stereo strips in the SCT. If there are dead modules, then the constraints which may have been lost looking at the overlaps for a given ring of modules may be partially recovered by "hopping around" dead modules in the R/z direction on the same barrel/disk or by reference to modules in neighbouring barrels/disks.

##### 2.1.5.1 Obtaining a Solution

The standard way to solve problems of alignment is by minimising a \(\chi^{2}\) describing the measurements. Here, "measurements" correspond to the means (or higher moments) of residual distributions (averaged over many tracks) with errors corresponding to _width_/\(\backslash\)_tracks_. Since the residuals are with respect to the fitted tracks, and the fitted tracks are formed using the same set of hits, there is some bias in the residuals which will tend to reduce residuals - see discussion in Section 2.3.1.

If we consider for example N = 2112 barrel SCT modules which are considered to have perfect internal alignment, then there are 6N parameters to be found. Therefore for each module, at least 6 measurements will have to be derived to allow a determination of all the parameters. Just for the SCT barrel, this would lead to solutions involving the inversion of 6N\(\times\)6N matrices.

This may seem unthinkable, however this is precisely what ALEPH [14] and SLD have done with 1000\(\times\)1000 matrices. Reassuringly, the solutions are well defined and stable and the matrices inverted in time-scales \(\sim\)1 hour. Because the approach is rigorously correct (although only as good as the assumptions) and democratic (does not give any part of the detector preferential treatment), it is worth considering - at least as a tool for parts of the bigger problem. Further, different types of measurements could be used corresponding to regions where overlaps could or could not be used.

#### 2.1.6 Sagitta Corrections

Sagitta distortions can occur even when the detector elements seem to be perfectly aligned using particle tracks. They can arise from differential twists of structures and have the form \(R\Delta\phi\propto R^{2}\). Ideally these should be corrected in a more global way by rotating complete SCT barrels or rings. However, inevitably corrections are made not in the alignment but to reconstructed tracks in the form: \(Q/p_{T}\to Q/p_{T}+\Delta\).

To avoid sagitta distortions, it is necessary to ensure that tracks of opposite charge have the same behaviour. In principle, one could compare the measured spectra of oppositely charged particles - in practice this is not so precise. If one looks at the means of \(1/p_{T}\) for oppositely charged particles, the uncertainties on the correction to \(Q/p_{T}\) will be \(\sim 1/(\overline{p}_{T}\backslash N)\). If the \(p_{T}\) spectra can be normalised by an estimate of \(p_{T}\), then the uncertainties on the correction will be \(\sim\sigma/(\overline{p}_{T}\backslash N)\), where \(\sigma\) is the width of the normalised distribution and may be much less than unity. The estimate of \(p_{T}\) need not be unbiassed, the only condition is that it should not be correlated to the track charge.

One approach, followed by CDF, is to use electrons and the energy determination from the calorimeter, i.e. one studies E/p for e\({}^{+}\) and e\({}^{-}\) separately. Another possibility would be to normalise muons by the momentum measured in the muon system (the uncertainty on the energy loss correction is around 0.5 GeV). The drawback with this is that the momentum measured in the Muon System is also charge dependent and could suffer even worse from sagitta corrections.

#### 2.1.6.1 Simple Estimates

The sagitta correction is of the form: \(Q/p_{T}\to Q/p_{T}+\Delta\) can be estimated from the comparison of the normalised signed distribution: \(Q\nu=Q\rho/p\) where \(\rho\) is an estimate of \(p\) from another detector, such as E measured in the EM Calorimeter:

\[\Delta=(Q\overline{\nu}\mid_{+\mathrm{e}\nu}+Q\overline{\nu}\mid_{-\mathrm{e }\nu})\ /\ (\overline{\rho}\mid_{+\mathrm{e}\nu}+\overline{\rho}\mid_{-\mathrm{e}\nu})\]

It can be seen that any scale error on \(\rho\) vanishes. The uncertainty on \(\Delta\) is \(\sigma(\nu)/\overline{\rho}/\backslash N\) where \(N\) is the total number of tracks used for correction. The uncertainty on the momentum scale for \(M_{W}\) measurements is \(p_{W}\sigma(\Delta)\), where \(p_{W}\sim\,M_{W}/2\). So the requirement is that

\[p_{W}/p\,\sigma(\nu)/\backslash N<2\times 10^{-4}\]

* For electrons from \(Z\to ee\) measured by the EM Calorimeter, \(p\to p_{W}\) and \(\sigma(\nu)=6\%\) (determined from E/p distribution at \(\eta=0.3\)). This implies \(N\) must be greater than \(\sim\)\(9\times 10^{4}\), which is much less than \(3\times 10^{6}\) Z's per year.
* 10%. Since \(p\sim 6\) GeV, \(N\) must be greater than \(\sim\)\(10^{7}\), which is much less than \(10^{9}\) muons per year.
* For muons from \(Z\to\mu\mu\) measured by the muon chambers, \(p\sim p_{W}\) and \(\sigma(\nu)\) is probably less than \(\sim\)\(4\%\) (see Figure 8-4 in [1]). This implies \(N\) must be greater than \(\sim\)\(3\times 10^{5}\), which is much less than \(3\times 10^{6}\) Z's per year.

The above calculations are fairly hand-waving, but clearly there are sufficient statistics within one year to measure the sagitta corrections globally. Further, the statistics would allow the corrections to be determined with coarse bins in \(\eta\)\(\to\)\(\phi\) or time.

For this method to work, it is not necessary that \(\rho\) is accurate, but it must be insensitive to charge. To a good approximation, this will be the case for energies measured by the EM Calo-rimeter, but may not be the case for the Muon Spectrometer.

#### Radial Distortions

Radial distortions stretch tracks and cause scale errors in 1/p\({}_{T}\). Since 1/p\({}_{T}\)\(\propto\) 1/R\({}^{2}\), \(\delta\)p\({}_{T}\)/p\({}_{T}\) = 2\(\delta\)R/R so if we want \(\delta\)p\({}_{T}\)/p\({}_{T}\) better than 0.02%, need \(\delta\)R better than 100 \(\mu\)m at R = 1 m. It is not clear to what extent it is appropriate to apply this directly to the SCT, since the TRT is essential in measuring p\({}_{T}\), however it would suggest we need to know the outer barrel radius to ~50 \(\mu\)m.

The radii may depart from their nominal value due to:

* Imperfections in the construction of support cylinders.
* Gravitational sagging causing elliptical or pear shaped distortions.
* What actually is required is the location of reconstructed coordinates and this will be affected by the location of charge deposition in the silicon.

Handles on the radial distortions should come from:

* FSI alignment.
* Overlaps: The circumference determined from the sum of the wafer widths can be compared with 2\(\pi\)R. This can only constrain the circumference and not detect deviations from cylindricity. See Appendix 10.3. (Note: this method is potentially compromised by rotations of the modules.)
* General alignment: If relative shifts of residuals within a module can be understood to O(1) \(\mu\)m, then since modules subtend O(0.1) rad, it should be possible to obtain some 10's of microns precision.

### Questions

This section includes some questions, yet it is undoubtedly far from complete.

#### Specifications

1. Is ~1 \(\mu\)m the appropriate scale for the alignment in the x-y plane?
2. What does it mean? With what frequency in time does it need to be determined?

These are difficult questions to consider and answers will probably only become clear after a lot more work.

#### Hardware Alignment

3. What is a suitable starting point for knowing the alignment from the surveys? Not obvious! Probably more important to know relative alignment of neighbouring modules quite accurately, but absolute positions and effect of global distortions may be less critical. See discussion after Point 13. Since the pattern recognition programs used in the ID need to allow for multiple-scattering and bremsstrahlung, the roads are quite wide (several hundred microns), so displacements of this order should not be a big problem.
4. How precisely can the FSI system determine the barrel radii and cylindricity?

#### Local Alignment with Overlaps

1. What is the width of the distribution of the difference of the residuals? Should be \(\sim\)30 (14) \(\upmu\)m for SCT (Pixels) - see Table 2-1 and Section 3.7.2 of [1].
2. How quickly can we align within barrels (or rings in the end-cap)? Answer: see Section 2.3.1.
3. How does alignment (residual distribution) vary with \(\mathrm{p_{T}}\) and \(\eta\)? Answer: see Section 2.3.1.
4. How easy would it be to break each of the 12 cm sets of strips into the two 6 cm wafers, and what can be deduced about their relative orientations in the plane of the module?

#### Local Alignment using the Complete Detector

Similar questions to those for the overlaps could be foreseen.

1. What is the width of the distribution of the residuals? Answer: see Section 2.3.1.
2. How quickly can we align within barrels (or rings in the end-cap)? Answer: see Section 2.3.1.
3. To what extent can one use the variation of residuals across the width of a detector to constrain the radial location?
4. How does alignment (residual distribution) vary with \(\mathrm{p_{T}}\) and \(\eta\)? Answer: see Section 2.3.1.
5. The 6 GeV muons from the B-physics triggers could be significantly contaminated with \(\pi\)/K decays in the ID. Is this a problem? Certainly seems undesirable. If quality cuts are applied to muon candidates, such as \(\chi^{2}\) cuts, then they will appear to be continuous helices - however, then this is biassed. The \(\pi\)/K rejection which can be achieved is shown in Figure 8-19 of [1] - 3 and 10 for pions and kaons, respectively, at

[MISSING_PAGE_FAIL:17]

Residuals were studied in both the complete modules and just in the overlaps. The nature of the residuals is described in more detail in [1] - their behaviour was as expected. It is worth pointing out that the residuals in the B-layer were larger than those of the other Pixel layers, since there was no constraint on the track at smaller radii. Also the effects of the discrete nature of the binary readout could be seen - causing some quantisation effects [9].

The alignment precision which can be achieved after **one day** of running at low luminosity was derived from the widths of the residual distributions and the expected rates. Alignment between layers was studied assuming muons from \(\mathrm{W}\rightarrow\mu\nu\) and \(\mathrm{p_{T}}\geq 6\)\(\mathrm{GeV}\) muons (from the B physics sample) can be used. Alignment within layers requires tracks crossing the overlaps with higher rates to compensate for the smaller solid angle. Since multiple scattering is less important, it was assumed that \(\mathrm{p_{T}}\geq 6\)\(\mathrm{GeV}\) muons and all particles (predominantly pions) with \(\mathrm{p_{T}}\geq 2\)\(\mathrm{GeV}\) in triggered events could be used. Assuming events triggered by \(\mathrm{p_{T}}\geq 6\)\(\mathrm{GeV}\) muons for B physics will be reasonably characteristic of any triggered events, it was found that each event had 10 particles with \(\mathrm{p_{T}}\geq 2\)\(\mathrm{GeV}\) in the absence of pile-up. For the three types of tracks (\(\mathrm{W}\rightarrow\mu\nu\), \(\mathrm{p_{T}}\geq 6\)\(\mathrm{GeV}\) muons and all particles with \(\mathrm{p_{T}}\geq 2\)\(\mathrm{GeV}\)) the residuals from the three muon samples (\(\mathrm{p_{T}}=45\), 6 and 2\(\mathrm{GeV}\) respectively) were assumed. The corresponding rates at peak luminosity were assumed to be 3\(\,\mathrm{Hz}\) (after triggering and all cuts), 50\(\,\mathrm{Hz}\) (where the \(\mathrm{p_{T}}\geq 6\)\(\mathrm{GeV}\) muon trigger is assumed to take half of the available DAQ bandwidth) and 10\(\times\)100 Hz (allowing for 10 tracks in every event). It was assumed that the running time in one day would be 2\(\times\)8 hours and that for \(\mathrm{W}\rightarrow\mu\nu\), the rate of which is limited by the luminosity, a 60% efficiency should be used to allow for the luminosity lifetime. These two effects were accounted for by a scale factor. To make the calculations transparent, a 1% R\(\phi\) overlap of the sensitive area for all modules has been assumed (that is 1% on each edge of each module).

The calculations are illustrated for the middle Pixel barrel in Table 2-1, which shows the precision which can be obtained for the R\(\phi\) alignment after one day of low luminosity running. A summary of the precisions which can be obtained for the different barrel layers using all the hits in a module is shown in Figure 2-1. The "error bars" show the spread of the values for the different modules of a given layer arising from different rates and residual distributions. Figure 2-2 shows the precision which can be obtained in the Pixel barrel layers using the overlaps. The results are summarised in Table 2-2. For the averages in the first column (precision for Pixel barrels using complete modules), the B-layer was excluded because of the large variations in the precision arising from modules at large \(|\,\eta\,|\).

#### 2.3.2 Conclusion of Studies

From Table 2-2, it would seem that at low luminosity it should be possible to align the Precision Tracker to O(1)\(\mu\)m in one day using tracks crossing the complete detector (anywhere in a module) or using the overlaps (assumed to be 1%). The information provided by the overlaps is

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Type of track** & **Rate (Hz)** & **Scale fact** & **Width of resid. (\(\mu\)m)** & \multicolumn{2}{c}{**Precision (\(\mu\)m)**} \\  & & & **Module** & **Overlap** & **Module** & **Overlap** \\ \hline W\(\rightarrow\mu\nu\) & 3 & 0.4 & 19 & 17 & 0.9 & \\ Single muons, \(\mathrm{p_{T}}\geq 6\)\(\mathrm{GeV}\) & 50 & 0.66 & 21 & 19 & 0.3 & 2 \\ Low-\(\mathrm{p_{T}}\) tracks, \(\mathrm{p_{T}}\geq 2\)\(\mathrm{GeV}\) & 10\(\times\)100 & 0.66 & 26 & 27 & & 0.8 \\ \hline \hline \end{tabular}
\end{table}
Table 2-1: Components of the calculation of the alignment precision in R\(\phi\) which can be obtained in the middle Pixel barrel after one day of low luminosity running. See text for more details.

[MISSING_PAGE_FAIL:19]

Nevertheless, at the LHC, it is certain that there will be many low-\(\mathrm{p_{T}}\) tracks which will be valuable for alignment with the overlaps and sufficient high-\(\mathrm{p_{T}}\) tracks for more global alignment. It would seem that the statistical precision which can be obtained using track-based alignment will allow the more complex systematics in the ID to be studied and corrected.

## 3 Stability

This chapter deals with the mechanical stability and is manifestly correlated to the previous chapter on alignment. To a large extent, the alignment and the understanding of the imperfections of the detector would be relatively straightforward in the absence of time-dependent changes.

The other stability issue relates to the **Magnetic Field**. Presumably the field strength stability will be determined by measuring the current in the coil as well as directly using NMR probes at the centre of the ID. These probes will also be sensitive to shifts of the coil within the cryostat.

### Discussion

#### 3.1.1 Introduction

The specifications on the stability must take into account:

* The requirements for Physics and the need to be able to understand other sources of systematic uncertainty.
* The frequency and nature with which in situ alignment measurements can be made.

The specifications should not be set in isolation from the alignment strategy, since if movements can be followed, they are not a problem. In situ alignment constraints will come from

1. Hardware alignment, i.e. FSI which will provide a limited number of measurements at particular times and with fixed precision. Each measurement set takes \(\sim\)30 minutes.
2. Track based measurements which will have statistical precisions which decrease roughly like \(\backslash\)_time_.

Understanding the requirements on alignment is difficult; to understand the stability is even more difficult. One should be careful not to:

* Over-estimate the requirements, since constraints (e.g. modules on a barrel will move together to a large extent) will reduce relative motion of modules. Also averaging out may remove many effects.
* Under-estimate the requirements, since instabilities will only make a complex problem even worse. One should not forget that in x-y, the alignment tolerance of \(\sim\)1 \(\upmu\)m is a factor 10 worse than what is actually required, and we are assuming that statistical processes will permit the achievement of the goal and will not be prevented by additional systematics.

Figure 3-1 shows how the measured displacements of a slowly moving detector may appear as a function of time. The displacements correspond to the alignment corrections which are deduced from the residuals from each track. To produce the alignment constants, these measurements would be averaged over a period of time T. Each individual measurement has a resolution \(\sigma_{1}\). If track measurements are made at a rate r, then after T, N = rT (\(>\)1) measurements will be collected. This statistical resolution will be \(\sigma_{1}\)/\(\backslash\)N. If the true position of the detector changes at a rate \(\rho\), the uncertainty on the mean displacement averaged over the period T will be

\[-(\sigma_{1}\oplus\rho\mathrm{T}/\diagup 12)/\diagup\mathrm{N}\] (3-1)

and the rms difference between the averaged and the true displacement of the detector would be

\[-(\sigma_{1}/\diagup\mathrm{N}\oplus\rho\mathrm{T}/\diagup 12)\] (3-2)

#### Requirements

In the discussion above concerning the slowly varying displacement of a detector, the period T represents a time-interval from which by definition a single set of alignment constants are derived. Any time dependence would be determined from a sequence of measurements from separate intervals. (Also, one could "overlay" several time intervals to study variations within an interval.) It is not obvious what requirement one should impose on the stability:

* Is it good enough to determine the average with the required accuracy?
* Or do we want to understand the true position of a detector at any point in time with the required accuracy?

Since there will be plenty of averaging anyway, one might argue that approach a should be sufficient, and yet precisely for this reason, in order to untangle different contributions to the systematics, it may prove very desirable to insist on approach b.

Taking the second approach leads to:

Figure 3-1: Illustration of sets of measurements to correct slowly varying position of a detector. The gray band represents the 1 \(\sigma\) extent of the corrections derived from individual tracks; the true position is moving at a rate \(\rho\).

\[(\sigma_{1}/\sqrt[]{N}\oplus\rho T/\sqrt[]{12})<\sigma_{T}\] (3-3)

where \(\sigma_{T}\) is the target precision to be achieved in time T. This implies \(\sigma_{1}/\sqrt[]{N}<\sigma_{T}\) - corresponding to the discussion of Section 2.3. This implies that the time must be \(T>(\sigma_{1}/\sigma_{T})^{2}/r\). However, during this period, the displacement due to the detector motion should not be too great: \(\rho T/\sqrt[]{12}<\sigma_{T}\). This implies \(\rho<\sqrt[]{12}\sigma_{T}/T\) and hence \(\rho<\mathrm{R}\sigma_{T}^{3}/\sigma_{i}^{2}\). We see that the limits on the acceptable stability vary like the cube of the target precision.

In a previous version of this note, I forgot the factor of \(\sqrt[]{12}\) - nevertheless, since one probably wants that the effect of instabilities is less than the target precision, and there are a large number of uncertainties, I will drop this factor.

The goal for the alignment set out in this note is 1 \(\mu\)m in R\(\phi\) and 10 \(\mu\)m in z/R (see Section 2.1). It would appear that the statistical precision for the alignment with track measurements can be met on a time-scale of the order of 1 day (see Section 2.3.1)1, hence the stability requirements for the Precision Tracker are:

Footnote 1: The alignment was considered in R\(\phi\) only - however, since the resolutions are approximately an order of magnitude larger in the R-z plane, it seems plausible that if the goals can be met in R\(\phi\), they can also be met in R-z.

* Stable in R\(\phi\) to 1 \(\mu\)m/day.
* Stable in z (barrels) and R (disks) to 10 \(\mu\)m/day.

Having made these statements, there are a number of qualifications which need to be made (see Section 3.1.3). In particular, it is not obvious how these number should be interpreted. It seems plausible that one would not want the relative position of neighbouring modules on the same support structure to move at a rate greater than these specifications. However, it is probable that somewhat greater motion could be tolerated between different structures. Having said that, the determination of the alignment constants and the correction procedures will be much simpler if displacements can be parametrised by fixed numbers in a given calibration period (a day, a run or whatever), without the complication of time dependence within the period. This would argue for the requirements being applied to all elements of the ID.

#### 3.1.3 Different Types of Time-dependent Distortions

This section overlaps to some extent with Section 2.1.3. The types of movement which will be easiest to deal with are those which are small and smoothly varying in space and time; however not all motions will be like this.

In what follows, emphasis will be put on R\(\phi\) displacements, but in many cases, the corresponding estimates for R-z quantities can be obtained by multiplying by 10.

#### 3.1.3 Vibrations

As stated earlier, vibrations should not lead to systematic effects. The principle requirement is that they should not spoil the track parameter resolutions and therefore should have amplitudes less than the specifications set out in Table 3.1 in the ID TDR: 7 \(\mu\)m (12 \(\mu\)m) in the Pixels (SCT).

Since it will be necessary to understand the momentum resolution to O(1)%, it will be important for any vibrations to have a stable amplitude.

##### 3.1.3.2 Local Distortions

These correspond to changes at the level of individual modules.

##### Jumps

Random jumps where modules move in a discrete way (for example, they suddenly overcome the friction provided by a grease joint associated with a cooling pipe) are highly undesirable and should be less than 1 \(\upmu\)m in total in R\(\theta\) in a day.

Thermal distortionsWhat matters here is variations within normal running conditions - the changes which take place in cooling from room to operating temperature are not the issue. These distortions should be minimised as much as possible by a) isothermal running (all modules of the same type should run at the same temperature which is constant during a fill) and b) designing modules to reduce thermal distortions. These conditions may be difficult to fulfil and it may prove necessary to parametrise the effects as a function of measured temperature or luminosity. This could be done by superimposing data as a function of temperature from different runs or modules. For this to work, it is important that from run to run, the modules should operate at the same temperature.

Nevertheless, the effect of turning on the HV at the start of a run will be small (in the absence of significant leakage currents). Further, if an FE chip is left configured and with the clock on, the power dissipation should not change significantly as collisions commence at the start of a fill. Hence the temperature of the silicon detectors should remain fairly constant for the start and duration of a fill. In deed, every reasonable effort will be made to ensure that this is the case.

Any distortions will be greatest at the edges of modules, where the motion will be normal to the surface. Since the overlaps at the edges will provide a very valuable alignment constraint, it is important that motion at the edges is not too great - the results reported in Section 2.1.3 are encouraging, as they indicate motions -0.5 \(\upmu\)m/\({}^{\circ}\)C. How the detectors move will be a function also of their support.

##### 3.1.3.3 Global Distortions

These correspond to larger scale distortions associated with the structure.

GravityThere will be some sagging of the ID. Due to differences between the loading and supports used at the time of the external survey, the distortions will inevitably be somewhat different from those measured in the survey. This is not likely to be a significant problem; what is less desirable is the time dependence of this: after any installation/access, the ID will relax. It is important that when data-taking resumes a few (?) days later, that the rate of change is small: less than a few microns per day.

HumidityShould be reduced by keeping ID in dry nitrogen (or CO\({}_{2}\)). Again, after interventions, damp air may enter and there will be a relaxation time. Similar requirements to Gravitational effects.

#### Thermal distortions

Similar remarks to those in Section 3.1.3.2 can be made.

The requirements on understanding the radius of a hit are quite loose O(50) \(\mu\)m (see Section 2.1.7). However, any expansion of the structure will not necessarily correspond to a radial expansion for each point on the detector. The finite azimuthal width of the detector will transform a radial motion of the centre of the detector into an azimuthal motion at the edges. Similar considerations to those in Section 3.1.3.2 lead to the same requirement that \(\Delta\)R \(\leq\) 10 \(\mu\)m.

#### Measuring Time-dependent Distortions

##### 3.1.4.1 Local Distortions

Local distortions will need to be followed using charged tracks.

##### 3.1.4.2 Global Distortions

Global distortions should be monitored by charged tracks, although this will be difficult for some modes.

##### Global translations

These will not affect mass measurements within the ID, but will affect:

* Matching to the EM Calorimeter. The \(\eta\)-strips have a resolution of \(\sim\)300 \(\mu\)m at 50 GeV (see Figure 4-37 of [1]).
* Matching to the Muon System tracks. If the ID is to be able to contribute to the combined momentum estimate, it is essential that translations in z should be controlled to better than \(\sim\)50 \(\mu\)m.

In the absence (7) of some in situ measurements of the position of the ID with respect to external reference frames, the Muon System itself may provide the best measurements of global longitudinal translations.

##### Global Longitudinal Expansions

If the complete ID expands uniformly in z, then straight tracks will continue to look like straight tracks and it will not be possible to deduce distortions from charged track residuals. However such effects should be readily observed with the FSI which will have a longitudinal resolution O(1) \(\mu\)m. Although the FSI will probably only be on the SCT, the connection to the Pixels with charged tracks will enable the Pixels to be understood. Of course it is unlikely that with its different subdetectors, the complete Precision Tracker will have a uniform expansion, but nevertheless, a uniform expansion is the lowest order component of a more general distortion.

The FSI will be associated with the SCT and so it should be possible to understand the overall longitudinal expansion of the SCT. However, since the z resolution of the SCT (\(\sim\)500 \(\mu\)m) will be much worse than the alignment precision in z required for the Pixels (\(\sim\)10 \(\mu\)m), it may prove difficult to transfer this information to the Pixels. Therefore, if the Pixels are not equipped with FSI, they need to be stable in z over the complete length of the barrels to 10 \(\mu\)m.

##### Global Radial Expansions

These uncertainties should be well constrained by the use of the overlaps (see Section 2.1.7).

## 4 B-field and Momentum Scale

The momentum scale depends critically and directly on the B-field. In addition, there will be some residual effects arising from imperfections in the alignment and some of these will be mapped up with the scale corrections.

### Discussion

#### 4.1.1 Magnetic Field

The original goal was to understand the B-field to the 0.1% level [2] - now we need to know it at least a factor of five better. One of the limitations for CDF was the knowledge of inhomogeneities in the field - of course our field is much worse. It is not clear what the spatial scale is over which the field must be known locally. Further, it should not be assumed that the field is axially symmetric.

Since the field is a continuous spatial function (unlike displacements of individual detector modules), knowledge of the field at one point will constrain the field at a neighbouring point. More explicitly, it is probably very valuable to express the field in terms of solutions of **Maxwell's Equations**. This expression should be limited to a reasonable number of higher order terms and the result of calibrations will be to determine the coefficients of the various terms.

#### 4.1.2 Direct Measurements of the Field

Uncertainties arising from the imperfect knowledge of the scale of the magnetic field will feed directly into those on the absolute momentum scale (before calibration). To be confident that the calibration is well understood, it is desirable to reduce the uncertainty coming from the magnetic field to a level such that it does not mask other problems. A goal of better than 0.1% appears to be reasonable as a starting point.

The standard way to map a magnetic field such as that from the ATLAS solenoid is with a machine which scans an array of Hall and nuclear magnetic resonance (NMR) probes over the full ID volume. This will be done after all of the surrounding magnetic materials have been installed and just before the installation of the ID itself. The accuracy required is similar to that which has been achieved in the past (_e.g._ a measurement accuracy of 0.01% was achieved in [10] and an accuracy of the magnetic field map of a few parts in 10 000 was achieved in [11], although the desired goal was only 0.1%).

The ATLAS solenoid is more difficult to map than other solenoids because of its non-uniform field. NMR probes require uniform fields and hence will only function within ~70 cm from the centre. Therefore, over most of the ID volume, Hall probes will have to be relied upon. Hall probes can be calibrated to better than 0.05% (there is considerable on-going work within ATLAS to produce a large number of calibrated Hall probes to monitor the toroid field [12]). The high field gradients expected in the ID volume will require the position of the probes in the mapping machine to be known to ~0.5 mm, which is more precise than was necessary for previous maps. Some understanding of the field can be derived from simulation, but such calculations are typically only accurate to ~0.5%.

In addition to mapping the field at the time of installation, it will be essential to monitor it with the ID in place and during the lifetime of the experiment. This will be done with a small number of NMR probes placed in the more uniform part of the field. The probes are sensitive to 1 part in \(10^{5}\); although they will not be very sensitive to the non-uniformities at larger \(\mid\!\eta\!\mid\), they should also be able to determine the position of the ID with respect to the magnetic axis. It seems that Hall probes will not be useful for in situ measurements.

The following effects may cause distortions of the field at the 0.1% level:

* Tiny quantities of ferro-magnetic material in the ID, which may be unavoidable, as for example nickel coating on the aluminium power cables, where they join the custom-made connectors on the SCT modules or at the level of the patch-panels. More recent thinking indicates that since the magnetisation will saturate, this will not be an issue.
* The mapping of the solenoid with the barrel toroid (most probably) switched off. Simulations predict that when the toroid is switched on, the solenoid field will decrease slightly because the extra field from the toroids will bring the Tile Calorimeter beams close to saturation and reduce their effectiveness at returning the solenoid field.
* What about the solenoidal field from the quadrupoles (ALPEH are concerned about this possibility) and the iron girders in the concrete floor (the Muon Spectrometer team are aware that these modify the toroidal fields)?

Other effects are considered to induce variations much smaller than 0.1%: para-magnetic and dia-magnetic effects due to the other components of the ID, the Earth's magnetic field, variations in the positions of the end-cap Calorimeters relative to the barrel Calorimeter when they are opened and shut, and hysteresis of the iron components.

The first-order effect of ferro-magnetic material (provided that it is uniformly distributed) and of the barrel toroid field, is to change the scale of the solenoid field map without changing its shape. This scale could be measured by building a small number of NMR probes into the SCT Barrel [13]. Such a system of probes would also provide a check of the position of the ID relative to the central axis of the magnetic field, which would be useful if the position of the solenoid coil were not absolutely stable.

#### Calibration with Z Mass

CDF used \(\mathrm{J/\psi\to\mu\mu}\) to understand their tracker calibration. This suffers from the fact that there are systematic uncertainties in extrapolating from the \(\mathrm{J/\psi}\) mass scale to the W mass. ATLAS will benefit from the plentiful statistics available in \(\mathrm{Z\to ll}\). Using \(\mathrm{Z\to ll}\) has the distinct advantage that the leptons have similar (but not identical) distributions to those from \(\mathrm{W\to lv}\). If the B-field can be directly measured with sufficient precision, this will enable the Z mass constraint to "mop up" any residual problems. It should be noted that the reconstructed Z mass is not very sensitive to sagitta distortions because they tend to cancel when multiplying the momenta of +ve and -ve leptons.

The **EM Calorimeter** will use \(\mathrm{Z\to ee}\) (in conjunction with E/p) to determine the absolute energy scale locally (see section 4.6 of [1]). If one scales the statistical precision obtained using \(\mathrm{Z\to ee}\) decays over about 400 \(\mathrm{\SIUnitSymbolMicro m}\)\(\mathrm{\SIUnitSymbolMicro\phi}\) bins for the study of the EM Calorimeter calibration to the expected statistics of 3x\(10^{6}\) reconstructed \(\mathrm{Z\to ee}\) decays for one year of operation at low luminosity, this leads to a statistical precision of 0.02% in each of these \(\mathrm{\SIUnitSymbolMicro\phi}\) bins. Since the experimental resolution for \(\mathrm{Z\to\mu\mu}\) decays reconstructed in the ID (\(\sigma(\mathrm{p_{T}})/\mathrm{p_{T}}=2.1\%\)) will be similar to that for \(\mathrm{Z\to ee}\) decays reconstructed in the EM Calorimeter (\(\sigma(\mathrm{E})/\mathrm{E}=1.5\%\))1, it should be possible to understand the momentum scale in the ID with comparable statistical precision over a similar number of \(\eta\)\(\mathrm{\to\phi}\) bins; or with greater precision for fewer bins - for example, 40 bins could be calibrated in one month. Things are conceptually straightforward for the Calorimeter: electrons of given \(\eta\)\(\mathrm{\to\phi}\), calibrate cells on the 2D surface of a cylinder. For the ID, the magnetic field extends through the 3D volume of the detector and the effect on charged particles is via an integral of the field along the trajectory.

Footnote 1: \(\Gamma_{\mathrm{Z}}/\mathrm{M_{Z}}=2.7\%\).

It is not clear how to proceed with studies of this. The momentum which will be reconstructed for a given track depends in a complicated way on the B-field at all points along the trajectory and the precision of measurements at detector planes. The effect of perturbations, other than those to the overall scale, cannot be trivially estimated. In principle, to formulate a \(\chi^{2}\) which could be used to determine coefficients of the field components would require an understanding of how the reconstructed track momenta depend on the coefficients2. This would seem very difficult. Instead, it might be necessary to consider tracks in \(\eta\)\(\mathrm{\to\phi}\) bins and determine a "measurement" of the average field - what this "average" would mean is unclear. Then, these "measurements" could be used to constrain the field coefficients. One would need to know how the average field depends on the coefficients - and again this would be equivalent to understanding the reconstructed momentum dependence. The trouble is that since the tracks effectively provide 2D information (as a function of \(\eta\)\(\mathrm{\to\phi}\)), it may prove difficult to constrain the 3D field. However, since the situation is exactly the same for leptons from W's, this probably does not matter too much. It may prove necessary and sufficient simply to derive **ad hoc corrections to the 1/\(\mathrm{p_{T}}\) scale as a function of \(\eta\)\(\mathrm{\to\phi}\)**.

Footnote 2: ALEPH have succeeded in fitting their magnetic field with Bessel functions.

While it would be nice to fit solutions of Maxwell's equations, this would only be possible if the alignment was sufficiently well controlled. This would imply understanding it to the level of 0.1 \(\mathrm{\mu\mu}\) (see Section 2.1.2). While this would seem completely hopeless, when averaged over the coarse \(\eta\)\(\mathrm{\to\phi}\) bins, this may be achievable.

Lastly, as was indicated in Section 2.1.2.2, in the approximation that Z's are produced at rest, the Z mass is only sensitive to the 3-momenta, whereas the W transverse mass is sensitive to the transverse momenta - the difference coming from the use of the polar angle. However, this should not be a problem, since sensitivity to the angles should come from boosted Z's.

#### 4.1.4 Calibration with J/\(\psi\) Mass

CDF used the \(\mathrm{J/\psi}\to\mathrm{\mu\mu}\). This has the feature (advantage or disadvantage?) that the muons are close together in space and will see similar global effects. The problem with \(\mathrm{J/\psi}\) is that CDF found they were limited by understanding the energy losses experienced by the muons. Muons at ATLAS will tend to be of higher energy, but then there is much more material to traverse. Nevertheless, at some time, the resonance should be studied. Generally, every piece of information will be valuable.

### Questions

#### Magnetic Field

1. How confident are we that the mapped field will not be modified by the insertion of the ID into ATLAS?
2. What are the systematics of the field measurements? Is there a chance that they could constrain the field sufficiently well?
3. How could a parametrisation of the field be constructed? Answer: see Section 4.3.1.

#### Calibration with Z Mass

1. Is it plausible that the alignment can be understood sufficiently well so that by using the Z mass constraint, the B-field components can be determined in the final calibration step?
2. Given a parametrisation of the B-field, think of a strategy to understand the B-field with Z \(\rightarrow\)\(\mu\mu\). A necessary tool for this work is the ability to fit tracks in an inhomogeneous field - this has been demonstrated by Igor Gavrilenko using xKalman (see Section 3.5.4 of [1]).
3. It might be possible/worthwhile to design some studies at the KINE level - although I cannot think what.

### Studies

#### Fitting the Magnetic Field

The studies reported in this section correspond to work undertaken by **Steve Snow**.

Rather than simply measuring the field at a set of points in space, it would be preferable to describe it in terms of components satisfying Maxwell's Equations: \(\nabla\cdot\mathrm{B}=\nabla\times\mathrm{B}=0\). These components could be constrained using the combination of data from: the field map, the monitoring probes and pairs of reconstructed charged particle tracks from the decays of resonances of known mass (J/\(\mathrm{\psi_{I}}\), \(\mathrm{\Upsilon}\), \(\mathrm{Z}\)). One set of functions, suitable for axial symmetry [10], are the modified Bessel functions, \(\mathrm{I_{0}}\) and \(\mathrm{I_{1}}\):

\[\mathrm{B_{z}}\,=\,\sum_{i}\mathrm{a_{i}}\ \cos\frac{\mathrm{i}\mathrm{z}}{ \mathrm{c}}\ \ \mathrm{I_{0}}(\frac{\mathrm{i}\mathrm{R}}{\mathrm{c}})\]

\[\mathrm{B_{R}}\,=\,\sum_{i}\mathrm{a_{i}}\ \ \sin\frac{\mathrm{i}\mathrm{z}}{ \mathrm{c}}\ \ \mathrm{I_{1}}(\frac{\mathrm{i}\mathrm{R}}{\mathrm{c}}).\]In practice, the constraint of axial symmetry would probably need to be relaxed for the actual field configuration in the ID. As an exercise to test the feasibility of fitting the field, an attempt was made to fit the _simulated_ field. Despite using eight terms, the fit was poor and unstable, suggesting that more terms are needed. Better results were achieved starting from a field corresponding to an ideal solenoid. A scale factor was associated with this field, which was then augmented by two terms from the above power series.

The residuals between the fitted and simulated fields are shown in Figures 4-1 and 4-2, respectively for the axial and radial field components. The comparison is shown at a radius R = 75 cm, where the field deviates more strongly from uniformity than along the beam axis. It can be seen that, over most of the length of the coil, the field can be described to better than \(\pm\)0.02%, corresponding to \(\pm\)4 Gauss. There are significant deviations at values of \(\mid\) z \(\mid\) larger than 2.6 m (the actual half-length of the solenoid coil is 2.65 m), but these would cause little effect on the momentum determination because they occur at the end of a track.

This study does not show how the field can be determined but it does show that it can be described sufficiently accurately with a small number of parameters.

## 5 Material

Knowledge of the ID material is important to understand the energy loss suffered by electrons. Unless corrected, this reduces the reconstructed \(\mathrm{p_{T}}\) and leads to a bias in the energy scale derived from E/p. What is important is the shift of the **peak of the E/p distribution** - this is caused by small but continuous energy losses as an electron traverses the ID material. The tail of the E/p distribution tends to be caused by one or more hard bremsstrahlung radiations, however there is no clear cut distinction between the soft and hard brem, The reconstruction of \(\mathrm{p_{T}}\) in the ID is not well defined but depends intimately on the assumption of the fitting procedure. The observed shift in the peak of E/p depends in a complicated manner on the integral of the material. The largest contribution to this shift will come from the Precision Tracker.

If the EM Calorimeter scale is to be accurate to 0.02%, it would be good to control the uncertainties arising from the material to the level of 0.01%.

### Discussion

#### 5.1.1 Understanding \(\mathrm{X_{0}}\) from Construction

CDF understood their material by considering the E/p distribution and confirmed it by the use of conversions. They had \(\sim\)10% \(\mathrm{X_{0}}\) in front of their main tracking volume. By contrast, ATLAS has an average of \(\sim\)40% within the volume of the active tracking. By having so much material, we will be liable to second order complications, i.e. electrons undergo multiple brem, converted electrons brem and brem photons convert. Further, CDF estimated from first principles that they had 6.4% \(\mathrm{X_{0}}\), whereas they measured \(8.1\pm 0.4\)% with conversions - a discrepancy of a factor of 1.3. Although we are going to great lengths to understand the material contents of the ID, there will be significant uncertainties on the numbers - things like hybrids can be notoriously tricky to understand. Various cross-checks must be made of the material:

* Detector elements should be weighed.
* They should be placed in test-beams to measure the material.
* In particular, reference planes (see comments on conversion) should be placed in test-beam.

#### 5.1.2 \(\mathrm{X_{0}}\) Determination with E/p Tails

The size of the tail in E/p distributions should be a measure of the amount of hard brem which electrons undergo. This is in contrast to the smaller energy losses suffered by electrons in the peak.

Let the E/p distribution for electrons whose hardest brem is at a radius R be F(E/p,R) (normalised to unity) - this can be determined from the KINE information for a reference sample. The complete E/p distribution is

\[\Sigma\]

C(R) F(E/p,R) + C( \[\infty\]

F(E/p, \[\infty\]where the sum is over the radii of the hardest brem, and R\(=\infty\) allows for cases where there is no brem above a certain threshold. To first order, the coefficients C(R) are proportional to the amount of material. Hence by fitting real E/p distributions by a sum of terms, it ought to be possible to estimate the corresponding coefficients and the relative amounts of material.

#### X\({}_{0}\) Determination with Conversions

Conversions provide an obvious way to determine the amount of material. However, for this to work, it is necessary to understand the efficiency very accurately up to some radius (probably around 80 cm). This is easier if efficiencies can be made very high, but currently, these efficiencies are typically 85%. Ideally, a dedicated conversion finding program should be developed with very high (or well understood) efficiency. Further, it is necessary to normalise the rate

* either absolutely using a known flux of photons
* or relative to some component whose material can be accurately estimated from first principles.

A known flux of photons could be determined from taking all EM clusters above some thresholds and subtracting all prompt electrons. This would then leave conversion photons and photons which are unconverted in the ID. However, it is probably difficult to avoid miscounting photons arising from \(\pi^{0}\) decays. Alternatively, it may prove possible to extract a well defined rate of photons using FSR in the decay Z \(\rightarrow\) ee? where the presence of the photon is deduced by reconstruction. It seems plausible that reasonable rates of a few percent of Z \(\rightarrow\) ee could be obtained after cuts to define reasonable energies and isolation for the photon. This could yield \(\sim\)10\({}^{5}\) photons/year at low luminosity1, but whether this would be enough remains to be seen.

Footnote 1: I dont recall where this number came from. In Section 12.3.2, it is estimated that 1.5\(\times\)10\({}^{4}\) photons with E\({}_{\rm T}\)\(>\) 30 GeV will be produced - for this study, a lower threshold would be fine, giving many more photons.

CDF normalised their conversion rate to the rate from a wall in the tracker volume. It is necessary to find a component whose material is well understood and which can be clearly identified as a peak in the conversion distribution. The CDF conversion radial position resolution was -4 mm, whereas in ATLAS it is somewhat larger with large tails due to electron brem. This may make it difficult to resolve clearly components in the tracker and generally, many composite components will have radiation lengths which are difficult to estimate. Alternatively, one could try normalising to the beryllium beam pipe. This is well defined, however it may be difficult to resolve due to its proximity to thermal shields and the B-layer and the conversion rate may be buried under the Dalitz decays at the primary vertex.

#### Other Possibilities

Another possibility may be to examine the tails of the mass distribution for electron pairs coming from J/\(\psi\) or even conversions.

ALEPH have located the material in their tracker by reconstructing hadronic interactions. However, the rate of interactions is related to the number of interaction lengths, rather than the radiation lengths.

### Questions

#### General

1. Do we believe GEANT describes electron energy loss sufficiently accurately?
2. How does the shift of E/p peak depend on the integrated material? Presumably it is associated with an integral of X0 along the track weighted by some function of the radius.
3. At what level do we need to know X0 to control p and hence E at the 0.02% level? Answer: see Section 5.3.1.
4. At what level can we calculate material budgets from first principle? Richard Apsimon believes that the module material will be known to \(<\)1% (relative, not percent of X0) and away from the ends, the complete SCT barrels will be known to \(<\)5%.
5. Can we measure material, at least for a reference, using an X-ray source?

#### X0 Determination with E/p Tails

1. Is it possible to understand the E/p tails for the modified layouts in terms of the components of the distribution characterised by different radii for the hardest brem?
2. It would be nice to use the electrons from W \(\rightarrow\) ev for this - but is this biassed?

#### X0 Determination with Conversions

1. What does the radial distribution of conversion points in the ID look like? Answer: see Section 5.3.3.
2. Is there some well defined component of the ID which could be used for normalising the rate? The need for a single simple component would be reduced if complete subdetector planes could be measured with an X-ray source.
3. Is it plausible that we could understand the conversion finding efficiency sufficiently accurately? Also, one needs to understand the photon flux as a function of radius: it is reduced by conversions, but is regenerated by electron brem from primary electrons (not many) and from conversion electrons.
4. Could Z \(\rightarrow\) eev be useful?

### Studies

To facilitate studies, datasets with several geometries could be generated to compare with the standard geometry corresponding to 98_2. It turns out to be easy to increase SCT material in a fairly uniform way by increasing the density of the supporting cylinders and wheels. However, it is not so easy to increase Pixel material uniformly (it is dangerous to increase the silicon density, since this will modify the ionisation deposited, affecting hit resolution due to the effects of charge sharing).

Two sets of data with extra SCT material have been created1:

Footnote 1: Jo Pater modified the SCT geometry; Jorgen Beck Hansen generated the datasets.

* 8.8\(\times\)10\({}^{3}\) events passing track quality cuts.
* 4.7\(\times\)10\({}^{3}\) events passing track quality cuts.

Information about these datasets can be found on the Web by following the links:

\[\mathrm{ATLAS}\rightarrow\mathrm{ID}\rightarrow\mathrm{Performance} \rightarrow\mathrm{Material}\]

The datasets which have been generated contain changes which are much greater than the required sensitivity in order to study the issues with much fewer number of events than will be available as real data.

#### Calibration Sensitivity

The studies reported in this section and the next correspond to work undertaken by **Stephen Haywood** and cross-checked by **Matevz Tadel**.

For the two rapidity intervals studied, the effect of increasing the material for the E/p distribution can be seen in Figures 5-1 and 5-2. There are small shifts in the peaks of the distributions and the tails have increased, with extra events around E/p = 1.05. The movement of the peak was determined by making a fit in the interval \(\pm\)1 \(\sigma\) about the peak. At \(\eta\) = 0.3 (1.8), the peak shifts by 0.0025\(\pm\)0.0012 (0.0068\(\pm\)0026), so if it is required to understand this shift at the level of 0.01%, the material of the SCT in the barrel must be understood to ~2% (0.5%) of its value. This requirement is likely to be even tighter for the Pixel System, which is located at small radius, and in the end-caps, where there is more material.

#### X\({}_{0}\) Determination with E/p Tails

To determine the X\({}_{0}\) distribution, the E/p distributions should be fitted with a distribution like that of Equation 5.1. This is quite complicated. To get a feel for the sensitivity which can be obtained, the distributions shown in Figures 5.1 and 5.2 were fitted with Gaussians. The range was increased to \(\pm 1.5\)\(\sigma\) to increase the sensitivity to the effect of extra SCT material. One such fit is shown in Figure 5.1. The width of the fitted distributions provides a measure of the difference resulting from extra material. For \(\eta\) = 0.3 (1.8), the difference in the widths of the fitted Gaussians was 0.0052\(\pm\)0.0007 (0.0139\(\pm\)0.0014) - the two distributions can be distinguished at the level of 7 (10) standard deviations with these datasets. While the difference between the pairs of distributions does not look very significant, different fits have been tried and they all indicate that the distributions really are very different. To increase the sensitivity by a factor of 25 (68) to ensure that the peak of the E/p distribution could be corrected with sufficient precision (see previous section), would require 12 (46) times more data, corresponding to 1.0\(\times\)10\({}^{5}\) (2.2\(\times\)10\({}^{5}\)) electrons. With 30\(\times\)10\({}^{6}\) reconstructed W \(\rightarrow\) ev events expected for each year of low luminosity running, this should allow a satisfactory determination of the ID material as a function of pseudorapidity.

Although it will be more difficult to resolve deviations from the material expected at different radii, sensitivity to the different components should be possible by fitting the E/p distribution by functions with more parameters, in particular taking more care with the tail.

#### 5.3.3 X\({}_{0}\) Determination with Conversions

The studies reported in this section correspond to work undertaken by **Monika Wielers**.

Figure 5-4 shows the radial positions of single photon conversions of \(\mathrm{p_{T}=50\GeV}\), as reconstructed in the barrel part of the ID (note that there is no background, in particular from Dalitz decays). The three pixel barrel layers at their respective radii of 4, 11 and 14 cm, a support cylinder at a radius of 25 cm, and the first two SCT barrel layers at their respective radii of 30 and 38 cm can be clearly distinguished above the very small background. The beam pipe and the pixel B-layer are barely resolved. The cut-off around the third SCT barrel layer is due to the requirement that a reconstructed track have at least four SCT hits.

The width of the peaks in Figure 5-4 is dominated by the resolution on the radius of the reconstructed photon conversion, but is also affected by the effects of the azimuthal tilts of the planar detectors. Figure 5-5 shows the radial resolution for conversions in the pixel barrel layers. The resolution is \(-4\) mm, although there is a significant tail due to bremsstrahlung of the conversion electrons. In the first two barrel layers of the SCT, this resolution degrades somewhat to \(-6\) mm. Studies of photon conversions in b-jets, where the photon energies are much smaller, indicate that the radial resolution improves at lower \(\mathrm{p_{T}}\), as might be expected from the larger angular separation between the electrons. At very low \(\mathrm{p_{T}}\), the radial resolution is dominated by multiple scattering effects. Photon conversions can be reconstructed at larger radii, up to \(\mathrm{R_{c}=80\ cm}\), using the TRT alone, but the radial resolution is significantly degraded.

## 6 Resolutions

It is claimed that we need to understand the actual \(\mathrm{p_{T}}\) resolution at the percent level. It is not clear over what distributions this is allowed to be averaged, in particular is it averaged over pseudorapidity? Inevitably, the resolutions will be obtained from understanding the convolution of the resolution functions with the Z lineshape.

### Questions

1. At what level is it necessary to know the \(\mathrm{p_{T}}\) and E resolutions and in what intervals?
2. How many events are required to understand the resolutions to the required (1%?) precision?

## 7 Strategy

In principle, if a super-dooper calibration procedure can be developed, including all survey information, all alignment information from FSI etc. and all track residual measurements, then all corrections can be derived in a single process. However this is implausible and such a procedure may fail to identify some problems. Instead, it seems likely that calibration will be broken down into certain sub-problems. Undoubtedly, to derive a complete (or best) understanding will involve iteration.

The obvious strategy is as follows. With time it can be refined.

1. Local alignment of ID starting from survey, FSI and previous alignment data.
2. If necessary, a fast global alignment of barrels and wheels relative to each other using high-\(\mathrm{p_{T}}\) tracks (muons from W and Z) to ensure efficient track finding. This assumes a sufficiently high luminosity (at least, at the very beginning it is unlikely that the luminosity will be as high as \(10^{-33}\) cm\({}^{-2}\)s\({}^{-1}\)). Nevertheless, there will be plenty of other sources of high-momentum tracks, e.g. the 6 GeV muons.
3. Local alignment using track data. 1. Alignment with respect to overlaps in Precision Tracker sublayers (barrels and disks). 2. Local alignment using all modules in Pixels, SCT and combined Pixels and SCT. 3. Alignment of TRT straws and modules with respect to Precision Tracker.
4. Estimation of sagitta corrections using E/p.
5. Estimation of scale corrections using Z\(\rightarrow\)\(\mu\mu\). Cross-check with J/\(\psi\)

On a longer time-scale, the following can be done:

* Estimation of ID material using E/p and conversions.
* Determination of ID resolution using Z\(\rightarrow\)\(\mu\mu\).

## 8 Conclusions

There is one big unanswered question in this note: "what constitutes a satisfactory interval (in space and time) for forming an average". This question is intimately linked to several crucial suggestions:

* rather than 0.1 \(\mu\)m.
* It is necessary ensure the stability of the detector (at least, module to module) to this precision over the time interval required to obtain alignment constants from track data.
* All residual uncertainties can be mapped up by using the Z mass.

While the first two points remain uncertain, the last point seems plausible. CDF obtained a scale uncertainty on M\({}_{\rm W}\) of 120 MeV - it would be surprising if with 10\({}^{3}\) more events, ATLAS could not reduce this by at least a factor of 10. Further, in this note, we have seen that we should have the tools to understand the detector (alignment, B-field, material) with sufficient precision.

### Summary

The positions of modules of the Precision Tracker (and within them) should be understood to

* 1 \(\mu\)m in R\(\phi\) and
* 10 \(\mu\)m in R-z.

Further, they should be stable to this precision over the period of \(\sim\)1 day.

### Work Programme

In several of the chapters, questions are asked - these indicate work which needs to be done at some time. A more immediate list of tasks which can be undertaken by interested peoples is given on the Alignment Web page:

[http://atlasinfo.cern.ch/Atlas/GROUPS/INNER_DETECTOR/PERFORMANCE/ALIGN/align.html](http://atlasinfo.cern.ch/Atlas/GROUPS/INNER_DETECTOR/PERFORMANCE/ALIGN/align.html)

### Acknowledgements

In addition to those people mentioned in the text, I would like to thank Richard Apsimon, Dario Barberis, Daniel Froidevaux, Fabiola Gianotti, Todd Huffman, Bill Murray, Peter Phillips, Armin Reichold David Rousseau and Fred Wickens who have contributed to the thinking in this document.

## 9 References

* [1] ATLAS Overview and Physics Potential TDR, CERN/LHCC/99-14 and 15 (1999).
* [2] S. Snow, "Magnetic Field Measurement Requirements of the Inner Detector", InDet-No-196.
* [3] F. Gianotti, "Measurement of the W Mass at LHC", talk at the Grenoble 1998 ATLAS Physics Workshop.
* [4] CDF Collaboration, "Measurement of the W Boson Mass", Phys. Rev. D 52 p4784.
* [5] ID TDR Vol I, CERN/LHCC/97-16 (1997).
* [6] ID TDR Vol II, CERN/LHCC/97-17 (1997).
* [7] S. Rajagopalan and M. Rijssenbeek, "Measurement of \(\mathrm{M}_{\mathrm{w}}\) using Transverse Mass Ratio of W and Z", contribution to Snowmass 96. See [http://atlasinfo.cern.ch/Atlas/GROUPS/PHYSICS/LHCCSMWS/lhccewwg.html](http://atlasinfo.cern.ch/Atlas/GROUPS/PHYSICS/LHCCSMWS/lhccewwg.html)
* [8] Private communication from Chris Nelson (RAL). Based on study using vibrational spectrum taken from ISIS.
* [9] S. Peeters, "Alignment of the ATLAS Precision Tracker Using Tracks", ATL-COM-INDET-99-007.
* [10] ALEPH Collaboration, ALEPH Handbook, Vol. 1 (1995).
* [11] D. Newton, H1 Internal Note, H1-8/90-145 (1990).
* [12] Barrel Toroid Technical Design Report, CERN/LHCC 97-19 (1997).
* [13] S. Snow, "Magnetic field measurement requirements of the Inner Detector", ATL-INDET-97-196 (1997).
* [14] A. Bonissent et al., ALEPH Internal Note, ALEPH-97-116.
* [15] Pixel TDR, CERN/LHCC/98-13 (1998).
* [16] J.Jackson, "Classical Electrodynamics".

## 10 Appendix

### Dilution Factors

See discussion in Section 2.1.

Just because errors on alignment constants are diluted, does not necessarily mean that the errors on reconstructed coordinates and hence track parameters are significantly diluted.

_I need to write up my rough notes, but since this is a bit academic, I will leave it for a rainy day._