# Atlas Internal Note DAQ-No-113

18 June 1998

Paper Models of the ATLAS Second Level Trigger

**M. Dobson, S. George; J. A. Strong**

Royal Holloway and Bedford New College,

University of London, Egham, Surrey, UK.

**J. C. Vermeulen**

NIKHEF, Amsterdam, Netherlands.

email S.George@rhbnc.ac.ukemail i73@nikhef.nl

18 June 1998

###### Abstract

Simple "paper" models of some of the options studied in the ATLAS second level trigger demonstrator programme are presented. The aim is to allow quick investigation of different options and parameters to guide the computer modelling and technology studies.

This note describes paper models of architectures B and C of the ATLAS LVL2 demonstrator programme. The architectures, protocols, parameters and assumptions of the models are described. Physics input is taken from a selection of trigger menus for low and high luminosity. Rates, occupancies, loads and the average latency are estimated, and potential bottlenecks identified. The impacts of different trigger strategies on the system requirements are studied. The effects of varying some of the parameters characterising the hardware are shown, and critical parameters identified.

## 1 Introduction

### The second level trigger demonstrator programme

The design of the ATLAS second level trigger is currently proceeding with an R&D programme to investigate possible architectures and study technologies. The aim of the programme is to inform the architecture choices and technology choices to be made in the next few years.

There are three demonstrator projects: A, B and C; the latter two are studied in this note. Both demonstrator architectures conform to the generic trigger architecture of data sources and processing nodes which are connected by a network. The partitioning of thenetwork, the distribution of processes on processors, and the message passing models are the key differences between B and C.

Demonstrator B is the investigation into a "local-global" or multi-farm architecture. Feature extraction is done on local processor farms, while event building and decision algorithms run on a global processor. This note fulfils part of the Demonstrator B work plan, in which the first work package is to collect and understand physics, detector and technology parameters, and to use these to make "paper model" calculations.

The Demonstrator C architecture is based on a single processing farm, which is connected to the data sources by a single switch. A single processor does all the processing for one event. Requests for data and the data itself all pass through the same switch.

### Paper Models

With the wide variety of choices faced in the demonstrator programme, in architectures, technologies, trigger strategies and protocols, a fast method of assessing the impact of different options is needed.

A paper model is the next step up from a back-of-the-envelope calculation. It should begin to answer questions about the pros and cons of the architectures, trigger menus, trigger strategies and protocols. The rates, loads, occupancies and latencies around the system will be calculated (see section 2 for their definitions). They provide preliminary indications of possible bottlenecks and hardware requirements. Paper models produce information that can guide other activities such as modelling, emulation, detailed systems analysis and design. The model should be flexible enough to allow comparison between different trigger strategies and modifications to the architecture and messages.

Paper models have some obvious limitations. They are static, dealing only with averages over time, so the effect of queueing or network congestion cannot be included in the model. Averages are used instead of distributions for most of the inputs. The architecture is somewhat abstract, with more complex units such as the supervisor treated as black boxes and technology kept as generic as possible with simple parameterisations. These aspects are being addressed by computer modelling [1], emulation [2] and technology studies.

### What this note contains

The model results presented in this note have been produced in agreement1 by two independent spreadsheets [3, 4], with the common set of input parameters specified in [5]. A paper model for demonstrator C predates this work [6]. It is not directly comparable with these results because many of the input parameters have since changed. However, the modelsused here have been verified against this previous work and comparable results obtained by using the appropriate parameters.

This paper includes a description of the demonstrator B and C architectures and protocols. Common 'technology independent' parameterisations are used to model the components of the architectures. These are outlined in this document, and fully described in [5]2. The sensitivity of some of these parameters is studied. The physics input to the models in the form of trigger menus is described. The impact of the trigger menus and several trigger strategies are compared with each architecture. Low (\(10^{33}\ \mathrm{cm}^{-2}\mathrm{s}^{-1}\)) and high (\(10^{34}\ \mathrm{cm}^{-2}\mathrm{s}^{-1}\)) luminosities are considered.

Footnote 2: It is recommended that readers of this note are familiar with ATLAS DAQ-No-70 [5].

## 2 Definitions

Some of the quantities which the paper model sets out to calculate are defined below.

The LVL2 trigger system is that shown in figures 1 and 2.

The **rate** is the number of messages per second at a point in the system. It is independent of message size and is for whole messages, not packets. No account has been taken of packets in the paper model. The rates do not include extra messages that may be necessary for a particular message passing protocol, for example acknowledgement messages.

The **occupancy** of a processor is the fraction of available CPU time which is used.

A **farm size** is defined as the number of processors at 100% occupancy that would meet the processing requirements of the farm.

Rates, bandwidths and occupancies given for ROBs, processors and the links between them are averaged over the whole group or farm. 'Hot-spots' would inevitably exist which cannot be taken into account in these simple models.

The **decision latency** of the system is defined as the time elapsed between the LVL1 trigger being received by the supervisor and the final accept/reject being processed in the supervisor for the same event. It is averaged over the expected types of event, weighted by their estimated rates.

## 3 Architecture models

### Messages

Table 1 is a list and description of the messages used in the paper models. All the message sizes include headers and padding. The sizes are calculated using the message formats specified in [7].

When several ROIR or ROIRSF messages are to be sent out of the supervisor at the same time, they could be grouped together into one big message record to reduce the rate (and therefore the overhead). This is not currently in the model.

The event decision message (T2DR) is sent every 100 events, which means every 1 ms at the maximum design rate of 100 kHz.

### Architecture B

Figure 1 shows the simplified architecture diagram used and the points at which rates, loads, occupancies and latencies are calculated. The system of messages and processing shown in the diagram is described in table 2. The messages are defined in section 3.1.

The key characteristic of architecture B is the partitioning of the system into local and global subsystems. There are four local parts which map onto the four main subdetectors (calorimeter, muon, TRT and SCT). It is envisaged that an event would be processed as follows.

The supervisor will know how the pattern of RoIs input by the LVL1 trigger should be processed, in terms of what sequential and parallel steps should be executed. The RoI requests for the first sequential step (which could be the only step, or the first of many) are sent to an intelligent network where they are distributed to the relevant ROBs in each subdetector. The local networks switch all the ROB data for a feature into one FEX processor. The FEX sends its output via the global network to the global processor assigned to the event. The global processor sends a decision (accept/reject/continue) to the supervisor back via the global network, and then the RoI requests for the next sequential step (if there is one) are sent around the system in the same way. Reject or final accept signals which mark the end of each event are collected together by the supervisor and broadcast to ROBs at

\begin{table}
\begin{tabular}{l l l} \hline
**Message** & **Size** & **Description** \\  & **(bytes)** & \\ \hline ROIR & 24 & RoI requests sent from supervisor to RoI Distributor (B) or \\  & & from processor to ROBs (C) \\ ROIRSF & 36 & send RoI records to global processor (for average 5 RoIs per event) \\  & & (architecture C) \\ ROID & 32 & data sent from ROB to processor, header only; the full size \\  & & includes the ROB data \\ FEXD & 150 & feature record sent from local to global processor (B only) \\ GPR1 & 20 & event decision/continue from global processor to supervisor \\ GPR2 & 96 & block of ROIRâ€™s produced in the local or global processor and sent to the supervisor for distribution to ROBs; this is the size of a \\  & & message containing 20 RoIs, which corresponds to one event \\  & & (B only) \\ T2DR & 408 & block of 100 event decisions sent from supervisor to all ROBs \\ \hline \end{tabular}
\end{table}
Table 1: _Description of messages_.

\begin{table}
\begin{tabular}{l l} \hline
**Node** & **Processes** \\ \hline
**Supervisor** & handle event decisions \\  & and send out ROIR messages \\
**RoI Distributor (intelligent network)** & distribute ROIR/T2DR \\  & to relevant ROBs \\ LVL2 Readout Buffer (ROB) & handle data requests \\  & and event decisions \\ Local network & switch ROID messages \\ Local feature extraction (FEX) processor & RoI fragment building \\  & and feature extraction \\ Global network & route FEXD (down) and \\  & GPR (up) messages \\ Global processor (GTP) & event building and decision \\ \hline
**Link** & **Messages** \\ \hline Link 1 from LVL1 to supervisor & LVL1 trigger message \\ Links 2 from supervisor to RoI Distributor & ROIR/T2DR \\ Links 3 from RoI Distributor to each ROB & ROIR/T2DR \\ Links 4 from ROBs to local net & ROID \\ Links 5 from local net to local processors & ROID \\ Links 6 from FEX processors to global net & FEXD \\ Links 7 from global network to global processors & FEXD \\ Links 8 from global processor to global network & GPR1/GPR2 \\ Links 9 from global network to supervisor & GPR1/GPR2 \\ \hline \end{tabular}
\end{table}
Table 2: Description of architecture B.

longer intervals to avoid overloading them.

The supervisor is a complex system in its own right. It is currently expected to consist of a farm of CPUs and special-purpose hardware for I/O. For simplicity we treat it as a single processing unit with standard i/o characteristics. The supervisor sends RoI requests for the muon detector, calorimeter, TRT and SCT as separate messages. For the muon precision and trigger detector it is assumed that per RoI a single message is sent. This also holds for the electron and hadron calorimeter.

The RoI distributor network is an intelligent fan out to take ROIR messages from the supervisor and send them directly and exclusively to whichever ROBs are involved in the RoI. This means that the minimum number of ROB interrupts is caused. The RoI distributor network is treated as a normal network in the model.

The links and processors shown in the diagram are only pictorial representations and do not attempt to show the actual number of processors or links in the model.

### Architecture C

Figure 2 shows architecture C as it is modelled. The system of messages and processing shown in the diagram is described in table 3. The messages are defined in section 3.1.

Figure 1: Diagram of paper model for architecture B. The numbered links and labelled boxes are described in table 2.

\begin{table}
\begin{tabular}{l l} \hline
**Node** & **Processes** \\ \hline
**Supervisor** & handle event decisions \\  & and sending RoIRSF messages \\ ROB & handle data requests \\  & and event decisions \\ RSI processor & RoI data fragment merging \\  & and ROIR/T2DR distribution \\ SFI processor & more ROI data merging, ROIRSF \\  & and GPR1 routing, and ROIR grouping \\ Network (down) & route ROIRSF and merged ROID \\ Network (up) & route T2DR and grouped ROIR \\ Global processor (GTP) & feature extraction, event building \\  & and event decision \\ \hline
**Link** & **Messages** \\ \hline Link 1 from LVL1 to Supervisor & LVL1 trigger message \\ Links 2 from Supervisor to Network & ROIRSF/T2DR \\ Links 3 from Network to Supervisor & GPR1 \\ Links 4 from each ROB to a RSI & ROID \\ Links 5 from each RSI to its ROBs & ROIR and T2DR \\ Links 6 from RSIs to network & merged ROID \\ Links 7 from network to each RSI & grouped ROIR and T2DR \\ Links 8 from network to SFIs & ROIRSF and merged ROID \\ Links 9 from SFIs to network & GPR1 and grouped ROIR \\ Links 10 from each SFI to its GTPs & ROIRSF and merged ROID \\ Links 11 from each GTP to a SFI & GPR1 and ROIR \\ \hline \end{tabular}
\end{table}
Table 3: Description architecture C.

Architecture C introduces RSI (ROB to Switch Interface) and SFI (Switch to Farm Interface) processors.

An example of event processing in architecture C is as follows. After the LVL1 trigger message is received by the supervisor, all the RoI information is passed via the network to a global processor. This processor then decides which data is required for the first sequential step and sends out the request.

On each side of the switch there is a 'farm' structure. Data requests go to the SFI, from where they are sent to the relevant RSIs across the switch. The RSIs will gather the requested data from their the ROBs, merge it together and send it back across the switch to the SFI. The SFI will collect and merge the fragments from all the RSIs involved, and sent them all in a single message back to the global processor. The global processor can then execute algorithms on this set of RoI data, make a decision about the event, and request the data for the next sequential step if necessary.

The final reject or accept decision must be sent back to the supervisor, which will still manage the task of sending decision blocks to the ROBs.

Although all the processing for an event is done on a single processor, it will work on several events at once, changing context to an event which requires processing while waiting for requested data for another event.

Figure 2: Diagram of paper model for architecture C. The numbered links and labelled boxes are described in table 3.

## 4 Input information

This section gives an overview of the information taken as input to the paper models. Full details have already been published in [5], which the reader is urged to consult alongside this note.

### Detector and readout parameters

The detector and readout parameters are taken mainly from [8] although some have since been revised.

There are 1462 readout buffers (ROBs) divided between the subdetectors as described in table 4. The average number of ROBs holding data for a region of interest (RoI) depends on the type of the RoI (as determined by the LVL1 trigger) as well as the subdetector. This information is given in table 5.3 The number of RSIs per RoI for architecture C has also been calculated.

Footnote 3: Since this note was written an error was found in the number of ROBs for a muon RoI; it has consequently been changed slightly to 4.63. The results in this note are consistent with the original number in the table.

The average amount of data in a ROB varies from 0.1 kByte in a Muon RPC ROB to 1.3 kByte in a ECal ROB. Tracking data is zero suppressed and therefore varies with occupancy, for example a TRT ROB contains on average 0.75 kByte of data at high luminosity, but only 0.28 kByte of data at low luminosity.

\begin{table}
\begin{tabular}{l l l}
**Subdetector** & **no. of ROBs** & **no. of RSIs\({}^{\dagger}\)** \\ \hline Muon MDT & 192 & 64 \\ Muon RPC & 22 & 6 \\ ECal & 432 & 108 \\ HCal & 48 & 12 \\ TRT & 512 & 128 \\ SCT & 256\({}^{\dagger}\) & 64\({}^{\dagger}\) \\ \end{tabular}
\end{table}
Table 4: ROB and RSI layout. \({}^{\dagger}\) Applicable to Arch. C only. \({}^{\dagger}\) Not including pixels.

\begin{table}
\begin{tabular}{l l l l}
**Detector** & \(\mu\)**RoI** & \(e/\gamma\), \(\tau\)**RoI** & **jet RoI** \\ \hline muon MDT & 3.24 & â€” & â€” \\ muon RPC & 2.44 & â€” & â€” \\ ECal & 6.19 & 6.12 & 16.50 \\ HCal & 2.04 & 2.32 & 3.75 \\ TRT & 12.40 & 6.88 & â€” \\ SCT & 4.44 & 3.94 & â€” \\ \end{tabular}
\end{table}
Table 5: _Average number of ROBs per RoI [9]_

### Processes

Process models have been designed for the supervisor, global processor, ROB, RSI, SFI and local processor, as required for each architecture. They are almost fully described in [5, 10]. For architecture C the following extensions have to be made:

* The RSIs are receiving, replicating and sending out T2DR decision blocks sent by the supervisor. For each message receive and send the standard I/O overhead is taken into account.
* The SFIs pass RoIRSF messages to the global processors and T2 decisions to the supervisor. For each message receive and send the standard I/O overhead is again taken into account.
* It is assumed that T2DR messages may be broadcast over the network to the RSIs.

All the models have a simple set of processes for which times are estimated and are assumed to be interrupt driven. The CPU time used for an i/o interrupt and context switch is set at 50 \(\mu\)s per message, in addition to any other times discussed below.

Overheads are the dominant use of CPU resources in the ROBs; preprocessing algorithms are minimal or non-existent. Other ROB processes include 100 \(\mu\)s per 100 events to manage decision blocks, and 10 \(\mu\)s to extract data for LVL3, at the LVL2 accept rate (see table 9).

Algorithm times are taken from benchmarking results for the feature extraction algorithms. They are all extrapolated to a nominal 500 MIPS processor. The FEX algorithms are executed on the global processors for architecture C and the local processors for architecture B. A summary of algorithm times is given in table 6.

Architecture C SFI/RSI processors and architecture B local/global processors have to merge data fragments from ROBs. Merging is done at 50 MByte/s in addition to the standard i/o overheads.

The global processor also has event building and decision algorithms for which times of 10 - 100 \(\mu\)s are estimated.

### Technology

Hardware is described as generically as possible for paper models, although it would be quite easy to replace the simple parameterisations with something specific to a particular technology if desired.

For estimating the decision latency, network links are assumed to have a bandwidth \(B\) of 10 MByte/s each, and the elapsed time \(T_{0}\) to set up a network transfer is 100 \(\mu\)s. The elapsed time for a transfer of data over links and via a network is thus

\[T=T_{0}\ +\frac{\text{message size}}{B}\]A simple treatment of congestion would be to reduce the available bandwidth, but this has not been done. Instead, the model is used to determine the average required bandwidth and the average rates of messages. This provides information about ATLAS LVL2 traffic for technology studies. The elapsed time when sending several messages from the same source or to the same destination is the sum of the times, otherwise messages can be sent in parallel and only the longest time counts.

A processor is taken to be a general purpose CPU accompanied by a dedicated i/o co-processor, such that the overhead to send/receive a message plus the accompanying context switch is 50 \(\mu\)s. The nominal processing times are extended to account for an operating system overhead of 10 % of the total processing time.

### Physics

The LVL2 trigger is driven by the output from LVL1. This is modelled with trigger menus, which give the rates and combinations of the RoIs expected from level one. They are estimated from physics simulations and fast simulations of the LVL1 trigger. As such, the trigger menus provide the physics input to the LVL2 paper model.

A menu item consists of a combination of types of LVL1 trigger object (RoIs) and the rate at which that combination is expected to arise. The total rate of each type of RoI gives the rates and occupancies in the trigger system. The patterns of RoI combinations are needed to give the average decision latency. The trigger objects delivered by the LVL1 trigger are listed in table 7.

There are currently two main variations in the trigger menus considered for the demonstrator programme. Menus can be for high and low luminosity, and they can be either

\begin{table}
\begin{tabular}{l l l l l l}
**RoI type** & **sub-** & \multicolumn{2}{l}{**Measured times (\(\mu\)s)**} & \multicolumn{2}{l}{**Extrapolated**} & \multicolumn{1}{l}{**Reference**} \\  & **detector** & **FEX** & **DAF** & **Time (\(\mu\)s) (total)** & \\ \hline MU & MUON & 220 &??? & 100 & [11] \\ MU/EM/TAU & CALO & 100 & 200 & 100 & [12, 13] \\ J & CALO &??? &??? & 100 & \\ ME & CALO &??? &??? & 100 & \\ MU & TRT & 700 & 2970 & 590 & [14, 15] \\ EM/TAU & TRT & 700 & 1552 & 310 & [14, 15] \\ scan & TRT & 680000 & 50000 & [16] \\ MU & SCT & 1500 & 650 & 500 & \\ EM/TAU & SCT & 1500 & 650 & 500 & \\ \(b\)-jet tag & SCT &??? &??? & 250000 & \\ \end{tabular}
\end{table}
Table 6: _Feature extraction algorithm (FEX), data formatting (DAF) and extrapolated total times. For times given as â€˜???â€™, no measurement was available and an estimate has been used to give the total time. Measured times are normalised to 100 MIPS, but the total time is extrapolated to 500 MIPS_minimal, listing only LVL1 trigger RoIs, or extended to include secondary RoIs flagged by LVL1. This distinction is shown in table 7.

Low luminosity menus have lower thresholds. The minimal menu is derived from the ATLAS TP [18] and the extended menu is taken from [19]. The full trigger menus are listed in appendix A. Note that the high luminosity minimal menu has been corrected with respect to the menu listed in citemodelling-inputs, with the effect of reducing the Jet RoI rate.

The menus are designed for a target rate of around 40 kHz input to LVL2. However, LVL2 is required to cope with 75 kHz design rate (scalable to 100 kHz), so a safety factor of two is allowed for cross section uncertainties.

Menus are distinct from the processing strategy adopted to deal with them.

### Selection

A selection strategy, while sometimes constrained by architecture choices, can be generally considered as architecture independent. The selection strategy is linked closely to the order of processing, which may be in parts parallel; this possibility is a feature of the architecture. If selection is made in a single decision, it is desirable but not essential to process in parallel. The optimum processing strategy is usually dictated by the architecture. Hence for architecture B, it is natural to have a single decision step with parallel processing of all RoIs. In the special case of the B physics trigger, sequential selection is implemented with parallel processing of the RoIs in each step, but the steps themselves are processed sequentially. In this context, processing refers to the time and resources taken to fetch RoI data, data transfers, i/o and algorithms. With architecture C, only completely sequential

\begin{table}
\begin{tabular}{l l l}  & **Low luminosity** & **High Luminosity** \\ \hline
**Trigger RoIs** & & \\ \(\mu\) & MU6, MU20 & MU6+MU6, MU20 \\ \(c/\gamma\) & EM15I+EM15I, EM20I, EM80 & EM20I+EM20I, EM30I \\ lepton & MU6+EM15I & MU6+EM20I \\ \(\tau\) & TAU80, TAU150 & â€” \\ jet & J100, J200, J50+J50+J50 & J150 \\ \(B_{T}\) & ME100, ME150 & ME100 \\
**Secondary RoIs** & & \\ \(\mu\) & â€” & MU6 \\ \(c/\gamma\) & EM7I, EM15I & EM10 \\ \(\tau\) & TAU40 & â€” \\ jet & J15 & J40 \\ \end{tabular}
\end{table}
Table 7: _The notation for LVL1 trigger objects comprises of a few letters indicating the object type (e.g. MU, J, EM), a number giving the threshold in GeV, and an optional T indicating that isolation is required._processing of RoIs is possible and hence sequential selection is the natural choice. However, all the RoI data for a given step can be requested in parallel and the RSIs will merge the ROB data fragments in parallel.

Secondary RoIs can be used with either selection model. They are implemented in the models by introducing extended trigger menus because the RoI rates change when secondary RoIs are added.

A further dimension to the selection strategy is the possibility of running more complex algorithms on LVL2 processors to reduce the bandwidth to the event filter (LVL3). These algorithms -- \(\mathit{E}_{T}\)  recalculation and \(b\)-jet tagging -- are much slower or make intensive use of the trigger system, so they can be run only on a small fraction of events. They are therefore considered only with sequential selection. For the purposes of this study it has been assumed that a \(b\)-jet tag requires all of the SCT data for that event to be read out.

Table 8 lists the combinations which were studied for this note. Implementations of the parallel and sequential selection strategies are given below, with further details in [5]. Table 9 gives the estimated LVL2 accept rate arising from the different options. Where these are unknown for high luminosity (indicated by "?") the corresponding low luminosity rate is used.

\begin{table}
\begin{tabular}{c l l l}
**Arch.** & **RoIs** & **Selection** & **Additional algorithms** \\ \hline
**B** & **trigger only** & **partially sequential** & **â€”** \\
**B** & trigger + non-trigger & partially sequential & **â€”** \\
**B\({}^{\prime}\)** & trigger + non-trigger & sequential steps & **â€”** \\
**B\({}^{\prime}\)** & trigger + non-trigger & sequential steps & \(\mathit{E}_{T}\) \\
**B\({}^{\prime}\)** & trigger + non-trigger & sequential steps & \(\mathit{E}_{T}\) + \(b\)-jet tag \\
**C** & trigger + non-trigger & partially sequential & **â€”** \\
**C** & **trigger + non-trigger & sequential steps & \(\mathit{E}_{T}\) \\
**C** & trigger + non-trigger & sequential steps & \(\mathit{E}_{T}\) + \(b\)-jet tag \\ \hline \end{tabular}
\end{table}
Table 8: _The combinations of trigger menus and selection strategy that have been modelled. The lines picked out in bold are the default ways to use each architecture._

\begin{table}
\begin{tabular}{l c c c}
**RoIs** & **Additional algorithms** & **luminosity** \\  & & **low** & **high** \\ \hline trigger only & â€” & 1.5 & 1.4 \\ trigger + non-trigger & â€” & 1.0 &? \\ trigger + non-trigger & \(\mathit{E}_{T}\) & 1.0 &? \\ trigger + non-trigger & \(\mathit{b}\)-jet tag & 0.1 &? \\ \hline \end{tabular}
\end{table}
Table 9: _LVL2 accept rates derived from the ATLAS technical proposal [18] and trigger menus [6]._

#### 4.5.1 Low luminosity

**TP' (for minimal menus or extended menus)**

This is the strategy outlined in the ATLAS Technical Proposal [18]. Most RoIs are processed in parallel, except muons. For an event in which there is a muon RoI, the following sequential processing and selection procedure should be followed, while at the same time all the other RoIs are analysed in parallel.

1. Confirm muon RoI in muon detector,
2. confirm muon RoI in calorimeter and tracking (in parallel),
3. full TRT scan,
4. analyse LVL2 RoIs generated by TRT scan.

Global processing is then done sequentially.

**'Sequential' (for extended menus)**

This sequence is copied from [6].

1. Confirm LVL1 trigger using calorimeter and muon data from trigger RoIs,
2. verify non-jet triggers in inner tracking,
3. full TRT scan for confirmed muon trigger,
4. verify trigger muon isolation in calorimeter,
5. analyse non trigger RoIs (requesting all data in a single step),
6. recalculate \(\not{E_{T}}\) if required by the LVL2 trigger menu,
7. \(b\)-jet tags if required by the LVL2 trigger menu4, Footnote 4: \(b\)-jet tags are done on events with at least one accepted lepton and one accepted jet of any threshold, or \(\geq\)4 accepted jets.
8. combine features for global selection criteria.

Note that verification of muon isolation (step 4) provides additional information about the muon RoI but doesn't give any further rejection before the final step.

#### 4.5.2 High luminosity

**'TP' (for minimal menus or extended menus)**

All RoIs are processed in parallel.

### Sequential' (for extended menus)

The sequential strategy of 4.5.1 without the TRT scan.

## 5 Results

### Baseline Technical Proposal trigger (architecture B), low luminosity

The architecture B described in section 3.2 and figure 1 is the baseline design for the ATLAS LVL2 trigger proposed in the Technical Proposal [18]. It has a local/global architecture and rejects events based on parallel processing and selection of trigger RoIs with simple algorithms only, i.e. secondary RoIs are not used and there is no \(b\)-jet tagging or missing energy re-calculation. The paper model has been used to calculate the system requirements and properties of this trigger, which are shown in the table 10.

### Bandwidth and computing

The total bandwidth inside this LVL2 system (local + global) is 1.3 GByte/s. The majority of this is RoI data in the local networks, in particular TRT data for the full scan and calorimeter data. For example, the local TRT network illustrated in figure 3 will have to switch data from 512 ROBs at 725 MByte/s to 375 processors. The message rate is 2.32 MHz and the average message size is 0.3 kByte. The global network is comparatively unchallenging in terms of bandwidth.

The total amount of computing required in the farms (local + global) is 536 processors (at 500 MIPS). Two thirds of this is attributed to the processing associated with the TRT scan.

### ROBs

The ROB input rates are related directly to the RoI rates in the trigger menu. The numbers include the RoI requests and the T2DR message from the supervisor to all the ROBs. This adds an extra 1% of the LVL1 trigger rate to the total rate of input to each ROB. The average ROB receives around 1-2 kHz of messages from LVL2 but the HCal has an average of 2.86 kHz and the TRT 4.87 kHz.

The ROB CPU occupancy reflects the request rate; the overhead for the ROB to receive a request and send out the data is the dominant load on the CPU. The HCal and the TRT suffer high CPU occupancies of 51% and 70% respectively, which are likely to result in bottlenecks in a dynamic system. When the overall rate safety factor of two is allowed, the TRT ROB CPU is overloaded. The high rate in the TRT ROBs is due to the full scan for the B-physics trigger. The high HCal load is due to the combination of the high EM/jet RoI frequency and the small number of HCal ROBs.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|l|} \hline  & \multicolumn{2}{|c|}{Calorimeter} & \multicolumn{2}{|c|}{Muon} & \multicolumn{2}{|c|}{Tracking} \\ \cline{2-7}  & **ECal** & **HCal** & **MDT** & **RPC** & **TRT** & **SCT** \\ \hline RoI distributor & & & & & & \\ output rate (kHz) & 575 & 137 & 91.6 & 27.9 & 2494 & 519 \\ output bandwidth (MByte/s) & 69.4 & 9.5 & 26.9 & 3.50 & 125.7 & 45.4 \\ \hline ROB request/clear & & & & & & \\ CPU occupancies (\%) & 32.3 & 51.0 & 16.3 & 26.0 & 70.0 & 35.2 \\ receive rate (kHz) /ROB & 1.33 & 2.86 & 0.477 & 1.27 & 4.87 & 2.03 \\ receive bandwidth (MByte/s) /ROB & 0.161 & 0.197 & 0.140 & 0.159 & 0.246 & 0.177 \\ data send bandwidth (MByte/s) /ROB & 0.570 & 2.33 & 0.0898 & 0.123 & 1.42 & 0.629 \\ \hline  & **Calorimeter** & **Muon** & **TRT** & **SCT** \\ \hline Local network bandwidth (MByte/s) & 358 & 20.0 & & 725 & 161 \\ \hline Local farm & & & & & \\ size (\# processors) & 42 & 5 & 375 & 92 \\ input rate (kHz) /FEX & 13.1 & 9.57 & 6.19 & 4.71 \\ input bandwidth (MByte/s) /FEX & 8.52 & 3.99 & 1.93 & 1.75 \\ output rate (kHz) /FEX & 1.03 & 1.68 & 0.247 & 1.19 \\ output bandwidth (MByte/s) /FEX & 0.154 & 0.253 & 0.0371 & 0.178 \\ output rate (kHz) total & 43.2 & 8.42 & 92.8 & 109 \\ output bandwidth (MByte/s) total & 6.47 & 1.26 & 13.92 & 16.4 \\ \hline  & & **Global** & & \\ \hline Global network bandwidth (MByte/s) & 39.3 & & & \\ \hline Global farm & & & & \\ size (\# processors) & 24 & & & \\ input rate (kHz) /CPU & 10.6 & & & \\ input bandwidth (Mbyte/s) /CPU & 1.58 & & & \\ output rate (kHz) /CPU & 2.01 & & & \\ output bandwidth (kHz) /CPU & 0.0531 & & & \\ \hline Supervisor & & & & \\ processing required (\# processors) & 20 & & & \\ input rate from LVL1 (kHz) & 33.5 & & & \\ input rate from LVL2 (kHz) & 48.1 & & & \\ input bandwidth from LVL2 (MByte/s) & 1.274 & & & \\ \hline RoI distributor & & & & \\ input rate (kHz) & 254 & & & \\ input bandwidth (MByte/s) & 6.22 & & & \\ \hline Average decision latency (ms) & 5.8 & & & \\ \hline \end{tabular}
\end{table}
Table 10: Properties and requirements of the TP low luminosity LVL2 trigger.

The ROBs also have to send data to LVL3, and this is included in the ROB CPU occupancies given. The rates depend on the type of trigger as shown in table 9. In this case the extra load is 60 \(\mu s\) at 1.5 kHz per ROB, i.e. an extra 9% CPU occupancy.

### Supervisor

The main task of the supervisor is i/o and the overheads associated with this are what determine the farm size of 20 processors. The internal architecture of the supervisor has not been taken into account so in this respect the model is over simplistic. It certainly cannot indicate potential internal bottlenecks. Messages from the LVL2 system outnumber those from LVL1 because of the requests for additional RoIs by the sequential muon and TRT scan procedure.

### RoI distributor

The input rate of ROIR messages, 254 kHz, is very high but comparable to other networks in the system. This could be reduced if the messages were grouped together. The model assumed that the RoI distributor would be capable of fanning out the ROID messages to the required ROBs, which is why the total output rate (3.8 MHz) is much higher than the input rate. Again it is the TRT scan (2.5 MHz) that makes the dominant contribution to this. Since this is essentially a broadcast to all TRT ROBs, there may be a more efficient way of doing it.

### Decision Latency

The average decision latency is 5.8 ms. Remember that this is defined as the elapsed time from the RoI information being received from LVL1 by the supervisor until the supervisor receives and processes the final accept/reject decision on the event; it is estimated on the basis of average numbers and without any effects of queueing or congestion taken into account.

The average time an event remains in the ROBs (the buffer latency) is different from the decision latency for a number of reasons. The data may arrive at the ROBs at a different time than the LVL1 RoI information arrives at the supervisor. After the supervisor has received a decision on the event from the global processor, it adds it to a list which is sent to all the ROBs once 100 decisions have been accumulated. This step corresponds to an additional \(1-3\) ms at most, depending on the rate of events from LVL1. Events which are to be accepted may then be pushed to LVL3 straight away, or they could be held until requested.

The trigger menu items which contain a muon RoI take the longest time in the LVL2 trigger, because half of them incur the large algorithm time of the TRT scan. Such events cause a significant increase the average decision latency. The average for these events alone is about 18 ms.

The average number of events held simultaneously in a global processor can be calculated from the time that an event occupies the processor and the event rate. The time spent in the global processor is not easily calculated for architecture B in a static model. If sequential processing is ignored and all the FEXD messages arrive within a short time interval, the average must be one event per processor. This would be expected to increase with sequential processing and network congestion. Both of these will introduce periods of idle time for an event in the global processor.

### Baseline Technical Proposal trigger (architecture B), high luminosity

The main difference at high luminosity is the absence of the TRT scan. This compensates for the much higher EM RoI rates in the high luminosity trigger menu. The results are now dominated by EM RoIs and jets. Full baseline results are given in table 11.

The total bandwidth of 0.9 GByte/s is slightly less than at low luminosity. Although there is no longer the data from the TRT scan, transfer of calorimeter data through the local network now uses \(\sim\)0.5 GByte/s. The sum of the farm sizes drops significantly to 141 processors without the CPU intensive TRT scan.

The ROB request/clear rates are roughly the same as at low luminosity, with the obvious exception that the rates to the tracking detector ROBs are reduced. The highest CPU occupancy is now found in the HCal ROBs at 50%. Since this is an average over time and position it is likely to act as a bottleneck in a dynamic model which uses distributions (or indeed a real trigger system). When the safety factor of two is allowed, the average rate and occupancy exceed the system's capabilities.

At high luminosity the RoI distributor has to deal with fewer RoIs so the input and output rates are down to 165 kHz and 1.6 MHz respectively. The supervisor farm size has shrunk by a third for the same reason.

The average decision latency is 3.7 ms. This is slightly lower than at low luminosity, the main reason being that there is no TRT scan.

\begin{table}
\begin{tabular}{|l|l l|l l|l|l|} \hline  & \multicolumn{2}{|c|}{Calorimeter} & \multicolumn{2}{|c|}{Muon} & \multicolumn{2}{|c|}{Tracking} \\ \cline{2-7}  & **ECal** & **HCal** & **MDT** & **RPC** & **TRT** & **SCT** \\ \hline RoI distributor & & & & & & \\ output rate (kHz) & 525 & 137 & 97.0 & 30.1 & 510 & 254 \\ output bandwidth (MByte/s) & 70.7 & 9.7 & 28.1 & 3.68 & 81.0 & 40.5 \\ \hline ROB request/clear & & & & & & \\ CPU occupancies (\%) & 29.6 & 50.3 & 16.1 & 26.7 & 22.1 & 22.1 \\ receive rate (kHz)/ROB & 1.22 & 2.85 & 0.505 & 1.37 & 0.995 & 0.993 \\ receive bandwidth (MByte/s) /ROB & 0.164 & 0.203 & 0.147 & 0.167 & 0.158 & 0.158 \\ data send bandwidth (MByte/s) /ROB & 0.795 & 2.31 & 0.0981 & 0.135 & 0.505 & 0.664 \\ \hline  & **Calorimeter** & **Muon** & **TRT** & **SCT** \\ \hline Local network bandwidth (MByte/s) & 454 & 21.8 & & 258 & 170 \\ \hline Local farm & & & & & \\ size (\# processors) & 42 & 5 & 41 & 37 \\ input rate (kHz) /FEX & 11.8 & 10.5 & 8.06 & 4.45 \\ input bandwidth (MByte/s) /FEX & 10.8 & 4.36 & 6.30 & 4.59 \\ output rate (kHz) /FEX & 1.15 & 1.84 & 0.991 & 1.10 \\ output bandwidth (MByte/s) /FEX & 0.172 & 0.276 & 0.149 & 0.165 \\ output rate (kHz) total & 48.2 & 9.20 & 40.6 & 40.6 \\ output bandwidth (MByte/s) total & 7.23 & 1.38 & 6.10 & 6.10 \\ \hline  & & **Global** & & \\ \hline Global network bandwidth (MByte/s) & 21.5 & & & \\ \hline Global farm & & & & \\ size (\# processors) & 16 & & & \\ input rate (kHz) /CPU & 8.7 & & & \\ input bandwidth (Mbyte/s) /CPU & 1.30 & & & \\ output rate (kHz) /CPU & 2.19 & & & \\ output bandwidth (kHz) /CPU & 0.0438 & & & \\ \hline Supervisor & & & & \\ processing required (\# processors) & 13 & & & \\ input rate from LVL1 (kHz) & 35.0 & & & \\ input rate from LVL2 (kHz) & 35.0 & & & \\ input bandwidth from LVL2 (MByte/s) & 0.700 & & & \\ \hline RoI distributor & & & & \\ input rate (kHz) & 165 & & & \\ input bandwidth (MByte/s) & 4.08 & & & \\ \hline Average decision latency (ms) & 3.7 & & & \\ \hline \end{tabular}
\end{table}
Table 11: Properties and requirements of the TP high luminosity LVL2 trigger.

### Architecture C, baseline low and high luminosity triggers

While looking at these results for architecture C, it is very important to bear in mind that it is not modelled with the same strategy as architecture B. The natural strategy for C is to use secondary RoIs and sequential selection, which cannot be compared directly to the trigger RoIs and single decision strategy used with B above. The aim in showing these two configurations for comparison is that the architectures be judged with the strategy that suits them best. A full comparison of the different selection strategies for both architectures follows this section.

Architecture C is distinguished by a significantly longer average decision latency at low luminosity. This is because of the TRT scan which benefits from parallel processing in architecture B but not in C. At high luminosity the latency is still estimated to be around twice that in B.

Total bandwidth is very similar between B and C with the same data (TRT at low and calorimeter at high luminosity) dominating. In fact, architecture C actually requires about a third less bandwidth than B at high luminosity. This is an advantage of sequential selection significantly reducing the frequency at which tracking data is requested. The control messages flowing in the opposite direction through the network are around 10% of the total required bandwidth.

The number of processors required by architecture C (farm + SFIs) is about the same as B. The difference is that all the algorithm processing for C is concentrated in the single farm and the merging and i/o overheads in the SFIs. However, C requires 382 extra processors to act as RSIs.

Sequential processing reduces access to the tracking data for primary RoIs and access to all data for secondary RoIs. Accessing the secondary RoI data adds to the rate in the ROBs, and since most secondary RoIs in the menu are jets this is mainly felt in the calorimeter. These observations explain the differences between the ROB rates for B and C. The resulting CPU occupancies reveal the overloading in the same places as B -- the HCal at low and high luminosity and the TRT at low luminosity. The worst bottleneck is in the HCal ROBs at low luminosity which are occupied at 74%.

Since the RSIs concentrate the ROB data, usually collecting from 4 ROBs, it is not surprising to find that they too are overloaded. This will most likely be avoided by a more suitable i/o method for ROB to RSI communication, for example using polling rather than the interrupt driven i/o assumed in this model. However, the problem remains most challenging.

The disadvantages of the RSI/SFI strategy are the increased computing resource requirements and a very challenging rate in the RSIs. The point of these devices is to merge messages and hence reduce message frequency in the switch. This would be an advantage for some switch technologies. For example, B and C need about the same total network bandwidth at low luminosity, but the total message rates in the networks are 3.6 MHz for B and 1.5 MHz for C. The use of concentrators also reduces the number of switch ports required. These factors will have to be optimised for specific technologies taking cost into

\begin{table}
\begin{tabular}{|l|l l|l l|l|l|l|} \hline  & \multicolumn{2}{|c|}{Calorimeter} & \multicolumn{2}{|c|}{Muon} & \multicolumn{2}{|c|}{Tracking} \\ \cline{2-7}  & **ECal** & **HCal** & **MDT** & **RPC** & **TRT** & **SCT** \\ \hline \multicolumn{7}{|l|}{_ROB request/send data/clear_} \\ \multicolumn{7}{|l|}{CPU occupancies (\%)} & 45.3 & 74.2 & 13.8 & 23.6 & 65.4 & 30.0 \\ \multicolumn{7}{|l|}{receive rate (kHz) /ROB} & 2.24 & 4.67 & 0.511 & 1.31 & 4.73 & 1.84 \\ \multicolumn{7}{|l|}{receive bandwidth (MByte/s) /ROB} & 0.195 & 0.253 & 0.153 & 0.172 & 0.254 & 0.185 \\ \multicolumn{7}{|l|}{data send bandwidth (MByte/s) /ROB} & 0.632 & 3.97 & 0.0911 & 0.125 & 1.36 & 0.546 \\ \hline \multicolumn{7}{|l|}{No. of RSIs} & 108 & 12 & 64 & 6 & 128 & 64 \\ \multicolumn{7}{|l|}{RSI CPU occupancy (\%)} & 122 & 314 & 14.1 & 66.1 & 239 & 128 \\ \multicolumn{7}{|l|}{RSI data (down) per RSI} \\ \multicolumn{7}{|l|}{input rate (kHz)} & 7.48 & 17.2 & 0.432 & 3.79 & 17.5 & 5.88 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 3.27 & 10.1 & 0.187 & 1.80 & 4.42 & 5.58 \\ \multicolumn{7}{|l|}{throughput (MByte/s)} & 2.53 & 15.9 & 0.273 & 0.500 & 5.45 & 2.19 \\ \multicolumn{7}{|l|}{RSI requests (up) per RSI} \\ \multicolumn{7}{|l|}{input rate (kHz)} & 3.63 & 10.4 & 0.554 & 2.17 & 4.79 & 5.94 \\ \multicolumn{7}{|l|}{input bandwidth (MByte/s)} & 0.228 & 0.392 & 1.54 & 0.193 & 0.256 & 0.284 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 8.95 & 18.7 & 1.53 & 5.26 & 18.92 & 7.34 \\ \multicolumn{7}{|l|}{output bandwidth (MByte/s)} & 0.778 & 1.01 & 0.460 & 0.690 & 1.02 & 0.740 \\ \hline \multicolumn{7}{|l|}{**Network \& farm**} \\ \multicolumn{7}{|l|}{Network bandwidth control (MByte/s)} & 93.2 & \multicolumn{7}{|c|}{} \\ \multicolumn{7}{|l|}{Network bandwidth data (MByte/s)} & 1320 & \\ \hline \multicolumn{7}{|l|}{\# SFIs required at 100\% occupancy} & 195 \\ \multicolumn{7}{|l|}{SFI data (down) per SFI: RoI fragments + ROIRSFs} \\ \multicolumn{7}{|l|}{input rate (kHz)} & 7.50 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 1.14 \\ \multicolumn{7}{|l|}{throughput (MByte/s)} & 6.81 \\ \multicolumn{7}{|l|}{SFI requests (up) per SFI: RoI requests + T2 decisions} \\ \multicolumn{7}{|l|}{input rate (kHz)} & 1.14 \\ \multicolumn{7}{|l|}{input bandwidth (MByte/s)} & 0.0266 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 7.50 \\ \multicolumn{7}{|l|}{output bandwidth (MByte/s)} & 0.179 \\ \hline \multicolumn{7}{|l|}{Single farm} \\ \multicolumn{7}{|l|}{size (\# processors)} & 333 \\ \multicolumn{7}{|l|}{input rate (kHz) /CPU} & 0.665 \\ \multicolumn{7}{|l|}{input bandwidth (MByte/s) /CPU} & 3.97 \\ \multicolumn{7}{|l|}{output rate (kHz) /CPU} & 0.665 \\ \multicolumn{7}{|l|}{output bandwidth (MByte/s) /CPU} & 0.0155 \\ \multicolumn{7}{|l|}{\hline \multicolumn{7}{|l|}{Supervisor} \\ \multicolumn{7}{|l|}{\# processors required} & 14 \\ \multicolumn{7}{|l|}{input rate (kHz) from LVL1} & 36.7 \\ \multicolumn{7}{|l|}{input rate (kHz) from LVL2} & 36.7 \\ \multicolumn{7}{|l|}{input bandwidth (MByte/s) from LVL2} & 0.734 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 177 \\ \multicolumn{7}{|l|}{output bandwidth (MByte/s)} & 58.4 \\ \multicolumn{7}{|l|}{\hline Average decision latency (ms)} & 27 \\ \hline \end{tabular}
\end{table}
Table 12: Properties and requirements of the architecture C low luminosity LVL2 trigger.

\begin{table}
\begin{tabular}{|l|l l|l l|l|l|l|} \hline  & \multicolumn{2}{|c|}{Calorimeter} & \multicolumn{2}{|c|}{Muon} & \multicolumn{2}{|c|}{Tracking} \\ \cline{2-7}  & **ECal** & **HCal** & **MDT** & **RPC** & **TRT** & **SCT** \\ \hline \multicolumn{7}{|l|}{_ROB request/send data/clear_} \\ \multicolumn{7}{|l|}{CPU occupancies (\%)} & 34.3 & 58.5 & 14.5 & 25.0 & 15.1 & 14.6 \\ \multicolumn{7}{|l|}{receive rate (kHz) /ROB} & 1.63 & 3.58 & 0.555 & 1.42 & 0.610 & 0.569 \\ \multicolumn{7}{|l|}{receive bandwidth (MByte/s) /ROB} & 0.193 & 0.240 & 0.167 & 0.188 & 0.168 & 0.167 \\ \multicolumn{7}{|l|}{data send bandwidth (MByte/s) /ROB} & 0.751 & 2.93 & 0.0981 & 0.135 & 0.165 & 0.174 \\ \hline \multicolumn{7}{|l|}{No. of RSIs} & 108 & 12 & 64 & 6 & 128 & 64 \\ \multicolumn{7}{|l|}{RSI CPU occupancy (\%)} & 87.2 & 239 & 15.3 & 71.3 & 22.2 & 24.5 \\ \multicolumn{7}{|l|}{RSI data (down) per RSI} \\ \multicolumn{7}{|l|}{input rate (kHz)} & 4.90 & 12.7 & 0.466 & 4.08 & 0.841 & 0.676 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 2.22 & 7.84 & 0.201 & 1.94 & 0.251 & 0.635 \\ \multicolumn{7}{|l|}{throughput (MByte/s)} & 3.00 & 11.7 & 0.294 & 0.539 & 0.658 & 0.698 \\ \multicolumn{7}{|l|}{RSI requests (up) per RSI} \\ \multicolumn{7}{|l|}{input rate (kHz)} & 2.62 & 8.24 & 0.601 & 2.34 & 0.651 & 1.03 \\ \multicolumn{7}{|l|}{input bandwidth (MByte/s)} & 0.216 & 0.351 & 0.168 & 0.210 & 0.169 & 0.178 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 6.50 & 14.3 & 1.67 & 5.68 & 2.44 & 2.28 \\ \multicolumn{7}{|l|}{output bandwidth (MByte/s)} & 0.770 & 0.958 & 0.501 & 0.751 & 0.673 & 0.669 \\ \hline \multicolumn{7}{|l|}{**Network & \& farm**} \\ \multicolumn{7}{|l|}{Network bandwidth control (MByte/s)} & 74.8 \\ \multicolumn{7}{|l|}{Network bandwidth data (MByte/s)} & 616 \\ \hline \multicolumn{7}{|l|}{\# SFIs required at 100\% occupancy} & 72 \\ \multicolumn{7}{|l|}{SFI data (down) per SFI: RoI fragments + ROIRSFs} \\ \multicolumn{7}{|l|}{input rate (kHz)} & 6.55 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 1.73 \\ \multicolumn{7}{|l|}{throughput (MByte/s)} & 8.60 \\ \multicolumn{7}{|l|}{SFI requests (up) per SFI: RoI requests + T2 decisions} \\ \multicolumn{7}{|l|}{input rate (kHz)} & 1.73 \\ \multicolumn{7}{|l|}{input bandwidth (MByte/s)} & 0.0394 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 6.55 \\ \multicolumn{7}{|l|}{output bandwidth (MByte/s)} & 0.155 \\ \hline \multicolumn{7}{|l|}{Single farm} \\ \multicolumn{7}{|l|}{size (\# processors)} & 42 \\ \multicolumn{7}{|l|}{input rate (kHz) /CPU} & 2.96 \\ \multicolumn{7}{|l|}{input bandwidth (MByte/s) /CPU} & 14.7 \\ \multicolumn{7}{|l|}{output rate (kHz) /CPU} & 2.96 \\ \multicolumn{7}{|l|}{output bandwidth (MByte/s) /CPU} & 0.0672 \\ \hline \multicolumn{7}{|l|}{Supervisor} \\ \multicolumn{7}{|l|}{\# processors required} & 15 \\ \multicolumn{7}{|l|}{input rate (kHz) from LVL1} & 40.0 \\ \multicolumn{7}{|l|}{input rate (kHz) from LVL2} & 40.0 \\ \multicolumn{7}{|l|}{input bandwidth (MByte/s) from LVL2} & 0.800 \\ \multicolumn{7}{|l|}{output rate (kHz)} & 193 \\ \multicolumn{7}{|l|}{output bandwidth (MByte/s)} & 63.7 \\ \hline \multicolumn{7}{|l|}{Average decision latency (ms)} & 8.3 \\ \multicolumn{7}{|l|}{} \\ \end{tabular}
\end{table}
Table 13: Properties and requirements of the architecture C high luminosity LVL2 trigger.

account.

Note that the data bandwidth in and out of the RSIs and SFIs are not strictly the same as the merged data will have fewer headers, but it turns out that these form a negligible part of the data volume, both numbers are given as a single throughput bandwidth.

At high luminosity the model calculations show that only 42 processors are needed to meet the CPU requirements of the single farm, while 72 SFIs are required to perform the necessary i/o and data merging. This arises because the algorithms require less CPU time than i/o and data merging. It is clearly an impossible configuration. For modelling/emulation one could assign 72 processors in the farm. These would be running at about 50% occupancy with reduced rate and bandwidth into each processor.

The length of time that the GP is occupied by an event is essentially the same as the decision latency for architecture C, which means there are on average 3 events simultaneously held in a processor at low luminosity and 9 at high luminosity.

### Trigger strategies

Several variations in trigger strategy have been implemented in the paper model and their impact on the trigger requirements and properties investigated. The results are presented by focusing on several key indicators:

1. ROB/RSI CPU occupancies,
2. farm sizes,Figure 7: ROB request/clear rates at low luminosity for various trigger strategies. They are independent of architecture. Figure 8: ROB request/clear rates at high luminosity for various trigger strategies. They are independent of architecture.

Figure 11: Total no. of LVL2 processors required and breakdown by farms, at low luminosity for various trigger strategies for architecture B.

Figure 14: Total no. of LVL2 processors required and breakdown by farms, at high luminosity for various trigger strategies for architecture C.

Figure 12: Total no. of LVL2 processors required and breakdown by farms, at high luminosity for various trigger strategies for architecture B.

Figure 13: Total no. of LVL2 processors required and breakdown by farms, at low luminosity for various trigger strategies for architecture C.

Figure 17: Average decision latencies of architectures B and C at high and low luminosities for the studied trigger strategies. The first strategy is not applicable to Architecture C.

3. bandwidths,
4. average decision latency.

They are repeated for low and high luminosity and both architectures B and C.

Strategy variants (shown in table 8) are:

1. trigger RoIs only or trigger and secondary RoIs
2. single decision step, partially sequential or several sequential selection steps
3. simple algorithms only or additional complex algorithms (\(b\)-jet tag \(\mbox{$E\!\!\!/_{T}$}\))

The figures 5 - 17 show the strategies from left to right in the order they are given in table 8. The 'partially sequential' strategy is the default for architecture B at low luminosity, where all the RoIs are handled in a single step but the B-physics trigger is done with sequential steps.

The ROB input rates (figures 7, 8) and occupancies (figures 5, 6) are independent of architecture while the RSI rates (figures 9, 10) only apply to architecture C.

As we have already seen, it is the calorimeter ROB/RSI occupancies which are highest. This problem is exacerbated by the addition of data requests for secondary RoIs, but sequential selection can compensate for this. Selection strategy has little effect on the Muon and SCT ROBs. Sequential selection can be seen as a partial solution to the rate problem that causes the calorimeter ROB bottleneck. However, the high TRT ROB/RSI occupancy due to the low luminosity scan is still too high, despite the sequential selection.

Figures 11, 12, 13, 14 show the farm sizes as a function of selection strategy. At low luminosity the number of LVL2 processors is dominated by the TRT scan, so the strategy does not have a big effect on the overall number. At high luminosity, sequential selection reduces the no. of processors by about half, because the most time consuming algorithms are now the SCT/TRT feature extraction and they run at a reduced rate. Secondary RoIs require approximately 20% additional computing, most of which is for calorimeter feature extraction.

The bandwidths in figures 15 and 16 have the same features as the farm sizes. Bandwidth, like ROB rates and occupancy, is independent of architecture. The only exception to this is an additional 80-100 MByte/s of control messages which go through the network in architecture C. The first column in the figures ('trig RoIs') only applies to architecture B.

The figures discussed above also showed the effect of adding more complex LVL2 algorithms (\(\mbox{$E\!\!\!/_{T}$}\) and \(b\)-jet tagging). This is summarised in tables 14 and 15. The \(\mbox{$E\!\!\!/_{T}$}\) recalculation requires full readout of the calorimeter at 2 kHz and the associated data merging and algorithms times for this process. The result is an additional 127 MByte/s bandwidth and \(30-60\) CPUs at low luminosity, and around half that at high luminosity; this just comes down to differences in the low and high luminosity trigger menus. Although the increase in processors required is consistently less for architecture C, there is a dramatic increase in calorimeter RSI occupancy of 120% (50%) at low (high) luminosity. The calorimeter ROBssuffer a lot of extra CPU load, with increases of 46% (ECal) and 39% (HCal) at low luminosity. The average decision latency (not shown in the table) increases by \(\sim\)1 ms for architecture C and \(\sim\)0.1 ms for architecture B.

The \(b\)-jet tag requires full SCT readout at 3.70 kHz (low lumi) or 1.30 kHz (high lumi). This results in major additional ROB/RSI requests and bandwidth and needs a lot of computing time. Hence the no. of processors is increased by about 1000 at low luminosity and 400 at high luminosity. The average decision latency increases dramatically for architecture C, but B is able to take advantage of its parallel processing capability so that it only goes up by the order of a ms.

Even with the same sequential decision process, architecture C has a longer decision latency than B (figure 17). As explained in the previous section, at low luminosity this is attributable to the parallel processing of the TRT scan in architecture B. This is not possible for C because a single processor is assigned for all the processing of an event. Even with the same sequential decision process, C still takes about twice as long as B. Again, the difference is due to the possibility of parallel processing with architecture B. When there is more than one RoI to process between decision steps, B can run the algorithms in parallel in the local farms and reach the decision more quickly.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline
**Arch.** & **Lumi.** & \multicolumn{4}{c|}{**Additional resources**} \\ \cline{3-6}  & & **Bandwidth** & **SCT ROB CPU occ.** & **CPUs** & **Latency** \\ \hline Bâ€™ & low & 353 MByte/s & 40\% & 1090 & 2.3 ms \\ C & low & 353 MByte/s & 40\% & 1040 & 30 ms \\ Bâ€™ & high & 344 MByte/s & 11\% & 388 & 0.8 ms \\ C & high & 344 MByte/s & 11\% & 370 & 11 ms \\ \hline \end{tabular}
\end{table}
Table 15: _The effect of running a \(b\)-tag algorithm at LVL2 on the SCT subsystem. These results are shown for a model using trigger and non-trigger RoIs with sequential decision steps_.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline
**Arch.** & **Lumi.** & \multicolumn{4}{c|}{**Additional resources**} & **Additional CPU occupancy** \\ \cline{3-6}  & & **Bandwidth** & **Farm CPUs** & **ECal ROB** & **HCal ROB** \\ \hline Bâ€™ & low & 127 MB/s & \(\sim\)63 & 46\% & 39\% \\ C & low & 127 MB/s & \(\sim\)32 & 46\% & 39\% \\ Bâ€™ & high & 55 MB/s & \(\sim\)27 & 20\% & 17\% \\ C & high & 55 MB/s & \(\sim\)14 & 20\% & 17\% \\ \hline \end{tabular}
\end{table}
Table 14: _The effect of recalculation of \(\not{B}_{T}\) at LVL2 on the calorimeter subsystem. These results are shown for a model using trigger and non-trigger RoIs with sequential decision steps_.

### Technology parameters

The generic technology parameterisations used in the model can be adjusted to reflect different technologies and different extrapolations into the future. The main aim here is to identify which parameters the trigger requirements and properties depend on critically.

#### 5.5.1 Processor i/o

The CPU overhead per message received or sent accounts for a large fraction of the total CPU load. This has been varied from the nominal value of 50 \(\mu\)s to show how the results depend on this parameter. It was found that at 50 \(\mu\)s the overheads account for about half the computing resources and decision latency.

This overhead is a product of two model parameters, \(\alpha\) and \(T_{0}\). \(T_{0}\) alone affects the latency and \(\alpha\times T_{0}\) affects the CPU load. To study the CPU overhead alone, \(T_{0}\) was fixed to 100 and \(\alpha\) set to 0, 25, 50, 75, 100. The occupancy of the ROBs in architectures B and C are shown as a function of overhead (\(\alpha T_{0}\)) in figures 19 and 20. It is seen that some of the ROB occupancies and depend critically on this parameter. The TRT and HCal are most sensitive to it.

To study the contribution of \(T_{0}\) to the decision latency, \(\alpha\) was set to 0. For the baseline model at low luminosity the decision latency drops from 5.8 to 2.4 ms when \(T_{0}\) is changed from 100 to 0 \(\mu\)s.

The occupancy of the RSIs of architecture C at low and high luminosity are shown as a

Figure 18: No. of LVL2 processors required as a function of i/o overhead, assuming 100% CPU utilisation. The nominal overhead is 50 \(\mu\)s.

Figure 21: RSI CPU occupancies at low luminosity (arch. C, secondary RoIs, sequential selection) as a function of the i/o overhead.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline
**i/o overhead** (\(\mu\)s) & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 \\ \hline SUPER & 2 & 11 & 20 & 28 & 36 \\ CAL & 12 & 27 & 42 & 57 & 71 \\ MU & 1 & 3 & 5 & 6 & 7 \\ TRT & 254 & 314 & 375 & 434 & 496 \\ SCT & 64 & 77 & 92 & 105 & 118 \\ GLOBAL & 8 & 16 & 24 & 31 & 38 \\ \hline _total_ & 341 & 447 & 554 & 660 & 766 \\ \hline \end{tabular}
\end{table}

_Low Luminosity, architecture B, trigger RoIs only, parallel RoI processing, partially sequential._

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline
**i/o overhead** (\(\mu\)s) & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 \\ \hline SUPER & 2 & 11 & 20 & 28 & 36 \\ CAL & 12 & 27 & 42 & 57 & 71 \\ MU & 1 & 3 & 5 & 6 & 7 \\ TRT & 254 & 314 & 375 & 434 & 496 \\ SCT & 64 & 77 & 92 & 105 & 118 \\ GLOBAL & 8 & 16 & 24 & 31 & 38 \\ \hline _total_ & 341 & 447 & 554 & 660 & 766 \\ \hline \end{tabular}
\end{table}
Table 16: Variation of farm sizes with CPU i/o overhead for the different architectures and trigger strategies.

function of overhead (\(\alpha T_{0}\)) in figures 21 and 22. At low luminosity the occupancy of all RSIs except those of the muon system depend critically on this parameter. They become heavily overloaded even with very small overheads. This is because as concentrators they see about four times the rate in the ROBs, some of which have also been seen to be overloaded at high overheads. As with the ROBs, the biggest problems are with the HCal, and the TRT due to the full scan. At high luminosity the dependence on the i/o overhead is not so steep. Only the ECal and HCal are can really be described as critical.

The total no. of processors required for the LVL2 trigger is shown as a function of the i/o overhead in figure 18. Table 16 gives more detailed information. Just to emphasise again how critical this parameter is, the plot shows that 1 \(\mu\)s longer i/o time requires approximately four more LVL2 processors, in the baseline LVL2 trigger at low luminosity.

#### 5.5.2 Link bandwidth

Links with speed 10 MByte/s are assumed in the models. To investigate the sensitivity of the results to this parameter, the modelling was repeated with 1 - 30 MByte/s links. This parameter only affects the decision latency.

The variation of the average decision latency of the baseline LVL2 trigger for architectures B and C was calculated as a function of the link speed, and the results are shown in figures 23, 24. They show that in the studied range the link bandwidth does not have a critical effect on the decision latency except at very low values. It is important to remember that the decision latency is estimated without taking queueing or network congestion into account.

Although the figures show link speeds as low as 1 Mbyte/s, the bandwidth requirements of the trigger exceed that in places. In architecture B the input links to the feature extraction processors will have to carry up to 10.8 MByte/s. In architecture C the concentration of data in the RSI and SFI means that a maximum of 16 MByte/s bandwidth is required for some links.

Another interpretation of these results is that if congestion reduced the effective network bandwidth to less than about 5 MByte/s, the latency would increase significantly.

#### 5.5.3 Processor MIPS

It is assumed that commodity processors running at 500 MIPS will be available and cost effective at the time when the ATLAS LVL2 trigger hardware is purchased. This is only one possible projection, and it may be quite conservative.

The processor speed was varied from 250 MIPS to 4 GIPS to show how sensitive the results are to this parameter. The processing speed was only allowed to affect the algorithm time; the i/o overhead and data merging times were left the same. In reality these would also be reduced, but it is not obvious how they would scale with the internal CPU speed.

Results for the baseline architecture B trigger are shown in figures 25 and 26: lo Figure 23: Average decision latency of architecture B as a function of link speed. The nominal speed is 10 MByte/s. Figure 24: Average decision latency of architecture C as a function of link speed. The nominal speed is 10 MByte/s.

cal/global/total farm sizes and average decision latency vs. the processor MIPS rating.

At 500 MIPS, the systems are in general not limited critically by CPU power compared to other factors such as i/o overheads. Although today's commodity processors (250 MIPS) would be required in vast numbers, even a conservative estimate of the possible improvement over the next five years would make CPU power a less important issue. There is little to be gained from using processors faster than 1000 MIPS when the i/o overhead does not decrease for increasing CPU power. The only thing that would really benefit from much faster processing is the TRT scan algorithm, so this would be the place to target any resources for especially fast processors.

At high luminosity there is no TRT scan, so the algorithm times are less significant and the dependence on CPU power is expected to be less important.

#### 5.5.4 FPGAs

FPGAs can be used for fast local preprocessing of data. In the 'hybrid' architecture described in [20], data requested from the ROBs of the tracking detectors is pushed directly to the global network through Enable++ processors. This idea could be used to speed up suitable parts of the slow tracking algorithms: the TRT scan and \(b\)-jet tagging. The effect of this is similar to the increased processing power considered above.

#### 5.5.5 Interrupts vs. polling

So far all the models have assumed that i/o is handled by interrupts. In such a high frequency environment with relatively short algorithm times, polling may be a more appropriate method. In this case, input to processors is either handled completely by an i/o coprocessor or by external DMA. The processor just runs a loop to take the next item from its input queue. Each item is dealt with in turn, and messages are sent out as required, then the next item is taken from the buffer. If there are a number possible processes, they can be done in turn or driven by polling flags that have been set externally.

In the simplified context of the paper model, the effect of handling i/o by polling is equivalent to a shorter i/o time - say \(5-10\ \mu\)s. The effect of this on occupancies and farm sizes can be seen in figures 18, 19, 20. What the paper model cannot show is the effect that polling has on the queues and decision latency. This will be investigated by computer modelling.

Conclusions

Two paper model spreadsheets have been made which allow quick investigation of the properties and requirements of the ATLAS LVL2 trigger. They are now available as tools to the trigger community. This note has presented a selection of the results obtained with these models.

The calorimeter ROBs have been identified as potential bottlenecks in the system. In many configurations, the HCal ROBs are found to be overloaded, particularly at low luminosity. Solutions to this problem are being sought in several different ways. The first is that most of the HCal CPU occupancy is i/o overhead time, so methods of fast communication using polling rather than interrupts are being studied. It has been assumed that 'indexing' (the bookkeeping associated with each event fragment received) is done completely in hardware, while for present Rob-in designs there is a certain amount of CPU processing time needed for this task for each event fragment arriving, so although context switch times may be shorter than assumed the indexing task makes the situation worse. Remapping of the HCal ROBS is also being investigated, to reduce the frequency of ROB requests. A change in the size of EM RoIs could also help here. The third approach is to change the trigger menus and/or use sequential processing to reduce the request rate of HCal ROB data.

The processor i/o overhead has been identified as a critical parameter of the trigger. This implies that the network interface technology will be a critical part of the trigger system. It has been shown that the parameters assumed for the model, including a 50 \(\mu\)s i/o overhead, result in half the computing power being used for i/o.

The link speed is not a critical factor in the average decision latency. This is virtually independent of the link speed above the minimum of \(\sim\)10 MByte/s required in architecture B and \(\sim\)20 MByte/s in architecture C.

The presence of several compute-intensive algorithms mean that the farm size and latency depend on the processor speed. For the TRT scan, this is a critical parameter. However, above 1000 MIPs there is little left to gain overall from further decreases in algorithm execution times. The variation in the average decision latency with processor speed is within an acceptable range. The cost of technology must be weighed up against the desirability of smaller farms.

At low luminosity, the no. of processors required are mostly for the computing involved in the TRT scan algorithm and the i/o overhead. ROB and RSI processor occupancy also depends strongly on the i/o overhead.

At low luminosity the LVL2 trigger will be a system of 500-700 processors linked by a 1300-1800 MByte/s network, independent of architecture. High luminosity requires a system of 100-250 processors linked by a 600-1300 MByte/s network. These systems can be partitioned according to the different architectures.

The strength of architecture B lies in the low average latency associated with parallel processing, which shows little change with respect to processing strategy. The advantages of partitioning the networks will be reflected in the traffic patterns which cannot be studied with paper models. A disadvantage of B is that sequential selection involves the supervisor for each step, which increases the complexity and rate for what is already one of the most challenging parts of the system.

Architecture C handles sequential selection very naturally and keeps the function of the supervisor as simple as possible. However, it has a longer decision latency than B because it cannot do parallel processing. The RSIs used in this model of architecture C suffer even more than parts of architecture B from the high rates. As data concentrators, this is to be expected. Optimising the balance between concentration on either side of the switch and the number of ports on the switch can be done only for specific technologies and the cost will be an important factor. This problem is therefore left for future consideration.

Both architectures suffer from problems of high rate readout for TRT scan and calorimeter. They both show a strong dependence on the i/o time parameter which characterises the computer to network interface.

While studying all the results, it should be borne in mind that the LVL2 trigger must be designed for the possibility of handling an input rate of up to 100 kHz. The menus used for this study give approximately 40 kHz input rate, so to get some idea of the impact of the maximum 100 kHz one could increase all the results by a factor of 2.5, since all results are proportional to the trigger rates. This is an equal increase in rate across all the items in the trigger menu. In the absence of congestion the latency is unaffected by an equal scaling of the trigger rates.

It is also important to bear in mind that these results are for average rates, data sizes, etc, ignoring fluctuations over time and in the detector. For example, in the calorimeter barrel/endcap transition region it is calculated [12] that twice as many HCal ROBs would have to be read out for a RoI.

### Future work

One of the interesting directions for future work is the implementation of some specific technologies in the model.

When building a real trigger system it would be expected that various optimisations which were specific to the technology used could be implemented. Throughout this model, technology dependence has been avoided as much as possible to try to make the results generally applicable. Also, the technology with which the final trigger is built is regarded as a separate issue from the choice of architecture and selection strategy. It is however worth taking a brief look at the impact of a few technology specific implementations on the results.

For example:

1. Replace local network (messages between ROBs and RSIs) with DMA over PCI, as it is done demonstrator vertical slices.
2. Allow broadcast and/or multicast in networks.

A next step for this work would be to try out some of these technology parameters in the paper models, and interested readers are invited to try this for themselves using one of the spreadsheets referenced [3, 4].

## 7 Acknowledgements

The authors wish to acknowledge the help of all the LVL2 demonstrator groups and other members of the ATLAS collaboration.

## References

* [1] Pilot project modelling page [http://www.nikhef.nl/pub/experiments/atlas/daq/modelling.html](http://www.nikhef.nl/pub/experiments/atlas/daq/modelling.html)
* [2] N. Madsen et al, Emulation of the Level-2 trigger, architecture B, on the Macrame Testbed, DAQ-NO-102, June 1998.
* [3] Excel spreadsheet available from [http://www.hep.ph.rhbnc.ac.uk/atlas/](http://www.hep.ph.rhbnc.ac.uk/atlas/)
* [4] Excel spreadsheet available from [ftp://ftp.nikhef.nl/pub/experiments/atlas/tdaq/Paper/](ftp://ftp.nikhef.nl/pub/experiments/atlas/tdaq/Paper/)
* [5] S.George, J. R. Hubbard, J. C. Vermeulen, Input Parameters for Modelling the ATLAS LVL2 Trigger, DAQ-NO-70, June 1997
* [6] J. Bystricky et al, A Model for Sequential Processing in the ATLAS LVL2/LVL3 Trigger (Demo C paper model), DAQ-NO-55 [http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/NOTES/note55/daq55.ps.Z](http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/NOTES/note55/daq55.ps.Z)
* [7][http://hepwww.rl.ac.uk/atlas/l2/demonstrator/docs/demons_data_defs.html](http://hepwww.rl.ac.uk/atlas/l2/demonstrator/docs/demons_data_defs.html) F Wickens, August 1996.
* [8] R Bock & P Le Du, Detector and readout specifications for the level-2 trigger demonstrator program, ATLAS DAQ-NO-62, February 1997.
* [9] J. Vermeulen, Calculation of no. of ROBs per RoI [http://www.nikhef.nl/www/pub/experiments/atlas/daq/ROBsperRoI.html](http://www.nikhef.nl/www/pub/experiments/atlas/daq/ROBsperRoI.html)
* [10] J. Vermeulen, LVL2 Process Models [http://www.nikhef.nl/www/pub/experiments/atlas/daq/process.html](http://www.nikhef.nl/www/pub/experiments/atlas/daq/process.html)
* [11] S. Falciano et al, the ATLAS Muon trigger algorithm for Level-2 Feature Extraction, ATLAS DAQ note in preparation???
* [12] R. Bock & B. Kastrup, Realistic Calorimeter Feature extraction: Algorithm, Benchmarks and Implementation Options, DAQ-NO-65. [http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/NOTES/note65/fexbenchs.ps.gz](http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/NOTES/note65/fexbenchs.ps.gz)
* [13] B. Thoris, private communication.