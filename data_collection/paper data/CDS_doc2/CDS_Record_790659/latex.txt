# Measurement of SUSY Masses via Cascade Decays for SPS 1a

B. K. Gjelsten

Department of Physics, University of Oslo, P.O.B. 1048, Blindem, N-0316 Oslo, Norway

E-mail: B.K.Gjelsten@fys.uio.no

D. J. Miller

Department of Physics and Astronomy, University of Glasgow, Glasgow G12 8QQ, UK.

and

School of Physics, University of Edinburgh, Edinburgh EH9 3JZ, UK.

E-mail: D.Miller@physics.gla.ac.uk

P. O Island

Department of Physics, University of Bergen, N-5007 Bergen, Norway

E-mail: Per.Osland@ift.uib.no

###### Abstract:

If R-parity conserving supersymmetry exists below the TeV-scale, new particles will be produced and decay in cascades at the LHC. The lightest supersymmetric particle will escape the detectors, thereby complicating the full reconstruction of the decay chains. In this paper we expand on existing methods for determining the masses of the particles in the cascade from endpoints of kinematical distributions. We perform scans in the mSUGRA parameter space to delimit the region where this method is applicable. From the examination of theoretical distributions for a wide selection of mass scenarios it is found that caution must be exerted when equating the theoretical endpoints with the experimentally obtainable ones. We provide analytic formulae for the masses in terms of the endpoints most readily available. Complications due to the composite nature of the endpoint expressions are discussed in relation to the detailed analysis of two points on the SPS 1a line. Finally we demonstrate how a Linear Collider measurement can improve dramatically on the precision of the masses obtained.

SUSY,BSM,MSM +
Footnote †: preprint: ATL-PHYS-2004-029

Edinburgh 2004/12

###### Contents

* 1 Introduction
* 2 Cascade decays in SUGRA scenarios
	* 2.1 The SUGRA mass hierarchy
	* 2.2 Gluino and squark decays: the upper part of the chain
	* 2.3 Neutralino and slepton decays: the lower part of the chain
	* 2.4 Other constraints
* 3 Summary of SPS 1a
	* 3.1 The SPS 1a line and points
	* 3.2 Sparticle production
	* 3.3 The cascade
* 4 Mass distributions
	* 4.1 Theory curves of invariant mass distributions
	* 4.2 Formulae for kinematic endpoints
	* 4.3 Inversion formulae
* 5 Data' generation and reconstruction
	* 5.1 Event generation
	* 5.2 Di event flavour (DF) subtraction
	* 5.3 Selection cuts
	* 5.4 Multiple squark masses
	* 5.5 Invariant mass distributions
* 6 Extraction of masses from edges
	* 6.1 10,000 ATLAS experiments
	* 6.2 Mass estimation via
	* 6.3 Minima of
	* 6.4 SPS 1a ( )
	* 6.5 SPS 1a ( )
* 7 Linear Collider inputs
* 8 Conclusions

## 1 Introduction

The Standard Model (SM ) of particle physics has been remarkably successful in describing the physics probed by modern day particle accelerators. No deviation from the SM has thus far been confirmed by experiment and only the Higgs mechanism, the SM's instrument for the breaking of the electroweak symmetry, remains to be discovered. Nevertheless, the SM's enters from considerable theoretical difficulties, not least of which is the hierarchy problem [1], the extreme sensitivity of the electroweak scale to new physics. Such difficulties imply that the SM is only an effective low-energy theory (albeit a highly successful one) applicable only up to a few hundred GeV or so, and will need to be extended in order to describe physics at higher scales.

One extension which has attracted a lot of attention is supersymmetry [2, 3, 4, 5]. Supersymmetry not only solves the hierarchy problem but has many other attractive features: it is the only non-trivial extension to the Poincare symmetry of space-time [6]; it is essential to the formulation of superstring theories [7]; it provides a low-energy theory which is more amenable to the unification of the fundamental forces into a Grand Unified Theory (GUT) at some high energy scale [8]; it provides a natural mechanism for generating the Higgs potential which breaks the electroweak symmetry [9, 10, 11, 12, 13, 14]; and it supplies a good candidate for cold dark matter [15]. Furthermore, if it is to be relevant in solving the hierarchy problem it must exhibit experimental consequences at the TeV-scale, and therefore can be tested by experiment at the Large Hadron Collider (LHC). For an overview of supersymmetry searches at LEP, the Tevatron and HERA, see Ref. [16].

If supersymmetric particles are produced at the LHC, thus confirming supersymmetry, it will become important to identify them and accurately measure their masses. This will be essential for identifying the low-energy model and hopefully distinguishing the Minimal Supersymmetric Standard Model (MSSM ) from other non-minimal extensions. Furthermore, since no supersymmetric particles have so far been discovered, supersymmetry must be broken by some as yet unknown mechanism. Only an accurate determination of the supersymmetric particle masses and couplings will allow us to determine the low-energy soft supersymmetry breaking parameters. It is hoped that extrapolation of these masses and couplings to high energies using the renormalisation group equations will provide an insight into the mechanism of supersymmetry breaking and, more generally, physics at the GUT scale [17]. Since errors in the mass measurements will be magnified by the renormalization group running it is absolutely essential that these masses be determined as accurately as possible.

Here we will discuss supersymmetric measurements with reference to one particular model of supersymmetry breaking, minimal super-gravity (mSUGRA ) [18, 19, 13, 14]. In this model, the supersymmetry is broken by the interaction of new particles at high energy which are only linked to the usual particles by gravitational interactions; this new sector of physics is often referred to as the hidden sector. These gravitational interactions transmit the supersymmetry breaking from the hidden sector to our own sector, producing TeV scale effective soft supersymmetry breaking terms in the GUT scale Lagrangian, quantified by parameters which run logarithmically down to the probed TeV scale. At theGUT scale, the scalar supersymmetric particles are assumed to have a common mass, \(m_{0}\), while the gauginos have a common mass \(m_{1=2}\). The trilinear couplings are also taken to be universal at the GUT scale and denoted \(A_{0}\).

Mass measurements in the MSSM are complicated by R-parity conservation, which is introduced to prevent unphysical proton decay. R-parity conservation requires that supersymmetric particles are produced in pairs and causes the lightest supersymmetric particle (LSP) to be stable. Consequently the LSP is inevitably the end product of every supersymmetric decay and, if electrically neutral, will escape the detector leaving no track or energy deposit. While this provides a very distinctive missing energy signature, it makes it very difficult to measure masses at the LHC since one cannot fully reconstruct decays.

Instead, mass measurements rely on continuous mass distributions of decay products which attain extrema for certain configurations of the particle momenta that are unam - biguously determined by the masses of initial, intermediate and natal particles involved. These relations may often be inverted to give the masses of unstable particles. This is analogous to the way a bound on the neutrino mass can be obtained from the end-point of the beta-decay spectrum of \({}^{3}\)H [19], but is usually more complex, since a long decay chain is often involved.

In this study we will consider supersymmetric mass measurements made by examining the mass distribution endpoints or edges' of the long decay chain1\(q\)! \(\sim_{2}^{0}q\)! \(\sim_{1}^{0}\)!

[MISSING_PAGE_EMPTY:5]

[MISSING_PAGE_EMPTY:6]

[MISSING_PAGE_EMPTY:7]

bol represents the sum of the masses of the scalar and its SM partner. Also shown (mave) is a region where the LSP is charged and therefore ruled out, as well as a theoretically forbidden (TF) region (gray) for low m\({}_{1=2}\).

It is interesting to note that there are no regions where a squark is heavier than the gluino and \(\sim_{2}^{0}\) is heavier than one of the sleptons. This is simply because the gluino and [the gaugino part of] the neutralino have a common mass, m\({}_{1=2}\), at the high scale, so if the gluino is light, the neutralinos will also be light.

However, for more general non-SUGRA unification scenarios one could still expect hierarchies of the type \(m_{\alpha}>m_{\beta}>m_{\alpha}>m_{\alpha}>m_{\alpha}>m_{\alpha}\) to be realised. It would then be inportant to be able to distinguish one hierarchy from the other; this should be possible using the kinematic endpoints, number of b-quarks in the final state, etc. Also it is possible to distinguish \(\sim_{2}^{0}\)! m! \(\sim_{11}^{0}\)! m! (m\({}_{\alpha}>m_{\alpha}\)) from \(\sim_{2}^{0}\)! \(\sim_{11}^{0}\)! m! (m\({}_{1}>m_{\alpha}\)). The first has the wellknown triangular shape of m! \(\sim_{11}\) while the second has a typical 3-body shape. All in all it should therefore be possible to distinguish the various hierarchies (2.2) before continuing to determine the masses themselves.

Region (i) is the only one that has a useful' squark decay together with a decay of \(\sim_{2}^{0}\) to a slepton, and is shown in light and dark green in Fig. 1. We see that there is therefore a large region where the mass hierarchy is compatible with the methods presented here, and even though we will only perform the analysis for the points ( ) and ( ) on the SPS la line, one would expect these methods to be widely applicable.

However, these plots of the mass hierarchies really only show the regions in which the masses are such that the decay chain may occur. If the full decay chain is to be useful, it must have a sufficiently large branching ratio to be seen above the many backgrounds. We will therefore go on to examine the branching ratios of the pertinent decays over (a restricted range of) the SUGRA parameter space. As a first taste, we have highlighted in a brighter green (and denoted (i)\({}_{\alpha}\)) the part of region (i) corresponding to where the overall branching ratio for the decay chain \(q\)! \(\sim_{2}^{0}q\)! \(\sim_{11}^{0}\)! m! exceeds a tenth of that at the SPS la ( ) reference point. Although the decay chain is available over a rather large region of the parameter space, using this decay for large values of m\({}_{1=2}\) and m\({}_{0}\) will be extremely challenging due to the small branching ratio.

### Gluino and squark decays: the upper part of the chain

The decay branching ratios of the gluino are shown in Fig. 2 over the m\({}_{1=2}\) (m\({}_{0}\) plane for two different scenarios. The representation is such that the branching ratio of a given decay channel in a small neighbourhood of the m\({}_{1=2}\) (m\({}_{0}\) plane is equal to the fraction which the corresponding colour occupies in that neighbourhood. Since the gluino only feels the strong force, it has to decay into a quark and a squark. If no squark is light enough, a three-body decay through an \(\alpha\)-shell squark will take place; this is what happens in the green/white region of Fig. 2 at small m\({}_{1=2}\).

For the rest of the m\({}_{1=2}\) (m\({}_{0}\) plane the gluino decays fairly democatically into the accessible squarks. In considerable parts of the plane only one two-body decay is open, \(\Sigma_{1}\)b (red) or it (yellow), in which case the allowed decay takes close to the full decay width. Although one can in principle obtain information about the gluino mass by analysing its decay chain, we will only consider here the decay chain starting from a parent squark, and leave the gluino case for a separate publication [33].

As already intimated, squarks may decay by the strong force into a quark and a gluino (if the gluino is lighter), or decay by weak interactions into a quark and a chargino or neutralino, or via a loop into a gluon and a lighter squark. If kinematically allowed, the strong interaction takes a large fraction of the branching ratio, but since the (lighter) charginos and neutralinos are typically much lighter than the gluino, there will always be some neutralino production.

Within SUGRA models, the squarks \(\tilde{G}_{L}\) and \(\tilde{u}_{L}\) are very close in mass and behaviour. Furthermore, the second generation squarks, \(\tilde{s}_{L}\) and \(\tilde{e}_{L}\), are almost identical copies of the former two. It is therefore useful to have the common notation, \(\tilde{q}_{L}\), for these four squarks. In a similar manner \(\tilde{q}_{R}\) is used for \(\tilde{G}_{R}\), \(\tilde{u}_{R}\) and their second generation copies. The right-handed squarks differ from the left-handed ones in that they do not feel weak interactions, which again makes their decay pattern different. In Fig. 3 the decay branching ratios of \(\tilde{u}_{L}\) and \(\tilde{u}_{R}\) are shown in the \(m_{1=2}\{m_{0}\) plane for two different scenarios.

For \(m_{1=2}\)\(m_{0}\), when the gluino mass is smaller than the squark mass, both \(\tilde{q}_{L}\) and even more so \(\tilde{q}_{R}\) have strong-interaction decays. In the rest of the \(m_{1=2}\{m_{0}\) plane, when the strong decay is forbidden or suppressed by phase space, their decay patterns are very different: while \(\tilde{q}_{R}\) decays directly into the LSP, \(\tilde{q}_{L}\) prefers \(\sim_{2}^{0}\) and \(\sim_{1}\).

For both low and high tan, the \(\sim_{1}^{0}\) is predominantly bino, with only a tiny admixture of wino and higgsino, while \(\sim_{2}^{0}\) and \(\sim_{1}\) are mainly wino. For quite low mass parameters,

Figure 2: Decay channels of \(g\) with \(A_{0}=m_{0}\), \(tan=10\) (left) and \(A_{0}=0\), \(tan=30\) (right). In the left panel the SPS 1a line is shown together with the two points ( ) and ( ) marked with triangles. In the right panel the triangle marks the SPS 1b point. The branching ratio of a given decay channel in a small neighbourhood of the \(m_{1=2}\{m_{0}\) plane is equal to the fraction which the corresponding colour occupies in that neighbourhood. The region where \(\sim_{1}^{0}\) is not the LSP is denoted Charged LSP’ and is discarded. Some regions are also forbidden theoretically, in that \(eg\). it is not possible to obtain electroweak symmetry breaking (labeled TF’).

m 1-2. 100 GeV, they become more mixed. Since \(\alpha_{1}\) generally has a much larger SU (2) coupling than U (1) coupling, decays to \(\sim_{1}\) and \(\sim_{2}^{0}\) will be preferred unless the difference in phase space makes the decay to the lighter \(\sim_{1}^{0}\) competitive. In contrast, since the \(\alpha_{1}\) has no SU (2) interaction it will decay predominantly to the bino \(\sim_{1}^{0}\), except at quite low mass parameters where the neutralinos change character.

The third generation squarks after from the others in two aspects. First, the mass eigenstates can have more even admixtures of both handedness components. For B this is the case for low mass parameters, \(m_{0}\), \(m_{1=2}\). 200 GeV, where the branching ratios into \(\sim_{1}^{0}\) and \(\sim_{2}^{0}\) are of comparable size. At higher masses \(B_{1}\) and \(B_{2}\), \(B_{3}\), giving a \(B_{1}\) which prefers to go to \(\sim_{2}^{0}\) rather than \(\sim_{1}^{0}\). Second, due to large splitting, the third generation squarks can decay into other third generation squarks together with a weak gauge boson. The drastic change observed in figure 4 as \(m_{0}\), \(m_{1=2}\) becomes less than 200 GeV, is due both to the more mixed mass eigenstates for lower masses, and to the closing of certain channels involving \(t\) or a heavy gauge boson.

While \(B_{1}\) has a large branching ratio into \(\sim_{2}^{0}\) throughout the entire plane, \(B_{2}\) produces \(\sim_{2}^{0}\) at a much smaller rate, except for small mass parameters.

To summarize, the squark decays that are useful for kinematic endpoint analyses, are those of left-handed first and second-generation squarks, as well as those of \(B_{1}\) and to a lesser extent \(B_{2}\). These occur in the entire \(m_{1=2}(m_{0}\) plane, except for extreme values \(m_{1=2}\)\(m_{0}\), and for both low and high tan values. For quite low mass parameters also \(B_{1}\) contributes.

### Neutralino and slepton decays: the lower part of the chain

The reasons why \(\sim_{2}^{0}\) often plays an important role in the reconstruction of SUSY events are many. Experimentally situated midway between the initially produced gluinos/squarks and the LSP, it is abundantly decayed into, as we have seen. What makes it so useful, usually more so than the \(\sim_{1}\), which is produced in similar ways and amounts, is the fact that its decay products, in addition to easily setting \(\sim_{2}\) the trigger, also reconstruct well.

In Fig. 5 the main decay channels of \(\sim_{2}^{0}\) are shown for two values of tan. A two-body decay is preferred over a three-body decay, and the coupling to \(\Gamma_{1}\sim\) or \(\sim\) is usually stronger than the coupling to \(\sim_{1}^{0}\).

For \(m_{0}\) & \(0.5m_{1=2}\) all of \(\Gamma_{1}\sim\) and \(\sim\) are heavier than \(\frac{0}{2}\), so only the decay into the LSP is possible. In yellow, to the very left, no two-body channel is open, and \(\sim_{2}^{0}\) undergoes a three-body decay, proceeding through an \(\sim_{2}\)-shell squark or stau/slepton, or involving an \(\sim\)-shell \(Z\), \(W\) or \(h.s.m_{1=2}\) increases, also \(m_{2}\sim_{2}^{0}\) increases and more decay channels become available. First the \(Z\) channel opens and takes the full decay width, then the h channel opens to dominate. The mass difference between \(\sim_{1}^{0}\) and \(\sim_{2}^{0}\) is mostly independent of \(m_{0}\), which is why, to a good approximation, the yellow, red and green regions are stacked horizontally.

In the blue regions decays into \(\Gamma_{1}\sim\sim\) are kinematically allowed. Following a clockwise movement, the scalar masses are reduced relative to \(\sim_{2}^{0}\). The right-handed scalars are lighter and become available first (dark blue region). In the light blue region the left-handed scalars have become available and, despite less phase space, take most of the width due to their SU (2) coupling. The black part of the blue regions shows the decay into \(\Gamma_{1}\) (dark blue region) and \(\Gamma_{2}\) (light blue region). These are the decays of interest to us. At low tan (left panel) the slepton channels can be used in most of the blue regions. At high tan the situation is less optimistic. The \(\sim_{1}\) channel totally dominates the \(\Gamma_{1}\) channel, and only in a small region is the \(\Gamma_{1}\) channel open.

Decay products which involve tau particles are more difficult to use since their reconstruction is always incomplete due to undetected neutrinos. However, in some parts of the parameter space, especially at high tan, these channels take the full decay width, so one 

[MISSING_PAGE_FAIL:13]

In addition, we define two points ( ) and ( ) on the SPS 1a line according to:

\[(\ ):\ \ m_{0}=100\GeV;\ \ \ \ \ m_{1=2}=250\GeV;\] \[(\ ):\ \ m_{0}=160\GeV;\ \ \ \ m_{1=2}=400\GeV. \tag{3.2}\]

The rstpoint ( ) is the basic' SPS 1a point of Ref. [21]and studied in Ref. [27, 28], while the second ( ) is a new, less optimistic scenario with a reduced cross-section for the decay chain.

The masses of particles relevant for our analysis are shown in Fig. 6 (left), moving along the SPS 1a line by varying \(m_{1=2}\). The values of the masses at points ( ) and ( ) can be seen from the vertical dotted lines. As expected, all the masses except the lightest Higgs boson mass increase linearly with \(m_{1=2}\). Neither the heaviest neutralino mass nor the chargino masses are shown; to a good approximation, \(m_{{}_{2}}\)' m\({}_{{}_{3}}\)' m\({}_{{}_{4}}\)' and m\({}_{{}_{1}}\)' m\({}_{{}_{2}}\)'. Similarly, the masses of H and A are not shown, but m\({}_{{}_{H}}\)' m\({}_{{}_{A}}\)' m\({}_{{}_{H}}\).

For the points ( ) and ( ), these masses are further detailed in Table 1, with the masses of the particles in our chosen decay chain displayed in bold.

The relative widths (with divided by the mass) of the decaying sparticles are shown in Fig. 6 (right), and are everywhere less than 2% of the mass. The wiggles in some of these curves (as well as in some of the branching ratio curves below) are due to limited precision in ISAJET [38]. As will be discussed, these widths contribute to a blurring of the kinematical endpoints, and will thus be rejected in the mass determination.

### Sparticle production

The cross-sections for producing supersymmetric particles at the LHC are for moderate values of \(m_{1=2}\) rather high. This can be seen in Fig. 7 which shows the dominating sparticle

Figure 6: Masses (left) and relative widths (right) of relevant sparticles as \(m_{1=2}\), \(m_{0}\) and \(A_{0}\) are varied along the SPS 1a slope, defined by Eq. (3.1). The vertical dotted lines represent SPS 1a points ( ) and ( ).

pair production cross-sections, as m\({}_{1=2}\) is varied along the SPS 1a line. Notice that these cross-sections fall very rapidly as m\({}_{1=2}\) is increased, which will cause repercussions in the analysis of SPS 1a ( ).

The cross-sections for gluino(gluino, gluino(squark and squark(squark pair productions are detailed in Table 2 for the two chosen analysis points, together with the SUSY total rate. Of course since other supersymmetric particle pairs may contribute to the total SUSY rate it is not simply a sum of the other numbers in the table.

These supersymmetric particle pairs are predominantly produced by QCD interactions of quarks and gluons in the colliding protons. For gluino pairs this is mainly due to gg! gg via t-channelgluino exchange and s-channelgluons, and at a much smaller rate qq! gg via s-channel gluons. Squark pairs with the same handedness have the dominant production

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline Point & g & \(\tilde{\alpha}_{L}\) & \(\tilde{\alpha}_{R}\) & \(\tilde{\alpha}_{L}\) & \(\tilde{\alpha}_{R}\) & \(\tilde{\alpha}_{R}\) & \(\tilde{\alpha}_{L}\) & \(\tilde{\alpha}_{R}\) \\ \hline ( ) & 595.2 & 543.0 & 520.1 & 537.2 & 520.5 & 524.6 & 491.9 & 574.6 & 379.1 \\ ( ) & 915.5 & 830.1 & 799.5 & 826.3 & 797.3 & 800.2 & 759.4 & 823.8 & 610.4 \\ \hline \hline  & \(\tilde{\alpha}_{L}\) & \(\tilde{\alpha}_{R}\) & \(\tilde{\alpha}_{L}\) & \(\tilde{\alpha}_{L}\) & \(\tilde{\alpha}_{L}\) & & H & A \\ \hline ( ) & 202.1 & 143.0 & 206.0 & 133.4 & 185.1 & 185.1 & 401.8 & 393.6 \\ ( ) & 315.6 & 221.9 & 317.3 & 213.4 & 304.1 & 304.1 & 613.9 & 608.3 \\ \hline \hline  & \(\tilde{\alpha}_{4}^{0}\) & \(\tilde{\alpha}_{3}^{0}\) & \(\tilde{\alpha}_{2}^{0}\) & \(\tilde{\alpha}_{1}^{0}\) & \(\tilde{\alpha}_{2}\) & \(\tilde{\alpha}_{1}\) & H & h \\ \hline ( ) & 377.8 & 358.8 & 176.8 & 96.1 & 378.2 & 176.4 & 394.2 & 114.0 \\ ( ) & 553.3 & 538.4 & 299.1 & 161.0 & 553.3 & 299.0 & 608.9 & 117.9 \\ \hline \end{tabular}
\end{table}
Table 1: Masses [ GeV] for the considered SPS 1a points ( ) and ( ) of Eq. (32).

Figure 7: Cross-sections as m\({}_{1=2}\), m\({}_{0}\) and A\({}_{0}\) are varied along the SPS 1a slope, defined by Eq. (31). The vertical dotted lines represent SPS 1a points ( ) and ( ).

[MISSING_PAGE_EMPTY:16]

### The cascade

The cross-sections and branching ratios of our chosen decay chains

\[\alpha_{1}\!\ \ \sim_{2}^{0}q\!\ \ \bar{\chi}_{1}q\!\ \ \bar{\chi}_{ 1}\bar{q}\!\ \ \bar{\chi}_{ 1}\bar{q} \tag{3.3}\] \[\bar{\chi}_{1}\!\ \ \bar{\chi}_{2}^{0}\ \!\ \bar{\chi}_{1}\bar{q}\!\ \ \bar{\chi}_{ 1}\bar{q}\!\ \ \bar{\chi}_{ 1}\bar{q}\] (3.4) \[\bar{\chi}_{2}\!\ \ \bar{\chi}_{2}^{0}\ \!\ \bar{\chi}_{1}\bar{q}\!\ \ \bar{\chi}_{ 1}\bar{q}\!\ \ \bar{\chi}_{ 1}\bar{q}\ \!\ \bar{\chi}_{ 1}\bar{q} \tag{3.5}\]

are summarized in Fig. 9 for the two SPS 1a points. Since the left-handed up and down squarks, \(\alpha_{1}\) and \(\bar{\chi}_{1}\), have very similar masses (at 5372 GeV and 5430 GeV respectively), for ( ), they are in the above jointly referred to as \(\alpha_{1}\), and for this analysis will be grouped

Figure 8: Branching ratios of \(\sim_{2}^{0}\) as m\({}_{1=2}\), m\({}_{0}\) and A\({}_{0}\) are varied along the SPS 1a slope. The verticaldotted lines represent SPS 1a points ( ) and ( ).

Figure 9: The SPS 1a cascade decay chain.

together. For the fraction of decay chains which commence from a bottom, \(\beta_{1}\) is responsible for 78% or so, leaving us rather insensitive to the contribution from \(\beta_{2}\).

## 4 Mass distributions

The longer a decay chain is, the more information it contains. To extract the masses of the supersymmetric particles in the decay we require at least as many kinematic endpoint measurements as unknown masses. In the lower part of the decay chain, where the second-lightest neutralino decays via \(\sim_{2}^{0}\): \(\beta_{1}\): \(\beta_{1}\): \(\sim_{1}^{0}\)ll, there are three unknown masses: \(m_{\odot}^{0}\), \(m_{\pm}\) and \(m_{\odot}^{0}\). However, only two particle momenta are measured, those of the two leptons, from which only one mass distribution can be constructed, \(m_{\pm}\). The system is highly underdetermined; one cannot extract the three masses, only a relation between them.

When a squark is added to the head of the decay chain, \(q\): \(\sim_{2}^{0}q\): \(\beta_{1}\): \(\beta_{1}\): \(\sim_{1}^{0}k_{1}q\): \(\sim_{1}^{0}k_{1}q\), three particles can be collected, and one can construct four mass distributions, \(m_{\pm}\), \(m_{\pm}\), \(m_{\pm}\) and \(m_{\pm}\), where following the notation of Refs. [31, 32], we denote the rest emitted lepton \(l_{h}\) ('\(h\)' for 'near') and the second \(\lambda_{1}\) ('\(\bar{\nu}\) for 'far'). In principle this is just a cient for extracting the four unknown masses: \(m_{q}\), \(m_{\odot}^{0}\), \(m_{\pm}\) and \(m_{\odot}^{0}\). However, in order to use the distributions \(m_{\pm}\) and \(m_{\pm}\), we need to be able to distinguish \(l_{h}\) from \(l_{h}\). Since this is usually not possible, two alternative distributions are defined, \(m_{\pm}\), \(m_{\pm}\), and \(m_{\pm}\), [31], constructed by selecting for each event the largest and smallest values of \(m_{\pm}\) respectively.

As will be detailed later in this section, the expressions for these kinematic endpoints are not always linearly independent, so these four endpoints are not always su cient to determine the masses in the decay chain. In this circumstance one must look for other endpoint measurements.

Correlations between different mass distributions can provide further measurements. For example, one may define the mass distribution \(m_{\pm}\): \(\sim_{\frac{1}{2}}\) identically to the \(m_{\pm}\) distribution but with the additional constraint

\[m_{\pm}^{\max}=\frac{P}{2}<m_{\pm}<m_{\pm}^{\max}: \tag{4.1}\]

This cut on \(m_{\pm}\) translates directly into a cut on the angle between the two leptons in the rest frame of \(\beta_{1}\)[41]. In terms of this angle, \(m_{\pm}\) is given by

\[m_{\pm}=m_{\pm}^{\max}P\frac{(1-\cos\theta)}{(1-\cos\theta)=2} \tag{4.2}\]

so a constraint of the form (4.1) directly corresponds to \(\sim_{2}\). The simplicity of this constraint allows one to find an analytic expression for the minimum of the \(m_{\pm}\): \(\sim_{2}^{0}\) distribution.

In principle, other correlations between mass distributions could be used, but they are limited by the lack of analytic expressions for the associated extrema. It is no doubt possible to construct simple constraints for which analytic expressions form in a or a form as distributions are possible, but this will not be investigated further in this study.

If we were to also include a parent gluino at the head of the decay chain, \(g\): \(\alpha_{1}\): \(\alpha_{2}^{0}\): \(\beta_{1}\): \(\sim_{1}^{0}\)ll, we would have an extra quark momentum at our disposal and could construct with its seven more (primary) mass distributions.

### Theory curves of invariant mass distributions

In Fig.10 we show "theory" versions of the "ve mass distributions discussed above for SPS 1a ( ) and ( ), and three other mass scenarios. These distributions reflect the parton level only, where the quark and leptons are assumed to be perfectly reconstructed, and particle widths have been neglected, suppressing a mild smearing of the distributions. Leptons and

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

endpoints. While the endpoints are given by single-valued functions of the masses, albeit with different expressions for different mass regions, the inverse is not true. A given set of endpoint values can in principle correspond to several sets of mass values. This is equally true for the numerical method, and has not received much attention previously (see, however, Ref. [32]). This complication will be faced in Section 6.

## 5 Data' generation and reconstruction

### Event generation

The SPS points are defined by the low-energy MSSM parameters produced by ISAJET 7.58 [30], given a set of high-energy input parameters. In our analysis PYTHIA 6.2 [44] with CTEQ 5L [45] is used to generate the Monte Carlo sample.3 To allow for this the low-energy parameters from ISAJET are fed into PYTHIA via the standard interface. PYTHIA in turn calculates the decay widths and cross-sections. Each event produced is passed through ATLFAST 2.60 [47], a fast simulation of the ATLAS detector. In ATLFAST the output particles of PYTHIA are mapped onto a grid of calorimetric cells' of a given spacing in pseudorapidity and azimuth angle. Next, the cells are combined into clusters, after which particle identification takes place, including smearing of the four-momenta according to particle type. Jets are built by a cone algorithm with R = 04, where R = \(\overline{\rho}\) ( \(\overline{\rho}\)) \({}^{2}+(\overline{\rho})\) \({}^{2}\). Acceptance requirements are imposed: j j \(<\) 25 for \(\epsilon\) = and j j \(<\) 5 for jets as well as p \(>\) 5=6=10 GeV for \(\epsilon\) = \(\epsilon\) jets. Leptons are marked as isolated if there is no other cluster within a distance \(R=0.4\) of the lepton, and if additional energy inside a cone \(R=0.2\) is below 10 GeV. While ATLFAST captures quite well the main features of the full simulation, some important effects are left out. Lepton identification efficiencies are not parametrized. A conventional 90% eciency per lepton is therefore included by hand in the analysis. Also, the possibility of misidentifying a jet as a lepton is absent in the fast simulation, and has not been included in our analysis. The effect of pile-up on the jet energy resolution is accounted for in ATLFAST when run at high luminosity, as in this analysis, but pile-up events have not been simulated, and the underlying event is probably too'slin'. However, as the selection criteria on jets and leptons are quite hard, we do not expect a more realistic detector simulation to change the results very much. Nevertheless, the numbers quoted at the end of this section should be validated with these effects included.

Footnote 3: The main parts of the analysis have been confirmed with HERWIG [46], see [28].

The signature of a signal event is two opposite-sign same-avour (SF) leptons, considerable missing p\({}_{\rm T}\) from the escaping LSPs, and at least two hard jets, one from the signal chain, the other from the decay of the squark nearly always present in the other decay chain. The most important standard model process to have the same features as the signal, is it production. Also W = Z together with jets, one of which is a b-jet, can mimic the signal, and in combination with the underlying event, pile-up and detector effects, other processes will also now and then result in the given signature. Together with the three methods, QCD, Z = W + jet as well as Z Z /ZW /W production. No 

[MISSING_PAGE_EMPTY:26]

shows the part of the di erent- avour-subtracted distribution which contains a signal chain (SC '). The reason why the blue distribution of m qll does not have the form of the theory distribution in Fig. 10, is that the jet is only correctly selected in roughly half of the cases. In solid/dashed green the Standard Model contributions to the same- avour/di erent- avour distributions are shown (SM '). They are statistically identical and will cancel each other through the di erent- avour subtraction.

If the samples contained only background events with uncorrelated leptons (and the di erent- avour-subtraction procedure removed all of these), the di erent- avour-subtracted distribution should fall exactly on top of the blue line. When this does not happen, it implies that the sample also contains background events with correlated leptons. The

Figure 11: Di erent– avour subtraction for m m and m qll at ( ) (left) and ( ) (right). The solid/dashed red curves are the same– avour (SF ')/di erent– avour (DF ') distributions. In black, their di erence, the di erent– avour-subtracted distribution (SF-DF '), is shown with error bars. The blue curve shows the part of the subtracted distribution which contains a signal chain (SC '). The solid/dashed green curves (SM ') give the Standard Model part (completely dominated by tt) of the same– avour/di erent– avour distributions. They are statistically equal and will cancel each other.

[MISSING_PAGE_FAIL:28]

[MISSING_PAGE_EMPTY:29]

[MISSING_PAGE_FAIL:30]

[MISSING_PAGE_EMPTY:31]

[20], Fig. 10-41. Low/high b-tagging efficiencies come with high/low rejection factors and allow for high-purity b/non-b-samples, respectively. The higher the purity, the smaller the sample. In this analysis we have used the following simplistic b-tagging prescription for both purposes: For a b-tagging efficiency of 50% the rejection factors against jets from ghons/three lightest jets and from c-jets are set to 100 and 10, respectively.

With b-tagging one can to a certain extent separate the \(\phi_{\perp}\) and the \(\beta\) distributions, thus opening for a disentanglement of the squark masses. Nevertheless, even though a high purity separation has been accomplished, each of the two distributions will still contain contributions from two squark masses. Typically a kink can be observed at the position of the lowest endpoint. For SUGRA scenarios \(\beta_{\perp}\)=\(\beta_{\perp}\) and \(\beta_{\perp}\)=\(\beta_{\perp}\) only if they are few GeV, so the kink will appear very near the end of the distribution. Since the proton contains more than d-quarks, \(\beta_{\perp}\) will be produced at a higher rate than the heavier \(\beta_{\perp}\). This reduces further the visibility of the kink. Then with the general smearing due to physics and detector effects in addition to background near the endpoint, it may be very difficult to identify such a kink. In case of the two b-squarks the separation will be larger. Whether it is possible to identify it or not depends on the rate of \(\beta\) production as well as the level of impurity from \(\phi_{\perp}\)-events in the b-tagged distribution.

### Invariant mass distributions

SUSY processes for ( ) and ( ) as well as the Standard Model background have been produced for 300 fb [1]. This corresponds to 3 years at design (high) luminosity. The mass distributions of the available edges for SPS 1a ( ) and ( ) are shown in Figs. 13(14.

For all plots the black points with errors show the total dierent-avour-subtracted distribution (SF-DF'). Solid green marks the SUSY background (SUSY'), and in green with error bars the Standard Model background (SM') is shown. The solid blue curve then shows the original theory distribution (TH'), normalised to the dierent-avour-subtracted distribution. The fitted function appears in red. In cases where mixed events are used to model the background, the smooth function fitted to the high-statistics mixed-event sample is shown in dashed red. When additional distributions are plotted, they will be described in the accompanying discussion.

For each distribution the endpoint estimation will be discussed. In most cases the edges are fitted to a straight line in combination with a simple background hypothesis, and in some cases convoluted with a Gaussian distribution. At ( ) this procedure gives numbers in reasonably good agreement with the nominal values. At ( ), where the SUSY cross-section is much smaller and also the branching fraction of the signal is reduced, the estimated endpoint values depend more strongly on the fitting method chosen. To control and reduce this systematic effect, a better understanding of the whole chain is required; physics effects, detector effects, multiple masses at dierent rates, background, prevents. After some years of LHC operation one can expect these issues to be understood soiently that the systematics of endpoint estimation is controlled and corrected for in the fitting procedure, up to some small uncertainty.

If this is achieved, it is the statistical error of the endpoint values, in combination with the uncertainty on the absolute energy scale, expected to be 1% for jets and 0.1% for 

[MISSING_PAGE_EMPTY:33]

other distributions, it does not do so well. Nevertheless, there are many ways to construct the mixed sample. Further study of the method seems worthwhile.

## 6 Conclusions

Figure 13: Invariant mass distributions for SPS 1a ( ). See the text for details.

It is tempting to suggest that it is mainly the \(u_{1}\)=\(u_{2}\) endpoint we measure, and that the extra events just above the endpoint (see the figure), make up the hard-to-detect kink from the \(d_{1}\)=\(u_{1}\) edge. At the present level of detail it is impossible to say if such an effect could be isolated to give an additional measurement for the heaviest mass. It does seem di cult, though, as the kink will be washed out by other e eets, e.g. sparticle widths. At ( ) the intrinsic width of \(u_{1}\) is 5.3 GeV, see Fig. 6 (right), to be compared with \(m_{d_{1}}\) = 5.8 GeV. Also detector eets will result in a general smoothing of the distributions.

\(m_{q1(low)}\) : For \(m_{q1(low)}\) the mixed sample is ne above the endpoint, but overestimates for lower masses. While the edge can be fitted and the endpoint measured by a mixed sample subtraction, due to its good behaviour in the edge region, we have here instead used the inconsistency cut \(m_{q1(low)}\) > 440 GeV to purify the sample, Fig. 13 (middle left). The ability of the inconsistency cut to bring the distribution very close to the original one, was already discussed in relation to Fig. 12.

For a zero background hypothesis a straight line \(\tau_{q1(low)}\) = 300.7(0.9) GeV, to compare with the nominal 298.5/302.1 GeV for \(u_{1}\)=\(d_{1}\). If the few bins around the endpoint are also included, the value increases by 1(2 GeV. Whether these high-mass events are signal or background is not easy to tell from the given distribution, since there is virtually no background structure to extrapolate from. This is because a "consistency cut" has been in posed which requires \(m_{q1(low)}\) < 440 GeV, in accordance with the already measured \(m_{q1}\) endpoint. From Eq. (4.10) this implies \(m_{q1(low)}\) < 440= 2 GeV = 311.1 GeV. The consistency cut takes away a large part of the background, but also has the e ect that it becomes di cult to see what structure the \(m_{q1(low)}\) background has. If the consistency cut is dropped, the usual background tail appears and can be modeled, although at the cost of a slight increase in the statistical error.

\(m_{q1(high)}\) : Following the same procedure as for \(m_{q1}\) the background of the \(m_{q1(high)}\) distribution was modeled by the mixed event sample. In Fig. 13 (middle right) the relevant distributions are shown. Also the result from subtracting the mixed sample is shown (mixed subtr.'); the lower-black points with error bars. This subtracted distribution follows the original theory distribution (blue) closely in the edge region, but overestimates at lower values.

In the range \(m_{q1(high)}\), 2 (320;550) GeV the endpoint was estimated to 374.0(2.0) GeV. The nominal value is 375.8/380.3 GeV for \(u_{1}\)=\(d_{1}\). At ( ) the \(m_{q1(high)}\) edge consists of two parts. This is clear from the theory distribution (blue), but also in the reconstructed distributions (black). In the \(t\) only the lowest near-linear stretch was used. It is clearly incorrect to apply a straight line for the whole edge, but if done, the statistical error would be reduced to 1 GeV. When a good signal function is at hand, the whole edge will be described. It may therefore be reasonable to expect a statistical error of 1 GeV rather than 2 GeV.

\(m_{q1(low)}\) : The \(m_{q1(low)}\) distribution differs from the previous ones in that a minimum is to be measured. This has two important consequences. One can be seen from Fig. 11

[MISSING_PAGE_EMPTY:36]

[MISSING_PAGE_FAIL:37]

m qll: The m\({}_{\rm{dow}}\) l distribution is shown in Fig. 14 (top right). With the mixed sample as a rough background estimate a straight-line t gives an endpoint at 640 GeV with a

Figure 14: Invariant mass distributions for SPS la ( ). See the text for details.

systematic shift of 1 GeV from varying the bin size and \(\tau\) range. The statistical error is 5 GeV. If, however, the same fitting procedure is applied to the \(m_{\rm q_{bin}\mu}\) distribution, the endpoint value increases to 655 2 GeV and the statistical error is 9 GeV.

Surely more study would bring these values closer, and optimally have them converge near the nominal value of 649.1/652.5 GeV for \(\mu_{\rm L}\)=\(\rm d_{L}\). For later use we take an optimistic statistical error of 5 GeV, but also include a systematic error of 3 GeV for a more conservative estimate.

\(m_{\rm q1(low)}\): The \(m_{\rm q1(low)}\) distribution is shown in Fig.14 (middle left). Both a consistency cut \(m_{\rm q1(low)}\) < 670 GeV and an inconsistency cut \(m_{\rm q1(high)}\) > 670 GeV are used. A straightforward fit with no background hypothesis gives a statistical error around 6.3 GeV. The actual value is 443 GeV, which overshoots by a few GeV since the background plateau has not been included. The nominal value is 436.6/438.9 GeV for \(\mu_{\rm L}\)=\(\rm d_{L}\).

\(m_{\rm q1(high)}\): The \(m_{\rm q1(high)}\) distribution with the consistency and the inconsistency cut is shown in Fig.14 (middle right). The mixed sample was again used to roughly model the background under the edge. A straight-line fit gives an average value of 520.5 GeV with systematics from binning and \(\tau\) range of 3 GeV. The statistical error is at 5.5 GeV.

The nominal value is 529.9/532.7 GeV for \(\mu_{\rm L}\)=\(\rm d_{L}\), some 10 GeV above our estimate. One could argue that the current endpoint measurement has considerable uncertainties and that this discrepancy is not dramatic at the present level of detail. However, such an underestimation is actually to be expected. In Fig.10 the theoretical \(m_{\rm q1(high)}\) distribution is shown for \(\mu_{\rm L}\) at ( ). There is a long vertical fall towards 517 GeV (for \(\rm d_{L}\) it is at 519 GeV), then just before the bottom is reached, a small foot appears, as anticipated in Sect. 4.1, and takes us up by 11(13 GeV. To detect such a small foot would require more statistics than is available at ( ). Experimentally it is therefore expected to get an endpoint near 517(519 GeV. The incorrect endpoint measurement will have in portant consequences for the determination of masses from the endpoints. This situation is further discussed in Section 6.

\(m_{\rm q1(~{}>~{}\tau)}\): The \(m_{\rm q1(~{}>~{}\tau)}\) distribution is shown in Fig.14 (bottom ). The same-avour (SF') and dierent-avour (DF') distributions are shown in solid and dashed black. Clearly, there are large uncertainties from the dierent-avour subtraction. The statistical uncertainty of the dierent-avour-subtracted sample was estimated with a straight-line fit, giving 13 GeV. In addition there will be a systematic error, here conservatively set to 10 GeV.

### Endpoint measurement values

The results of the endpoint estimation for ( ) and ( ) are summarized in Table 4. The last column contains an estimate of the systematic error from dierent fitting techniques, ranges and bin widths. These values are not used in the following, but are included for completeness.

The column with the heading Energy Scale Error shows the expected error on each endpoint estimation from the uncertainty on the absolute energy scale for jets and leptons.

This e ect has not been taken into account in the simulation. The uncertainties of the energy scales are here set to 1% for jets and 0.1% for electrons and muons, see Ch. 12 of [20]. For an invariant mass which consists of only jets or only leptons, this will give the same uncertainties, 1% and 0.1%, respectively. If the invariant mass is constructed from one jet and one lepton, the endpoint uncertainty is

\[\frac{(m_{\mathrm{q1}})}{m_{\mathrm{q1}}}=\frac{(m_{\mathrm{q1}}^{2})}{2m_{ \mathrm{q1}}^{2}}=\frac{1}{2}\quad\frac{(E_{\mathrm{j}})}{E_{\mathrm{j}}}^{2}+ \quad\frac{(E_{\mathrm{1}})}{E_{\mathrm{1}}}^{2}=0.50\% \tag{5.2}\]

where \(E_{\mathrm{j}}\) and \(E_{\mathrm{1}}\) are the jet and the lepton energies, respectively. For an invariant mass involving a higher number of jets and leptons, the error on the endpoint value from the energy scale uncertainty will be different for each event. The error of \(m_{\mathrm{q1}}\) will depend on the relative size of the three terms on the right-hand side of Eq. (4.10). Since at ( ) and ( ) we are in the region where the mass ratio \(m_{\mathrm{q1}}=m_{\mathrm{q2}}\) dominates the two other mass ratios, see Eq. (4.4)-(1), the quark will usually be very energetic, leaving one or both \(m_{\mathrm{q1}}\) terms to dominate. This is particularly true at large values, so near the edge of \(m_{\mathrm{q1}}\) one can show that the energy scale error will result in an endpoint error between 0.35% and 0.5% for each event. For \(m_{\mathrm{q1}}(>m_{\mathrm{q2}})\) the average energy scale error will be slightly lower in our two scenarios. For simplicity we have set the energy scale error to 0.5% for all endpoints involving jets.

\begin{table}
\begin{tabular}{|c c c c c c|} \hline  & \multicolumn{2}{c|}{Nominal} & \multicolumn{1}{c|}{Fit} & \multicolumn{1}{c|}{Energy Scale} & \multicolumn{1}{c|}{Statistical} & \multicolumn{1}{c|}{Syst.Fit} \\ Edge & Value & Value & Error ( scale) & Error ( stat) & Error \\  & [GeV] & [GeV] & [GeV] & [GeV] & [GeV] \\ \hline ( ) & & & & & \\ \(m_{\mathrm{l}}^{\,\mathrm{max}}\) & 77.07 & 76.72 & 0.08 & 0.04 & 0.1 \\ \(m_{\mathrm{q1}}^{\,\mathrm{max}}\) & 425.9 & 427.7 & 2.1 & 0.9 & 0.5 \\ \(m_{\mathrm{q1}}^{\,\mathrm{max}}\) & 298.5 & 300.7 & 1.5 & 0.9 & 0.5 \\ \(m_{\mathrm{q1}}^{\,\mathrm{max}}\) & 375.8 & 374.0 & 1.9 & 1.0 & 0.5 \\ \(m_{\mathrm{q1}}^{\,\mathrm{min}}\) & 200.7 & – & 1.0 & 2.2 & 2.0 \\ \(m_{\mathrm{q1}}^{\,\mathrm{min}}\) & 183.1 & – & 0.9 & 4.5 & 4.0 \\ \hline ( ) & & & & & \\ \(m_{\mathrm{l}}^{\,\mathrm{max}}\) & 137.9 & 137.4 & 0.14 & 0.5 & 0.1 \\ \(m_{\mathrm{q1}}^{\,\mathrm{max}}\) & 649.1 & 647.0 & 3.2 & 5.0 & 3.0 \\ \(m_{\mathrm{q1}}^{\,\mathrm{max}}\) & 436.6 & 443.0 & 2.2 & 6.3 & 4.0 \\ \(m_{\mathrm{q1}}^{\,\mathrm{max}}\) & 529.9 & 520.5 & 2.6 & 5.5 & 3.0 \\ \(m_{\mathrm{q1}}^{\,\mathrm{min}}\) & 325.7 & – & 1.6 & 13.0 & 10.0 \\ \hline \end{tabular}
\end{table}
Table 4: Endpoint values found from fitting the edges in Figs. 13(14, for 300 fb\({}^{-1}\). The nominal values correspond to the mass of \(m_{\mathrm{l}}\), which due to the proton content is produced at higher rates than the heavier \(\mathcal{G}_{\mathrm{l}}\). For the thresholds not values are shown, only the errors. This reects the fact that a reasonable \(t\) function is lacking for this edge.

## 6 Extraction of masses from edges

### 10,000 ATLAS experiments

In the simulation study described in Sect. 5 values for the endpoints and their statistical uncertainties were found, together with a "systematic" uncertainty". Although not so far from the nominal standpoint values, the values in Table 4 are somewhat uncertain due to the as yet not-understood systematics of the fitting procedures. Also, the systematic error on the energy scale has not been addressed.

A assuming that one will eventually be able to control the systematics of the fitting, only the statistical errors together with the systematic error from the energy scale uncertainty will be what determine the LHC potential to measure the SUSY masses. To estimate this potential, consider an ensemble of typical LHC experiments, i.e. where the deviation of each endpoint measurement from the nominal value is based on a Gaussian distribution of width equal to the statistical error estimated for that endpoint, as well as a jet/lepton energy scale error picked from a Gaussian distribution for each experiment, in line with what is done in [31],

\[E_{i}^{\rm exp}=E_{i}^{\rm nom}+A_{i}^{\rm stat}+B_{i}^{\rm scale} \tag{6.1}\]

Here \(E_{i}\) denotes the position of the \(i^{\rm th}\) endpoint. The coefficients A and B are picked from a Gaussian distribution of mean 0 and variance 1. Each experiment will pick as many A's as there are endpoint measurements as well as one B for the \(m_{\rm 1}\) endpoint and one for the endpoints involving jets, thus neglecting the effect of the lepton energy scale error on the latter.

When a set of edges \(E^{\rm exp}\) has been found, the task is to find the masses of which best correspond to the measurements. If only four endpoints are measured, the inversion formulae straight away return the possible mass combinations. If more endpoints are available, no mass combination will in general reproduce the edge measurements, and a numerical approach is required, where the measurements are weighted according to their uncertainties. Note that in this procedure we do not make use of the \(t\) values given in Table 4.

It should also be emphasised that the systematics of the endpoint measurements are here assumed to be under control, i.e. the Syst. Fit Error' of Table 4 is neglected. The precision we will not in this section and the next for the determination of masses and mass differences at the LHC must be understood in this context. If the endpoint systematics turn out to be comparable to the combined statistical and energy scale errors, then the precision will be worse.

### Mass estimation via

In our case, where the jet energy scale error produces a correlation between the endpoint measurements, the method of least squares is appropriate. The best mass estimate of \(t\) is then the one which minimises the function

\[=[E^{\rm exp}\ \ \ \ \ \ E^{\rm th}(m\ )]\bar{\rm\rm\,W}\ [E^{\rm exp}\ \ \ \ \ E^{\rm th}(m\ )] \tag{6.2}\]where \(E^{th}(m)\) contains the theoretical edge values for a set of masses. The weight matrix \(W\) is the inverse of the correlation matrix or enorm matrix of the observations, which is given by the variances and covariances of the endpoint measurements,

\[\begin{array}{l}(W^{1})_{ii}=\sum_{ii}^{stat}+\sum_{ii}^{scale}=(\sum_{i}^{stat}) ^{2}+(\sum_{i}^{scale})^{2}\\ (W^{1})_{ij}=\sum_{ij}^{scale}=\frac{1}{i!}E_{ij}^{exp}E_{j}^{exp}i\quad hE_{i}^ {exp}iE_{j}^{exp}i=\sum_{i}^{scale}\frac{scale}{j};\quad i6j\\ (W^{1})_{ii}=0;\quad i6\end{array} \tag{6.3}\]

where \(j=1\) refers to \(m\frac{m\max}{1},\) which to a good approximation is uncorrelated with the other measurements. The covariances are similar to the variances in size, and so cannot be neglected. If the endpoint measurements were uncorrelated, W would become diagonal, and the least-squares method would reduce to the normal \({}^{2}\) minimum method.

The ensemble distributions obtained by such a procedure can be interpreted as probability density functions. From these the 'inverse probability problem' can be addressed, which is that of stating something about the true masses on the basis of the ones obtained in one experiment. We will be interested in the mean values of the ensemble distribution, their standard deviations, skewness, as well as the correlation between masses.

### Minima of

Because many endpoints are given by different expressions for different mass regions, see Eqs (4.4)(4.5), the minimisation function is a composite function. For the endpoint measurements used in this paper, is made up of nine individual functions, \((i,j)\), one for each of the nine regions (i,j). Considered separately each function \((i,j)\) has one or more minima. For these to also be minima of the composite function (physical minima'), they need to be situated in the region of validity (home region') (i,j) of the corresponding function. Physical minima can also occur on the borders between regions, in which case they will be referred to as borderline minima'.

If the threshold endpoints are left out, there are four measurements for four masses. The clear failure of the endpoint measurements of SPS 1a ( ) and ( ) to comply with Eq. (4.11) already discards the three regions where these four measurements are not so - cient to determine the masses. In each of the other six regions the minima can be sought by use of the inversion formulae. Such solutions correspond to \(=0\). In cases where no physical solutions are found in this way, border minima exist at \(>0\), and will have to be found by a least-square minimisation.

When the threshold measurement is included, the system of equations becomes overconstrained. This will give a non-uniform increase in the value of, which may destroy or create minima. Another effect will be to move the minima of around in mass space, possibly moving them into or out of their home regions. One way to picture the effect is to "tune in" the new measurement by letting its uncertainty go from infinity, in which case the measurement has no effect, to the value specified in Table 4. The masses and height of each minimum will then move continuously from the old to the new position.

Even though composite, is continuous, so its realisations in two neighbouring regions attain the same value at their common border. Assume that the endpoint measurements

[MISSING_PAGE_FAIL:43]

would thus be two solution sets, eg. for 1 we will have two solutions in 12% of the experiments. Whether or not it would be possible to select one of the solutions, and preferably the correct one, hinges on other measurements. In this case, where the masses of the two sets are quite close, they might be very difficult to distinguish, by eg. cross-section considerations.

The upper part of Table 6 shows the ensemble means of the masses, \(\mathrm{m}\), the root-mean-squared distances from them,, and skewness 14 of the two solutions for 1. The values are relatively stable with respect to the cut: The same table for 99 would for (1,1) show a decrease in themselves by 0.1{0.2 GeV, and for (1,2) an increase by 1{1.3 GeV.

Footnote 4: Skewness is defined by 1 = 3+( 2)^2 = 3+ 3+, where 1 = (x x)\({}^{i}\) is the \(i^{th}\) moment about the mean.

The inclusion of the threshold measurement has very little e ect on the ensemble values of the (1,1) solution. For the (1,2) solution, to better comply with the additional measurement, the masses have increased, and are now 10{15 GeV below the nominal ones. Also the \(m_{\mathrm{D}}\) threshold was included in the \(i^{th}\) returned the values of Table 6. It is measured with much less precision than the other endpoints, so its inclusion has practically no e ect on the other masses, only on \(m_{\mathrm{b}}\), for which it is the only measurement here.

The fact that the ensemble means of (1,1) reproduce the nominal values, relates to the good average performance of the ensemble of experiments. The probability of doing well with only one experiment relies in addition on the spread of the ensemble values, given by. For SPS 1a ( ) the high precision of the endpoint measurements translates into rather small values. From the table we see e.g. that in 68% of the experiments the mass of \(\frac{0}{1}\) from the (1,1) solution will lie within 3.8 GeV of the nominal value. The root-mean-square

\begin{table}
\begin{tabular}{|c c|c c c|c c c|} \hline  & & \multicolumn{3}{c|}{(1,1)} & \multicolumn{3}{c|}{(1,2)} \\  & N cm & \(\mathrm{m}\) i & & 1 & \(\mathrm{m}\) i & & 1 \\ \hline \(m_{\mathrm{b}}\) & 96.1 & 96.3 & 3.8 & 0.2 & 85.3 & 3.4 & 0.1 \\ \(m_{\mathrm{b}}\) & 143.0 & 143.2 & 3.8 & 0.2 & 130.4 & 3.7 & 0.1 \\ \(m_{\mathrm{b}}\) & 176.8 & 177.0 & 3.7 & 0.2 & 165.5 & 3.4 & 0.1 \\ \(m_{\mathrm{b}}\) & 537.2 & 537.5 & 6.1 & 0.1 & 523.2 & 5.1 & 0.1 \\ \(m_{\mathrm{b}}\) & 491.9 & 492.4 & 13.4 & 0.0 & 469.6 & 13.3 & 0.1 \\ \hline \(m_{\mathrm{b}}\) & \(m_{\mathrm{-}}\)\({}_{1}^{0}\) & 46.92 & 46.93 & 0.28 & 0.0 & 45.08 & 0.72 & 0.2 \\ \(m_{\mathrm{-}}\)\({}_{2}^{0}\) & \(m_{\mathrm{-}}\)\({}_{1}^{0}\) & 80.77 & 80.77 & 0.18 & 0.0 & 80.18 & 0.29 & 0.1 \\ \(m_{\mathrm{b}}\) & \(m_{\mathrm{-}}\)\({}_{1}^{0}\) & 441.2 & 441.3 & 3.1 & 0.0 & 438.0 & 2.7 & 0.0 \\ \(m_{\mathrm{b}}\) & \(m_{\mathrm{-}}\)\({}_{1}^{0}\) & 395.9 & 396.2 & 12.0 & 0.0 & 384.4 & 12.0 & 0.1 \\ \hline \end{tabular}
\end{table}
Table 6: SPS 1a ( ): Min a for 1 in regions (1,1) and (1,2). Ensemble means, \(\mathrm{m}\) i, and root-mean-square distances from the mean,, are in GeV. The three lightest masses are very correlated. The mass of \(\mathrm{q}\), is fairly correlated to the lighter masses, but \(m_{\mathrm{b}}\) is essentially uncorrelated. The distributions are very close to symmetric.

\begin{table}
\begin{tabular}{|c|c|c|} \hline  & \# Min a & (1,1) & (1,2) \\ \hline
0 & 1.00 & 90\% & 10\% \\
1 & 1.12 & 94\% & 17\% \\
3 & 1.30 & 97\% & 33\% \\
99 & 1.88 & 99\% & 88\% \\ \hline \end{tabular}
\end{table}
Table 5: Number of minima for various cuts and their whereabouts.

[MISSING_PAGE_EMPTY:45]

squark masses are less correlated. Also the results in region (1,2) are closer to (1,1) and the nominal ones when considering mass differences. Fig. 15 shows the ensemble distributions corresponding to Table 6.

Because of this strong correlation between the masses, not only the mean values and their 1 uncertainties, but the entire error matrix should be considered if one wants to use the result obtained by this method as input for other analyses.

A less involved solution would be to use less dependent variables, e.g., \(m_{1}^{\circ}\) to set the scale, then differences for the remaining masses, \(m_{1}^{\circ}\), \(m_{2}^{\circ}\), \(m_{2}^{\circ}\), \(m_{3}^{\circ}\), \(m_{4}^{\circ}\), \(m_{5}^{\circ}\) and \(m_{6}^{\circ}\), \(m_{7}^{\circ}\).

Due to the high cross-section, most of the endpoints are determined with high precision, which in turn gives narrow and approximately symmetric ensemble distributions. The masses are thus determined with quite high precision. As a result of the strong correlations between in particular the lighter masses, even better estimates can be obtained for other combinations of the variables, e.g. mass differences. At SPS 1a ( ) there is however a fair chance that two sets of masses do equally well in the minisation procedure. Other considerations must in that case be made in order to choose between them, or both must be kept.

### SPS 1a ( )

In combination with the theory plots of Section 4 we found in Section 5 that the \(m_{q1(high)}^{\max}\) value of ( ) would not probably on the average be underestimated by 11(13 GeV. We will not that this has a dramatic effect on the masses returned. However, for easier comparison with nominal values we first consider the situation without such a systematic effect. Then, afterwards, the impact of the mismasurement will be shown, together with a way to mend the situation.

As in the previous case we start out without the threshold measurement. Also here two solutions are available for all experiments, one in (1,1), the other in either (1,2) or (1,3). The nominal region for ( ) is (1,2), but it is quite close to (1,3). This becomes clear from an inspection of the border parameter,

\[b=\frac{m_{k}^{2}}{m_{-1}^{\circ}m_{-2}^{\circ}}; \tag{6.4}\]

which is 1 on the border between these two regions. For ( ) we have \(b=102\). If \(m_{k}\) is reduced by 2.5 GeV, this ratio becomes unity and the the mass set sits on the border, see Eq. (4.5). The other point, ( ), was near the border between (1,1) and (1,2). There, both solutions (or none) were available. Here, the derivatives of \(m_{k}\) are such that only one of the two solutions is available. In 71% of the cases we get (1,2), in 29% we get (1,3). While the 'lowmass' solution, (1,2) or (1,3), is in the vicinity of the nominal masses, the (1,1) solution, which is always present, usually sits at much higher masses, \(m_{-1}^{\circ}i=514\) GeV. Because the two solutions are so separated one may hope that the incorrect one will be sufficiently disfavoured by other measurements, e.g. cross-sections, that it can be discarded.

For ( ) the solution in the nominal region on the average reproduced the nominal values to within 0.2(0.5 GeV. Here the (1,2)/(1,3) solution has a mean of \(m_{-1}^{\circ}\) at 183 GeV,same 22 GeV above the normal value. The most probable value of the ensemble distribution is much closer to the nominal value. The distributions are infested with considerable skewness. On the way from endpoint measurement to mass determination a systematic effect which favours higher masses has been introduced. In statistical language our estimators of the true masses are not consistent: they do not converge to the nominal values. This of course has implications for the interpretation of the masses we obtain. What can be said about the true masses on the basis of the measured ones? We will return to the reasons for the skewness later.

When the threshold measurement is included, the (1,1) minimum usually yields a large value. Only in a small fraction of the experiments does it challenge the other minima. In the other sector there is either one minimum, positioned in (1,2), (1,3) or on the border (B), or there are two minima, in (1,2) and (1,3). These minima are usually in good agreement with the threshold measurement and have low values.

\begin{tabular}{|c|c|c|c|c|c|c|} \hline  & & & & & 1 sol & 2 sol \\  & \# Min & (1,1) & (1,2) & (1,3) & B & (1,2)\& (1,3) \\ \hline
0 & 1.0 & 3\% & 60\% & 25\% & 12\% & 0\% \\
1 & 1.2 & 5\% & 52\% & 18\% & 12\% & 16\% \\
3 & 1.4 & 13\% & 46\% & 14\% & 12\% & 28\% \\
99 & 2.3 & 99\% & 41\% & 13\% & 12\% & 34\% \\ \hline \end{tabular}

Table 7: SPS 1a ( ): A average number of minima and the fraction of experiments with the special solution types, for different cuts.

Table 7 shows the average number of minima for different cuts. The three rightmost sets show the fraction of experiments which have the specified solution type. The two rightmost sets exclude one another. Either there is one solution in the low-mass sector, or there are two. For the one-solution case the threezehouts of the minimum is also shown. The home region of the nominal masses, (1,2), is seen to dominate. As it may well be possible to discard the (1,1) minimum on the basis of other observations, it is logically separated from the low-mass minima. Eg. in 13% of the cases, regardless of the low-mass solution type, there is a (1,1) minimum at 3. To get the average number of minima shown in column 2, sum horizontally, adding twice the two-solution percentage. For small cuts the two rightmost sets do not add up to 1. This simply means that in some cases the global minimum lies in (1,1), and no low-mass minimum is available in the given range. For the current set of endpoint measurements the (1,1) contamination is seen to be very moderate. However, the systematic error (column 6 of Table 4) is here assumed to be zero. If it should become in possible to obtain the threshold value with such optimistic precision, the fraction of (1,1) solutions at low will grow rapidly.

In Table 8 1, and 1 of the ensemble masses and mass differences are shown for the different solution types and the cut. The masses of the (1,1) solution are much higher than what the low-mass minima give. Even though the distributions are broad, allowing for low values to occur, it is very rare that the masses stretch down to the low-mass sector. In section 4 of the table the low-mass one-solution values are shown. Sincein such a case only one acceptable solution is available (discarding (1,1)), and since ( ) anyway is situated close to the border, it makes sense to show the combined distribution of the (1,2), (1,3) and border (B) minima. From Table 7 this situation is seen to occur in 52% + 18% + 12% = 82% of the experiments. The mean values of the masses lie same 15 GeV above the nominalones. This is an improvement compared to the non-threshold situation, but it remains an undesirable feature. The mass distributions are skewed and the most probable value is found close to the nominal value. The root-mean-square values of the ensemble distributions are large, nearly an order of magnitude larger than at ( ).

The rightmost sections show the values for the two-solution type. The ensemble means of the two distributions do not differ too much, and they are much closer to the nominal values than is the case for the one-solution type. Since the values are rather close, it will probably be quite difficult to find other measurements which favours one of the sets. On the other hand, since the root-mean-square values are larger than the differences between the two solutions (also within one experiment), taking the average value, perhaps somehow weighted with the value, might be a possible compromise.

Again, the mass correlation is very strong. This is evident from the lower part of Table 8, where the ensemble distributions of mass differences come with much smaller root-mean-square distance to the mean values than what the masses themselves do. They are also very close to the nominal values. Even more seems to be gained by using mass differences here than at ( ). Fig. 16 show the ensemble distributions for the masses of all solutions with 1 and which lie in the regions (1,2), (1,3) or on their common border. See figure caption for details.

Skewness

The ensemble distributions are not symmetric. While the most probable values are close to the nominal values, the means lie above. For ( ) the tendency of such an asymmetry is

\begin{table}
\begin{tabular}{|r|r|r r r r|r r r|r r r r|} \hline  & & \multicolumn{4}{|c|}{1 solution} & \multicolumn{4}{|c|}{2 solutions} \\ \cline{3-13}  & & \multicolumn{4}{|c|}{(1,1)} & \multicolumn{4}{|c|}{(1,2)/(1,3)/B} & \multicolumn{4}{|c|}{(1,2)} & \multicolumn{4}{|c|}{(1,3)} \\ \hline  & N \(\alpha\)m & \multicolumn{1}{|c|}{\(\mathrm{m}\,\mathrm{i}\)} & \multicolumn{4}{|c|}{1} & \multicolumn{1}{|c|}{\(\mathrm{m}\,\mathrm{i}\)} & \multicolumn{4}{|c|}{\(\mathrm{m}\,\mathrm{i}\)} & \multicolumn{4}{|c|}{\(\mathrm{m}\,\mathrm{i}\)} & \multicolumn{4}{|c|}{\(\mathrm{m}\,\mathrm{i}\)} & \multicolumn{4}{|c|}{\(\mathrm{m}\,\mathrm{i}\)} \\ \hline \(\gamma_{1}^{0}\) & 161 & 438 & 88 & 0.9 & 175 & 35 & 1.0 & 161 & 22 & 0.3 & 166 & 27 & 0.6 \\ \(\gamma_{2}^{0}\) & 222 & 518 & 85 & 0.7 & 236 & 37 & 0.8 & 221 & 24 & 0.3 & 223 & 28 & 0.5 \\ \(\gamma_{2}^{0}\) & 299 & 579 & 85 & 0.7 & 313 & 35 & 1.0 & 299 & 22 & 0.3 & 304 & 27 & 0.6 \\ \(\alpha_{1}\) & 826 & 1146 & 104 & 0.8 & 843 & 44 & 0.9 & 826 & 30 & 0.3 & 835 & 36 & 0.5 \\ \hline \(\gamma_{3}^{0}\) & 61 & 81 & 1.8 & 0.3 & 61 & 4.4 & 0.4 & 61 & 1.9 & 0.2 & 57 & 1.3 & 0.2 \\ \(\gamma_{2}^{0}\) & 138 & 141 & 0.9 & 0.1 & 138 & 0.6 & 0.2 & 138 & 0.5 & 0.0 & 138 & 0.5 & 0.0 \\ \(\alpha_{1}\) & 665 & 708 & 17 & 0.1 & 668 & 10 & 0.5 & 665 & 9 & 0.1 & 669 & 10 & 0.2 \\ \hline \end{tabular}
\end{table}
Table 8: SPS 1a ( ): N \(\alpha\)m \(\gamma\) and \(\gamma\) ensemble distribution values for the three solution types. High-mass sector: The (1,1) solutions return masses far beyond the nominal values. Low-mass sector: For the one-solution case the values are based on the common distribution of (1,2), (1,3) and border (B) solutions. In the two-solution case the ensemble variables of both solutions are shown. Ensemble means, \(\mathrm{m}\,\mathrm{i}\), and root-mean-square values,, are in GeV.

[MISSING_PAGE_EMPTY:49]

when the unphysical (1,2) solutions are replaced by the physical (1,3) solutions which lie at higher masses, the ensemble mean increases. Here, this effect brings the average value for \(m_{-1}\) to 183 GeV, an additional increase of nearly 20 GeV.

When the threshold measurement is added, the border effect is reduced, giving an average value of 173 GeV (for one solution). This is because a measurement will, unless there is any bias, on the average be conservative. It will try to keep the masses at their nominal values. Here, as the border effect is pushing the masses upwards, away from the nominal values, the threshold is holding back.

If, in a realistic situation, a set of endpoints has been measured, and the resulting set of masses is found to lie close to a border, caution should be exercised. Ad hoc procedures seem necessary for such a case. At least one is in a position to be aware of the danger. It is probably appropriate to consider unphysical minima as well.

Mism measured \(m_{\rm qln(high)}^{\rm\,max}\)

As pointed out at the start of this subsection it is very likely that, for \((\ ),m_{\rm qln(high)}^{\rm\,max}\) will be underestimated by 11(13 GeV. Without the threshold measurement, the effect of this as returned by the inversion formulae, is an increase of 50 GeV for the three lightest masses and 60 GeV for the squark! If the threshold measurement is included, the effect is reduced to 30(40 GeV for the lighter masses, depending on the solution type, and 40(55 GeV for the squark. The increase is still surprisingly large and represents a serious threat to the applicability of the method in the case where the nominal masses happen to sit near a border. (Not to be confused with the previously mentioned border effect.)

Figure 17: Border effect between region (1,2) and (1,3); the mass of \(\sim_{1}^{0}\) as a function of the border parameter b, see Eq. (6.4), demonstrating border effects. Filled boxes represent physical solutions, empty boxes represent unphysical solutions.

[MISSING_PAGE_EMPTY:51]

[MISSING_PAGE_FAIL:52]

neutralino \(\sim_{1}^{0}\) is the LSP in most interesting NUGRA scenarios, it will escape detection and only the quark and two leptons are available for the construction of invariant mass distributions. Nevertheless, the kinematic endpoints of these distributions have a well defined dependence on them asses of the particles in the decay chain and their measurement allows the extraction of the masses either by analytic inversion or numerical. The analytic expressions for the endpoints in terms of the masses were confirmed and presented together with their analytic inversions.

In order to measure the endpoints of the invariant mass distributions pertaining to the chosen decay chain, one must have the correct mass hierarchies for the decay chain and a large enough cross-section. To ensure that this decay chain could be used over a wide range of scenarios, we performed a scan over the SUSY parameters. We found that as long asm \({}_{0}\) was not too large in comparison to m\({}_{1=2}\), a large proportion of the allowed parameter space would display the correct mass hierarchy. Furthermore, on examination of the sparticle production cross-sections and decay branching ratios we found that a large cross-section for the decay was available over much of this region. The Snowmass m SUGRA SPS 1a line/point falls into this region and is a good candidate for study. However, we noted that the cross-section for the decay chain is particularly high for the SPS 1a point, and it is instructive to examine a second point on the SPS 1a line with a less optimistic cross-section. We have denoted this new points as SPS 1a ( ) while the original point became SPS 1a ( ).

The LHC measurements of the endpoints were simulated using PYTHIA and ATLFAST. Had kinematic cuts remove practically all Standard Model backgrounds, except tt. Up to statistical fluctuations the powerful element- avev subtraction then cancels the remaining tt as well as any lepton-uncorrelated background from other SUSY channels. The resulting distributions are however contaminated by lepton-correlated background from \(\sim_{2}^{0}\)'s not taking part in the decay chain under study and combinatorial background from choosing the wrong jet. The inconsistency cut was shown to address the latter part with greater efficiency, giving distributions much closer to the theoretical ones. Also mixed events were studied, revealing their potential to describe the background. More study is however needed.

The endpoints were found by simplistic fitting of the edges, usually with a straight line together with a reasonable background estimate. A Gaussian smearing was sometimes included as a first approximation of the various smearing effects which take place. The statistical precision of the edge was sought rather than an accurate determination of the endpoint. Still, the endpoints were seen to be in reasonably good agreement with the nominal values. However, the fitting procedure is clearly an area for improvement. On one hand, more realistic study of how the detector a _ects the distributions, and in particular the end regions, is called for. On the other hand, further study of the many realisations of the theory curves seems necessary. It is important to find good \(t\) functions for the signal. A central part of such a programme is the incorporation of multiple squark masses at different rates. At a less ambitious scale the theory distributions should be studied for shear acquaintance. The importance of such an awareness was demonstrated for the \(\mbox{m}_{\mbox{\tiny{q(high)}}}^{\mbox{\tiny{max}}}\) measurement at ( ).

In order to turn the endpoint measurements into particle masses, and understand the resulting errors of this procedure, we considered an ensemble of 10,000 _tgedanken'_ experiments. For each experiment a numerical \(t\) for the particle masses was performed, using the method of least squares, thus appropriately handling the correlation between measurements due to the common jet energy scale error. Where available the analytic expressions for the masses in terms of the endpoints were used to provide starting points for the _ts_.

The least squares function was found to often have two or even three minima of comparable importance, a consequence of the multiple realisations for many of the endpoints. Without the threshold measurement there are for both scenarios usually two equally good minima, one in the correct region and one in another region, giving different masses. When the threshold endpoint is added, the minimum in the correct region is usually preferred. Still, in a noticeable fraction of experiments there will be more than one solution. Due to less precise measurements this applies more to ( ) than to ( ).

At ( ) the minima of the correct region gives masses very close to the nominal values. The other (incorrect) region gives masses some 10(15 GeV lower. The ensemble distributions are symmetric. At ( ) there is one high-mass solution. The more precisely the threshold endpoint is determined, the less important this false minimum becomes. The low-mass solutions, one or two, are closer to the nominal values, but the distributions are skewed. This is a combined effect of the large endpoint uncertainties and the so-called border etc.

The obtained masses of the three lightest particles are found to be very strongly related. Furthermore one it was seen that mass differences are better variables, in the sense that they are less correlated than themselves themselves. Due to the form of the endpoint expressions, the LHC will measure mass differences at high precision, but leave the overall scale less certain. A Linear Collider measurement of the LSP mass effectively sets the scale, which is why the precision of the masses in proved drastically when the LHC and the Linear Collider measurements are combined.

### Acknowledgments

This work has been performed partly within the ATLAS Collaboration, and we thank collaboration members for helpful discussions. We have made use of the physics analysis framework and tools which are the result of collaboration-wide e orts. It is a great pleasure to thank Giacomo Polesello for his contributions in the early stages of this work, and continued interest and advice. BKG would like to thank Steiner Stapnes for useful discussions. DJM would like to thank Ben Allanach for useful discussions. This research has been supported in part by the Research Council of Norway.

## References

* [1] S. Weinberg, Phys. Rev. D 13 (1976) 974; Phys. Rev. D 19 (1979) 1277; L. Susskind, Phys. Rev. D 20 (1979) 2619; G. 't Hooft, in Recent developments in gauge theories, Proceedings of the NATO Advanced Summer Institute, Cargese 1979, ed. G. 't Hooft et al. (Plenum, New York 1980).
* [2] P. Fayet and S. Ferrara, Phys. Rept. 32 (1977) 249.
* [3] S. Dimopoulos and H. Georgi, Nucl. Phys. B 193 (1981) 150.
* [4] H. P. Nilles, Phys. Rept. 110 (1984) 1.
* [5] H. E. Haber and G. L. Kane, Phys. Rept. 117 (1985) 75.
* [6] R. Haag, J. T. Lopuszanski and M. Sohnius, Nucl. Phys. B 88 (1975) 257.
* [7] M. B. Green, J. H. Schwarz and E. Witten, 'Superstring Theory. Vol. 1: Introduction,' Cambridge University Press (1987) (Cambridge Monographs On Mathematical Physics).
* [8] N. Sakai and T. Yanagida, Nucl. Phys. B 197 (1982) 533.
* [9] K. Inoue, A. Kakuto, H. Komatsu and S. Takeshita, Prog. Theor. Phys. 68 (1982) 927 [Erratum-ibid. 70 (1983) 330]; Prog. Theor. Phys. 71 (1984) 413.
* [10] N. K. Fakk, Z. Phys. C 30 (1986) 247.
* [11] L. E. Ibanez and G. G. Ross, Phys. Lett. B 110 (1982) 215.
* [12] L. E. Ibanez, Phys. Lett. B 118 (1982) 73.
* [13] J. R. Ellis, D. V. Nanopoulos and K. Tam vakis, Phys. Lett. B 121 (1983) 123.
* [14] L. Alvarez-Gaume, J. Polchinski and M. B. Wise, Nucl. Phys. B 221 (1983) 495.
* [15] H. Gokhberg, Phys. Rev. Lett. 50 (1983) 1419; L. M. Krauss, Nucl. Phys. B 227 (1983) 556; J. R. Ellis, J. S. Hagelin, D. V. Nanopoulos, K. A. Olive and M. Srednicki, Nucl. Phys. B 238 (1984) 453.
* [16] F. Gianotti, New J. Phys. 4 (2002) 63.
* [17] B. C. Allanach, G. A. Blair, S. Kram, H. U. Martyn, G. Polesello, W. Porod and P. M. Zerwas, arXiv:hep-ph/0403133.
* [18] A. H. Chamseddine, R. Amowitt and P. Nath, Phys. Rev. Lett. 49 (1982) 970.
* [19] V. M. Lobashev et al., Phys. Lett. B 460 (1999) 227.
* [20] ATLAS Collaboration, ATLAS Detector and Physics Performance Technical Design Report 2. No. CERN-LHCC-99-014 ATLAS-TDR-14. May,1999.
* [21] B. C. Allanach et al., in Proc. of the APS/DPF/DPB Summer Study on the Future of Particle Physics (Snowmass 2001) ed. N. Graf, Eur. Phys. J. C 25 (2002) 113 [eConf C 010630 (2001) P125] [arXiv:hep-ph/0202233].