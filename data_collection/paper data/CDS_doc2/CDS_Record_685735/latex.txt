**ATLAS Internal Note DAQ-No-19 EAST Note 94-34 1 December 1994**

SCI with DSPs and RISC Processors for LHC 2nd Level Triggering

P.E.L.Clarke, R.Cranfield, G.J.Crone

University College London, Gower Street, London, WC1E 6BT, UK.

B.J.Green, J.A.Strong

Royal Holloway, University of London, Egham, Surrey, TW20 0EX, UK.

R.E.Hughes-Jones, S.Kolya, R.Marshall, D.Mercer

University of Manchester, Manchester, M13 9PL, UK.

K.Korcyl

Institute of Nuclear Physics, Cracow, Poland.

R.Hatley, R.P.Middleton, F.J.Wickens

Rutherford Appleton Laboratory, Chilton, Didcot, Oxon, OX11 0QX, UK.

A.Guglielmi

Digital Joint Project - CERN, \(\mbox{$\mbox{}^{\mbox{c}}$/$\mbox{}^{\mbox{o}}$}\) CERN, Bldg 513 1-026, CH-1211 Geneve 23, Switzerland.

## Introduction

Detectors at the Large Hadron Collider (LHC) at CERN will have to handle raw data rates of order \(10^{15}\) bytes per second. To this end, a 3-level trigger system [1] is under study to reduce this rate by \(10^{7}\), so that only interesting event data from proton - proton collisions is recorded for subsequent physics analysis. The first level, consisting of custom designed hardware is expected to reduce the rate by a factor of \(10^{4}\), the second level by a factor of \(10^{2}\) and the third level by a factor of \(10\).

This paper is concerned with studies of a candidate level-2 system based on particular choices of technology.

## Architecture

The level-2 system (see figure 1) is split into local and global parts. A processor in the local system are used to process data from a specific, small region of a detector. whilst a global processor is used to process data derived from all regions of all detectors

In the local part, guidance is taken from the level-1 system to extract fine grain raw data from the level-2 buffer to produce specific regions of interest (RoI) of the detector. The data are processed in feature extractors (FEX) to produce a feature (e.g. for a calorimeter this would be a cluster energy and position with some associated 'particle' classification).

The resulting features from all detectors participating in the level-2 system are then gathered together in a data concentration phase prior to passing through a network to the global sub-system. Here, features from different detectors which correspond to the passage of a particular particle or jet through the detector are combined and a probable particle identification assigned. Certain physical quantities are then evaluated for use in classification of events according to topology and likely under-lying physics processes. The end result is a decision as to whether to accept or reject the data.

The essential features to note are :-

* Localised processing of fine grain raw data
* Reduction of required level-2 system input bandwidth by limiting processing to specified Regions of Interest.
* Parallel processing in both local and global sub-systems to achieve a design decision frequency of

Figure 1: Functional Architecture of a Level-2 System\(10^{5}\) Hz, but with a processing time of a few milliseconds for each event.
* System scalabilty to track the evolution of algorithms and corresponding physics goals

## Initial Studies

### Selection of Technologies

The Texas Instruments TMS320C40 digital signal processor [2] is a leading floating point processor designed primarily for image processing applications. It has a simple (RISC-like) instruction set, parallel operations and six 20 Mbyte/s communications links (each supported by a separate DMA channel). The combination of a high performance processor with integral communication capabilities makes it an excellent choice for both FEX processor, data routing

### Building Blocks

Commercial units, namely DBV42 & DBV44 modules [4], provided a maximum of either two or four C40s per card, and were used for all C40 processors. The level-2 buffer [5] was implemented as a sister module to a C40 board and was designed to operate at up to 100 kHz.

All SCI nodes were custom built, using SCI NodeChips(TM)1[6] and high performance fifos to decouple the processor bus from the NodeChip. Figure 2 shows a block diagram of one of two types of interface used. To ensure deadlock free operation of both request and response sub-actions, four fifos were used. The SCI standard defines specific packet formats for each transaction type.

Footnote 1: NodeChip is a trademark of Dolphin Interconnect Solutions

A processor wishing to initiate a transaction constructs a packet in the Request Output fifo and then initiates packet transmission through the NodeChip. Responses from remote nodes return to the originating node and pass through the Response Input fifo to be handled by the processor.

A node also responds to an external request received through the Request Input fifo and returns any data through the Response Output fifo.

The SCI interface logic described was implemented on a 6U Eurocard and was combined with additional logic to interface either to VME (as in figure 2) or to the C40 global bus. The SCI to VME interface, thus formed, was used both with an embedded VME controller and through a memory mapped interface into a DEC Alpha system.

Figure 2: Block Diagram of an SCI Node

### Test Set-up

Figure 3 shows the interconnection of modules used to build the first test system. Data was derived from a HIPPI spy unit which formed part of a router system [7] placed in the data readout path of a Transition Radiation Detector (TRD) [8]. Two channels were equipped to feed data into two level-2 buffers under the supervision of a C40 based buffer manager.

Data was fed through a link unit to enable routing of data to the appropriate FEX (again both C40 based). After feature extraction (a null algorithm in initial tests) the data was concentrated in the global gateway (another C40) where it was sent on to a SCI node for final processing by a global processor. Events were recorded at the global gateway by an

Figure 4: SCI Move64 Timing Trace

Figure 3: Beam Test Set-upOS9 system and at the target SCI node consisting of a DEC Alpha processor [9] running the VxWorks2 real-time kernel.

Footnote 2: VxWorks is a trademark of Wind River Systems Inc.

The whole system was self-triggered by the passage of an event on the RD-6 HIPPI lines.

The C40 sub-system was controlled from a PC through a daisy-chained JTAG interface. Nodes in the SCI sub-system were each self-configuring.

### Tests

The complete system, consisted of two level-2 buffers, one link unit, two FEXs, global gateway and a four node SCI ring. It was first successfully operated in the ATLAS test beam line at CERN during September/October 1994 using data from the TRD of the RD-6 collaboration. Parasitic operation ensured minimal disruption to other parts of the ATLAS tests and enabled the work programme to remain independent of other activities. Invaluable experience was gained in integrating with real detectors providing real data.

It is believed that this is the first time that 'live' detector data has been passed round an SCI ring at a beam line.

Figure 4 shows a logic analyser trace of a move-64 byte transaction between two SCI nodes (64 bytes of user data with a 16 byte header). The time taken for the transmitting node to send the move request on to the SCI ring and receive the response from the remote node was 2.1\(\upmu\)s. The time from when the data is placed on to the SCI to when the data is clocked into the receiver's fifo is 1.48\(\upmu\)s. The SCI ringlet was occupied for 850ns in transmitting the request.

## Future Plans

It is planned to expand the scope of the project in the near future by enlarging the system to accept data from more than one detector and to field an adequate set of RISC processors to handle the data. Proper feature extraction algorithms will be implemented, tailored to individual detectors. Features from different detectors for the same RoI must be matched and subsequently processed by a global algorithm to yield an overall Level-2 decision.

## Acknowledgements

The authors would like to extend thanks to the RD-6 collaboration for permitting data access through a spy mechanism, to colleagues in the University of Jena and JINR Dubna for use of the router system and to the ATLAS test beam organisers for the tests. The authors would also like to thank colleagues in the RD-24 and EAST collaborations at CERN for their help and support and Digital Equipment Corporation (through the CERN-DEC Joint Project Office at CERN) for their collaboration in the development of the SCI sub-system and Alpha processing node. Financial support from the UK Particle Physics and Astronomy Research Council for this project and partial support for one of the authors (KK) from the Polish State Committee for Scientific Research (grant No. 2 P302 047 06) is gratefully acknowledged.

## References

* [1] ATLAS Collaboration Letter of Intent, CERN/LHCC/92-4, Chapter 5, p67.
* [2] TMS320C40 Users Guide, Texas Instruments.
* [3] "SCI, Scalable Coherent Interface", IEEE standard 1596-1992.
* [4] DBV42 & DBV44 Technical Reference Manuals, Loughborough Sound Images Ltd, Loughborough, UK.
* [5] A Second Level Data Buffer with LHC Performance; B.J.Green et al; 6th Pisa meeting on Advanced Detectors
* [6] L64601 SCI NodeChip Technical Manual, LSI Logic Corporation.
* [7] CERN/EAST note 92-09.
* [8] RD-6 Collaboration, CERN/DRDC/P8.
* [9] Alpha Architecture Reference Manual, ISBN 1-55558-098-X, Digital Equipment Corporation.