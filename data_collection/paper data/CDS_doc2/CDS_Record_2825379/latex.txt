# Point Cloud Deep Learning Methods for Pion Reconstruction in the ATLAS Experiment

The ATLAS Collaboration

The reconstruction and calibration of hadronic final states in the ATLAS detector present complex experimental challenges. For isolated pions in particular, classifying \(\pi^{0}\) versus \(\pi^{\pm}\) and calibrating pion energy deposits in the ATLAS calorimeters are key steps in the hadronic reconstruction process. The baseline methods for local hadronic calibration were optimized early in the lifetime of the ATLAS experiment. This note presents a significant improvement over existing techniques using machine learning methods that do not require the input variables to be projected onto a fixed and regular grid. Instead, Transformer, Deep Sets, and Graph Neural Network architectures are used to process calorimeter clusters and particle tracks as point clouds, or a collection of data points representing a three-dimensional object in space. This note demonstrates the performance of these new approaches as an important step towards a low-level hadronic reconstruction scheme that fully takes advantage of deep learning to improve its performance.

## 1 Introduction

A fundamental task in hadronic final state reconstruction in the ATLAS detector [1] is the identification and calibration of the detector response to single particles. As the hadronic showers produced in proton-proton (\(pp\)) collisions at the CERN Large Hadron Collider (LHC) are primarily generated by pions, it is essential to accurately characterize the calorimeter response to both charged and neutral pions.

Neutral pions (\(\pi^{0}\)) decay promptly to photons and develop compact showers with relatively small intrinsic fluctuations. These showers are mostly captured by the electromagnetic calorimeter. On the other hand, showers emanating from charged pions (\(\pi^{\pm}\)) generally fluctuate more dramatically in the course of their development. They also penetrate deeper into the detector than electromagnetic showers, thereby necessitating an additional hadronic calorimeter outside of the electromagnetic calorimeter to measure their energy deposits.

In the ATLAS experiment, three-dimensional clusters of topologically-connected calorimeter cell signals called topo-clusters are employed as the baseline signal definition used in the reconstruction of hadronic final states. These hadronic final states comprise isolated hadrons and jets in addition to full-event observables such as missing transverse momentum (\(E_{T}^{\rm miss}\)) [2].

The ATLAS calorimeters, described in Section 2, are non-compensating. Their response to hadrons is smaller than their corresponding response to electrons and photons depositing the same amount of energy. The hadronic calibration for topo-clusters, as described in Section 3.2, is a multi-step process called Local Cell Weighting (LCW) that aims to correct this non-compensating calorimeter response to hadrons.

In the past few years, machine learning has transformed various methods designed for hadronic final states at the LHC [3] such as pile-up mitigation [4], jet calibration [5; 6], calorimeter simulation [7], hadronic tau identification [8; 9], jet tagging [10; 11; 12; 13], and background estimation [14]. A variety of studies using simplified detector geometries have investigated the use of deep learning for separating charged and neutral pions as well as calibrating the pion energy response [15; 16; 17]. Machine learning techniques designed for point cloud data such as Deep Sets [13], Graph Neural Networks [18; 19; 20], and Transformers [21; 22; 23] have also been developed for various tasks related to hadronic final states at the LHC. Recent reviews of machine learning for high energy physics in general can be found in References [24; 25; 26].

As described in Section 4.3, deep learning techniques including densely-connected neural networks (DNNs) and convolutional neural networks (CNNs) were recently explored for image-based pion classification and energy regression in the context of the complex ATLAS detector geometry in the central barrel region (\(|\eta|<0.7\)) [27]. In these studies, single pion events were presented to the DNN and CNN as a series of images with nonuniform geometries reflecting the varying granularities of each calorimeter layer. Pixel intensities in each image represented energy deposits in each calorimeter layer.

This note explores new perspectives for pion identification and energy calibration, extending the previous work on deep learning for image-based representations of calorimeter deposits shown in Reference [27]. The results presented here represent pions in a new way that more naturally suits the nonuniform three-dimensional structure of the calorimeter deposits: graph and point clouds representation. This representation also has the advantage of being flexible enough to include new information beyond the calorimeters such as inner detector tracking information. In this study, the pseudorapidity range is extended to \(|\eta|<3\), and results are shown for both calorimeter-only information and for the scenario of combining calorimeter and tracking information. These studies are also a stepping stone toward a future iteration of Particle Flow that can leverage machine learning techniques to better take advantage of complementarydetector information from both the calorimeters and trackers. The methods considered are Deep Sets [28], Graph Neural Networks (GNNs) [29], and Transformers [30].

This note is organized as follows. The ATLAS detector is briefly described in Section 2 and the simulated dataset used for training these deep learning models is described in Section 3. Section 4 introduces the deep learning methods themselves, and the numerical results for classification and energy regression are presented in Section 5. The note ends with conclusions in Section 6.

## 2 ATLAS Detector

The ATLAS detector [1] at the LHC covers nearly the entire solid angle around the collision point.1 It consists of an inner tracking detector surrounded by a thin superconducting solenoid, electromagnetic and hadron calorimeters, and a muon spectrometer incorporating three large superconducting air-core toroidal magnets.

Footnote 1: ATLAS uses a right-handed coordinate system with its origin at the nominal interaction point (IP) in the centre of the detector and the \(z\)-axis along the beam pipe. The \(x\)-axis points from the IP to the centre of the LHC ring, and the \(y\)-axis points upwards. Cylindrical coordinates \((r,\phi)\) are used in the transverse plane, \(\phi\) being the azimuthal angle around the \(z\)-axis. The pseudorapidity is defined in terms of the polar angle \(\theta\) as \(\eta=-\ln\tan(\theta/2)\). Angular distance is measured in units of \(\Delta R\equiv\sqrt{(\Delta\eta)^{2}+(\Delta\phi)^{2}}\).

The inner-detector system (ID) is immersed in a 2 T axial magnetic field and provides charged-particle tracking in the range \(|\eta|<2.5\). The high-granularity silicon pixel detector covers the vertex region and typically provides four measurements per track, the first hit normally being in the insertable B-layer (IBL) installed before Run 2 [31; 32]. It is followed by the silicon microstrip tracker (SCT), which usually provides eight measurements per track. These silicon detectors are complemented by the transition radiation tracker (TRT), which enables radially extended track reconstruction up to \(|\eta|=2.0\). The TRT also provides electron identification information based on the fraction of hits (typically 30 in total) above a higher energy-deposit threshold corresponding to transition radiation.

The calorimeter system covers the pseudorapidity range \(|\eta|<4.9\). Within the region \(|\eta|<3.2\), electromagnetic calorimetry is provided by barrel and endcap high-granularity lead/liquid-argon (LAr) calorimeters, with an additional thin LAr presampler covering \(|\eta|<1.8\) to correct for energy loss in material upstream of the calorimeters. Hadron calorimetry is provided by the steel/scintillator-tile calorimeter, segmented into three barrel structures within \(|\eta|<1.7\), and two copper/LAr hadron endcap calorimeters. The solid angle coverage is completed with forward copper/LAr and tungsten/LAr calorimeter modules optimised for electromagnetic and hadronic energy measurements respectively.

The muon spectrometer (MS) comprises separate trigger and high-precision tracking chambers measuring the deflection of muons in a magnetic field generated by the superconducting air-core toroidal magnets. The field integral of the toroids ranges between 2.0 and 6.0 T m across most of the detector. Three layers of precision chambers, each consisting of layers of monitored drift tubes, covers the region \(|\eta|<2.7\), complemented by cathode-strip chambers in the forward region, where the background is highest. The muon trigger system covers the range \(|\eta|<2.4\) with resistive-plate chambers in the barrel, and thin-gap chambers in the endcap regions.

Interesting events are selected by the first-level trigger system implemented in custom hardware, followed by selections made by algorithms implemented in software in the high-level trigger [33]. The first-leveltrigger accepts events from the 40 MHz bunch crossings at a rate below 100 kHz, which the high-level trigger further reduces in order to record events to disk at about 1 kHz.

An extensive software suite [34] is used in the reconstruction and analysis of real and simulated data, in detector operations, and in the trigger and data acquisition systems of the experiment.

## 3 Datasets

### Monte Carlo Simulations

In this study, the detector response to single pions was studied using the full ATLAS detector simulation [35] based on Geant4 [36]. Single pions were generated starting at the origin of the detector with uniform kinematic distributions in azimuthal angle \(\phi\), pseudo-rapidity with \(|\eta|<3\), and the logarithm of truth pion energy \(E_{\pi}^{\text{true}}\) from 0.2 to 2,000. In total, 10 million \(\pi^{0}\), 5 million \(\pi^{+}\), and 5 million \(\pi^{-}\) events were produced.

The presence of pile-up (multiple \(pp\) collisions in the same LHC bunch crossing) manifests as noise in the calorimeter cell energy measurements. While the simulations used in this note contain only single pion events and no pile-up events, the reconstruction of topologically-connected calorimeter clusters [2] was performed using the expected pile-up noise levels present during 2018 data-taking to account for the effects of noise suppression on the cluster energy scale.

### Input Variables

#### 3.2.1 Topo-clusters

Topo-clusters are topologically-connected groups built out of calorimeter cells that follow cell signal-significance patterns generated by the showers [2]. The clustering algorithm that forms topo-clusters removes cells with insignificant signals that are not close to the cell with significant signal to noise ratio, i.e. signal-significance. The clustering algorithm has predefined minimum signal significances that vary based on what portion of the topo-cluster they represent. For instance, the default ratio threshold of \(S=4\) is used for the seed of a topo-cluster as follows:

\[|E_{\text{seed, cell}}^{\text{EM}}|>S\sigma_{\text{noise, cell}}^{\text{EM}} \rightarrow\frac{|E_{\text{seed, cell}}^{\text{EM}}|}{\sigma_{\text{noise, cell}}^{\text{EM}}}>4\]

Additional significance thresholds are applied to control the topo-cluster growth and the collection of marginal cell signals important for full hadronic energy reconstruction.

Hadrons can generate more than one cluster in their shower development. Once a topo-cluster has been formed, its shape and location information can be exploited to apply a local energy calibration and corrections depending on whether the cluster is electromagnetic or hadronic. The motivation for a local energy calibration arises from the intention to provide a calorimeter signal for physics object reconstruction which is calibrated outside any particular assumption about the type of object.

Electromagnetic (EM) ScaleThe energy calibration of topo-clusters in the ATLAS detector begins with a basic correction of the raw deposited energy at the electromagnetic (EM) scale. This ensures that particle showers from a photon and an electron with equal energies should yield the same energy at the EM scale: \(E_{\text{cluster}}^{\text{EM}}\)[2]. This treatment, however, does not account for differences between the ATLAS calorimeters that can cause significant deviations from the true cluster energy for clusters depositing a significant amount of energy in the hadronic calorimeter. The energy calibration for hadronic clusters is greatly improved through the introduction of the Local Hadronic Calibration.

Local Hadronic CalibrationThe local hadronic cell weighting (LCW) calibration method classifies topo-clusters as electromagnetic or hadronic, then applies an appropriate cell signal weighting. Using single neutral and charged pion MC simulations, the likelihood of a topo-cluster to be generated by electromagnetic energy deposit, \(\mathcal{P}_{\text{cluster}}^{\text{EM}}\), is determined by measuring the efficiency for detecting an electromagnetic-like cluster in bins of topo-cluster observables: the EM-scale energy \(E_{\text{cluster}}^{\text{EM}}\), \(\eta\), longitudinal depth, and the first energy-weighted moment of the cell energy density distribution inside the cluster [2]. After this stage, additional corrections are applied to clusters determined using the \(\mathcal{P}_{\text{cluster}}^{\text{EM}}\) classification scheme to be hadronic in nature. This energy correction step yields the calibrated LCW-scale cluster energy \(E_{\text{cluster}}^{\text{LCW}}\). Both \(\mathcal{P}_{\text{cluster}}^{\text{EM}}\) and \(E_{\text{cluster}}^{\text{LCW}}\) serve as benchmarks for the classification and regression tasks studied here. The full LCW energy calibration scheme also includes energy corrections that address signal losses from energy deposited in inactive material or outside of the topo-cluster boundary. The cluster-only results here include only the initial hadronic calibration step, while the full LCW calibration is used for the combined cluster and track results.

#### 3.2.2 Tracks

This note includes results for pion energy regression using only calorimeter topo-clusters as well as topo-clusters with inner detector tracking information included. Track reconstruction with the ATLAS detector begins with the formation of a track seed consisting of hits from silicon tracker layers [37; 38]. A pattern recognition algorithm is then run with the goal of extending the track seed to a full track. A broad search for track candidates is first run, then an ambiguity-solver algorithm assigns each of the track candidates a track score. Reconstructed tracks are then chosen in order of descending track scores. Once these well-reconstructed tracks are identified, a fine-grained track-fitting algorithm is applied to the remaining strong candidates [38]. Then, to account for the energy loss resulting from charged particles interacting with detector materials, a track-fitting method based on a Kalman filter called the Gaussian-sum filter (GSF) is run [39].

## 4 Methods

Three different machine learning architectures were studied for pion classification and energy regression in the previous image-based results [27]: a Merged Deep Fully Connected Network (DNN), a Densely Connected Convolutional Neural Network (DenseNet), and a Convolutional Neural Network (CNN). It was shown that all three deep learning architectures considerably outperformed the LCW calibration [27]. For the classification task, at a fixed \(\pi^{+}\) efficiency of 90%, the DNN had a factor of approximately 5 times better background rejection, while the CNN and DenseNet architectures gave approximately 8 times better background rejection. The DNN, CNN, and DenseNet energy regressions also perform extremely well, correctly reconstructing the target cluster energy \(E_{\text{cluster}}^{\text{true}}\) for a wide range of energy values.

In the present note, three point cloud deep learning architectures are investigated: Deep Sets, Graph Neural Networks (GNNs), and Transformers. An overview of these methods can be found in Sections 4.4, 4.5, and 4.6, respectively. Versions of the CNN and DNN architectures from [27] are also revisited for comparison purposes with the new methods for pion classification and energy regression, respectively. The CNN and DNN baselines are chosen because they tended to yield the strongest performance for the classification and regression tasks on the image-based pion representations. Though the CNN differs from the other methods in terms of the model architecture and type of inputs, all methods are trained using the available calorimeter topo-cluster information framed in different ways: for the CNN, as a series of images, and for the DNN and other new methods, as point clouds.

### Point Clouds

One of the the most powerful advantages of deep learning techniques is the ability to process large numbers of correlated inputs. For the hadronic calibration task, each cell of a topo-cluster can be treated as a potential input variable. To date, the deep learning approaches to pion classification and calibration in ATLAS have only considered image-based representations of pions [27], as is summarized in Section 4.3. However, this image-based approach may be suboptimal, considering that calorimeter layers have different spatial granularities, deposition geometries are irregular, and calorimeter images are sparse with most cells not passing the selection criteria. Furthermore, image-based representations of the calorimeter restrict studies to using calorimeter information only.

Pion deposits in the ATLAS detector can also be thought of as point clouds, or collections of points in space often representing a three-dimensional object. Each point in a point cloud representation has unique position coordinates (e.g. \(x\), \(y\), and \(z\)). In this view, pions are represented not as a series of images, but as a complex three-dimensional form comprised of many (typically on the order of 100) individual topo-cluster cells. Figure 1 shows an example point cloud of topo-cluster cells activated in a di-jet collision event.

When only calorimeter signals are considered, all topo-clusters are used, and each of them is considered separately for the classification and regression tasks. When tracking information is included, the

Figure 1: A dijet collision event rendered as a 3-dimensional point cloud of calorimeter cells, as seen from two orientations.

dataset is filtered to events with a single track and the clusters are limited to those that fall within \(\Delta R(\text{track},\text{cluster}<1.2)\) of the track. Tracks can then be included as an additional point or points in the point cloud by extrapolating them to the calorimeters, for instance by introducing a boolean flag indicating whether a point is a topo-cluster cell or track, or as a global feature for the graph as a whole.

### Deep Neural Network (DNN)

A simple fully-connected deep neural network (DNN) [40] with 3 hidden layers of 50 nodes is used as a baseline to be compared with the more sophisticated point cloud deep learning methods. The DNN method, and all other arquitectures considered in this note, uses the same single pion Monte carlo simulation dataset described in Section 3. It is studied in the context of pion energy regression, and trained using point clouds inputs with topo-cluster and track information as will be described in Section 5.2.2. A selection criteria is applied to ensure events have exactly one track and all topo-clusters in an event are included if they are geometrically matched around the track by requiring \(\Delta R(\text{cluster},\text{track})<1.2\). The model is trained on approximately 3.5 million events, validated on approximately 500,000 events, and tested on approximately 1 million events. The inputs variables used are the track \(p_{T}\), \(\eta\), \(\phi\), and \(z_{0}\), and all available topo-cluster energies. This method do not include cell-level information as input variable as it is the case for the other point clouds methods. The inputs variables are reprocessed by standardizing them to a mean of zero and a unit variance, also, the logarithm of topo-cluster energies and track \(p_{T}\) are used. The DNN uses ReLU activation functions, Adam optimizer [41], and mean absolute error loss. It is trained until convergence (approximately 100 epochs) using a patience of 30 epochs and a batch size of 1,024.

### Convolutional Neural Network (CNN)

In the image-based approach, energy deposits in the calorimeters are treated as pixel densities. The cells in a given calorimeter sampling layer are projected into a two-dimensional image in the \((\eta,\phi)\) plane with extent \(0.4\times 0.4\) centered on the topo-cluster's centroid.

In the previous study [27], only the central barrel of the detector is considered, with \(|\eta|<0.7\). In this region, there are three layers of the EM calorimeter and three layers of the hadronic calorimeter. The full granularity of each layer of the calorimeter in the central barrel is preserved as shown in Table 1.

\begin{table}
\begin{tabular}{c|c} Calorimeter Layer & (\(\Delta\eta\), \(\Delta\phi\)) Granularity \\ \hline EMB1 & 128 x 4 \\ EMB2 & 16 x 16 \\ EMB3 & 8 x 16 \\ Tile1 & 4 x 4 \\ Tile2 & 4 x 4 \\ Tile3 & 2 x 4 \\ \end{tabular}
\end{table}
Table 1: A summary of the calorimeter granularity (cells per image) used in these studies. The sampling layers of the EM calorimeter are denoted as EMB1 - EMB3, and the Tile calorimeter as Tile1 - Tile3, from the inner layers to the most forward.

All models in the image-based approach were implemented in Keras [42] with a Tensorflow [43] backend and trained using binary cross-entropy loss for classifiers, and the mean-squared error loss for regressions, with the Adam optimizer [41].

The CNN architecture studied in these results is the same as the one implemented in Reference [27]. It considers the EMB1 layer separately, taking into account its extremely fine segmentation in \(\eta\), while the EMB2 and EMB3 layers are considered together with shared convolutional filters, with the EMB3 image up-scaled to the granularity of the EMB2 layer. The Tile layers are considered together, and they are upscaled to the granularity of the Tile1 layer. Each input layer is followed by convolutional layers and then fully connected layers prior to the final sigmoid activation. Figure 2 shows a diagram of the CNN classifier model.

### Deep Sets

A \(pp\) collision event at LHC can be seen as a variable-length unordered set of particles. The Deep Sets Theorem for point clouds [45] demonstrates how permutation-invariant functions of variable-length inputs can be parameterized in a fully general way. Therefore, any physics observable that is symmetric

Figure 2: A diagram of the CNN classifier model. For simplicity, dropout layers [44] are not displayed.

with respect to the ordering of the considered particles, can be approximated arbitrarily well with a parameterization of permutation-invariant functions of variable-length inputs [46].

The general parameterization for a permutation-invariant observable of sets of particles can be written as [46]:

\[\mathcal{O}(\{p_{1},...,p_{M}\})=F\left(\sum_{i=1}^{M}\Phi(p_{i})\right); \tag{1}\]

where \(p_{i}\) represents the particle properties such as four-momentum or quantum numbers. Each particle is mapped by \(\Phi\) to an internal particle representation or latent space. The latent representation is then summed over all particles to arrive at a latent event representation, which is mapped by a continuous function \(F\) to the value of the observable. A schematic representation is shown in Figure 3.

The Deep Sets Theorem can be applied to the context of pion classification and energy calibration where observables are viewed as functions of sets of clusters composed of calorimeter cells. In Equation 1, a cluster comprises \(M\) cells, where \(p_{i}\) contains the relevant attributes of cell \(i\). The cell-level features considered in this study are: the logarithm of the cell energy, the \(\eta\) and \(\phi\) angular positions of the cell relative to the cluster center, and an index that labels each calorimeter layer, called "SampleID".

In the case in which the inner detector tracks are considered, the track position is projected on the calorimeter layers and is normalized to the EMB1 position. The track features considered are: transverse momentum (\(p_{T}\)), angular positions \(\eta\) and \(\phi\), and track radius \(r_{\perp}\).

Particle Flow Networks (PFNs) adapt the "Deep Sets" framework to particle physics [46]. Dense neural networks are used to parameterize \(\Phi\) and \(F\), since a sufficiently large neural network can approximate any well-behaved function. The particular dense networks used here to parameterize the per-particle mapping \(\Phi\) uses three dense layers with 100, 100, and \(\ell\) = 128 nodes, where \(\ell\) refers to the latent space. Several network architectures with more or fewer nodes were tested, achieving similar performance with the dataset considered. A diagram of the Deep Sets model can be found in Figure 4.

Figure 3: A visualization of the decomposition of an observable in the Particle Flow Network architecture [46].

Each dense layer uses the ReLU activation function [47] and He-uniform parameter initialization [48]. In the output layer of the classifier, a two-unit layer with a SoftMax activation function is used [49]. The EnergyFlow Python package [50] was used for implementing the Deep Sets used in the classification study. For the regression task, an analogous implementation was built using the Keras functional API.

When track \(p_{T}\) and \(\eta\) are introduced as inputs, the Deep Sets model is approximated by using the regression-only GNN setup described in Section 4.5 without edges.

### Graph Neural Network (GNN)

A typical form of Graph Neural Network (GNN) takes a graph-structured input \(G=(V,E)\), where \(V\) is a set of nodes and \(E\) a set of edges, and learns a hidden representation of the graph that is repeatedly updated via a method called message passing. These updates occur in separate stages for edges, as a function of nodes \(x_{i}\), and edges \(x_{(i,j)}\):

\[x^{\prime}_{(i,j)}=f_{\text{edge}}(x_{i},x_{j},x_{(i,j)}) \tag{2}\]

and nodes:

\[x^{\prime}_{i}=f_{\text{node}}(x_{i},\sum_{j\in\mathcal{N}_{i}}x^{\prime}_{(j,i)}). \tag{3}\]

Figure 4: A diagram of the Deep Sets model architecture considered in the pion classification and energy regression tasks.

Both \(f_{\text{edge}}\) and \(f_{\text{node}}\) are themselves neural networks. The edge update step in the message passing algorithm creates a hidden or updated representation \(x^{\prime}_{(i,j)}\) of the information contained in each edge of the graph as a function of the values of the nodes connected by that edge. Subsequently, the node update step aggregates the edge messages incoming to a particular node from its neighborhood of immediately-connected nodes \(\mathcal{N}_{i}\). This message passing scheme can be repeated such that the latent graph representation is updated based on information propagated from throughout the whole graph. Additional layers can then be added to address the task at hand, for instance to aggregate the hidden node embeddings in order to quantify the entire graph structure for topo-cluster classification or energy regression. Such aggregation of node embeddings can be performed by the use of global nodes \(g\) that encode graph-level attributes and are inherently connected to all nodes \(\mathcal{N}\) in the graph.

\[g^{\prime}=f_{\text{global}}(g,\sum_{i\in N}x^{\prime}_{i}). \tag{4}\]

Here \(f_{\text{global}}\) is again a neural network and \(g^{\prime}\) is the updated global representation to be passed to additional message passing updates or used for quantifying the entire graph structure. The global nodes can be also be used as inputs to \(f_{\text{edge}}\) and \(f_{\text{node}}\) if the graph-level attributes are useful for updating edge and node representations.

The GNN model trained on topo-cluster information is designed to concurrently accomplish the classification task of pion identification (\(\pi^{0}\) versus \(\pi^{\pm}\)) and regression task of energy calibration. The classification and regression losses are balanced together in the same model using a loss function that accommodates both tasks:

\[\mathcal{L}=(1-\alpha)\mathcal{L}_{\text{classification}}+\alpha\mathcal{L}_{ \text{Regression}} \tag{5}\]

The hyperparameter \(\alpha\), specifying the importance of the regression task versus the classification task in terms of the overall loss penalty to the model, is set to 0.75 following an optimization study. This study found that classification accuracy was essentially constant for \(\alpha<0.75\) and decreased slightly for \(\alpha\geq 0.75\), while regression loss reached its minimum value around \(\alpha=0.5\).

Each pion topo-cluster is represented as a graph with nodes, edges, and a global node. Every cell in the topo-cluster is used as a node in the cluster graph and associated with node features. The node features for the calorimeter clusters include cell energy, sampling layer, \(\eta\), \(\Delta\eta\), \(\phi\), \(\Delta\phi\), and shortest radial distance of the cell to the shower axis, \(r_{\perp}\). \(\Delta\eta\) and \(\Delta\phi\) refer to the angular distance from the topo-cluster barycenter. These nodes are connected by edges defined by the calorimeter cell geometry, i.e. neighboring cells are connected to one another. Each edge is associated with edge features that define the type of connection between the associated nodes. The global node is implicitly connected to all the other nodes in the graph and has the total topo-cluster energy as its only feature.

The GNN architecture consists of four GNN blocks that use multi-layer perceptrons (MLP) and permutation-invariant aggregation functions to update the edge, node, and global features of the input graphs. Each MLP in the GNN block consists of three dense layers of size 64 each. The model is trained with a batch size of 1,024 for 100 epochs using the Adam optimizer [41]. The learning rate was initialized at 0.001 and was halved after every 20 epochs.

A schematic of the GNN block, inspired from the GNN modeling approaches described in [51] is shown in Figure 5(a). The _Edge MLP_ uses the edge, node, and global features as inputs. The output of the _Edge MLP_ is aggregated across all edges and used along with the node and global features as an input to the _Node MLP_. The output of the _Node MLP_ is then aggregated across all nodes and used along with the global features as an input to the _Global MLP_. The updated edges, nodes, and global features define the updated output graph of the GNN block. Figure 5(b) shows the overall model architecture used for the simultaneous classification and energy regression of the pion.

The model uses cluster graphs as inputs. The output features of each GNN block are concatenated with the input features to maximize the information flow through the network. This approach is similar to the concepts used in the DenseNet [52] architecture used in [27]. After the four GNN blocks, only the global features are used to make pion classification and energy predictions. The global features from each stage of the model, including the input graph, are concatenated to be used as inputs to two dense layers that map to a single output neuron. The output from the first neuron is used as the classification output of the model. To inform the model about the type of pion before predicting the energy, the classification output is used to weight the energy output of the network using an additional concatenation operation.

When track features are introduced as additional inputs, the GNN architecture is modified to only address the energy regression task, as classification is no longer needed when training on only charged pion datasets. The architecture is also modified to only use one GNN block. The following four different graph

Figure 5: A schematic of the simultaneous classification and regression GNN model trained on topo-cluster information only.

representations of data with varying levels of information were considered for this approach (directly compared in Figure 12), with the "All clusters, with cells" ultimately being the model selected for comparison with the other ML approaches:

1. **Leading cluster, no cells:** This is the simplest case where the event is represented as two nodes, a cluster node with the cluster energy of the highest energy cluster (leading cluster) associated with the track, and a track node with the track features track \(p_{T}\) and \(\eta\). These nodes are connected with a bidirectional edge with no edge features. There is no global node.
2. **Leading cluster, with cells:** This case used a graph representation very similar to the one used in cluster classification/regression task. Cells in the leading topo-cluster are represented as nodes of graph connected by the edges defined by the calorimeter cell geometry. The node and edge features are kept same as the cluster classification/regression task. The global node has three features: cluster energy, track \(p_{T}\), and \(\eta\).
3. **All clusters, no cells:** This representation is similar to the "Leading cluster, no cells" representation but with all the clusters associated with the track and satisfying the \(\Delta R(\text{track},\text{cluster}<1.2)\) criterion. The cluster nodes and the track node form a fully connected graph with no edge features. There is no global node.
4. **All clusters, with cells:** This representation has the most event information incorporated in it. It is similar to the representation in "Leading cluster, with cells" case, but with cells from all clusters associated with the track and satisfying the \(\Delta R(\text{track},\text{cluster}<1.2)\) criterion. The cells within any cluster are connected with edges defined by the calorimeter cell geometry. Individual clusters form sub-graphs within the whole event graph representation, but are disconnected from one other. A fully connected graph with all the \(n\) nodes would have edges on the order of \(n^{2}\), which would be significantly more difficult to train with on the typical \(n\sim 100\) cells. As there are multiple clusters in the representation, a single cluster energy cannot be included as one of the global node features. Therefore cluster energy is appended as an additional feature to nodes of sub-graph representing any cluster. The global node has two features: track \(p_{T}\) and \(\eta\).

### Transformer

Transformer networks [53] have shown promising performance in identifying relationships between sequences of elements within a set. The core building block of a transformer network is the self attention mechanism [30] that computes the encoding between different elements of a set and decides their mutual importance for performing a statistical learning task. In a point-cloud representation of a calorimeter shower, the mutual position and energy content of the calorimeter cell within a topo-cluster represent the basic features of the particle shower. Through the attention mechanism, the model learns the mutual encoding of the calorimeter cells and eventually uses this information to predict the calibrated cluster energy or the energy of the original truth particle causing the energy deposit.

In this case, the transformer model has been implemented as a message passing layer. The node features \(h\) are first transformed to three different latent representations \(\text{Q}(=\Theta_{1}(h))\), \(\text{K}(=\Theta_{2}(h))\) and \(\text{V}(=\Theta_{3}(h))\) via three MLP blocks \(\Theta_{i}\) (\(i\in 1,2,3\)), where the vectors \(\text{Q},K,V\) refer to the _query_, _key_ and _value_ for each node. The self attention score between the \(i\)-th and \(j\)-th node is \(e_{ij}=\sigma(Q_{i}K_{j}^{T}/\sqrt{d})\). Here, \(\sigma\) is the ReLuactivation function and \(d\) is the size of the vectors \(Q_{i},K_{j}\). The final updated node features are obtained through the permutation invariant operation:

\[h^{\prime}_{i}=\Phi\Big{(}V_{i},\sum_{j\in\mathcal{N}(i)}e_{ij}\cdot V_{j}\Big{)}. \tag{6}\]

Here, \(\Phi\) is another MLP block used for the node update operation, and \(\mathcal{N}(i)\) is the neighborhood of the \(i\)-th node in the graph. After several message passing layers, a global pooling operation is performed to predict the target truth particle energy. The workflow of a single message passing layer is demonstrated in Figure 6.

Unlike the other point cloud models, the Transformer is only trained for the task of pion energy regression using cluster and track inputs. The selection criteria consider events with one or zero tracks, and all clusters with \(E^{\text{EM}}_{\text{cluster}}>0.5\) GeV are used regardless of their relative position to the track (no \(\Delta R\)(cluster, track) requirement).

Figure 6: An overview of the transformer operation on the graph, implemented as a message passing network, as described in Equation 6.

## 5 Results

### \(\pi^{0}/\pi^{\pm}\) Classification

The following \(\pi^{0}\) vs. \(\pi^{\pm}\) shower classification results are obtained from training the CNN, Deep Sets, and GNN models described in Section 4 using only calorimeter topo-cluster information, including calorimeter cell geometry information.

Figure 7(a) shows a comparison of the classification performance for the various models in the appropriate pseudo-rapidity range of \(|\eta|<0.7\) suitable for a comparison to the CNN's training dataset. Even though the CNN model was trained for the central barrel as a point of comparison with respect to previous result [27], both the GNN and Deep Sets models were trained without any pseudo-rapidity selection. It should also be noted that the previous result's CNN performance is stronger than the results shown here, because the CNN in Reference [27] was trained on simulation datasets that did not include electronic noise. The datasets used in this analysis do include electronic noise, rendering these results not directly comparable. Performance is shown as \(\pi^{0}\) rejection (defined as the inverse of \(\pi^{0}\) selection efficiency) versus \(\pi^{\pm}\) efficiency with respect to all truth pions, where higher rejection indicates better classification performance for the same selection efficiency. Across the full range of \(\pi^{\pm}\) efficiencies, all machine learning models notably outperform the \(\sigma^{\text{EDM}}_{\text{clus}}\) baseline classifier. At high \(\pi^{\pm}\) efficiencies, the CNN pion classification performance is comparable to the performance of the new Deep Sets model, while the GNN shows the highest rejection across all \(\pi^{\pm}\) efficiencies. It should be noted that while these trends in model performance are fairly robust, they do not indicate an ultimate optimized performance of each architecture based on a detailed hyperparameter search.

Figure 7(b) shows a direct comparison of all the new classification models studied in this note across the full \(|\eta|<3\) range of the simulated datasets. In this case, with a wider range of \(\eta\) values used, both the Deep Sets model and the GNN outperform the \(\mathcal{P}^{\text{EM}}_{\text{clus}}\) classifier, and the GNN in particular shows strong rejection across all \(\pi^{\pm}\) efficiencies.

Table 2 shows the neutral pion rejection for a working point of a fixed 90% charged pion selection efficiency for the considered methods. In the central pseudo-rapidity range, compared to the baseline \(\mathcal{P}^{\text{EM}}_{\text{clus}}\) classification, the Deep Sets and CNN architectures have approximately 4 times better background rejection, while GNN has almost 8 times better neutral pion rejection. When considering the full \(\eta\) range, Deep Sets outperforms \(\mathcal{P}^{\text{EM}}_{\text{clus}}\) by a factor \(\approx 2\), and the GNN network improves the pion classification by \(\approx 5\) times.

Pion topo-cluster classification performance for the GNN model and the Deep Sets models are shown in Figure 8 for different exclusive ranges of \(E^{\text{EM}}_{\text{cluster}}\) and \(|\eta|\). Performance increases with higher topo-cluster energies \(E^{\text{EM}}_{\text{cluster}}\) due to both higher sampling statistics and reduced stochastic fluctuations. Performance is

\begin{table}
\begin{tabular}{|c|c|c|} \hline Model & Rej. @ 90\% Eff. for \(|\eta|<0.7\) & Rej. @ 90\% Eff. for \(|\eta|<3\) \\ \hline \hline CNN & 26.584 & - \\ \hline GNN & 46.419 & 20.500 \\ \hline Deep Sets & 24.814 & 7.608 \\ \hline \(\mathcal{P}^{\text{EM}}_{\text{clus}}\) & 6.123 & 3.977 \\ \hline \end{tabular}
\end{table}
Table 2: Neutral pion rejection at a fixed charged pion efficiency of 90% for the various classification models.

more mixed in bins of \(|\eta|\), likely due in part to the varied detector materials and geometries across the \(|\eta|\) range.

### Pion Energy Regression

The performance of the regression models can be quantified by measuring the energy response, i.e. \(R=E_{\text{predicted}}/E_{\text{true}}\), as a function of \(E_{\text{true}}\). \(E_{\text{predicted}}\) should be close to the target value \(E_{\text{true}}\) after calibration, leading to a mean response \(R\) close to the unity.

The energy resolution is also a relevant metric used to evaluate regression performance. An ideal calibration would have a small resolution of predicted values, meaning that its predictions are more precise and stable. The resolution of the energy measurement can be quantified with the interquantile range (IQR), representing the width of the response data from \(1\sigma\) to \(-1\sigma\) (84% - 16%) of the median. Examining the semi-interquantile range, i.e. one-half the IQR, is particularly useful for skewed distributions. Since the resolution is used to determine the size of the response fluctuations, it is calculated as one-half the IQR divided by the median of the response in order to reduce the effects of varying levels of non-closure in the energy scale between methods that may bias the resolution estimation.

#### 5.2.1 Regression using only cluster information

The energy calibrations for both the baseline LCW and the point cloud regression methods trained on calorimeter top-cluster information only target the true topo-cluster energy. The true topo-cluster energy is defined as the sum of all energy deposits, as given by the Geant4 simulation, within the physical extent of the topo-cluster.

Figure 7: Comparison of topo-cluster classification performance of all methods for \(|\eta|<0.7\) (7(a)) and \(|\eta|<3\) (7(b)). The CNN is only trained on datasets for which \(|\eta|<0.7\). Performance is measured as \(\pi^{0}\) topo-cluster rejection (defined as the inverse of \(\pi^{0}\) selection efficiency) versus \(\pi^{\pm}\) topo-cluster efficiency, where higher rejection indicates better classification performance for the same selection efficiency.

For charged pions, the EM energy calibration tends to underestimate the true energy for energies larger than 1 GeV due to the non-compensating nature of the ATLAS calorimeters. At energies lower than 1 GeV, it tends to overestimate the true cluster energy due to both a selection bias towards higher-response clusters near the noise threshold as well as the fact that low-energy charged pions deposit most of their energy in the calorimeters via direct ionization, like a minimally-ionizing particle. These deposits from a minimally-ionizing particle require a different energy calibration than the EM scale. However, for neutral pions, the EM scale does not show the same under-estimation behavior - instead, it primarily exhibits an over-estimation behavior due to the noise thresholds at lower energies. For charged pions, the LCW scale shows much better predictions at energies greater than 1 GeV, as it is designed to help mitigate the calorimeter non-compensation, but for neutral pions the performance is essentially the same as for the EM scale.

The point cloud deep learning methods introduced here (both GNN and Deep Sets) trained on topo-cluster information only have very strong performance. Their performance compared to the EM and LCW baselines are shown in Figure 9. The median energy response ratio for the GNN is significantly closer to

Figure 8: Topo-cluster classification performance in bins of topo-cluster energy \(E^{\text{EM}}_{\text{cluster}}\) and \(|\eta|\). Performance is measured as \(\pi^{0}\) topo-cluster rejection (defined as the inverse of \(\pi^{0}\) selection efficiency) versus \(\pi^{\pm}\) topo-cluster efficiency, where higher rejection indicates better classification performance for the same selection efficiency. These results indicate that the model performance clearly increases with increasing \(E^{\text{EM}}_{\text{cluster}}\), but the trend with increasing \(|\eta|\) is less clear.

unity throughout the full energy spectrum considered than the baselines EM or LCW calibration schemes for charged and neutral pions. The IQR is narrower for the GNN than for the baselines across both charged and neutral pions. The Deep Sets approach trained on cluster information only for charged pions also has good performance, as shown in Figures 9(e) and 9(f). In these figures, the Deep Sets response curve has been smoothed with a Savitzky-Golay filter [54] to account for its evaluation on a lower-statistics dataset than that of the GNN. The median energy response ratios for the Deep Sets is in general closer to unity throughout the full energy spectrum considered, and especially at energies below 1 GeV, than the baseline EM or LCW calibration schemes, and its IQR is comparable to the baselines.

#### 5.2.2 Regression using cluster and track information

There are multiple ways of incorporating track information into the point cloud structure used for each of these machine learning models, but in general the strategy involves treating tracks as additional points with features that are distinct from the cluster points or as global features of the point cloud.

Tracking information is expected to complement and interplay differently with model performance than calorimeter cluster information. For instance, while the calorimeter energy resolution will generally increase with the shower energy due to higher sampling statistics and lower intrinsic (stochastic) fluctuations, the track \(p_{T}\) resolution will decrease with higher momenta. Requiring a track also increases the minimum energy for the pions involved, resulting in relatively fewer statistics for pion energies below 1 GeV than for the cluster-only studies. The networks studied here are able to use information from all energy regimes on a continuous basis in order to achieve better overall performance.

Since the regression target in the cluster-only results \(E_{\text{cluster}}^{\text{true}}\) will not benefit from the introduction of tracking information, for these studies the truth particle energy \(E_{\pi^{\pm}}^{\text{true}}\) is used as the regression target. The baseline EM and LCW comparisons are also slightly modified to account for the new \(E_{\pi^{\pm}}^{\text{true}}\) target: \(E_{\text{cluster}}^{\text{EM}}\) represents the sum of all EM-scale cluster energies for the pion, while \(E_{\text{cluster}}^{\text{LCW}}\) represents the sum of all calibrated cluster energies, including hadronic as well as out-of-cluster and dead material corrections.

The results shown here for the Deep Sets (i.e. GNN with no edges) and GNN models are trained on events with a single track and multiple clusters for which \(E_{\text{cluster}}^{\text{EM}}>0.5\) GeV and \(\Delta R\)(track, cluster) \(<0.2\). The Transformer is trained with \(E_{\text{cluster}}^{\text{EM}}>0.5\) GeV, but no \(\Delta R\) requirement, and one or zero tracks.

A comparison of the median response ratios with respect to the target energies for the point cloud methods versus the baseline methods is shown in Figure 10. A comparison of the relative IQR values as a function of \(E_{\pi^{\pm}}^{\text{true}}\) is shown in Figure 11.

The point cloud deep learning models significantly outperform the EM and LCW calibration scales as a function of \(E_{\pi^{\pm}}^{\text{true}}\) along the metrics of both median of the energy response and energy resolution (IQR/median). In terms of median accuracy, they are comparable to the DNN for energies below approximately 30 GeV and outperform the DNN for energies greater than approximately 30 GeV. In terms of IQR/median, they outperform the DNN for very high (\(E>100\) GeV) truth pion energies. Compared to the cluster-only trainings, the spread in predictions with the inclusion of track information is much tighter, with IQR values consistently less than 0.1 versus a maximum of around 0.4 for the cluster-only GNN training, and the median response curves are even more closely aligned with the target energy.

The improvement in the pion energy resolution by including granular cell-level information into the point cloud models in addition to cluster-level energies and track information, is shown in Figure 12. The GNN models, both with and without edges, have comparable performance on equivalent training information throughout. The largest energy resolution comes from trainings using only the leading cluster information. Next, improved performance is achieved at high energy when introducing cell-level information for the leading cluster. Finally, further improvements can be seen from including all cluster energies with no cell-level information. Ultimately, the best performance is obtained from all clusters with cell-level information included. When cell-level information is not present, the models perform essentially identically. When cell-level information is present, some small differences emerge. The GNN with edges performs better on leading cluster energy and cell information, but when all clusters with cell information are included, both models perform equally well.

Figure 9: Median energy response \((a,c,e)\) and IQR \((b,d,f)\) for the EM and LCW baselines as well as the GNN and Deep Sets as a function of true cluster energy. The GNN shows better performance than the baseline across both charged and neutral pions. The Deep Sets median energy response ratio \((e)\) is in general closer to unity throughout the full energy spectrum considered, and especially for \(E<1\) GeV, than the baseline. Its IQR \((f)\) is comparable to the baseline.

## 6 Conclusion

Figure 10: 10(a): A comparison of the median ratios of the predicted energy to the truth pion energy for multiple charged pion energy regression methods shows the effectiveness of the point cloud deep learning methods trained on both top-cluster and track information. The EM and LCW scales are also shown with respect to \(E_{\pi^{\pm}}^{\text{true}}\) instead of the typical \(E_{\text{cluster}}^{\text{true}}\). Also shown is the performance of a simple DNN with the same inputs but no cell-level information. The GNN without edges is a version of the Deep Sets model. The point cloud deep learning models have significantly more accurate predictions than the EM or LCW calibration schemes and slightly more accurate predictions than the DNN, particularly for energies above 30 GeV. 10(b): The same performance is shown without the EM and LCW baselines to allow for a more granular comparison of the different ML schemes.

Figure 11: 11(a): A comparison of multiple charged pion energy regression methods shows the effectiveness of the point cloud deep learning methods trained on both topo-cluster and track information. The metric shown is one-half the interquartile range (IQR) divided by the median predicted energy as a function of the truth particle energy, capturing a measure of the spread of energy predictions. The EM and LCW scales are also shown with respect to \(E_{\pi^{\pm}}^{\text{true}}\) instead of the typical \(E_{\text{cluster}}^{\text{true}}\). Also shown, for comparison, are the track resolution (\(p_{T}^{\text{track}}\cosh(\eta^{\text{track}})/E_{\text{truth}}^{\pi}\)) and the performance of a simple DNN with the same inputs but no cell-level information. The GNN without edges is a version of the Deep Sets model. The point cloud deep learning models have significantly smaller spreads than the EM or LCW calibration schemes, the DNN, and the track resolution for approximately \(E_{\pi^{\pm}}^{\text{true}}>100\) GeV. 11(b): The same performance is shown without the EM and LCW baselines to allow for a more granular comparison of the different ML schemes.

Figure 12: A comparison of the interquantile ranges (IQR) for various iterations of the GNN models with and without edges demonstrates the utility of including cell-level information in addition to cluster energies for each pion event. The GNN without edges is a version of the Deep Sets model. The GNN models with and without edges have comparable performance on equivalent training information throughout. The largest energy resolution comes from trainings using only the leading cluster information. Next, we see improved performance at high energy when introducing cell-level information for the leading cluster. Finally, we see further improvements from including all cluster energies with no cell-level information and ultimately the best performance from all clusters with cell-level information included.

## 6 Conclusion

A variety of machine learning methods designed for \(\pi^{0}\) vs. \(\pi^{\pm}\) classification and pion energy regression were studied using information from both calorimeter clusters and, in the case of energy regression, particle tracks. The architectures used ranged from Deep Sets to Graph Neural Networks to Transformers. All of these methods outperformed the existing baseline classifier in ATLAS, \(\mathcal{P}^{\text{EM}}_{\text{clus}}\), in terms of \(\pi^{0}\) rejection vs. \(\pi^{\pm}\) efficiency. Regarding the pion energy regression task, all the studied architectures outperform the baseline LCW calibration in terms of both the accuracy and precision of the predicted energy responses.

There are several advantages to using point cloud representations of pions instead of image-based representations. The point cloud-based approaches are more flexible, allowing the models to include particle tracking information that greatly improves their energy regression performance. They are also more expressive, meaning that they suit the variable-length and permutation-invariant natural structure of pion calorimeter showers and associated track hits across a detector with a nonuniform geometry.

These results show the potential of deep learning techniques for low-level hadronic reconstruction and are therefore an important step towards implementing a version of Particle Flow that optimally takes advantage of performance improvements from machine learning.

## References

* [1] ATLAS Collaboration, _The ATLAS Experiment at the CERN Large Hadron Collider_, JINST **3** (2008) S08003 (cited. on pp. 2, 3).
* [2] ATLAS Collaboration, _Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1_, Eur. Phys. J. C **77** (2017) 490, arXiv: 1603.02934 [hep-ex] (cited. on pp. 2, 4, 5).
* [3] A. J. Larkoski, I. Moult, and B. Nachman, _Jet substructure at the Large Hadron Collider: A review of recent advances in theory and machine learning_, Physics Reports **841** (2020) 1, issn: 0370-1573, url: [http://dx.doi.org/10.1016/j.physrep.2019.11.001](http://dx.doi.org/10.1016/j.physrep.2019.11.001) (cited. on p. 2).
* [4] ATLAS Collaboration, _Convolutional Neural Networks with Event Images for Pileup Mitigation in \(E_{T}^{miss}\) reconstruction with the ATLAS Detector_, ATL-PHYS-PUB-2019-028, 2019, url: [https://cds.cern.ch/record/2684070](https://cds.cern.ch/record/2684070) (cited. on p. 2).
* [5] ATLAS Collaboration, _Generalized Numerical Inversion: A Neutral Network Approach to Jet Calibration_, ATL-PHYS-PUB-2018-013, 2018, url: [https://cds.cern.ch/record/2630972](https://cds.cern.ch/record/2630972) (cited. on p. 2).
* [6] ATLAS Collaboration, _Simultaneous Jet Energy and Mass Calibrations with Neural Networks_, ATL-PHYS-PUB-2020-001, 2020, url: [https://cds.cern.ch/record/2706189](https://cds.cern.ch/record/2706189) (cited. on p. 2).
* [7] ATLAS Collaboration, _Deep generative models for fast shower simulation in ATLAS_, ATL-SOFT-PUB-2018-001, 2018, url: [https://cds.cern.ch/record/2630433](https://cds.cern.ch/record/2630433) (cited. on p. 2).
* [8] ATLAS Collaboration, _Identification of hadronic tau lepton decays using neural networks in the ATLAS experiment_, ATL-PHYS-PUB-2019-033, 2019, url: [https://cds.cern.ch/record/2688062](https://cds.cern.ch/record/2688062) (cited. on p. 2).
* [9] A. Tumasyan et al., _Identification of hadronic tau lepton decays using a deep neural network_, Journal of Instrumentation **17** (2022) P07023, url: [https://doi.org/10.1088%2F1748-0221%2F17%2F07%2Fp07023](https://doi.org/10.1088%2F1748-0221%2F17%2F07%2Fp07023) (cited. on p. 2).
* [10] ATLAS Collaboration, _Quark versus Gluon Jet Tagging Using Jet Images with the ATLAS Detector_, ATL-PHYS-PUB-2017-017, 2017, url: [https://cds.cern.ch/record/2275641](https://cds.cern.ch/record/2275641) (cited. on p. 2).
* [11] ATLAS Collaboration, _Identification of Hadronically-Decaying W Bosons and Top Quarks Using High-Level Features as Input to Boosted Decision Trees and Deep Neural Networks in ATLAS at \(\sqrt{s}=13\) TeV_, ATL-PHYS-PUB-2017-004, 2017, url: [https://cds.cern.ch/record/2259646](https://cds.cern.ch/record/2259646) (cited. on p. 2).
* [12] ATLAS Collaboration, _Performance of top-quark and \(W\)-boson tagging with ATLAS in Run 2 of the LHC_, Eur. Phys. J. C **79** (2019) 375, arXiv: 1808.07858 [hep-ex] (cited. on p. 2).
* [13] ATLAS Collaboration, _Deep Sets based Neural Networks for Impact Parameter Flavour Tagging in ATLAS_, ATL-PHYS-PUB-2020-014, 2020, url: [https://cds.cern.ch/record/2718948](https://cds.cern.ch/record/2718948) (cited. on p. 2).
* [14] ATLAS Collaboration, _Performance of mass-decorrelated jet substructure observables for hadronic two-body decay tagging in ATLAS_, ATL-PHYS-PUB-2018-014, 2018, url: [https://cds.cern.ch/record/2630973](https://cds.cern.ch/record/2630973) (cited. on p. 2).
* [15] L. De Oliveira, B. Nachman, and M. Paganini, _Electromagnetic Showers Beyond Shower Shapes_, Nucl. Instrum. Meth. A **951** (2020) 162879, arXiv: 1806.05667 [hep-ex] (cited. on p. 2).