# The Swiss ATLAS Computing Prototype

Szymon Gadomski, Christian Haeberli

LHEP, University of Bern

Frederik Orellana

DPNC, University of Geneva

Gian Luca Volpato

CSCS, Manno

On leave from the Institute of Nuclear Physics, Polish Academy of Science, Cracow.

June 22, 2005

###### Abstract

The Swiss ATLAS Computing prototype consists of clusters of PCs located at the universities of Bern and Geneva and at the Swiss National Supercomputing Centre (CSCS) in Manno. In terms of software, the prototype includes ATLAS off-line releases as well as middleware for running the ATLAS off-line in a distributed way. Both batch and interactive use cases are supported. The batch use case is covered by a country wide batch system, the interactive use case is covered by a parallel execution system running on single clusters. The prototype serves the dual purpose of providing resources to the ATLAS production system and providing Swiss researchers with resources for individual studies of both simulated data and data from the ATLAS test beam. In this note the solutions used for achieving this are presented. Initial experience with the system is also described.

## Motivation

The present project forms part of the preparations for the analysis of ATLAS data by physicists at the universities of Bern and Geneva at startup of LHC in 2007. A small prototype has been set up which will provide input to resource estimates of the future full Swiss ATLAS computing infrastructure. The practical near term objectives are to start building the infrastructure as well as to gain experience in the following areas:

* hardware of PC clusters
* management of Linux farms, choice of Linux distributions
* ATLAS off-line software in the ATHENA framework
* distribution kit of ATLAS off-line, issues related to working outside CERN
* installation and use of Grid middleware
* interactive analysis of ATLAS data
* software enabling distributed interactive analysis

The prototyping group consists of four persons working mostly part time on the project, active in other endeavours like ATLAS construction, teaching at the universities and other academic duties. Swiss ATLAS Computing can not become an important activity in itself for the university groups. The emphasis is on finding practical and currently possible solutions to problems at hand. The software involved must thus provide the needed functionality (or at least a well-defined subset of it), be documented, easy to install and to maintain. Development of Grid middleware and other "infrastructure" software is not in the scope of the project. Some development or customisation is in practice necessary, but must be very limited, always motivated by direct needs of the prototype.

The following use cases have been identified and tried on the system:

* simulation studies of ATLAS by Swiss researchers,
* analysis of combined test beam data by Swiss researchers,
* generation and processing of simulated ATLAS events in the framework of the ATLAS Data Challenges, controlled centrally from CERN by the ATLAS production system,
* interactive analysis of simulated ATLAS events,
* parallel semi-interactive analysis.

In the following sections we describe our hardware and software infrastructure as well as our experience with the chosen solutions.

Hardware infrastructure and network

### Bern ATLAS cluster

The Bern ATLAS cluster is owned and operated by the Laboratory for High Energy Physics of the University of Bern. It consists of 8 PCs:

* AMD Athlon 64 3400+ 2.4 GHz with a 2 TB hardware RAID serving as a file server
* Intel Pentium III 600 MHz serving as submission machine and Grid front-end
* six worker nodes (12 CPUs):
* 4x Intel XEON 3.0 GHz (32 bit) dual-CPU
* 1x Intel XEON EM64T 3.4 GHz dual-CPU
* 1x AMD Opteron 248 2.2 GHz (64 bit) dual-CPU The worker nodes are equipped with 1 GB of RAM per CPU.

The operating system is Linux, but more than one distribution is used. We have CERN Redhat 7.3 and SUSE 9.2 for the 64 bit machines. Redhat 7.3 is phasing out and the machines will be upgraded to SUSE 9.2 to obtain a more uniform system. The reasons for the choice of Linux distributions are explained in section 3.2. The machines are interconnected with Gigabit-ethernet.

### DPNC test cluster

The DPNC test cluster consists of one server and three worker nodes at the Departement de physique nucleaire et corpusculaire of the University of Geneva. The three worker nodes serve simultaneously as shared desktop work stations.

The hardware specification of the server is as follows:

* dual Pentium III, 1 GHz
* 1 GB of RAM
* 300 GB RAID array for data storage

The hardware specification of the worker nodes is as follows:

* AMD Athlon XP 2800+, 1.2 GHz
* 1 GB of RAMThis is a very modest hardware configuration, but seems to be fully sufficient for running the chosen Grid middleware and the ATLAS software, at least for testing purposes. All hosts run CERN Scientific Linux 3.0.3 and are interconnected with fast ethernet.

### Phoenix Cluster

The PHOENIX cluster is owned by CHIPP (Swiss Institute of Particle Physics) and it is accessible for all Swiss researchers participating to the ATLAS, CMS and LHCb experiments. It is physically installed and managed at CSCS.

PHOENIX consists of:

* 1 Intel Pentium IV 4.0 GHz serving as NorduGrid frontend
* 1 Intel Xeon EM64T 3.0 GHz dual-CPU serving as LCG frontend and local batch system master
* 1 Intel Xeon EM64T 3.0 GHz dual-CPU serving as file server
* 9.6 TB RAID array attached to the file server via fibre channel interface
* 10x Intel Xeon EM64T 3.0 GHz dual-CPU serving as worker nodes. The worker nodes are equipped with 2 GB of RAM per CPU.

All hosts run CERN Scientific Linux 3.0.4 and are interconnected with Gigabit-ethernet.

### Ubelix

UBELIX is a common cluster accessible to all researchers at the University of Bern. It is shared between astronomers, chemists and physicists of several disciplines. The cluster is managed by the IT services department of the University of Bern [4].

UBELIX consists of:

* AMD Opteron 848 2.2 GHz dual serving as the master
* 3 TB Hardware RAID attached to the master
* Intel XEON 2.0 GHz dual serving as the 32 bit submit host
* AMD Opteron 240 1.4 GHz serving as the 64 bit submit host
* 64 worker nodes (96 CPUs):
* 16x Intel Pentium 4 2.8 GHz (32 bit)
* 16x AMD Athlon 3400+ 2.2 GHz (64 bit)
* 32x AMD Opteron 250 2.4 GHz (64 bit) dual-CPU

The worker nodes are equipped with at least 1.5 GB of RAM per CPU.

The operating system is Gentoo 2004.2. The machines are interconnected with Gigabit-Ethernet

### The SWITCH Network

The resources of the Swiss ATLAS Computing Prototype and CERN are connected via the SWITCH network [5], which is providing a dark fibre infrastructure between CERN, the Swiss universities and CSCS. The dark fibres are owned and exclusively used by SWITCH. Currently the dark fibre links are equipped with a bandwidth between 4 GBit/s and 10 GBit/s, depending on the importance of the connection.

The advantage of the dark fibre infrastructure is that it allows SWITCH to ramp up the bandwidth continuously according to the growing needs by installing optical end devices using different colours on the same fibre. Max

Figure 1: Current dark fibre connections between Swiss research institutions and planned extensions in 2005, provided by SWITCH.

imally, SWITCH could provide a bandwidth of 16 \(\times\) 10 GBit/s per dark fibre link. Technically, even 32 \(\times\) 10 GBit/s would be feasible, but this would require a major investment.

The obtainable bandwidth is about two orders of magnitude above the foreseen output bandwidth of the ATLAS TDAQ system of 300 MB/s [2].

## 2 Country-wide batch system based on the NorduGrid ARC

By much the same reasoning as that underlying the original visions of Grid computing [1] it was judged that a sensible first goal was to establish a Swiss-wide batch system: a system for sharing computing resources, accessing data and submitting computing jobs, involving all Swiss institutes with ATLAS computing needs. Researchers should be enabled to submit jobs in a transparent way to all Swiss clusters using the same simple user interface.

Given the non-interoperability of the available Grid middleware systems, this implied that all Swiss clusters had to be integrated into one Grid system. In practice this meant LCG [6] or NorduGrid [7]. After an informal evaluation, drawing on the experience of the people involved, NorduGrid was chosen.

The reasons for this choice were:

* the DPNC test cluster and the Bern ATLAS cluster are too small to be integrated into LCG. An LCG cluster needs in the order of four PCs to run services. This would be 50% of the Bern ATLAS cluster and 100% of the DPNC prototype cluster.
* UBELIX is a resource shared within the University of Bern. But LCG requires a dedicated cluster, because it imposes cluster management and strict operating system requirements. The Laboratory for High Energy Physics cannot impose LCG cluster management and a CERN Linux distribution on a common university resource.
* NorduGrid ARC is far simpler to install and configure than LCG, but delivering all the functionality we need to build a country-wide batch system.
* Users should be able to submit jobs from their own desktop or laptop. It did not appear feasible demanding of normal users to install an LCG user interface.

Currently the country wide batch system unifies about 130 CPUs to one big batch cluster. The four clusters (see in section 1) making up this Swiss ATLAS Grid not only have different hardware configurations, they also have rather different usage patterns and have different levels of maintenance and support.

PHOENIX is located in a professional computing center with around-the-clock maintenance and a dedicated staff member dedicated to system, software and user administration. PHOENIX forms part of the Swiss contribution to LCG, supporting the three LHC experiments with Swiss participation (ATLAS, CMS, LHCb). The same local batch system (PBS) is used by LCG and NorduGrid, with different submission queues. PHOENIX is thus a hybrid LCG/NorduGrid cluster, offering the same resources to both Grids. This should pose no problems as the information systems in both cases get their information directly from PBS.

UBELIX is also located in a professional computing center, but supporting several other research groups than the Swiss LHC groups. The batch system used, SUN Grid Engine [8], was successfully brought to work with NorduGrid.

The Bern ATLAS cluster and the Geneva test cluster are smaller, local resources, adding to the country - and world wide pool of resources in a manner less controlled (at least in the case of Geneva, some down time may occur), but well in line with the original intentions [1] of optimising the use of available computing power.

The user interacts with the system through the NorduGrid command line tools, like "ngsub", "ngstat" and "ngkill", much the same way as with a normal batch system like PBS or LSF. Monitoring of jobs can also be done conveniently through the GUI on the NorduGrid web site. To use the NorduGrid tools, the user can either log on to one of the front-end machines, if she has an account there, or simply download and extract the tar-ball available on the NorduGrid web site.

### Prioritisation of use cases

The general idea is to provide significant resources to both local/national use and general Grid/ATLAS production, keeping the resources as busy as possible.

The different use cases are prioritised as follows:

1. interactive usage
2. batch jobs of Swiss physicists
3. centrally managed ATLAS production (production system jobs)

The interactive usage of resources has priority over the batch usage. This requirement is implemented by starting all the batch jobs with "nice 19".

Grid jobs submitted by Swiss physicists have priority over general Grid jobs like jobs from the ATLAS production system. This is implemented by mapping users from the Swiss Virtual Organisation (see section 2.2) and users from other acknowledged VOs to batch queues with different priorities. Currently resources are not fully used by Swiss physicists and are thus contributing to the common pool of ATLAS. In the future this is bound to change and a more strict/detailed control of resource usage will be necessary. Implementing such control is crucial in order to guarantee both the short-term availability of local/national resources while still providing significant contributions to general Grid/ATLAS production.

### User management and accounting

Currently there exists one Swiss VO, the Swiss Atlas VO. In the future, as needs arise, one for each of the two other LHC experiments with Swiss participation (CMS and LHCb) will be added. The management of this VO is technically simple: use is made of the already existing database of Swiss high energy physicists([http://www.chipp.ch/directory/login.php?login=demo&passwd=demo](http://www.chipp.ch/directory/login.php?login=demo&passwd=demo)). This database is maintained by one or several persons at each institute (typically a secretary) and the records have a field, "Public Key", which is used for keeping the subject of Grid certificates. An automatic job, run every 10 minutes, generates a list of all subjects and puts it on a web server (currently at [http://www.chipp.ch/vo/chipp.txt](http://www.chipp.ch/vo/chipp.txt)). The URL of this list can be specified directly in the NorduGrid configuration file ("/etc/grid-security/nordugridmap.conf").

Since the bulk of our resources are not dedicated to ATLAS, and not even to LHC, not only prioritisation, but also detailed logging and accounting will be needed. Currently, all grid-manager daemons are configured to log to the logging service at [https://grid.uio.no:8001/logger](https://grid.uio.no:8001/logger). This service provides useful statistical numbers and tables on the web site [http://grid.uio.no/atlas/nglogger/nglogger_info/](http://grid.uio.no/atlas/nglogger/nglogger_info/). Unfortunately this web site is very slow. Moreover, to do actual accounting, more detailed information is needed. In principle, this information is available in the log files of the local batch system on each cluster, and in the case of PBS, utilities exist to make this information usable for accounting purposes. To try this out, a simple cron job has been set up on the Geneva test cluster, which generates a daily PBS usage report and puts it on a web server, [http://grid01.unige.ch/pbs/report.txt](http://grid01.unige.ch/pbs/report.txt). This, however is only of limited practical use, because the information is limited to one cluster. For country wide accounting, a more general system needs to be set up, providing statistics on the number of jobs run by a given VO, submitted from a given subnet to a given cluster.

### Use cases

The batch system is providing resources to the ATLAS Production System, controlled centrally from CERN. The system is also available to researchers of the Universities of Bern and Geneva, who can submit jobs directly, not passing via the ATLAS Production System.

The Bern ATLAS cluster has been available to the ATLAS Production System through NorduGrid since July 2004. It was put to use for both DC2 and Rome production. In total 2624 DC2 jobs and 2667 Rome production jobs were successfully run on the cluster since July 2004.

In the following we describe how the system has been used by researchers of the Universities of Bern and Geneva. So far, production for the SUSY group and reconstruction of Combined Test Beam data have been carried out. Also, some reconstruction of Rome data has been carried out, primarily for testing purposes.

The use of file catalogues has not been explored, instead the simple approach has been taken of specifying input and output files in job description (.xrsI) files as gridftp URLs of the form "gsi[ftp://server.com/some/file.root](ftp://server.com/some/file.root)". The storage elements can be accessed directly via gridftp, conveniently using the semi-graphical client "gsincftp" provided with the NorduGrid distribution.

This far, roughly the following jobs have been run:

* 100 SUSY event generation jobs (5500 events per job)
* 500 SUSY simulation, digitisation and reconstruction jobs (50 events per job)
* 50 CTB reconstruction jobs (10000 events per job)

#### 2.3.1 SUSY Event Generation

Running SUSY event generation on the Swiss batch facility was straight forward and no problems occurred. However, due to a bug concerning how parameters are passed to the tauola Monte Carlo program, which models tau lepton decays, the initially generated events were not useful for physics.

#### 2.3.2 SUSY Simulation, Digitisation and Reconstruction

In this use case we tried to run simulation, digitisation and reconstruction (ESD and AOD) on the Swiss batch facility, using the production system transformations. Chaining these four transformations was very ambitious and thus problems appeared. Things went wrong when running the transformations and when downloading/uploading the input/output files.

In the case of the NorduGrid front-end being congested with parallel file transfers, transfers can fail, causing jobs to fail. This problem was solved by strictly limiting the number of active download/upload threads on the NorduGrid front-ends of the clusters.

In the area of the ATLAS transformations, running reconstruction was the most difficult part. The most obvious problem with reconstruction is a memory leak, which in case of release 9.0.4 let the process grow to more than 1024 MB for 50 events. The local scheduler of the clusters killed the jobs, because 1024 MBs were requested in the job description as in some cluster the worker nodes are equipped with only 1024 MB of RAM. The only solution to this problem was to run the jobs on worker nodes equipped with more than 1024 GB per CPU and request more memory in the NorduGrid job description. Another but rarely occuring problem is that the athena framework sometimes crashes during the closing process after AOD production. This problem is not understood but not serious, because the AOD sample is produced correctly.

#### 2.3.3 CTB Reconstruction

The challenge of running the CTB reconstruction on the Swiss batch facility was mainly to run it outside CERN - without AFS. It seems that CTB reconstruction was initially thought to be run only in the CERN/AFS environment. The following problems needed to be solved for release 9.1.2:

* The files listed in "jobOptions/MuonTBRecExample/MuonTBLoadCal_jobOptions.py" are hardcoded to an AFS path. If one wants to run without of AFS, one needs to change the paths to the files to local paths and copy over the files from AFS.
* The conditions data cannot be accessed from outside CERN. This problem was solved by the Database group by packaging the relevant files into an archive and make it available for download. One has to download the file "/afs/cern.ch/user/a/atlcond/ctbrep/poolcond.tar.gz", unpack it, change the file "poolcond.xml" according to the new path and replace the line

PoolSvc.ReadCatalog+= [ "file:PoolFileCatalog.xml",

"mysqlcatalog_mysql://atlcond:aCfC2004@atlobk02.cern.ch:3306/oflpoolcond" ]with the absolute path to "poolcond.xml", e.g.

PoolSvc.ReadCatalog+= [ "file:PoolFileCatalog.xml",

"file:/terabig/atlsoft/PoolCondDB/poolcond/poolcond.xml" ] in "RecExTB_CondDB.py".

## 3 Experiences with the installation and operation of the country-wide batch system

### NorduGrid middleware

Under the assumption that a Linux cluster with a scheduling system is already in place, it is straight forward to install the NorduGrid middleware. The installation procedure is simple, because NorduGrid is not invasive and needs to be installed only on the front-end PC. The worker nodes do not "know" anything about NorduGrid but are exclusively communicating with the local scheduling system of the cluster. In addition, the NorduGrid installation procedure is well documented at the NorduGrid page [7]. The installation and configuration can be carried out by an experienced administrator within half a working day.

A disadvantage of NorduGrid is that only the interface to the PBS scheduler is fully supported. Interfaces to other schedulers like SGE (UBELIX cluster) are buggy and only partly supported. A significant amount of work time (4 full working days) was needed to debug and customise NorduGrid for SGE on UBELIX.

Although in general the system is quite stable, the grid-manager does crash occasionally. It would be desirable to be able to sign up for a central service, which would send out email notifications to the responsible persons when services go down. [Frederik will rephrase]

### ATLAS software

The ATLAS software is installed using the ATLAS pacman distribution kit. The installation procedure is simple under standard conditions; however under more special conditions (platforms and Linux flavours) some problems may occur with the installation procedure or with the ATLAS release itself and workarounds have to be found. But so far all required ATLAS releases could be installed and made work on all below-mentioned platforms within a reasonable amount of time. Feed-back was given to the pacman developer and the ATLAS developers responsible for the distribution kit.

Our system is highly non-uniform. Currently we have the ATLAS software working on the following platforms:

CPU architecture:

* i686* x86_64 (AMD Opteron, AMD Athlon, Intel XEON EM64T)

Operating System:

* Redhat 7.3 (phasing out)
* CERN Scientific Linux 3
* SUSE 9.2
* Gentoo 2004.2
* Mandrake 9.2

The ATLAS software can be made work on all mentioned hardware and software platforms with a reasonable effort. Usually the following customisation needs to be done:

* GCC 3.2 needs to be installed and the LD_LIBRARY_PATH environment variable needs to be set accordingly. GCC 3.2 does not need to be installed on every single worker node, it is enough to install it on the shared file system of the cluster.
* Some soft-links need to be adapted. For instance "libssl.so.0.9.7" is soft-linked as "libssl.so.2" on Redhat Linux but as "libssl.so.0" on SUSE Linux. It is enough to add a soft-link to "libssl.so.0.9.7" and call it "libssl.so.2". The soft-link can be in any directory LD_LIBRARY_PATH points to. Other libraries which typically need to be soft-linked manually are "libcrypto.so.0.9.7" and and "libX11.so.6.2" (as "libX11.so.6").
* On 64 bit machines some 32 bit libraries the ATLAS software needs can be missing. This problem can be fixed by installing these libraries from the 32 bit release of the very same Linux distribution.

The only problem appears at the post-installation step of the ATLAS release. If "SealPluginRefresh" fails due to a missing soft-link (e.g. libssl.so.2) there is no way to rerun it. The release installation process needs to be run from scratch.

There are multiple reasons for the non-uniformity of our system:

* Computer centres prefer to buy 64 bit machines, because they can be equipped with more than 4 Gigabyte of RAM. That is the main reason why the x86_64 platform has become a quasi-standard for new hardware.
* A common university cluster like UBELIX is a shared resource. It is not possible to impose a CERN Linux flavour on such an installation. In the case of UBELIX the choice of the IT services department of the university was Gentoo.
* CERN Scientific Linux 3, despite that it was released recently (Feb 05), does not fully run on modern 64 bit computers (e.g. AMD Opteron and Intel XEON EM64T). That is the reason why a fall-back solution was needed for the two 64bit PCs in the Bern ATLAS Cluster (SUSE 9.2).

We think that our situation is a very typical example of the reality the ATLAS collaboration will meet after the start-up of LHC. University groups will try to find computing resources at various places and will face non-uniformities. This fact has to be taken into account for the future ATLAS release strategy.

One general problem with the management of ATLAS software releases on clusters outside CERN is that the releases are often patched. It is then very easy for the system administrator to loose track of which version of release x.y.z is really installed on the cluster. This problem is currently being solved with improvements in the "diff" and "update" capabilities of the pacman tool.

## 4 Distributed interactive analysis on a local cluster

### Motivation

It is expected that the volumes of data with which physicists will be working regularly during ATLAS operation, will be much larger than those used when exercising ATLAS simulation. To give an concrete example, the total production of fully simulated and reconstructed events for the Rome physics workshop of ATLAS (in June 2005) will be of the order of a few million events. In comparison the LHC accelerator will collide bunches of protons at the rate of 40 MHz. At the design luminosity, around 25 proton collisions will take place per bunch crossing. The ATLAS data acquisition and on-line event selection system will reduce the rate by nearly six orders of magnitude, writing the data corresponding to around 300 bunch crossings (i.e. 7500 proton collisions) per second. At this rate it will take ATLAS around 22 minutes to write the data of 10 million proton collisions to disk files. Thus, the data samples that will need to be analysed will indeed quickly become much larger than those used in the ongoing simulations of ATLAS.

At the same time there is an advantage in working with the data in an interactive way. Obtaining a result rapidly speeds up development, enabling physicists to correct their algorithms and to refine their selection criteria in a shorter time. Efficiency of working with the data is an important factor in producing good quality physics results.

These premises motivate us to look for ways of working with large data samples in an interactive way. We are investigating ways of working interactively with the ATLAS software framework ATHENA [12]. To process these large samples interactively it is necessary to extend our interactive session to multiple PCs, processing events in parallel. Use of a local cluster of PCs, instead of one PC, seems to be a natural first step.

### Choice of the software infrastructure

The objective of our exercise was to setup parallel processing of ATLAS data in an interactive session using computers of a local cluster. To simplify the problem we are making the following assumptions:

* All the software that a user wants to execute is already installed on a shared file system of the cluster. The software can be partly in an ATLAS release (installed in one place for all users) and partly in a directory owned by the user, but both are visible to all the nodes of the cluster.

* All the data that the user wants to process is also already available to all the nodes of the cluster.

With the above assumptions the task of the software infrastructure becomes relatively simple. It needs to be able to

* split the input data,
* distribute the processing,
* collect and merge the results.

The Bern group is involved in the ATLAS Distributed Analysis project [10]. This development aims to support a variety of use cases that is far beyond the simple scenario proposed here. ADA aims to enable users to submit jobs to all kinds of Grid systems distributed globally. In addition the ADA software is under heavy development and as such does not yet have the documentation and the ease of use that was required. Installing an ADA server (i. e. a DIAL server) in Bern did not seem to be a simple solution to the relatively simple problem that we have defined.

A simple, stable and documented piece of software infrastructure that we were able to use was DIANE [9]. In the following section we will describe our experience with this package.

### User experience, performance measurements

The installation of DIANE on the Bern cluster turned out to be relatively straightforward. It required only a little assistance from the developer, an exchange of a few email messages was sufficient. The setup was created in a couple of days starting from software used earlier for a demonstration of DIANE at CERN. Compared to the demonstration, two issues needed to be addressed in order to have DIANE working on the LHEP cluster in Bern. It was necessary to

* define and separate the "user" part, as opposed to the "central setup" part that can be common to all users of the cluster,
* replace references to files on AFS with the corresponding references that are valid on the LHEP cluster, which has no AFS.

In order to run ATHENA with DIANE in Bern a user needs to provide the following files:

1. A script to run ATHENA including environment setup. The environment variables define also the user's code to be run in ATHENA. A "job options" file, which describes the configuration of ATHENA, is also defined in the script.
2. A "job description" file that defines some parameters of the process, such as the name of the output file that should be fetched from each node after the processing is finished.

3. A list of input data files.
4. A list of cluster nodes to be used.

An example of a user setup together with user instructions are provided at [11].

Using the mentioned files the DIANE software, which is a set of Python scripts, can distribute processing to the nodes of the cluster. Splitting of input data is done on a per input file level. The result files are collected and copied to one directory in the user's home directory. Files containing histograms in Root format are merged together and the histograms are added. The user finds the merged result file as well as the individual histogram files produced by each process.

The processes run interactively (under the user's account) and the processing starts within a few seconds on all the worker nodes. An example of how the processing time depends on the number of processors involved is shown in Fig.2. The used processors were Intel XEON 3.0 GHz CPUs.

The processing time decreases with more processors but it is not inversely proportional to it. An analysis process in ATHENA has a long startup time. Around 40 seconds (real time) are spent to setup the run time environment in CMT. ATHENA then needs about 15 seconds to load and to initialise all the software. In total almost one minute is spent before the event processing can start. After the startup the processing speed becomes limited by the CPU.

Figure 2: Processing time for an example AOD analysis process of 59000 events in ATHENA as a function of processors used.

Because the startup time is significant the actual gain does not scale linearly with the number of CPUs. It will also depend on the size of the data sample, the larger the sample the larger the gain from processing it in parallel on many CPUs. The result shown in Fig. 2 should be seen as an example.

### Ideas for a future development

Our current parallel-interactive setup should perhaps be called quasi-interactive. It submits short processes that execute with the priority of an interactive session, but the user does not operate the system from the ATHENA Python prompt. The user sends commands from the shell prompt and ATHENA effectively works in batch mode. The interactivity is limited to starting the jobs quickly on all the nodes.

An interesting development would be to parallelise an interactive ATHENA session. In such a session the user would not leave the prompt of interactive ATHENA. While the user would work in the ATHENA environment the commands that cause event transition (theApp.run(), theApp.nextEvent(),... in the ATHENA prompt) would execute in parallel on several PCs. Other commands that change the state of ATHENA, for instance load a library, would also need to execute in all the sub-processes. Such a setup would give a truly parallel and interactive environment for ATLAS data analysis. The development would need to be tightly coupled to the ATHENA framework and we do not yet know if it would be feasible.

## 5 Documentation and Communication

In order to have all information about the Swiss ATLAS Computing Prototype collected at a single place a "Swiss ATLAS Computing" section [13] created in the ATLAS Wiki. It mainly contains a collection of HOWTOs and of presentations given in the context of the Swiss ATLAS Computing. The URL of the Wiki page is : [https://uimon.cern.ch/twiki/bin/view/Atlas/SwissAtlasComputing](https://uimon.cern.ch/twiki/bin/view/Atlas/SwissAtlasComputing).

For the communication between the users a mailing list was setup on the CERN listbox server: atlas-swiss-computing@cern.ch.

## 6 Conclusion and outlook

### Summary of the experience

In 2007 when LHC becomes operational, the amount of data will be truly enormous and will be accessed by research groups around the world. The current exercise represents the efforts of a small country to prepare for this, using Grid tools available. The result is the Swiss ATLAS Grid.

Our overall experience is that it is possible to build a useful and working ATLAS computing system with today's tools.

The main advantages of the Swiss ATLAS Grid are its simplicity to set up and use, its stability and little to no need of maintenance.

The main limitation is the lack of sophisticated file replica management. But so far none of the Grid solutions could provide a convincing solution to this problem.

Despite the global nature of the Grid paradigm, the reality of a deployment of a system like the present, is that some manual local prioritisation, VO management and accounting is necessary. We believe that useful tools for job prioritisation according to VO membership are crucial for the Grid approach to LHC data analysis to be successful. Moreover, a logging service allowing easy accounting on a country-wide level would be highly appreciated.

### The Swiss ATLAS computing model

Based on the experience with the prototype a Swiss ATLAS Computing model is taking shape:

* The Tier-2 facility in Manno will serve as a batch production resource and as the main storage facility (disk and tape). Thanks to the high bandwidth network between the Swiss sites the largest part of the required storage can be concentrated at the Tier-2 facility.
* Sizeable shared university clusters (e.g. UBELIX) will serve as batch facilities. They build a counterweight to the batch facility at the Tier-2. This allows for load balancing between a facility exclusively used by particle physicists (Tier-2) and shared resources at the universities (Tier-3). Tier-2 and the Tier-3 facilities need to be accessible via an identical user interface.
* Both batch facilities are not well suitable to be used interactively, given the lack of interactivity in the current middleware implementations. Therefore small dedicated clusters (e.g. the Bern ATLAS cluster) owned and operated by the HEP groups will serve for this purpose. At the same time these small clusters can serve as test facilities to validate new releases of grid middleware or ATLAS software, before deploying them on the large facilities.

The size of the different facilities in terms of CPU and storage is an important motivation for our ongoing studies on the Swiss ATLAS Computing prototype (SUSY production and CTB analysis). We hope to derive reliable resource estimates from these activities.

### Future choice of Grid middleware

Considerations of manpower and efficiency listed already in the motivation section lead to a set of requirements for the GRID middleware. In order to be useful for our small research community the GRID software must be:

1. deployable on shared resources not dedicated to LHC only
2. deployable on small clusters (\(<\) 20 CPUs)3. deployable on any underlying Operating System and Local Resource Management System
4. easy to install and maintain
5. easy to use
6. stable

Up to now we do not have a real choice of the grid middleware other than NorduGrid. The Swiss ATLAS Computing prototype could not have been built with LCG, because it does not fulfil the requirements 1, 2 by design. There seems to be evidence of difficulty with the other requirements as well.

Never the less, we are carefully observing the evolution of various Grid projects and we are open to any future solution, which fulfils our requirements.

### Outlook

We plan to increase the Swiss ATLAS Computing Grid facility significantly in the near future:

* The university of Geneva plans to acquire a cluster of around 20 CPUs in summer 2005.
* For UBELIX, the acquisition of 128 additional CPUs is planned in autumn 2005.
* PHOENIX should double its size to 40 CPUs later this year.

In addition we hope to get the Swiss CMS and LHCb groups on board to build a common Swiss LHC Grid.

With the evolving Swiss ATLAS Computing Grid we are confident to meet the challenge of real ATLAS data and to become competitive in data analysis from the very beginning of ATLAS data taking.

### Acknowledgements

The authors would like to thank the Swiss physicists who contributed time and effort in testing our system with real-world computing jobs. In particular we are indebted to Eric Thomas, Ignacio Aracena and Valeria Perez Reale from Bern and Imma Riu Dachs and Manuel Maria Diaz Gomez from Geneva.

## References

* [1] I. Foster and C. Kesselman: _The Grid: Blueprint for a New Computing Infrastructure_ (1998), Morgan Kaufmann; 1st edition (November 1998), ISBN: 1558604758
* [2] D. Adams, D. Barberis, C. Bee, R. Hawkings, S. Jarp, R. Jones1, D.Malon, L. Poggioli, G. Poulard, D. Quarrie, T. Wenaus on behalf of the ATLAS Collaboration, "THE ATLAS COMPUTING MODEL", 10 January 2005, CERN-LHCC-2004-037/G-085, ATL-SOFT-2004-007, [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/computing-model/Comp-Model-January10.doc](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/computing-model/Comp-Model-January10.doc)
* [3] Ketevi A. Assamagan, Dario Barberis, Davide Costanzo, Edward Moyse, Giacomo Polesello, David Quarrie, David Rousseau (chair), R.D. Schaerk, Peter Sherwood, "Final report of the ATLAS AOD/ESD Denition Task Force", 14 December 2004, Revision 4, ATLAS-SOFT-2004-006, [http://doc.cern.ch/archive/electronic/cern/others/atlnot/Note/soft/soft-2004-006.pdf](http://doc.cern.ch/archive/electronic/cern/others/atlnot/Note/soft/soft-2004-006.pdf)
* [4] The IT services department of the University of Bern, [http://www.id.unibe.ch/](http://www.id.unibe.ch/)
* Swiss Academic and Research Network, [http://www.switch.ch/](http://www.switch.ch/)
* [6] The LHC Grid Computing Project, [http://cern.ch/lcg/](http://cern.ch/lcg/)
* [7] NorduGrid Collaboration, NorduGrid web site, [http://www.nordugrid.org/](http://www.nordugrid.org/)
* [8] SUN Grid Engine, [http://gridengine.sunsource.net/](http://gridengine.sunsource.net/)
* [9] The DIANE (Distributed Analysis) Project, [http://cern.ch/diane](http://cern.ch/diane)
* [10] The ADA (ATLAS Distributed Analysis) Project, [http://www.usatlas.bnl.gov/ADA/](http://www.usatlas.bnl.gov/ADA/)
* [11] User instructions "How to run distributed analysis on the Bern ATLAS cluster using DIANE", [https://uimon.cern.ch/twiki/bin/view/Atlas/DIANEinBern](https://uimon.cern.ch/twiki/bin/view/Atlas/DIANEinBern)
* [12] Analysis of ATLAS data in interactive ATHENA (an exercise), [https://uimon.cern.ch/twiki/bin/view/Main/SUSYPhysUserPython](https://uimon.cern.ch/twiki/bin/view/Main/SUSYPhysUserPython)
* [13] The Swiss ATLAS Computing Wiki, [https://uimon.cern.ch/twiki/bin/view/Atlas/SwissAtlasComputing](https://uimon.cern.ch/twiki/bin/view/Atlas/SwissAtlasComputing).