Active Learning reinterpretation of an ATLAS Dark Matter search constraining a model of a dark Higgs boson decaying to two \(b\)-quarks

The ATLAS Collaboration

A reinterpretation of a search for dark matter produced in association with a Higgs boson decaying to \(b\)-quarks using Active Learning, a technique to facilitate efficient and comprehensive inference in multi-dimensional new physics parameter spaces, is presented. The dataset has an integrated luminosity of \(139\,\mathrm{fb}^{-1}\) and was recorded with the ATLAS detector at the Large Hadron Collider at a centre-of-mass energy of \(\sqrt{s}\)=\(13\,\mathrm{TeV}\). The reinterpretation refers to a model predicting dark matter production in association with a dark sector Higgs boson decaying to \(b\)-quarks. The Active Learning approach makes use of a Gaussian Process to determine the exclusion limit contour and a corresponding uncertainty in a four-dimensional new physics parameter space. Each exclusion limit is determined accurately by means of the RECAST protocol. The combined approach of RECAST and Active Learning provides a blueprint for accurate, efficient and comprehensive interpretations of new physics searches at the Large Hadron Collider.

## 1 Introduction

The main purpose of the Large Hadron Collider (LHC) and its experiments is to search for physics beyond the standard model of elementary particles (BSM). This effort is guided by theoretical extensions of the standard model (SM) which predict new experimental signatures. In the absence of sufficiently significant evidence for BSM signatures at the LHC during the 12 years of physics analysis at the energy frontier so far, a plethora of proposals for BSM signatures has emerged. Most of the these proposals contain a multitude of parameters which need to be constrained by experimental data analysis. In practice, the desired comprehensive constraints on these multidimensional parameter spaces cannot be achieved by a naive sampling approach due to the computational costs of running the physics analysis workflow.

By far the most popular approach to this challenge of dimensionality is to reduce the parameter space down to a low number of dimensions, usually one or two. The resulting subset of the parameter space is sampled by means of a regular grid. In consequence, the computational costs of the interpretation effort are sufficiently limited and exclusion curves are obtained either as upper limits on cross-sections depending on one BSM parameter or exclusion contours in a space spanned by 2 BSM parameters. This approach inherently limits the outcome of searches for new physics. In contrast, extensions of such BSM interpretations to higher dimensions can not only provide further constraints on new physics models, but also reveal gaps in coverage within the BSM physics landscape which in some cases only become noticeable when previously unexplored parameters are investigated, thus motivating future analyses.

This note presents a new, efficient approach to the challenge of interpreting BSM searches in higher dimensions, leveraging the technique of Active Learning. Its methodology belongs to the design of experiments domain [1] and follows the iterative design paradigm established in the field of Machine Learning [2]. In an iterative procedure, new BSM parameter space points are sampled in batches and the upper limit on the BSM signal strength is determined, running the physics analysis pipeline from event simulation, reconstruction and selection to statistical inference for every selected point. After every iteration, a regression on the signal strength upper limit in the multidimensional space is performed by means of a Gaussian Process. Apart from these limits, the Gaussian Process also provides corresponding uncertainties, which are used to derive an information entropy as a measure of confidence in the current exclusion contour. For the following iteration, a new batch of parameter space points is analysed such that this confidence is raised, until a sufficiently accurate exclusion contour is obtained.

Previous research in Active Learning for the purpose of BSM interpretations in high energy physics (HEP) using LHC data has been performed either on the basis of pre-determined results [3] or approximate exclusion limits [4], whereas the work presented in this note incorporates the Active Learning approach into a complete physics analysis workflow such that queries for new data trigger the execution of this workflow and result in faithful exclusion limits based on an automated execution of the preserved analysis pipeline.

The search for dark matter produced in association with a SM Higgs boson decaying into \(b\)-quarks (Mono-H(\(\overline{\mathrm{b}}\)\(\overline{\mathrm{b}}\))), based on the full Run 2 dataset from the ATLAS detector, is used to demonstrate the improvement of BSM interpretations achieved with Active Learning. In this search events are selected using sophisticated means of particle reconstruction, including large-radius jets with a 2-prong substructure in accordance with the decay of a boosted Higgs boson into two \(b\)-quarks. Therefore its accurate reinterpretation requires a detailed simulation of the ATLAS detector and cannot easily be achieved by approximate means. The required fully accurate and efficient re-execution of the analysis workflow is achieved by means of the RECAST protocol [5, 6]. The search is reinterpreted in terms of a dark Higgs boson model [7] including four non-trivial parameters which determine the couplings and the masses of dark matter particles,boson mediators and dark Higgs bosons. The requirement for the execution of the complete analysis workflow and the higher dimensionality of the BSM parameter space make this reinterpretation a prime use case for Active Learning.

This note is organised as follows. Section 2 presents the dark Higgs model used for the reinterpretation of the Mono-H(\(\mathrm{b\overline{b}}\)) search. Section 3 provides an overview of the Mono-H(\(\mathrm{b\overline{b}}\)) search, while Section 4 presents a simplified version of this search which is used in a pre-training in order to raise the efficiency of the Active Learning approach, followed by Section 5, which presents the analysis workflow. Section 6 presents the Active Learning approach and the resulting reinterpretation of the Mono-H(\(\mathrm{b\overline{b}}\)) search is presented in Section 7. Conclusions are drawn in Section 8.

## 2 Dark Higgs boson signal model

The Mono-H(\(\mathrm{b\overline{b}}\)) search is reinterpreted in view of a dark Higgs boson model [7], which has been investigated in view of several final states at LHC [8; 9] but in case of the Mono-H(\(\mathrm{b\overline{b}}\)) signature an investigation based on the full Run 2 dataset has not been provided so far. The model is motivated by the need to generate masses of dark sector particles, which it achieves by means of spontaneous breaking of a new \(U(1)^{\prime}\) gauge symmetry in the dark sector of particles. The associated scalar particle, the dark higgs boson \(s\), has a small degree of mixing with the Higgs boson of the standard model, which leads to similar decay channels for the two scalar particles. In addition, the model includes a neutral, heavy spin-1 particle \(Z^{\prime}\), which is the gauge boson of the spontaneously broken \(U(1)^{\prime}\) symmetry, and weakly interacting, heavy Majorana fermions \(\chi\), which are candidates for dark matter. The \(Z^{\prime}\) particle interacts with the \(\chi\) and \(s\) particles as well as with quarks, which leads to experimental signatures which can be probed at LHC (Fig. 1). In particular, the production of dark matter particles \(\chi\) together with a dark Higgs boson which decays into \(b\)-quarks leads to the Mono-H(\(\mathrm{b\overline{b}}\)) signature. The discovery of new physics in agreement with this model at LHC is not excluded by auxiliary constraints provided by the dark matter relic abundance in the universe and searches for \(Z^{\prime}\) resonances at LHC [10].

The parameters of the model are the masses of the dark Higgs, dark matter and \(Z^{\prime}\) particles, \(m_{s}\), \(m_{\chi}\) and \(m_{Z^{\prime}}\), respectively, and the coupling strengths of the interactions between the \(Z^{\prime}\) particle and the dark matter particles as well as quarks, \(g_{\chi}\) and \(g_{q}\), respectively, as shown in the model's Lagrangians (Eq. 1 and 2). Experimentally, the dark Higgs boson mass \(m_{s}\) can be reconstructed as the invariant mass of a pair of \(b\)-tagged jets \(m_{bb}\). Hence the Mono-H(\(\mathrm{b\overline{b}}\)) search achieves sensitivity to this model starting from its lower \(m_{bb}\) threshold at \(50\,\mathrm{GeV}\) up to a value of \(160\,\mathrm{GeV}\) above which the dark Higgs boson decays predominantly into a pair of \(W\)-bosons.

\[\mathcal{L}_{\chi}=-\frac{1}{2}g_{\chi}Z^{\prime\mu}\overline{\chi}\gamma^{5} \gamma_{\mu}\chi-g_{\chi}\frac{m_{\chi}}{m_{Z^{\prime}}}s\overline{\chi}\chi +2g_{\chi}Z^{\prime\mu}Z^{\prime}_{\mu}(g_{\chi}s^{2}+m_{Z^{\prime}}s). \tag{1}\]

\[\mathcal{L}_{q-Z^{\prime}}=-g_{q}Z^{\prime\mu}\overline{q}\gamma_{\mu}q. \tag{2}\]

Samples of collision events based on this model are simulated at leading order in perturbation theory using the event generator setup described in Ref. [11]. Systematic uncertainties on this signal model have been investigated in the context of Ref. [11] and were found to be negligible.

## 3 The Mono-H(\(\mathrm{b\overline{b}}\)) search

The physics analysis subject to the following reinterpretation selects events with missing transverse momentum or energy (\(E_{\mathrm{T}}^{\mathrm{miss}}\)) and at least two jets which contain decay products of \(B\)-hadrons. This signature targets the production of dark matter particles (or other weakly interacting particles) together with a SM Higgs boson which decays into a pair of \(b\)-quarks, denoted Mono-H(\(\mathrm{b\overline{b}}\)). The SM process of associated production of a Higgs boson together with a \(Z\)-boson which decays into neutrinos is considered as a background process, together with further, reducible backgrounds. Additional contributions due to new physics are searched for in this analysis, while a wide range of such scattering processes beyond the SM are allowed to enter the event selection. In particular, the selected values of the \(b\)-jet pair invariant mass are not restricted to the Higgs boson mass but range from 50 to 280. Therefore the analysis is well suited for further interpretations, including the dark Higgs model for which the mass of the new scalar particle decaying to \(b\)-quark pairs is unknown.

The event selection distinguishes between low and high transverse momentum of the \(b\)-jet pair, distinguished by \(E_{\mathrm{T}}^{\mathrm{miss}}\) as a measure of total transverse momentum in a collision event. For low \(E_{\mathrm{T}}^{\mathrm{miss}}\) events, the amount of background processes is reduced by the requirement of a lower threshold on the \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance, which is a measure of the likelihood that the observed \(E_{\mathrm{T}}^{\mathrm{miss}}\) stems from weakly interacting particles rather than mismeasurements. This \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance is derived from the resolutions of all objects which enter the \(E_{\mathrm{T}}^{\mathrm{miss}}\) calculation as well as their covariance matrix. For high \(E_{\mathrm{T}}^{\mathrm{miss}}\) events, an overlap of the \(b\)-jets is expected in signal events. Hence in this region a large-radius (large-R) jet with a substructure consistent with a \(b\)-jet pair is required. The substructure requirement refers to jets derived from inner detector tracks using a variant of the anti-\(k_{t}\) jet clustering algorithm where the radius parameter shrinks as the jet transverse momentum increases. Two of these variable-radius (VR) track-jets, contained in the large-R jet and identified as \(b\)-jets, are required for each high \(E_{\mathrm{T}}^{\mathrm{miss}}\) event.

In addition to the signal event selection which rejects events with charged leptons, control regions are defined similarly to the signal region but with the requirement of one or two charged leptons in order to constrain SM background contributions in the final statistical evaluation.

The selected signal region events are distinguished by \(E_{\mathrm{T}}^{\mathrm{miss}}\), starting at the value of 150 where the employed \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger becomes efficient. For each \(E_{\mathrm{T}}^{\mathrm{miss}}\) interval, a range of \(b\)-jet pair invariant masses \(m_{bb}\) is considered. The concluding statistical fitting procedure provides sensitivity in particular to new physics models which result in large \(E_{\mathrm{T}}^{\mathrm{miss}}\) and a resonant enhancement in the distribution of the \(m_{bb}\) variable.

Figure 1: Born-level Feynman diagrams of the production of a dark Higgs boson \(s\) together with a pair of dark matter particles \(\chi\), mediated by a \(Z^{\prime}\) particle which also interacts with the initial state quarks. The dark Higgs boson decays into a pair of \(b\)-quarks while the weakly interacting dark matter particles escape the detector without a trace, resulting in the Mono-H(\(\mathrm{b\overline{b}}\)) signature.

## 4 SimpleAnalysis implementation

A simplified implementation of the Mono-H(\(\mathrm{b\overline{b}}\)) search is implemented using the SimpleAnalysis framework [12]. The fast evaluation of a large number of different configurations of the model under study is enabled by simulating the signal process without considering the detector response in detail.

SimpleAnalysis provides simplified implementations of the detector response and the physics object reconstruction as well as the event selection logic for a given physics analysis. The distributions of simulated particles are smeared using parameterisations for object reconstruction and identification efficiencies, as well as for their resolutions. Typically these efficiencies and resolutions are parameterised as functions in \(p_{\mathrm{T}}\), \(\eta\) and \(\phi\) in correspondence with the working points employed in the physics object reconstruction and identification.

The SimpleAnalysis implementation of the Mono-H(\(\mathrm{b\overline{b}}\)) search is based on electrons, muons, tau leptons, jets, and missing transverse momentum. The object definitions are provided in Table 1. The flavour-tagging efficiencies of \(R=0.4\)-jets and VR track-jets are approximated using measured values from MC simulation of top-quark pair production events. The missing transverse momentum is computed using the transverse momenta of the smeared physics objects and an approximation of the inner detector track term contributing to \(E_{\mathrm{T}}^{\mathrm{miss}}\) as measured in \(\mathrm{Z}\to\mathrm{ee}\) events. The overlap removal procedure which prevents the same detector signals being interpreted as different objects, is applied to the smeared physics objects. The largest differences to the published search are in the implementation of the object-based \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance and in the association of VR track jets with the large-R jets.

The event selection of the simplified Mono-H(\(\mathrm{b\overline{b}}\)) search implementation resembles the published result except for the requirements on data quality, trigger, and the tau lepton veto. The event selection requirements are documented in Ref. [13]. The effect of the event selection requirements are shown in Figure 2 for two signal configurations with \(m_{\chi}=200\,\mathrm{GeV}\). The first configuration represents a boosted jet signature with \(m_{\chi^{\prime}}=2.5\,\mathrm{TeV}\) and \(m_{s}=50\,\mathrm{GeV}\). The second configuration represents a resolved jet signature with \(m_{\chi^{\prime}}=500\,\mathrm{GeV}\) and \(m_{s}=130\,\mathrm{GeV}\). Large differences between the full detector simulation and the SimpleAnalysis implementation are observed for the \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger requirement. The subsequent requirement on \(E_{\mathrm{T}}^{\mathrm{miss}}\) mostly alleviates this differences, while the approximate implementations of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance and the VR track jets result in residual differences of up to 20%.

\begin{table}
\begin{tabular}{l l} \hline \hline Object & Definition \\ \hline
**Electrons** & \(p_{\mathrm{T}}>7\,\mathrm{GeV},\abs{\eta}<2.47\), \(\mathrm{loose}\) identification working point \\
**Muons** & \(p_{\mathrm{T}}>7\,\mathrm{GeV},\abs{\eta}<2.5\), \(\mathrm{loose}\) identification working point \\
**Tau leptons** & \(p_{\mathrm{T}}>20\,\mathrm{GeV},\abs{\eta}<2.5\), \(\mathrm{very}\)\(\mathrm{loose}\) identification working point \\
**\(R=0.4\)-jets** & \(p_{\mathrm{T}}>20\,\mathrm{GeV},\abs{\eta}<2.5\) and \(\mathrm{medium}\) JVT working point; \\  & or \(p_{\mathrm{T}}>30\,\mathrm{GeV}\) and \(2.5<\abs{\eta}<4.5\); \(b\)-jet identification with 77\% efficiency \\
**\(R=1.0\)-jets** & \(p_{\mathrm{T}}>200\,\mathrm{GeV},\abs{\eta}<2.0\) \\
**VR track jets** & \(p_{\mathrm{T}}>10\,\mathrm{GeV},\abs{\eta}<2.5\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Object definitions of electrons, muons, tau leptons, and jets used in the SimpleAnalysis implementation of the Mono-H(\(\mathrm{b\overline{b}}\)) search.

## 5 RECAST workflow

The reinterpretation of the Mono-H(\(\mathrm{b\overline{b}}\)) analysis is performed in an iterative procedure where new upper limits on the dark Higgs boson signal strength are determined repeatedly for new parameter space points according to the following workflow. In a first step, configuration files for the Monte Carlo simulation of the signal scattering process according to the desired parameters are prepared and registered in a software repository available for computing processes running on the LHC computing grid. In a second step, these processes run the event generation described in Section 2, followed by a simulation of the ATLAS detector at full accuracy using the GEANT4 simulation toolkit [14]. Contrary to most of the new physics signal Monte Carlo event generation tasks at the LHC, the full simulation accuracy rather than an approximate, faster simulation is needed due to the jet substructure requirement involved in the event selection. Finally, the resulting simulated signal events are analysed on CERN's reusable analysis platform REANA [15] using the RECAST protocol [6; 16]. Based on a Linux container image of the physics analysis software, the experimental data and simulated background estimate is used together with the new signal estimation following a prewritten, fixed and automatised workflow, fully equivalent to the original physics analysis procedure [6; 17]. For the approximately 200 new physics parameter sets evaluated per iteration, this workflow takes approximately one week, with approximately equal durations for the three analysis steps.

Figure 2: Efficiency for applying event selection requirements for the merged (left) and resolved (right) event selection, comparing the full analysis based on a simulated detector response and the SimpleAnalysis implementation. Two signal configurations are displayed with \(m_{\chi^{\prime}}=2.5\,\mathrm{TeV}\), \(m_{\chi}=200\,\mathrm{GeV}\) and \(m_{s}=50\,\mathrm{GeV}\) (top), as well as \(m_{\chi^{\prime}}=500\,\mathrm{GeV}\), \(m_{\chi}=200\,\mathrm{GeV}\), and \(m_{s}=130\,\mathrm{GeV}\) (bottom).

## 6 Active Learning approach

Active learning is an iterative procedure to collect new training data for an optimisation task in an efficient manner [18]. In the reinterpretation effort discussed below, the optimisation taks is the classification of new physics parameter points \(\mathbf{\theta}\) regarding their exclusion by the Mono-H(\(\rm b\overline{b}\)) search. The four parameters from Eq.1 are used as they affect the cross-section exclusion limits in terms of changes of the event selection efficiency while the coupling strength \(g_{q}\) from Eq. 2 affects meraly the total cross-section:

\[\mathbf{\theta}=\left(m_{Z^{\prime}},m_{s},m_{\chi^{\prime}}g_{\chi}\right). \tag{3}\]

Within the space spanned by \(\mathbf{\theta}\), the logarithm of the signal strength upper limit is determined,

\[y\coloneqq\log\left(\frac{\sigma_{\text{experiment}}^{\text{Upper Limit}}}{\sigma_{\text{theory}}}\right), \tag{4}\]

to find the exclusion contour \(y=0\). The choice of the logarithm is made in order to obtain an approximately linear function \(y(\mathbf{\theta})\), such that the optimisation process is simplified.

Sets of data points \(\{y^{i}\}\) are obtained in iteratively where each value is computed using the complete physics analysis workflow described in the previous section.

In order to determine useful parameter space points \(\mathbf{\theta}\) already for the first iteration of data acquisition with RECAST, approximate estimations of the signal strength upper limits are determined. While total cross-sections of the signal scattering process provide such estimates in a particularly simple and still useful manner, the reinterpretation discussed in this note goes one step further and uses upper limits on the signal strength determined by means of the SimpleAnalysis implementation discussed in Section 4. An equidistant grid of approximately 5000 points is used with the following bounds: \(\text{m}_{Z^{\prime}}\in[500,5000]\) GeV, for Dark Higgs mass \(\text{m}_{s}\in[50,150]\) GeV, for Dark Matter mass \(\text{m}_{\chi}\in[100,1200]\) GeV and finally \(g_{\chi}\in[0.5,2.0]\).

### Gaussian Processes

Regression on the function \(y(\mathbf{\theta})\) makes use of a Gaussian Process [19][20]. In other words, any vector of \(N\) function values \(\{y^{i}\}_{i=1}^{N}\) follows a multivariate Gaussian distribution. As a key result, not only mean values but also uncertainty estimates on the function value are obtained. The active learning approach takes advantage of these uncertainties in order to progressively narrow the estimate on the exclusion contour \(y=0\).

To incorporate the SimpleAnalysis upper limits, a 2-task Gaussian Process is implemented, distinguishing the upper limit values by their association either with the SimpleAnalysis or the RECAST case. This combination takes advantage of the less accurate, yet computationally inexpensive limits determined with SimpleAnalysis such that fewer evaluations of RECAST are needed, reducing the overall computationalbudget required to determine the exclusion limit contour accurately. Using subscripts \(s\) and \(r\) accordingly, for instance SimpleAnalysis limits at \(\mathbf{\theta}\) and RECAST limits at \(\mathbf{\theta}^{\prime}\) are distributed according to

\[\begin{pmatrix}y_{s}(\mathbf{\theta})\\ y_{r}(\mathbf{\theta}^{\prime})\end{pmatrix}\sim\mathcal{GP}\left(\begin{pmatrix}m( \mathbf{\theta})\\ m(\mathbf{\theta}^{\prime})\end{pmatrix},\Sigma_{sr}(\mathbf{\theta},\mathbf{\theta}^{ \prime})\right),\quad\Sigma_{sr}(\mathbf{\theta},\mathbf{\theta}^{\prime})=\begin{pmatrix} k_{ss}(\mathbf{\theta},\mathbf{\theta})&k_{sr}(\mathbf{\theta},\mathbf{\theta}^{\prime})\\ k_{sr}(\mathbf{\theta},\mathbf{\theta}^{\prime})&k_{rr}(\mathbf{\theta}^{\prime},\mathbf{ \theta}^{\prime})\end{pmatrix}. \tag{5}\]

The Gaussian Process mean \(m\) and kernel \(k\) depend on hyperparameters according to

\[m(\mathbf{\theta})=\mathbf{w}^{T}\mathbf{\theta}+b\, \tag{6}\]

\[k_{ij}(\mathbf{\theta},\mathbf{\theta}^{\prime})=k(\mathbf{\theta},\mathbf{\theta}^{\prime}) \kappa_{ij}+\epsilon^{2}\delta(\mathbf{\theta},\mathbf{\theta}^{\prime})\,\quad\delta(\mathbf{\theta},\mathbf{\theta}^{\prime})=\begin{cases}1,& \text{if}\ \ \mathbf{\theta}=\mathbf{\theta}^{\prime}\\ 0,&\text{else}\end{cases}\, \tag{7}\]

\[k(\mathbf{\theta},\mathbf{\theta}^{\prime})=\exp\left(-\frac{||\mathbf{\theta}-\mathbf{ \theta}^{\prime}||^{2}}{2\,l^{2}}\right)\, \tag{8}\]

\[\kappa_{ij}=\begin{cases}\sigma_{s}\text{ if }i=j=s\\ \sigma_{r}\text{ if }i=j=r\\ \sigma_{sr}\text{ if }i\neq j\end{cases}. \tag{9}\]

The parameter \(\sigma_{sr}\) correlates limits obtained with SimpleAnalysis and RECAST such that the aforementioned reduction of the overall computational cost of the determination of the exclusion limit contour is achieved. The hyperparameter \(l\) controls the correlations between different parameter space points. With \(\mathbf{w}\) being four-dimensional and the remaining hyperparameters \(b\), \(l\), \(\varepsilon\), \(\sigma_{s}\), \(\sigma_{r}\) and \(\sigma_{sr}\) being one-dimensional real values, there are 10 hyperparameters in total. Given a predefined dataset \(\{(y_{i},\mathbf{\theta})\}\), these hyperparameters are determined by maximising the likelihood function in accordance with Eq. 5.

### Data Acquisition

For the active learning approach, the uncertainty estimate encoded in the kernel \(k\) is the key asset of the Gaussian Process. For each point in the parameter space \(\mathbf{\theta}\), the probability of it being excluded by the physics analysis is computed according to the Gaussian Process. While eq. 5 denotes the so-called prior Gaussian Process, a posterior Gaussian Process is computed in order to incorporate the evaluated exclusion limits. These conditional probabilities are again Gaussian distributions. Their mean values \(\mu\) and standard deviations \(\sigma\) at each parameter space point \(\mathbf{\theta}\) depend on the hyperparameters and on the evaluated exclusion limits. Hence the desired exclusion probabilities are

\[p_{\text{excl}}\left(\mathbf{\theta}\right)=\int_{-\infty}^{0}\,g\,\left(y\mid\mu (\mathbf{\theta}),\ \sigma(\mathbf{\theta})\right)dy \tag{10}\]

This exclusion probability is turned into the exclusion entropy,

\[H_{\text{excl}}(\mathbf{\theta})=-p_{\text{excl}}(\mathbf{\theta})\log p_{\text{excl}} (\mathbf{\theta})-\left(1-p_{\text{excl}}(\mathbf{\theta})\right)\log\left(1-p_{\text{ excl}}(\mathbf{\theta})\right)\, \tag{11}\]which is a measure for the uncertainty about the exclusion of the parameter space point \(\theta\). It vanishes in the cases of certainty \(p_{\rm excl}=0\) and \(p_{\rm excl}=1\), and it is maximal in case of maximal uncertainty \(p_{\rm excl}=0.5\). The objective of the active learning approach is to arrive at a low exclusion entropy across the parameter space.

The evaluation of a new signal strength upper limit and the corresponding update of the Gaussian process result in a reduction of the exclusion entropy in a volume around a new parameter space point. The size of this volume depends on its distance to the exclusion contour. In parameter space regions which are clearly excluded or clearly not excluded, only few points are needed to arrive at a low exclusion entropy, while close to the exclusion boundary more datapoints are needed to reduce this uncertainty measure.

The active learning approach used in this note takes advantage of this property of the exclusion entropy in order to efficiently determine the exclusion boundary. In each iteration, approximately 200 new parameter space points are evaluated and the Gaussian Process is updated accordingly. The resulting exclusion entropy is evaluated on a discrete grid of parameter points and \(N\) points of highest exclusion entropy are chosen for the next iteration. This approach reduces the exclusion entropy quickly along the currently estimated exclusion contour, but only leaves a small amount of variation for points to be spread around this contour. Such a variation is also required though, since the estimate of the exclusion entropy in a given iteration is not fully accurate. Furthermore, apart from the exclusion contour referring to the mean expected experimental data, also the variations of this contour by one standard deviation as well as the exclusion contour referring to the observed experimental data need to be determined, which gives further need for a variation in the acquisition of new data points.

For these reasons, only half of the new data points per iteration are acquired according to the exclusion entropy minimisation principle, while the other points are acquired according to a uniform random sampling procedure performed within a band around the expected exclusion contour. The width of this band is chosen to be twice as large as the estimated standard deviation of the expected limit contour. The random sampling of data points within this band is performed such that the distance between each pair of these points is bounded by a lower threshold in order to improve the coverage of the parameter space [21].

The first three out of the four iterations of data acquisition with RECAST were affected by a numerical instability which introduced noise regarding the estimated uncertainties of exclusion limits derived from the Gaussian Process. Mean values were not affected by this effect. For the final, fourth iteration of data acquisition this effect was spotted and removed by fixing the evaluation settings. Hence neither the mean values nor the uncertainty estimates of the Gaussian Process presented below are affected by it. However, a future application of Active Learning as discussed in this note would benefit from proper Gaussian Process uncertainty estimates throughout all iterations of data acquisition, namely in terms of more accurate exclusion entropy estimates and hence a lower number of data points to be acquired for the determination of sufficiently accurate exclusion limit contours.

## 7 Reinterpretation of the Mono-H(\(\overline{\rm b}\)\(\overline{\rm b}\)) search

The reinterpretation of the Run 2 Mono-H(\(\overline{\rm b}\)\(\overline{\rm b}\)) analysis regarding the dark Higgs boson model uses a Gaussian Process optimisation as described in Section 6.1 for the determination of each of the four desired exclusion limit contours, namely the limit given observed data as well as the limits given expected data and its up and downward variations by one standard deviation according to the SM prediction. The limits determined by means of SimpleAnalysis and RECAST are used together accordin to the following procedure:

* _Warm Start_: Signal strength upper limits are evaluated at 5 000 parameter space points on an equidistant grid using SimpleAnalysis. A Gaussian Process similar to the one described by eq. 5 but for only a single task is trained with this dataset in order to achieve a first, computationally inexpensive estimation of the exclusion contour.
* _RECAST iterations_: The following three steps are performed repeatedly until the precision of the obtained limit contour is sufficiently high. 1. _Limit evaluation_: Approximately 200 new parameter space points are determined according to the data acquisition procedure described in Sec. 6.2. The Mono-H(\(\mathrm{b\overline{b}}\)) analysis is evaluated for each of these points as described in Sec. 5.
2. _Training_: The Gaussian Process of eq. 5 is trained using all 5 000 limits obtained with SimpleAnalysis and all limits obtained with RECAST so far.
3. _Inference and diagnostics_: The intrinsic uncertainty on the expected limit contour described by the updated Gaussian Process is compared with the expected variations of this contour caused by fluctuations in the experimental data by a standard deviation. The intrinsic uncertainty of the Gaussian process is required to be the smaller one among these two uncertainties.

The resulting approximately 800 limit evaluations with RECAST mean a relevant reduction of computational cost compared to the conventional regular grid approach which would typically use approximately \(10^{4}\) such evaluations for this four-dimensional space. Further reductions of computational cost are possible by means of future research into improvements of the employed data acquisition function described in Section 6.2. The implementation of the Gaussian Process training makes use of the GPTorch program [22] and takes up to 3 hours using a single V100 NVIDIA graphics processing unit with 32 GB random-access memory.

Figure 3 shows the SM background prediction of the dark Higgs boson candidate mass fitted to the data in a signal region where the event selection includes a large-R jet. A dark Higgs boson signal distribution with a dark Higgs boson mass of 90 GeV as used for reinterpretations of the physics analysis is shown as well. Figure 4 shows how the newly determined exclusion limits approach the exclusion contour, proving the convergence of the Active Learning approach. Figure 5 compares Gaussian Process predictions with the actual exclusion limits on an independent test dataset, proving the accuracy of the Active Learning approach. Figure 6 shows the resulting exclusion limits in a two-dimensional plane, using the Gaussian Process defined in the four dimensional superset space. The Run 2 Mono-H(\(\mathrm{b\overline{b}}\)) analysis improves the previous exclusion limits based on a predecessor analysis and a fraction of the Run 2 dataset by approximately 300 GeV in terms of the \(Z^{\prime}\) mass. Figure 7 shows the exclusion limits in a one-dimensional slice of the parameter space. Figures 8 and 9 show the obtained exclusion limits in the complete four-dimensional parameter space. Figure 11 and 10 show the exclusion entropy, which demonstrates the convergence of the Active Learning approach as well.

Figure 12 compares the fully accurate exclusion limits with approximate exclusion limits. In addition to the exclusion limits obtained with SimpleAnalysis and RECAST, also contours where the total cross-section of the signal process amounts to 1 fb is presented. This value is of the order of magnitude of cross-section exclusion limits of the physics analysis. These contours provide a useful approximation of the actual exclusion limits and are obtained at a particularly low computational cost. Compared to these cross-section

contours, no significant improvement due to SimpleAnalysis is observed, demonstrating the need to perform fully accurate reinterpretations using RECAST.

Figure 3: Distributions of the dark Higgs boson candidate mass in the 2 \(b\)-tag signal region with \(E_{\mathrm{T}}^{\mathrm{miss}}\) between 500 and 750 GeV. The top panel compares the fitted background yields with data, while the bottom panel indicates the ratio of the observed data to the predicted Standard Model backgrounds. The background yields are obtained in a background-only fit to data. An example signal model from is overlayed for comparison and the corresponding cross-section is scaled by a factor of 3 for better visibility.

Figure 4: Convergence of the Active Learning algorithm. From one iteration to the next, the upper limit values at the newly investigated parameter space points \(\mathbf{\theta}\) approach the exclusion value \(y=0\).

Figure 5: Classification accuracy of the exclusion limits determined by the Gaussian Process in four dimensions. An independent test dataset of points not entering the Gaussian Process definition is used. The resulting classification accuracy amounts to 88 %.

Figure 6: Exclusion limits on the mediator masses \(m_{Z^{\prime}}\) and \(m_{s}\) in the plane \(m_{\chi}\) = 200 GeV, \(g_{\chi}\)=1.0 and \(g_{q}\) =0.25. The regions to the left of the contours are excluded. The exclusion limits are determined by a Gaussian Process in four dimensions where also \(m_{\chi}\) and \(g_{\chi}\) vary. The exclusion limits of the previous Mono-H(\(\mathrm{b\overline{b}}\)) search using a fraction of the Run 2 data are improved by approximately 300 GeV in terms of \(m_{Z^{\prime}}\).

Figure 7: Exclusion limits according to the Gaussian Process fit in four dimensions, presented in a one-dimensional slice of the parameter space, together with two evaluated exclusion limits.

Figure 8: Exclusion limits determined by a Gaussian Process in four dimensions. Each slice presents limits on \(m_{Z^{\prime}}\) and \(m_{s}\) for different values of \(g_{\chi}\) varying horizontally and \(m_{\chi}\) varying vertically. The off-shell region in the lower left corner of the figure where \(m_{\chi}\) is high compared to \(m_{Z^{\prime}}\) cannot be excluded. The grey bands indicate the intrinsic uncertainty of the Gaussian Process, which is negligibly small.

Figure 9: Exclusion limits determined by a Gaussian Process in four dimensions. Each slice presents limits on \(m_{Z^{\prime}}\) and \(m_{\chi}\) for different values of \(g_{\chi}\) varying horizontally and \(m_{s}\) varying vertically. The grey bands indicate the intrinsic uncertainty of the Gaussian Process, which is negligibly small.

Figure 10: Exclusion entropy according to a Gaussian Process in four dimensions. Large exclusion entropies are only observed close to the exclusion contours presented in Fig. 8, which shows the convergence of the Gaussian Process.

Figure 11: Exclusion entropy according to a Gaussian Process in four dimensions. Large exclusion entropies are only observed close to the exclusion contours presented in Fig. 9, which shows the convergence of the Gaussian Process.

Figure 12: Comparison of dark Higgs exclusion limits in two-dimensional slices. The fully accurate limits (dashed line) are approximated by SimpleAnalysis limits (solid line) and total cross-section contours (dotted line). The latter are particularly fast to compute and already provide a useful approximation. Still both approximations differ to a significant amount from the fully accurate limits, indicating the necessity for the physics analysis interpretation using the complete analysis workflow (RECAST).

## 8 Conclusion

In this note, an efficient reinterpretation of an existing dark matter search with the ATLAS detector using proton-proton collisions has been presented. The physics analysis investigates events with two \(b\)-jets and missing transverse momentum, relevant for extensions of the SM which predict associated productions of dark matter and a Higgs boson. A corresponding new physics model where the \(b\)-jets stem from a dark sector Higgs boson decay has been studied with respect to its four non-trivial parameters, namely the mass of this dark Higgs boson, the dark matter mass, the mass of a new spin-1 mediator \(Z^{\prime}\) and the strength of the coupling between the two latter particles. In order to determine the exclusion contour in this four-dimensional space using a limited computational budget, an Active Learning approach is used where a Gaussian Process fits the upper limit on the signal strength. Together with the estimation of this limit, the Gaussian Process also provides an uncertainty on it, which is used to determine new parameter space points where limits are evaluated. Within this iterative procedure, the exclusion limits are computed with full accuracy using the RECAST protocol. The resulting Gaussian Process captures the exclusion contour across the whole new physics parameter space under investigation. Combining Active Learning and RECAST, this note demonstrates how to interpret new physics searches at LHC in an accurate, efficient and comprehensive manner.

## References

* [1] R. Fisher, _The design of experiments_, Macmillan, 1935 (cit. on p. 2).
* [2] L. Heinrich, _Searches for Supersymmetry, RECAST, and Contributions to Computational High Energy Physics_, English, PhD thesis, 2019 330, isbn: 978-1-392-00471-5, url: [http://proxy.library.nyu.edu/login?qurl=https%3A%2F%2Fwww.proquest.com%2Fdissertations-theses%2Fsearches-supersymmetry-recast-contributions%2Fdocview%2F2201490026%2Fse-2](http://proxy.library.nyu.edu/login?qurl=https%3A%2F%2Fwww.proquest.com%2Fdissertations-theses%2Fsearches-supersymmetry-recast-contributions%2Fdocview%2F2201490026%2Fse-2) (cit. on p. 2).
* [3] S. Caron, T. Heskes, S. Otten, and B. Stienen, _Constraining the Parameters of High-Dimensional Models with Active Learning_, Eur. Phys. J. C **79** (2019) 944, arXiv: 1905.08628 [cs.LG] (cit. on p. 2).
* [4] J. Rocamonde, L. Corpe, G. Zilgalvis, M. Avramidou, and J. Butterworth, _Picking the low-hanging fruit: testing new physics at scale with active learning_, SciPost Phys. **13** (2022) 002, arXiv: 2202.05882 [hep-ph] (cit. on p. 2).
* [5] K. Cranmer and I. Yavin, _RECAST -- extending the impact of existing analyses_, Journal of High Energy Physics **2011** (2011), url: [https://doi.org/10.1007/jhep04](https://doi.org/10.1007/jhep04)(2011)038 (cit. on p. 2).
* [6] K. Cranmer, L. Heinrich, and on behalf of the ATLAS collaboration, _Analysis Preservation and Systematic Reinterpretation within the ATLAS experiment_, Journal of Physics: Conference Series **1085** (2018) 042011, url: [https://dx.doi.org/10.1088/1742-6596/1085/4/042011](https://dx.doi.org/10.1088/1742-6596/1085/4/042011) (cit. on pp. 2, 6).
* [7] M. Duerr et al., _Hunting the dark Higgs_, JHEP **04** (2017) 143, arXiv: 1701.08780 [hep-ph] (cit. on pp. 2, 3).
* [8] ATLAS collaboration, _Search for Dark Matter Produced in Association with a Dark Higgs Boson Decaying into \(W^{\pm}W^{\mp}\) or \(ZZ\) in Fully Hadronic Final States from \(\sqrt{s}=13\) TeV pp Collisions Recorded with the ATLAS Detector_, Phys. Rev. Lett. **126** (2021) 121802, arXiv: 2010.06548 [hep-ex] (cit. on p. 3).
* [9] ATLAS Collaboration, _Search for dark matter produced in association with a dark Higgs boson decaying to \(W^{+}W^{-}\) in the one-lepton final state at \(\sqrt{s}=13\) TeV using \(139\,fb^{-1}\) of pp collisions recorded with the ATLAS detector_, ATLAS-CONF-2022-029, 2022, url: [https://cds.cern.ch/record/2809721](https://cds.cern.ch/record/2809721) (cit. on p. 3).
* [10] ATLAS collaboration, _Constraints on mediator-based dark matter and scalar dark energy models using \(\sqrt{s}=13\) TeV pp collision data collected by the ATLAS detector_, JHEP **05** (2019) 142, arXiv: 1903.01400 [hep-ex] (cit. on p. 3).
* [11] ATLAS collaboration, _RECAST framework reinterpretation of an ATLAS Dark Matter Search constraining a model of a dark Higgs boson decaying to two b-quarks_, tech. rep., All figures including auxiliary figures are available at [https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PUBNOTES/ATL-PHYS-PUB-2019-032](https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PUBNOTES/ATL-PHYS-PUB-2019-032): CERN, 2019, url: [http://cds.cern.ch/record/2686290](http://cds.cern.ch/record/2686290) (cit. on p. 3).
* [12] ATLAS Collaboration, _SimpleAnalysis: Truth-level Analysis Framework_, ATL-PHYS-PUB-2022-017, 2022, url: [https://cds.cern.ch/record/2805991](https://cds.cern.ch/record/2805991) (cit. on p. 5).