# Software Performance of the ATLAS Track Reconstruction for LHC Run 3

The ATLAS Collaboration

The ATLAS Collaboration

The ATLAS Collaboration

This document summarises the main changes to the ATLAS experiment's Inner Detector track reconstruction software chain in preparation for LHC Run 3 (2022-2024). The work was carried out to ensure that the expected high-activity collisions (with on average 50 simultaneous proton-proton interactions per bunch crossing, pile-up) can be reconstructed promptly using the available computing resources. Performance figures in terms of CPU consumption for the key components of the reconstruction algorithm chain and their dependence on the pile-up are shown. For the design pile-up value of 60 the updated track reconstruction is a factor of 2 faster than the previous version.

## 1 Introduction

The reconstruction of charged particle trajectories (tracking) in the Inner Detector (ID) is a complex part of the ATLAS [1, 2, 3] experiment's event reconstruction chain, making it the most resource intensive component during Run 2 of the LHC (2015-2018). Given its close proximity to the interaction point (IP) and high granularity, the ID records up to 1500 hits per single proton-proton (p-p) collision, while each bunch crossing results in a number of simultaneous proton-proton collisions taking place (pile-up, \(\langle\mu\rangle\)). The average \(\langle\mu\rangle\) during Run 2 ranged from 20 to 40, with a peak luminosity of \(1.9\times 10^{34}\)cm\({}^{-2}\)s\({}^{-1}\)[4], twice the original LHC design value. As a result, in an average bunch crossing event, about 30000 to 60000 hits need to be processed, decoded, and combined into clusters. The clusters then need to be combined into short track seeds that are subsequently attempted to be extended through the entire ID to identify the charged particles (tracks) and precisely reconstruct their trajectories. This represents a complex combinatorial problem, which increases in difficulty with pile-up. In addition to the computational effort, the quality of the reconstructed track candidates becomes challenging to maintain under high pile-up, as the high density of clusters leads to incorrect cluster-to-track association, potentially pulling the reconstructed trajectories away from their true values. Additionally, largely random collections of clusters can be reconstructed as tracks, which happens more frequently as the number of available clusters increases with pile-up. The timing required for tracking scales rapidly with \(\langle\mu\rangle\). For the LHC Run 3 (2022-2024) data-taking, averages of around 50 interactions per bunch-crossing are expected. Preparing the reconstruction to cope with such conditions was a main focus of the ATLAS Run 3 reconstruction updates. One main direction not discussed here was the adoption of multi-threading to make more efficient use of the available resources. This however is not, a priori, expected to improve processing time required by the algorithms. In addition to this infrastructural change, a major effort was carried out to improve the per-thread performance of track reconstruction while maintaining comparable or even superior quality of the reconstructed tracks. The changes made in this effort and their impact are described in this note.

## 2 The ATLAS Detector

The ATLAS experiment [1] at the LHC is a multipurpose particle detector with a forward-backward symmetric cylindrical geometry and a near \(4\pi\) coverage in solid angle.1 It consists of an inner tracking detector surrounded by a thin superconducting solenoid providing a 2 T axial magnetic field, electromagnetic and hadron calorimeters, and a muon spectrometer. The inner tracking detector covers the pseudorapidity range \(|\eta|<2.5\). It consists of a high granularity silicon pixel detector, a silicon microstrip semiconductor tracking detector (SCT), and a transition radiation tracker (TRT). Lead/liquid-argon (LAr) sampling calorimeters provide electromagnetic (EM) energy measurements with high granularity, particularly in the pseudorapidity region corresponding to the ID coverage. A steel/scintillator-tile hadron calorimeter covers the central pseudorapidity range (\(|\eta|<1.7\)). The endcap and forward regions are instrumented with LAr calorimeters for both the EM and hadronic energy measurements up to \(|\eta|=4.9\). The muon spectrometer surrounds the calorimeters and is based on three large superconducting air-core toroidal magnets with eight coils each. The field integral of the toroids ranges between 2.0 and 6.0 T m across most of the detector. The muon spectrometer includes a system of precision tracking chambers and fast detectors for triggering.

A two-level trigger system is used to select events and reduce the incoming data rate of 40 MHz. The first-level trigger is implemented in custom-built hardware and uses a subset of the detector information to accept events at a rate below 100 kHz. This is followed by a software-based trigger that reduces the accepted event rate to 1 kHz on average depending on the data-taking conditions.

## 3 Benchmarking Methodology

The impact of the improvements discussed in the following sections on the software and reconstruction performance is evaluated using collision data and Monte-Carlo (MC) simulated \(t\bar{t}\) samples. For various pile-up values, the raw data of sets of 300 consecutive collision events are reconstructed, and processing time taken for the reconstruction as well as the size of the output written to disk is recorded. The samples are taken from a single LHC run (fill number 6291) recorded towards the end of the 2017 data-taking campaign and covering a range of pile-up values between \(\langle\mu\rangle=15.5\) and 60. This ensures consistent data-taking conditions across all pile-up values. All the events taken from this run fall under the so-called good-run list (GRL), meaning that they satisfy all data quality requirements to be considered part of the ATLAS physics dataset [5]. To extend the study towards even larger values of pile-up of up to \(\langle\mu\rangle=90\), an LHC run recorded in late 2018 (fill 7358) as part of a machine-development campaign is used in addition. Unlike the 2017 data, this run is not considered part of the ATLAS physics dataset due to the nonstandard data-taking conditions. Since the ID was fully operational, it is however possible to use the respective events to obtain an estimate of the scaling behaviour of track reconstruction performance under extreme pile-up conditions. Table 1 provides the full set of luminosity blocks, which each correspond to approximately 60 seconds of data-taking under consistent experimental conditions, contributing to the results presented in this paper.

When evaluating the track reconstruction physics performance where the true value of the parameter of interest is required, simulated MC \(t\bar{t}\) events are used. The production of these \(t\bar{t}\) events is modelled using the Powheg Box v2[6; 7; 8; 9] generator at NLO with the NNPDF3.0nlo[10] PDF set and the \(h_{\rm damp}\) parameter2 set

\begin{table}
\begin{tabular}{|c|c c|c c|} \hline \hline Year of data-taking & LHC Fill number & ATLAS Run number & Luminosity block & \(\langle\mu\rangle\) \\ \hline  & & & 1475 & 15.5 \\  & & & 1249 & 19.8 \\  & & & 1048 & 25 \\  & & & 905 & 30 \\
2017 & 6291 & 337833 & 785 & 35 \\  & & & 663 & 40 \\  & & & 584 & 45 \\  & & & 512 & 50 \\  & & & 405 & 55 \\  & & & 299 & 60 \\ \hline
2018 & 7358 & 364485 & 783 & 80.1 \\  & & & 725 & 90 \\ \hline \hline \end{tabular}
\end{table}
Table 1: ATLAS runs and luminosity blocks used for the evaluation of the technical tracking performance.

to \(1.5\,m_{\text{top}}\)[11]. The events are interfaced to Pythia 8.230 [12] to model the parton shower, hadronisation, and underlying event, with parameters set according to the A14 tune [13] and using the NNPDF2.3lo set of PDFs [14]. The decays of bottom and charm hadrons are performed by EvtGen 1.6.0 [15]. The effect of multiple interactions in the same and neighbouring bunch crossings (pile-up) was modelled by overlaying the simulated hard-scattering event with inelastic proton-proton (\(pp\)) events generated with Pythia 8.186 [16] using the NNPDF2.3LO set of parton distribution functions (PDF) [14] and the A3 set of tuned parameters [17].

All benchmarks described in this work were run as the only active user on a dedicated machine equipped with two Intel(R) Xeon(R) E5-2630 v3 8-core, 2.4 GHz processors and 128 GB of RAM, running the CERN CENTOS 7 operating system. CPU scaling was set to performance mode and hyper-threading disabled. A HS06 score of 278 is reported [18] for this processor without hyperthreading. The machine was kept at a stable 50% in capacity by running an appropriate number of reconstruction tasks simultaneously. To exclude the impact of multi-threading from the comparison all tests were run in single-thread mode

The binaries tested in this work are identical to those most commonly used for the experiment's regular data reconstruction. As a result, different versions of the software differ not only in terms of their own programming, but also in terms of the method of compilation as well as external libraries and the compilation thereof. All versions of the software were compiled with the default compilation settings as they were defined in the ATLAS software project at the time of their release. Since the optimisation flags are partially set according to compiler presets, the exact details may differ between compiler versions. In all cases, however, the code is compiled for a generic x86-64 architecture, implying support for vector instruction set extensions up to SSE2. The binaries produced are therefore unable to exploit more modern architectural features like AVX. The binaries for the Run 2 reconstruction were compiled using version 6.2.0 of the GNU Compiler Collection, whereas the Run 3 binaries utilise version 8.3.0 of \(gcc\).

## 4 Track Reconstruction

ATLAS track reconstruction as performed during LHC Run 2 is extensively documented in other sources [19, 20, 21, 22, 23]. A brief overview is provided in the following and shown schematically in Figure 1.

The procedure starts with a pre-processing stage. Signals from adjacent channels in the Pixel and SCT subdetectors are combined into clusters which are interpreted as the deposits left by individual traversing charged particles. Pairs of one-dimensional SCT clusters on either side of a sensor module or individual pixel clusters are further converted into 3-dimensional space-points, with position uncertainties determined by the detector geometry and sensor pitch.

The primary ATLAS track reconstruction pass starts by forming so-called track seeds consisting of triplets of space-points in the Pixel or SCT subdetectors which are compatible with originating from a charged particle track. Search roads (sets of detector modules that can be expected to contain clusters compatible with the seed) are built through the remaining detector based on the estimated seed trajectory, and the seeds are extended with additional clusters along the search road into silicon track candidates by means of a combinatorial Kalman Filter [24].

To resolve overlaps between track candidates and reject incorrect combinations of unrelated clusters ("fake tracks"), a dedicated ambiguity resolution step is performed, which scores track candidates based on a range of quality criteria and rejects lower-quality candidates sharing a large number of associated hits with higher-quality ones. A limited number of shared hits is permitted to retain high performance in dense topologies such as cores of high-energy jets, where the separation between charged particles is expected to reach below the magnitude of the sensor pitch. The estimated cluster positions and their uncertainties are updated using a neural network based algorithm [20, 25], and a probability is assigned for one, two, or at least three charged particles to have contributed to the cluster. Clusters judged to consist of more than one charged particle crossing are split among track candidates, with position and uncertainty estimates for each particle crossing provided by the algorithm.

The refined and purified track candidates resulting from the ambiguity resolution are then re-fit using a global \(\chi^{2}\) method to obtain the final, high-precision track parameter estimate. An extension of the track into the TRT subdetector is attempted, with a re-fit of the entire track being performed in case of a successful extension to profit from the additional measurements on track in particular for momentum resolution and particle identification.

This reconstruction pass is optimised for particles produced in the primary \(p\)-\(p\) interactions. To increase acceptance to particles produced at a greater distance from the beamline, such as electrons originating from photon conversions in the detector material, a secondary back-tracking pass is performed using the detector hits not already assigned to tracks from the primary pass. Here, track reconstruction is only attempted in regions of interest determined by deposits in the electromagnetic calorimeter. Unlike the first pass, this second pass starts with segments of hits in the TRT compatible with the region of interest. In the presence of such a segment, short silicon track seeds consisting of two space-points are constructed in the Pixel and SCT subdetectors, and extended into track candidates using the same procedure as for the primary pass. A dedicated ambiguity resolution pass among the track candidates in the second pass and a re-fit of the resulting tracks including their TRT extension complete the second pass.

Further tracking passes are performed to reconstruct short tracklets from muons in \(|\eta|>2.5\), where only the pixel detector is traversed, as well as short tracks compatible with decaying, short-lived charged particles. In each case, only left-over hits from prior passes are used to limit combinatorial complexity.

After all track candidates have been reconstructed, the locations of the underlying \(p\)-\(p\) interactions (vertices) are identified by a dedicated vertex reconstruction procedure [26]. A first step obtains an initial position estimate for a vertex from the distributions of the z coordinate of the tracks' closest approach to the

Figure 1: Simplified overview of the primary and secondary back-tracking chains used in ATLAS reconstruction. The primary reconstruction runs from inside-out, starting from silicon space-points in the innermost Pixel and SCT subdetectors. The secondary back-tracking chain runs from outside-in, seeded from leftover TRT hits within electromagnetic calorimeter regions of interest.

beamline. Then, a fit of the vertex location is performed taking into account all tracks loosely compatible with the initial position estimate. Before the changes reported in this paper, this was performed using an iterative procedure, constructing one vertex at a time and removing the associated tracks from consideration before repeating the procedure.

The track reconstruction procedure described above does not attempt to reconstruct tracks that have a very large distance of closest approach orthogonal to the beam line (transverse impact parameter, \(d_{0}\)). Measurements and searches requiring such tracks, for example to reconstruct decays of long-lived neutral particles within the ID volume, therefore run a dedicated version of track reconstruction [27] on a preselected sub-set of the collision data. This version uses the same algorithmic flow, but is configured with a wider search space in the transverse impact parameter, enabling reconstruction of displaced tracks at the price of drastically slowed execution speed.

## 5 Physics Performance and Software Optimisation

A number of changes to the tracking software were introduced in order to ensure that the computational performance and the size of the generated output will remain sustainable during LHC Run 3 data-taking. Apart from general algorithmic improvements, the guiding principle is to abort the track reconstruction as early as possible for candidates that are not expected to result in high-quality tracks, in order to minimise the number of executions of the down-stream algorithms, thus saving time and resources.

A first step was to apply stricter requirements on track candidates when determining which ones to retain after the initial track-finding stage. Instead of seven silicon clusters on track, at least eight are required in Run 3, and the permitted transverse impact parameter range of silicon-seeded tracks is restricted to \(|d_{0}|<5\) mm instead of \(|d_{0}|<10\) mm. This reduces the acceptance of track reconstruction in terms of both displacement and production radius by a small fraction, but significantly reduces the number of low-quality tracks written to storage as well as the required number of iterations of the ambiguity resolution and TRT extension phases.

A large contribution of falsely reconstructed tracks was previously generated by the TRT-seeded backtracking step. This was reduced by only performing the backtracking within regions of interest seeded by energy deposits in the electromagnetic calorimeter (\(E_{T}>6\) GeV). The recovery of late-appearing tracks from electron conversions, which is the main purpose of this reconstruction step, is only degraded at a negligible level since these topologies coincide with significant calorimetric deposits. However, the number of erroneously reconstructed track candidates is reduced and the execution speed of the backtracking phase is dramatically improved by a factor 20.

The seeding phase of the inside-out track reconstruction was optimised to prevent seeds unlikely to result in tracks from being passed into down-stream processing. This has a large impact on processing speed, since the number of executions of all following reconstruction steps is reduced. The optimisations include stricter requirements on the estimated impact parameters of the track seeds, narrower search roads used to extend the seeds, and a restriction of the number of mutually overlapping seeds to pass into further processing. The presence of a fourth pixel layer [2, 3] since 2015 is exploited by using confirmation space-points to detect promising seeds and treat them with preference. A confirmation space-point is a fourth space-point from a different layer which, if used to replace the outer-most space-point on a given seed, results in a new seed with a curvature compatible to the original, indicating it is likely part of the same charged-particle trajectory. These changes have only a minor impact on the number of correctly reconstructed tracks, while significantly suppressing the occurrence of falsely reconstructed tracks and improving execution speed dramatically. The seeding strategy was further optimised by adapting the size of the angular regions within which seeds are formed to correspond to the track curvature resulting from the bending in the magnetic field of the detector expected to occur at the lowest track momentum to be reconstructed, instead of the wider angular regions used previously. This improves execution speed by reducing the number of combinatorial permutations to process during the seed finding stage, without significantly changing the number of tracks being reconstructed.

The iterative vertex finding algorithm described in Section 4 was replaced by an adaptive multi-vertex fitter algorithm [28], in which vertex candidates are allowed to compete for tracks in order to reduce the chance of nearby \(p-p\) interactions being reconstructed as a single merged vertex. The initial vertex locations are estimated with high accuracy using a Gaussian resolution model for the track impact parameter. This updated algorithm is implemented within the ACTS [29] framework, and represents the first production use of this framework in an LHC experiment.

The TRT extension was sped up significantly by aborting the iterative track fit procedure early for candidates with insufficient compatible hits in the TRT. This change does not impact reconstruction efficiency or the rate of incorrectly reconstructed tracks, but speeds up the TRT extension step by nearly 30%.

Further execution speed was gained by carefully optimising the software implementation of each reconstruction step individually. Notable examples include a re-organisation of the search for holes on tracks performed as part of the precision fit, exploiting the navigation between detector surfaces already being performed by the track fit procedure, an optimisation of the space-point formation and the re-writing of parts of the Runge-Kutta propagator implementation used to extrapolate trajectories through the inhomogenous magnetic field of the detector to exploit vectorised instructions where possible.

The speed improvements achieved using the measures described above make it feasible to run an additional reconstruction pass to recover non-pointing tracks from displaced decays, using left-over hits unused by the earlier passes, as part of the standard ATLAS track reconstruction. This removes the need for inefficient pre-selection and re-reconstruction to reconstruct these tracks in the Run 2 implementation of the software described earlier. In the following study of computational performance, the impact of this Large Radius Tracking (LRT) step will be pointed out separately, as it is not being run in the previous reconstruction the updated software is being compared to.

The reduction in the single-thread CPU timing per event for each of the optimisations listed above are shown in Fig. 2 for a set of events recorded at the very high pile-up value of \(\langle\mu\rangle=90\). The reductions are given relative to the first iteration of the Run 3 reconstruction, which did not include any optimisations but was slower than the Run 2 implementation due to changes made to ensure the thread-safety of the code. Changes to the vertex finding, TRT extensions and further improvements are added under "Additional Optimisations". The purple shaded area indicates the increase in the CPU per event by adding in the LRT tracking pass. For these challenging conditions, a factor 4 improvement in speed has been achieved. After the addition of LRT, near a factor of 3 reduction in the timing requirement per event is observed compared to the initial Run 3 software implementation before the changes discussed above.

## 6 Performance Results

The comparison of the time taken for track reconstruction in the software release used during Run 2 data taking versus the new release prepared for Run 3 is shown in Fig. 3 as a function of \(\langle\mu\rangle\). The green curve shows the timing requirement for track reconstruction using the Run 2 release, the purple indicates the timing for the Run 3 release, while the blue shows the impact of including the LRT secondary pass in the default reconstruction chain. In both cases the performance for the new release is more than a factor of 2 faster. Near linear scaling of the CPU consumption with \(\langle\mu\rangle\) is now observed compared to the behaviour seen for Run 2.

A breakdown of the speed-up seen for the individual major parts of track reconstruction, as described in Section 4, is shown in Fig. 4. For Run 2 the track-finding step (violet) was by far the largest CPU consumer for track and total ATLAS reconstruction and scaled non-linearly with \(\langle\mu\rangle\). This behaviour has been rectified and the timing of the pattern recognition has been reduced up to a factor of 4. Nearly all of the major consumers see a reduction of around a factor of 1.5 to 2.0.

A comparison between the Run 2 and Run 3 track reconstruction for absolute values of time required per event and the fraction taken for each component are illustrated in Fig. 5 (a) and (b) respectively. The secondary pass added to Run 3 reconstruction is indicated in the white area superimposed on top for the Run 3 part of the figure. The violet area shows the dramatic reduction of absolute time required for the track finding part, with respect to Run 2. The \(\langle\mu\rangle\) dependency observed for Run 2 is closer to linear after the optimisations, showing that the Run 3 tracking software is well prepared for high \(\langle\mu\rangle\) data-taking. Similarly, Fig. 6 shows the fraction of total ID reconstruction taken by components.

Figure 2: Incremental decrease of the CPU time taken to reconstruct a set of \(\langle\mu\rangle=90\) events when adding improvements to the Run 3 track reconstruction. The blue shaded area indicates the time, relative to the initial Run 3 software implementation, taken for the tracking passes that were also performed during Run 2. The purple area indicates the time added by the additional LRT pass.

Figure 4: Breakdown of the speed improvement in the Run 3 software compared to the Run 2 iteration for key components of the track reconstruction as a function of \(\langle\mu\rangle\). The shaded area indicates data events taken from a 2018 machine development run not passing the full ATLAS data quality requirements.

Figure 3: Processing time taken per event versus average pileup to reconstruct the same data events, comparing the Run 2 (green) and Run 3 (purple) reconstruction software. The Run 3 numbers are also presented including the impact of the additional LRT tracking pass (blue). The bottom panel depicts the time taken as a fraction of the Run 2 result. The shaded area indicates data events taken from a 2018 machine development run not passing the full ATLAS data quality requirements.

Figure 5: Breakdown of CPU consumer for track and vertex reconstruction comparing Run 2 (a) and Run 3 (b) configurations versus \(\langle\mu\rangle\), shown as absolute units for the individual components.

Figure 6: Breakdown of CPU consumer for track and vertex reconstruction comparing Run 2 (a) and Run 3 (b) configurations versus \(\langle\mu\rangle\), shown as the fraction of the total runtime for the individual components.

Track reconstruction accounted for around 64% of the total ATLAS event reconstruction CPU time in Run 2. This fraction has been reduced to 40% for Run 3 at \(\langle\mu\rangle=50\). The breakdown of the total time between the different domains of reconstruction, as measured in the release used for the benchmarking3, is shown in Fig. 7, where the slices are defined as follows: INDET: Inner Detector track and vertex reconstruction. CALO: Preprocessing and clustering of cells in the Tile and LAr calorimeter. MUON: Muon spectrometer track reconstruction and ID combined muons. EGAMMA: Dedicated electron track reconstruction, \(\gamma\)-conversion secondary vertex finding, electron- and \(\gamma\)-object reconstruction. TAU: Tau reconstruction. PFO: Charged and neutral particle flow jet reconstruction. JETETMISS: Initial jet and missing \(E_{T}\) reconstruction. BTAG: Low level flavour tagging reconstruction for monitoring LRT: Summed contribution for all domains for producing displaced objects. OTHER: Data-writing, and isolation building.

Footnote 3: The software release for Run 3 is still being finalised, and the domains may evolve compared to the version evaluated in this work.

Storage capacity is also a limited commodity and heavily challenged by the vast amounts of collision data to be recorded. With the Run 3 track reconstruction improvements, fake track reconstruction rates have been drastically reduced, and the average quality of the tracks has increased. This amounts to a large reduction in the overall number of output tracks written to disk, reducing the needs for storage space. Even after including the additional tracks from the LRT a reduction of up to 50% is achieved at the highest pile-up values. The output size for tracks in kilobytes per event is shown for the standard ATLAS event data format in Fig. 8, and illustrates up to a \(20-50\%\) reduction in the required disk space. Additionally, the scaling with pile-up has been significantly reduced in the Run 3 release, leading to larger improvements at higher values of \(\langle\mu\rangle\).

The new deployment of the ACTS software framework in ATLAS for primary vertex reconstruction results in a significant reduction of primary vertexing CPU time. Fig. 9 shows a comparison of the primary vertex reconstruction time required per event between the ACTS-provided Adaptive Multi-Vertex Finder algorithm and the previously deployed ATLAS implementation. An average reduction of CPU time of more than a factor of 1.5 is seen while the ACTS implementation gives entirely identical physics results as the previous

Figure 7: Fraction of the total CPU requirement for full ATLAS reconstruction split by domain for Run 2 (a) and Run 3 (b) for one data run at \(\langle\mu\rangle=50\).

non-ACTS version. The now fully integrated ACTS vertexing software makes additional functionality with further potential for CPU improvements available, such as a newly developed vertex seeder that results in overall vertex reconstruction speed-ups of up to a factor of 3 in high pile-up environments.

The improved computational efficiency of the track seeding and finding stage is illustrated in Fig. 10. The figure shows the number of seeds processed divided by the final number of tracks created in the

Figure 8: Event size of the Inner Detector reconstruction output in the ATLAS Event data format for the same set of reconstructed data events as a function of average pile-up, comparing the Run 2 and Run 3 releases. The shaded panel on the right indicates data events taken from a 2018 machine development run not passing the full ATLAS data quality requirements.

Figure 9: Primary vertex reconstruction processing time taken per event versus average pileup to reconstruct the same data events, comparing the previous ATLAS (non-ACTS) version of the Adaptive Multi-Vertex Finder algorithm with the ACTS-provided implementation. The shaded area indicates data events taken from a 2018 machine development run not passing the full ATLAS data quality requirements.

silicon-seeded tracking pass. This ratio is reduced by a factor of almost two in the Run 3 reconstruction, implying that to obtain a similar number of output tracks, the costly algorithms responsible for extending a seed through the tracker into a candidate are only called half the number of times, which is one of the main drivers of the dramatic speed-up of the track finding stage shown in Fig. 4.

Finally, the software improvements have not negatively impacted the track reconstruction physics performance compared to Run 2. The tracking efficiency, defined as the fraction of charged particles originating from the primary \(p\)-\(p\) interaction that were successfully reconstructed, is shown for the Run 2 and Run 3 reconstruction in Fig. 11 as a function for the truth particle transverse momentum. The efficiency loss is smaller than 4% at low \(p_{T}\) and smaller than 1% at larger transverse momenta. The slight reduction compared to Run 2 is a result of the stricter requirements on the number of silicon clusters and impact parameter for a track to be retained, and is not expected to affect the reconstruction of muons as minimum ionising particles.

A measure of the rate of incorrect combinations of clusters reconstructed as tracks is the number of tracks per event as a function of \(\langle\mu\rangle\). The number of real tracks is expected to scale linearly with this quantity, since it is related to the number of charged particles produced in the collisions. The number of random combinations is expected to scale with a higher power, since the enhanced combinatorics allow for more such candidates to be formed. The mean number of reconstructed tracks as a function of \(\langle\mu\rangle\) is shown in Fig. 12. In order to visualise the linearity, a linear fit to the \(5<\langle\mu\rangle<20\) is superimposed as a dashed line. While a clear non-linear component amounting to up to 30% of the total number of tracks is visible for the Run 2 reconstruction, the Run 3 reconstruction is observed to show nearly ideal linear behaviour, demonstrating the dramatic improvement in purity of the reconstructed tracks. This is the case for both simulation and real data, and the relative change in track multiplicity as a function of \(\mu\) is very well reproduced in the simulation.

Figure 10: Number of seeds processed divided by the final number of tracks created in the silicon-seeded tracking pass as a function of \(\langle\mu\rangle\).

## 7 Conclusions

The upcoming LHC Run 3 represents a major challenge to the experiments' event reconstruction. In ATLAS, a major improvement of the Inner Detector track reconstruction software has been performed in order to ensure that reconstruction remains feasible within the available computing resources. An execution speed improvement between a factor of 2 and 4, depending on the pile-up, is achieved. These improvements allow the execution of an additional tracking pass benefiting long-lived particle searches while still retaining a significant overall performance improvement compared to the past software iteration. This ensures that ATLAS will be able to efficiently reconstruct collision data in LHC Run 3.

Figure 11: Tracking efficiency as a function of truth \(p_{T}\) in simulated \(t\bar{t}\) events with a \(\langle\mu\rangle\) distribution matching the conditions of LHC Run 2, comparing Run 2 and Run 3.

Figure 12: Mean number of reconstructed tracks per event as a function of \(\mu\) in simulated \(t\bar{t}\) events (a) or in collision data (b), comparing the Run 2 and Run 3 reconstruction. The additional large-radius tracking pass in Run 3 is not included to allow a like-to-like comparison. Dashed lines represent linear fits to the range of \(\mu\in\{5,\ldots 20\}\).

## References

* [1] ATLAS Collaboration, _The ATLAS Experiment at the CERN Large Hadron Collider_, JINST **3** (2008) S08003 (cited. on p. 2).
* [2] ATLAS Collaboration, _ATLAS Insertable B-Layer Technical Design Report_, ATLAS-TDR-19; CERN-LHCC-2010-013, 2010, url: [https://cds.cern.ch/record/1291633](https://cds.cern.ch/record/1291633) (cited. on pp. 2, 6).
* [3] B. Abbott et al., _Production and integration of the ATLAS Insertable B-Layer_, JINST **13** (2018) T05008, arXiv: 1803.00844 [physics.ins-det] (cited. on pp. 2, 6).
* [4] ATLAS Collaboration, _Luminosity determination in \(pp\) collisions at \(\sqrt{s}=13\) TeV using the ATLAS detector at the LHC_, ATLAS-CONF-2019-021, 2019, url: [https://cds.cern.ch/record/2677054](https://cds.cern.ch/record/2677054) (cited. on p. 2).
* [5] ATLAS Collaboration, _ATLAS data quality operations and performance for 2015-2018 data-taking_, JINST **15** (2020) P04003, arXiv: 1911.04632 [physics.ins-det] (cited. on p. 3).
* [6] S. Frixione, P. Nason and G. Ridolfi, _A positive-weight next-to-leading-order Monte Carlo for heavy flavour hadroproduction_, JHEP **09** (2007) 126, arXiv: 0707.3088 [hep-ph] (cited. on p. 3).
* [7] P. Nason, _A new method for combining NLO QCD with shower Monte Carlo algorithms_, JHEP **11** (2004) 040, arXiv: hep-ph/0409146 (cited. on p. 3).
* [8] S. Frixione, P. Nason and C. Oleari, _Matching NLO QCD computations with parton shower simulations: the POWHEG method_, JHEP **11** (2007) 070, arXiv: 0709.2092 [hep-ph] (cited. on p. 3).
* [9] S. Alioli, P. Nason, C. Oleari and E. Re, _A general framework for implementing NLO calculations in shower Monte Carlo programs: the POWHEG BOX_, JHEP **06** (2010) 043, arXiv: 1002.2581 [hep-ph] (cited. on p. 3).
* [10] R. D. Ball et al., _Parton distributions for the LHC run II_, JHEP **04** (2015) 040, arXiv: 1410.8849 [hep-ph] (cited. on p. 3).
* [11] ATLAS Collaboration, _Studies on top-quark Monte Carlo modelling for Top2016_, ATL-PHYS-PUB-2016-020, 2016, url: [https://cds.cern.ch/record/2216168](https://cds.cern.ch/record/2216168) (cited. on p. 4).
* [12] T. Sjostrand et al., _An introduction to PYTHIA 8.2_, Comput. Phys. Commun. **191** (2015) 159, arXiv: 1410.3012 [hep-ph] (cited. on p. 4).
* [13] ATLAS Collaboration, _ATLAS Pythia 8 tunes to 7 TeV data_, ATL-PHYS-PUB-2014-021, 2014, url: [https://cds.cern.ch/record/1966419](https://cds.cern.ch/record/1966419) (cited. on p. 4).
* [14] R. D. Ball et al., _Parton distributions with LHC data_, Nucl. Phys. B **867** (2013) 244, arXiv: 1207.1303 [hep-ph] (cited. on p. 4).
* [15] D. J. Lange, _The EvtGen particle decay simulation package_, Nucl. Instrum. Meth. A **462** (2001) 152 (cited. on p. 4).
* [16] T. Sjostrand, S. Mrenna and P. Skands, _A brief introduction to PYTHIA 8.1_, Comput. Phys. Commun. **178** (2008) 852, arXiv: 0710.3820 [hep-ph] (cited. on p. 4).
* [17] ATLAS Collaboration, _The Pythia 8 A3 tune description of ATLAS minimum bias and inelastic measurements incorporating the Donnachie-Landshoff diffractive model_, ATL-PHYS-PUB-2016-017, 2016, url: [https://cds.cern.ch/record/2206965](https://cds.cern.ch/record/2206965) (cited. on p. 4).