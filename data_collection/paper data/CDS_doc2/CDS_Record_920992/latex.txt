# Tests of the ATLAS Trigger and Data Acquisition System

G. Lehmann Miotto

_CERN PH-ATD, Route de Meyrin, 1211 Geneva, Switzerland_

On behalf of the ATLAS TDAQ Group

###### Abstract

The complexity of the Trigger and Data AcQuisition (DAQ) systems at LHC experiments has required a long phase of prototyping both for software and hardware technologies. The development of the core aspects of the DAQ for ATLAS is now well advanced and large effort is put into the validation of the performance and the scalability of the most crucial components. This paper presents the results obtained on two testbeds used to verify the functionality and performance of the read out system and the run control.

## 1 Introduction

The ATLAS experiment [1] at the Large Hadron Collider (LHC) [2] is scheduled to start taking data in 2007. The high center of mass energy (\(14\;TeV\)) and bunch crossing frequency (\(40\;MHz\)) of the LHC as well as the large amountof data produced by ATLAS (\(\sim 2\)\(MB\) formatted data per event) require the design of a performant Data AcQuisition (DAQ) system with powerful online event selection capabilities [3][4]. The online rejection of events is mandatory in order to reduce the data read out from the detector and put to mass storage to a manageable amount (\(\sim 200\)\(MB/s\)). In this paper we will briefly describe the mechanism by which interesting Physics events are selected as well as the architecture of the DAQ. We will then present the results obtained on two testbeds which serve to validate data taking performance and run control aspects of the Trigger and DAQ.

## 2 The Event Selection System and DAQ Architecture

The ATLAS experiment has been designed to cover the physics in proton-proton collisions with a centre-of-mass energy of \(14\)\(TeV\) at LHC. The online event-selection strategy has to define the proper criteria to cover efficiently the physics programme foreseen for ATLAS, while at the same time providing the required reduction in event rate. In order to guarantee optimal acceptance to new physics, an approach based on emphasizing the use of inclusive criteria for the online selection has been taken, i.e. having signatures mostly based on single- and di-object high-\(p_{T}\) triggers. Here 'high-\(p_{T}\)' refers to objects such as charged leptons with transverse momenta above \(O(10)\)\(GeV\).

The online selection system is organized in three levels. The first trigger level (LVL1) will carry out a rate reduction from \(40\)\(MHz\) down to at most \(100\)\(kHz\). The second level trigger (LVL2) will reduce the rate by another two orders of magnitude, and the Event Filter (EF) will bring down the data-recording rate to \(100\)\(Hz\) approximately. The DAQ is responsible for formatting, moving and storing the data in such a way that the selection algorithms can perform their task and finally for putting the accepted events to mass storage.

The LVL1 trigger is based on a system of purpose-built hardware processors which analyse multiple events in parallel to avoid the introduction of dead time and sustain the bunch crossing rate of \(40\)\(MHz\). It uses only information from the calorimeters and some muon detectors, with coarse granularity, to perform the selection, and passes information on to the LVL2 trigger about the areas of the detector which contained interesting features for an event, the so called Regions of Interest (RoI).

The raw data produced at each bunch crossing are kept on on-detector buffers for as long as the LVL1 does not issue an LVL1 accept signal. Only then data are read-out, formatted, and put into Read Out Buffers (ROBs). In ATLAS there are 1600 ROBs. ROBs are housed in groups of three on custom build PCI cards, the ROBINs [5]. ROBINs are installed into commercial rack mounted computers, called Read Out Systems (ROS). Each ROS contains four ROBIN cards, thus 12 ROBs. The function of the ROS is to provide event fragments on demand from the next trigger levels and to store them for as long as it is requested.

The other two trigger levels are based on software running on commercial computer farms. The LVL2 trigger has a decision latency of \(\sim 10\ ms\) per event. Several hundreds of software processes will perform data analysis in parallel, to sustain the event rate without introduction of dead time. They will use a sequence of highly optimized trigger selection algorithms which operate on only a fraction (typically \(\sim 2\%\)) of the event data. The RoI information received by the LVL1 enables the LVL2 algorithms to select precisely the region of the detector, i.e. the ROBs, in which the interesting features reside and therefore to request to the ROSs a limited amount of data for analysis. Events rejected by the LVL2 will be immediately deleted from the ROBs, while the accepted ones will be gathered by the event building system (EB), formatted, deleted from the ROBs and finally passed on to the EF. The EF disposes of an average event treatment time of \(\sim 1\ s\) and operates on the complete event. Several thousands of software processes will perform the filtering in parallel, to sustain the LVL2 accept rate. Compared to LVL2, more sophisticated reconstruction and trigger algorithms and more complete and detailed calibration information are used to make the selection. Events not selected by the EF are deleted and those accepted are passed to mass storage.

## 3 Validation of the Read Out System Performance

As already outlined in the previous section, the ROS is a central element in the ATLAS DAQ architecture. It has to be capable of receiving and storing event fragments at up to \(100\ kHz\) and to dispatch the requested data to the LVL2 and to the EB.

The requirements on the output performance of the ROS are not straight forward to determine, since the size of data fragments varies strongly from detector to detector, the LVL2 requests only a fraction of the data and the output to the EB depends on the rejection power of LVL2. Computer models have been used to infer the maximum performance required depending on the part of the detector the ROS is connected to and the LVL2 behaviour. Three types of ROS have been identified and labeled 'hot-spot', 'average' and 'cold-spot'. Figure 1 shows the sustained LVL1 accept rate (which corresponds to the ROS event input rate) as a function of the event building (EB) rate (which corresponds to the LVL2 accept rate). It should be noted that in the real experiment the LVL1 accept rate will never go beyond \(100\ kHz\), since this value is determined by the read-out electronics capabilities of the detectors. Also, studies performed with LVL2 algorithms show that the expected EB rate will vary between \(2-3\%\). In these conditions the ROS is already capable of satisfying the final ATLAS requirements.

## 4 Validation of the Run Control system on a Large Scale Testbed

Another aspect of a data acquisition system, which is as crucial as its data taking performance, is the ability to control and configure all elements of the experiment, ranging from the detector modules, to all computers and trigger

Figure 1: _Sustained LVL1 accept rate as a function of the LVL2 accept rate. The behaviour of all ROSs will lie within the band enclosed between a hot-spot and the cold-spot ROS curve. The grey line indicates the expected ATLAS region._

algorithms. In ATLAS there will be in the order of 2000 interconnected computers running \(O(10000)\) software processes. To validate this aspect of the DAQ, the exclusive use of a very large computer cluster has been obtained by the DAQ group at CERN for a few weeks. Up to 700 machines were made available to test the performance and scaling behaviour of the run control system. Figure 2 shows the results obtained: the most time consuming operation are the 'boot' and'shutdown', which correspond to the moment in which all software processes are launched and terminated, respectively. The other transitions complete in a negligible amount of time. These results show that already today the dispatching of run control commands is very fast. It should be noted nevertheless that in these measurements the real data taking and trigger applications were substituted by light-weight emulators. It is expected that the configuration time will grow to several minutes once the configuration of real detector hardware and analysis algorithms will be included. Also stopping a run will take longer than what was measured here, since time will be needed for all buffered events to go through the DAQ and event selection system.

Figure 2: _Time needed to boot and shutdown all DAQ and trigger processes and to perform the transitions within the run control finite state machine. Each transition consists of multiple commands which are being distributed over the network._

Conclusions

In this paper we have shown how different aspects of a complex system such as the ATLAS Trigger and DAQ can be tested prior to the implementation of the final system at the experiment site. Performance tests on the ROS indicate that this crucial component can safely deliver the nominal readout rates foreseen for the ATLAS experiment. Another set of tests have been carried out on a large computer cluster emulating the farm which will be put in place for the DAQ and trigger. The tests consisted in performing all run control operations on up to 700 machines with \(\sim\) 2000 processes. Also in this case the results have shown that the design and implementation of the mechanisms for starting and stopping processes and for distributing run control commands match the expectations. Despite the fact that these tests are not conclusive for the validation of the complete trigger and DAQ chain, they are essential for its detailed comprehension and debugging. The good results are a step in the right direction towards data taking in 2007.

## 6 Acknowledgements

The authors would like to thank the Information and Technology department of CERN for having made a large scale computer cluster available for the run control tests.

## References

* [1] ATLAS Collaboration, [http://atlas.web.cern.ch/Atlas/index.html](http://atlas.web.cern.ch/Atlas/index.html)
* [2] LHC, [http://lhc.web.cern.ch/lhc/](http://lhc.web.cern.ch/lhc/)
* [3] ATLAS Collaboration, ATLAS First-Level Trigger Technical Design Report, CERN/LHCC/98-14 (1998)
* [4] ATLAS Collaboration, ATLAS High-Level Trigger Data Acquisition and Controls Technical Design Report, CERN/LHCC/2003-022 (2003)
* [5] B. Green et al., ATLAS Trigger/DAQ RobIn Prototype, IEEE Trans. Nucl. Sci. **51**, 2004, pp. 465-469.