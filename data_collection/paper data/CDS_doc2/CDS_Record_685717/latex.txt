###### Abstract

The T9000 family of transputer units from INMOS/Thomson improves the processing power and the bandwidth of the point-to-point links between transputers by 5 times over the old version of the transputers and links. The high band width, the reliability and a user friendly software environment make a network of T9000-transputers a good candidate for the choice of the higher level trigger hardware at an LHC experiment. If it is configured correctly both the second level and the third level trigger decisions can be made in one transputer.

**EAGLE Internal Note DAQ-NO-003**

**7.2.1992**

A network of transputers as the basic structure for a second

and third level trigger-processor farm.

Contact person

John Renner Hansen/Niels Bohr Institute

February 7, 1992

## 1 Introduction

In this note we propose a transputer based data acquisition model for the high level triggering and data acquisition systems for the EAGLE experiment. Transputers and their accessories are briefly described in appendix A and in more detail in reference 1. A short summary of the main points is given below.

The basic component in our model is the T9000 transputer from INMOS/Thomson. It is a 50 MHz RISC type \(\mu\)-processor, performing up to 200 MIPS and 25 MFLOPs. It can access 4 Gbyte of external memory. The speciality of the transputer is its four point-to-point communication links. Each link can simultaneously transfer 10 MByte/sec into - and 10 MByte/sec out of the chip, and on all four links at the same time. The other basic component is the C104 Packet Routing Switch which allows us to connect infinitely large arrays of transputers. Relevant numbers for the building blocks are given in appendix A.

## 2 A transputers based model for the second level trigger farm.

In this "one event to one processor" DAQ model the transputer links are used to push the data from the front-end electronics up to a second level T9000 transputer.

The transputers are connected to a grid of C104 packet switch units, as illustrated in figure 1. The network has 400 C104-nodes arranged in a 20 by 20 torus, where all nodes are connected to nearest neighbor, figure 2. In such a network any node can send a message to any other node. The maximum distance between two nodes is 10 C104 connections, and the average distance is 5 connections. Each node hosts two transputers and 4 front-end read-out modules. Out of the 32 links available on the C104, 8 links are used for C104 network connections, 8 links establish the connections to the transputers and the read-outmodules use 8 links. The last 8 links are so far spare.

A module with a functionality like that of FEAST [2] reads a local section of the calorimeter and stores the data in a memory accessible through two T9000 links, figure 1. Two links per read-out module protect the local system from permanent black out in the case of a link damaged. Part of the front-end memory is also accessible from the two c-links.

Each of the 1600 front-end modules reads 300 calorimeter cells out of a total of 480.000 cells. A calorimeter cell which measures more than \(\mathcal{E}\) MeV of energy "belonging" to a bunch crossing selected by the first level trigger, stores 4 bytes of data in the front-end memory. 2 bytes represent the energy and 2 bytes are used for the address. We assume that on the average 1% of the cells will contain energy "belonging" to the same bunch crossing. After zero suppression, one read-out module will contain between 0 and 1200 bytes per level one trigger. The average data volume pushed up to the level two processor is roughly 20 Kbyte 1.

When a first level trigger is received, the read-out module sends the local information to the next transputer in the processing queue. Let us assume that no preselection of the data is done before transmission to a level two processor, i.e. no use of "regions of interest". In order to insure the full integrity of the event all 1600 read-out modules must send at least one message for each level one trigger. The 20 Kbyte of data per event is contained in the 1600 primary packets plus a number of extra packets from read-out modules, where 32 bytes are insufficient to contain the information from a local read-out module. In the following we assume, that 2000 packets will be sent for each event. This number is a compromise between 1600 packets, which is the minimal number of packets, we can have, when all the data are equally distributed among the readout modules, and 2200 packets, which is the maximum number, which occur, when 600 modules equally share the 20 Kbyte. As trigger rates we use the standard LHC rates here given in units of triggers per second; 10\({}^{5}\) first level triggers, 1000 second level triggers and 10 third level triggers. The total number of packets to be transferred inside the network per second is 10\({}^{9}\)2. Since the messages are equally distributed over the network, each link has to transmit \(\approx\) 160.000 packets per second 3. The maximum number of packets send in one direction on one link is according to the specifications 3.125.000 per second or 20 times the average load.

Footnote 1: 480000 - 4 bytes \(\approx\) 20 Kbyte

A bottle neck in the system is the input link to the T9000 transputer. Each of the 800 transducers has to receive 125 events per second 4. With 2000 packets per event, the total load is 250.000 packets per second or \(\approx\) 10% of the max. capacity.

On one T9000 link it will take \(\approx\) 1 msec 5 to get the full event in to the transputer. If all four links are used, the packet load is reduced to below 3% of the maximum capacity and the push in time is reduced to \(\approx 250\)\(\mu\)sec. Both these numbers are tolerable. During the push in time the data are prepared for processing and will be ready soon after end of transfer. The front-end read-out modules must keep the information until the level two decision is taken, which is about 1 msec after level one yes. If necessary a processor can go back to a read-out module through the c-link and ask for more detailed information about the event, a time frame. The c-links are connected through a small separate layer of C104 units.

The level two buffers can be released by a timeout or by a message from the transputer.

The use of "regions of interest" and a packing of the calorimeter date will reduce the overall network load significantly.

## 3 Third level trigger farm.

One T9000 must on the average take care of 125 second level calculations, so approximately 90% of the CPU time is available for other tasks. Apart from monitoring this free time could be used for third level trigger processing. The advantage of this construction is, that all information from the second level process directly can be used in the following third level process.

If an event is accepted by the third level trigger it is pushed out through one of the spare C104 links.

Both at level two and at level three more transputers could share the work load. Networks similar to the calorimeter network could be established for all subdetectors. The connections to the calorimeter network should be made through the spare link(s) of each C104 unit. A construction including similar networks for other subdetectors and the connection to the calorimeter layer is shown in figure 3.

## 4 Transputer development projects at the Niels Bohr Institute.

The group at the Niels Bohr Institute has over the last year gained experience with the hardware and software tools available for the T800 transputer. The software tools from INMOS for developing multitasking and multiprocessor applications have been well documented with very few problems.

In 1992/1993, when the T9000 and the C104 can be purchased, it is the idea to build a standard board with two C104 and four T9000 transputers each equipped with 1 MByte of DRAM, figure 4. This could be the basic building block of the high level trigger system. All links which are not used for the connections to the T9000's and between the two C104, are accessible on the edge of the board.

The component price of such a board would be 3200 SFr for the transputers, 1600 SFr for the C104, 2000 SFr for the memory and 700 SFr for the PC-board and connectors or a total of 7500 SFr. This is the production cost. The first board will be slightly more expensive.

The cost of the 200 boards needed to establish the proposed structure for the second level trigger, is 1.500.000 SFr.

Conclusion

The system which is described above, has several advantages. Some of them are here listed in random order.

* It is cheap,
* the hardware is designed for real-time applications,
* the data transport medium is an integral part of the processor hardware,
* the processing power is large,
* most of the hardware is available or will very soon be available,
* it is based on products heavily used by the civil- and military industry,
* the software is easy to write and a large number of support libraries already exist,
* it is easy to expand,
* a single architecture and software platform can be used for both the second and third level trigger.

As the next step we propose to establish a simulation- and hardware test bench for a further study of the proposed data acquisition model. This should be made in tight collaboration with the FERMI and FEAST projects.

The T9000 transputer products.

The T9000 family consist of a number of VLSI chips, which make it possible to connect large arrays of high performance transputers. These units could be used in a data acquisition and event building system for an LHC experiment. There are four basic building blocks,

* the T9000 transputer
* the C104 32 channel Packet Routing Switch
* the memory to link adapter.
* the C100 old to new link protocol converter.

each described in the following sections.

### The T9000 transputer.

The T9000 transputer [1] is the latest and until now the most powerful \(\mu\)-processor from INMOS. Figure A1 shows the building blocks of the processor. The three main components are,

* the CPU and its on-chip floating point unit (FPU)
* the 32-bit external memory bus
* the 4 bidirectional communication links and the 2 control links.

#### a.1.1 The CPU and the FPU.

The CPU performs up to 200 MIPS ( million instructions per second ) in short time intervals, and \(>\) 70 MIPS sustained. The FPU does 25 MFLOPs ( million floating point instructions per second ) peak and \(>\) 15 MFLOPs sustained. The performance is increased beyond the basic 50 MHz clock frequency level by the use of instruction pipelines. A 16 Kbyte on chip data and program cache is available for optimal programming.

The processor has a small and efficient instruction set, but in contrast to most other RISC machines it operates with only 6 CPU registers and 6 FPU registers. This makes context switching in a multi tasking application extremely fast.

Four external interrupt lines (events) can be served after less than one \(\mu\)sec.

Multi processing control and memory management are integral parts of the hardware.

High CPU/FPU performance, fast interrupt capability, hardware multitasking support together with the communication links, make it easy to establish a safe and efficient platform for data acquisition.

#### a.1.2 The memory bus.

Up to 4 Gbyte of external memory can be addressed from the T9000 transputer. An on chip programmable memory interface makes it possible to use up to 8 Mbyte of dynamic RAM without any extra hardware. This feature saves space on the PC-board and decreases the complexity of the hardware. Keeping the size of programs and date volumes below 8 Mbyte ensures access to the full processing power of the T9000 transputer.

#### a.1.3 Communication and control links.

A link is the physical communication path between two units. It is a point-to-point connection consisting of four wires, two used for outgoing and two for incoming messages. The previous version of the link had only two wires, but the increased link speed made it necessary to add two synchronization lines. This allows at the same time unknown length messages to be received. 100 Mbit's can be send and received every second simultaneously on all four links or a total of 800 Mbits/sec across the chip boundary. The Virtual Channel Processor (VCP) takes care of all the necessary protocol gymnastics to send and receive the messages. A short program example in appendix B shows, how easy it is the handle the link communication from a software point of view.

The logical data unit transported on the link is called a packet. A packet consists of three sections. A header, a max. 32 bytes data part and a trailer. The trailer can be an end_of_message_token if the message is short, or an end_of_packet_token if the message is longer than 32 bytes. In that case the message is continued in a following packet. A message can consist of infinitely many packets, where the last one always is terminated by an end_of_message_token. One physical link can be used by more than one process through a virtual link assignment. Packets from different processes can be interleaved on the physical link.

All T9000 units have two extra links (c-link), used to control the processor. These links are in particular useful when booting the system, and to communicate information which has to be placed on special memory addresses.

Apart from the software only a cable with four conductors is necessary to establish communication between two transputers.

#### a.1.4 Software.

Compilers exist for many high level languages including, C, C++, Fortran, Occam and Ada. A range of operating systems and real-time kernels are available for the transputers, for instance C Executive, VRTX and CROCUS distributed UNIX.

Also source level debugging is supplied for programs running out on the network.

As an example a fraction of a simple C program is shown in appendix B. It handles an interrupt, new process startup and two examples of link communication.

### The Packet Routing Switch, C104.

With the four links on each T9000 it is possible without extra hardware to connect any number of transputers in a network. But a dedicated process must reside on each node to perform the routing of messages through a node. This is CPU power consuming and slow.

A more elegant and efficient way to connect large arrays of transputers uses the C104 Packet Routing Switch, figure A2. It has 32 bidirectional links and 2 c-links. A message on any of the 32 inputs can be routed to any of the other 31 outputs. All 32 bidirectional links can be used at the same time and independent of each other. The routing algorithm used to determine the path through the unit is dynamically adjusted, and it is optimized to avoid hot spots. The worst case massage latency through the C104 is one _vec_, and up to 2 108 packets can path the switch each second.

The path list, which together with the message header determines the destination, is modified through the c-links.

### The link adapter, CXXX.

This unit makes it possible to connect external memory directly on a link. Such a unit can autonomously send a packet containing some or all the information stored in a memory to a node in the network.

### The system protocol converter, C100.

The T800, T400 and T200 transputers use an old version of the link. In order to integrate these transputers with a system of T9000's a protocol converter has be developed. In the early phase of the construction of a network such a unit will let existing systems work together with T9000 units.

### Cost and availability.

The T9000 family was announced in March 1991 and all members are expected to be commercially available in 1993. For the moment the prices of both the T9000-transputer and the C104 are expected to fall in the neighborhood of 500 US$. The prices of the two other units are for the moment unknown, at least to me.

[MISSING_PAGE_FAIL:8]

Figure 3.

Multi detector T9000/C104 trigger system. All subevents from one event are processed by CPU's located on the same column. This ensures minimal communication overhead for the third level processing.

Figure 4.

Logical layout of T9000/C104 test board.

Figure 1.

A C104 node equipped with two T9000 transducers and four front end read out modules (FERMI). The eight free links can be used for extra connections to neighbor C104 modules or to connect to other layers of C104 as illustrated in figure 3.

[MISSING_PAGE_POST]

Figure A1.

[MISSING_PAGE_EMPTY:12]

## References

* [1]_The T9000 transputer. Product overview and manual._ INMOS document DBTRANSPST/1
* [2]_A Digital Front-end and Readout Microsystem for Calorimetry at LHC._ R&D proposal. CERN/DRDC/90-7+, DRDC/P 19.