**Discovery Reach for Black Hole Production**

The ATLAS Collaboration1)

Footnote 1: This note prepared by N.D. Brett, J.A. Frost, D.M. Gingrich, R.H. Hakobyan, S.H. Han, M. Henke, C. Issever, M. Kaneda, B.T. King, V. Lendermann, K.F. Loureiro, J. Lu, J.P. Ottersbach, M.A. Parker, J. Tanaka.

_This note is part of CERN-OPEN-2008-020. This version of the note should not be cited: all citations should be to CERN-OPEN-2008-020._

Models with extra space dimensions, in which our Universe exists on a 4-dimensional brane embedded in a higher dimensional bulk space-time, offer a new way to address outstanding problems in and beyond the Standard Model. In such models the Planck scale in the bulk can be of the order of the electroweak symmetry breaking scale. This allows the coupling strength of gravity to increase to a size similar to the other interactions, opening the way to the unification of gravity and the gauge interactions. The increased strength of gravity in the bulk space-time means that quantum gravity effects would be observable in the TeV energy range reachable by the LHC. The most spectacular phenomenon would be the production of black holes, which would decay semi-classically by Hawking radiation emitting high energy particles. In this note, we discuss the potential for the ATLAS experiment to discover such black holes in the early data (1-1000 pb\({}^{-1}\)).

Introduction

In this study we simulate the search for black holes in the first \(100\,\mathrm{pb}^{-1}\) of LHC data with the ATLAS detector and software framework.

The document's structure is as follows: Section 2 gives an overview of the extra dimension models, present limits on the size of the extra dimensions and a discussion of black hole production and decay. In Section 3 the Monte Carlo simulation samples are described. Section 4 presents basic event properties, and is followed by Sections 5 and 6 dealing with the triggering and analysis selection, respectively. The expected systematic uncertainties are given in Section 7. Finally, the extraction of model parameters, especially of black hole properties, is covered in Section 8. A summary is given in Section 9.

## 2 Theory

### Theoretical Motivation

The electroweak energy scale and the Planck scale, at which gravitational interactions become strong, differ by about sixteen orders of magnitude. This large difference between the scales of the two fundamental interactions is known as the hierarchy problem. Explaining the hierarchy problem is one of the outstanding challenges in particle physics.

Arkani-Hamed, Dimopoulos and Dvali (ADD) [1, 2, 3], and Randall and Sundrum (RS) [4, 5] have pioneered approaches to solving the hierarchy problem by using extra-dimensional space. The hierarchy is generated by the geometry of the additional spatial dimensions. ADD models postulate additional flat extra dimensions, while RS models invoke a single warped extra dimension. The observed weakness of gravity is thus due to the gravitational field being allowed to expand into the higher-dimensional space (bulk), while the Standard Model particles are confined to our familiar three-dimensional space (3-brane). Extra-dimensional models can also be motivated by string theory.

In extra-dimensional models, the \(D\)-dimensional Planck scale \(M_{D}\) is the fundamental scale from which the Planck scale \(M_{\mathrm{Pl}}=1.22\times 10^{19}\) GeV in four dimensions is derived2. The relationship between the two scales is determined by the volume of the extra dimensions in ADD models or by the warp factor in RS models. For large extra dimensions or a strongly warped extra dimension, the fundamental scale of gravity can be as low as the electroweak scale. If the Planck scale is low enough, black holes could be produced at the Large Hadron Collider (LHC) [9, 10]. Detecting them will not only test general relativity and probe extra dimensions, but would also teach us about quantum gravity.

Footnote 2: Several conventions exist for the \(D\)-dimensional Planck scale in the ADD model. We denote by \(M_{D}\) the parameter defined by Giudice, Rattazzi and Wells [6] and used by the PDG [7]: \(M_{D}^{D-2}=(2\pi)^{D-4}/(8\pi G_{D})\), where \(G_{D}\) is the \(D\)-dimensional Newton gravity constant. In an alternative convention given by Dimopoulos and Landsberg [8] the \(D\)-dimensional Planck scale \(M_{\mathrm{DL}}\) is defined via \(M_{\mathrm{DL}}^{D-2}=1/G_{D}\).

### Experimental Limits

Assuming that low-scale gravity is due to the existence of extra dimensions3, most experimental searches for unusual low-scale gravity effects have focused on detecting evidence for extra dimensions. Current experimental limits allow the fundamental scale of gravity to be as low as about \(1\,\mathrm{TeV}\). In testing the ADD models and deriving limits in these models, the compactification radius of all extra dimensions is assumed to be the same. ADD models have been tested at length scales comparable to the radius of the compactified (i.e. curled-up) dimensions \(R\). Were the effective number of large extra dimensions to be \(n=D-4\), the inverse-square law would smoothly change from the \(1/r^{2}\) form for \(rggR\) to a \(1/r^{2+n}\) form for \(r\ll R\). Searches have been performed and constraints on the Planck scale have been set by tabletop and particle accelerator experiments, astrophysical observations, cosmic-ray measurements and cosmological considerations. Direct searches for black holes at collider experiments have not yet been performed. The only direct limits on black hole production in high energy interactions were obtained using cosmic-ray data. Table top experiments lead to an upper bound of \(R\leq 44\,\mathrm{\SIUnitSymbolMicro m}\), at the 95% confidence level [12]. The LEP bounds obtained with direct searches vary from \(1.5\,\mathrm{TeV}\) for \(n=2\) extra dimensions to \(0.75\,\mathrm{TeV}\) for 5 extra dimensions [13]. The latest direct search result from the CDF collaboration has set lower bounds with \(1.1\,\mathrm{fb}^{-1}\) of Run II data on \(M_{D}\) of \(1.33\,\mathrm{TeV}\) for \(n=2\) to \(0.88\,\mathrm{TeV}\) for \(n=6\)[14]. All four LEP experiments combined set a lower limit on \(M_{D}\) of \(1.2\,\mathrm{TeV}\) for positive interference, or \(1.1\,\mathrm{TeV}\) for negative interference between Standard Model diagrams and graviton exchange [15, 16]. Indirect searches by the DO [17] collaboration set lower limits around \(1.28\,\mathrm{TeV}\)4). Astrophysics places the most stringent lower limits on \(M_{D}\) in ADD models which however fall sharply with increasing number of extra dimension [18, 19, 20, 21, 22, 23, 24]. Considerations of neutron star dynamics imposes the strongest constraints: \(M_{D}>1760\), \(77\), \(9\) and \(2\,\mathrm{TeV}\) for \(n=2\), \(3\), \(4\) and \(5\) extra dimensions, respectively [18]. One should note that all astrophysical and cosmological constraints are based on a number of assumptions, whose uncertainties are not included in the limit derivations, so the results are reliable only as order of magnitude estimates. Ultra high-energy cosmic-ray particles, through their interaction with the Earth's atmosphere, offer a complementary probe of extra dimensions. Cosmic-rays interact with the atmosphere and earth's crust with centre-of-mass energies of the order of \(100\,\mathrm{TeV}\). The particles can produce black holes deep in the atmosphere, leading to quasi-horizontal giant air showers. So far a lower bound on \(M_{D}\) in the ADD model, ranging from \(1.0\) to \(1.4\,\mathrm{TeV}\) for scenarios with \(4\) to \(7\) extra dimensions has been set at 95% confidence level [25]. It is expected that the Pierre Auger Observatory will be able to set more stringent limits during the first five years of operation; the estimates place \(M_{D}\gtrsim 3\,\mathrm{TeV}\) for \(n\geq 4\)[26].

Footnote 4: This lower limit uses the Hewett approach for the calculation of \(M_{D}\) and implies a positive interference term \(\lambda\) with the Standard Model diagrams.

### Working Model

Our working model for black holes uses the black disk cross-section, which depends only on the horizon radius. The \((4+n)\)-dimensional Myers-Perry solution [27], similar to the 4-dimensional Schwarzschild radius, is chosen for the horizon radius \(r_{\mathrm{h}}\). It depends only on the number of dimensions and the Planck scale. The classical black hole cross-section at the parton level is

\[\hat{\sigma}_{ab\to\mathrm{BH}}=\pi r_{\mathrm{h}}^{2}\,, \tag{1}\]

where \(a\) and \(b\) are the parton types. In most cases, we work with initial black hole masses at least five times higher than the Planck scale at which the expression for the cross section should be valid.

The total cross-section is obtained by convoluting the parton-level cross-section with the parton distribution functions (PDFs), integrating over the phase space, and summing over the parton types.

Throughout this study we use the CTEQ6L1 (leading order with leading order \(\alpha_{s}\)) parton distribution functions [28] within the LHAPDF framework [29]. The momentum scale for the PDFs is set equal to the black hole mass for convenience.

The transition from the parton-level to the hadron-level cross-section is based on a factorisation ansatz. The validity of this formula for the energy region above the Planck scale is unclear. Even if factorisation is valid, the extrapolation of the parton distribution functions into this transplanckian region based on Standard Model evolution from present energies is questionable, since the evolution equations neglect gravity and possible KK states in the proton.

The details of horizon formation, and the balding and spin-down phases have been ignored5). The important effects of angular momentum in the production and decay of the black hole in extra dimensions are not accounted for in the Monte Carlo event generator. The black holes are considered as \(D\)-dimensional Schwarzschild solutions. Only the Hawking evaporation phase is generated by the simulation.

We can view the Hawking evaporation phase as consisting of two parts: determination of the particle species and assigning energy to the decay products. A particle species is selected randomly with a probability determined by its number of degrees of freedom and the ratio of emissivities6). The degrees of freedom take into account polarisation, charge and colour. The emitted charge is chosen such that the magnitude of the black hole charge decreases. All Standard Model particles are considered, including a Higgs boson7). The particles are treated as massless, including the gauge bosons and heavy quarks.

Footnote 6: Here emissivity is the fractional emission rate per degree of freedom. Mathematically it is ratio of \(dE/dt\) for different species, or roughly speaking the area under the black body distribution for a particular type of particle.

Footnote 7: Including a scalar Higgs boson is not significant since it has only one degree of freedom.

Gravitons have not been included in the simulation, which is another drawback of the current model. Because the graviton lives in the bulk, the number of degrees of freedom of the graviton becomes significant for high numbers of dimensions. In addition, the graviton emissivity is highly enhanced as the space-time dimensionality increases. Therefore the black hole may lose a significant fraction of its mass into the bulk, resulting in missing transverse energy.

The energy assignment to the decay particles in the Hawking evaporation phase has been implemented as follows. The particle species selected by the model described above is given an energy randomly according to its extra-dimensional decay spectrum. A different decay spectrum is used for scalars, fermions and vector bosons, i.e. the spin statistics factor is taken into account. Grey-body spectra are used without approximations [30]. The grey-body factors depend on the number of dimensions. The Hawking temperature is updated after each decay. It is assumed the decay is quasi-stationary in the sense that the black hole has time to come into equilibrium at each new temperature before the next particle is emitted. The energy of the particle given by the spectrum must be constrained to conserve energy and momentum at each step.

The evaporation phase ends when the chosen energy for the emitted particle is ruled out by the kinematics of a two-body decay. At this point an isotropic two-body phase-space decay is performed. In our simulation, the decay is performed totally to Standard Model particles and no stable exotic remnants survive.

Baryon number, colour and electric charge are conserved in the black hole production and decay in this model. Missing transverse energy in the generator comes only from the neutrinos, while in reality missing transverse energy is also possible due to the lost energy in inelastic production, graviton emission, a non-detectable black hole remnant and the possibility that the black hole can leave the Standard Model brane. For the black holes we consider, only a small amount of energy, on average, is lost due to neutrinos. If gravitons were considered, the average energy loss would be approximately 9% [31].

## 3 Monte Carlo Simulations

### Production of Signal and Background Events

The event generator CHARYBDIS [32, 33] version 1.003 was used within the ATLAS software framework to generate Monte Carlo signal samples. It was interfaced via the Les Houches accord [29] to HERWIG [34, 35] which provides the parton evolution and hadronisation, as well as Standard Model particle decays.

Table 1 shows the default CHARYBDIS parameters used, for which approximately 25000 events were generated. Three other black hole signal samples were generated with variations in the number of dimensions and in the black hole minimum mass. In all simulations, the parameter MSSDEF was set

equal to 2, setting the Planck scale MPLNCK to be the \(D\)-dimensional Planck scale \(M_{\rm DL}\) in the convention of Ref. [8]. The above samples subsequently underwent the full ATLAS detector simulation and reconstruction. Fast simulation using ATLFAST [36] was employed to widen the range of signal samples studied, enabling investigation of the many theoretical uncertainties modelled by generator parameter switches (see Table 2).

Black holes decay democratically to all particles of the Standard Model, so few Standard Model processes should produce the same particle spectrum. Black hole decays are characterised by a number of high energy and transverse momentum objects, so the primary Standard Model backgrounds are states with high multiplicity or high energy jets. The predominant backgrounds to our signal are described below and their datasets and cross-sections are listed in Table 3. Sizeable samples are required due to their large cross-sections at the LHC.

\begin{table}
\begin{tabular}{c|c|c} \hline \hline Name & Description & Value \\ \hline MINMSS & Minimum mass of black holes & 5 TeV \\ MAXMSS & Maximum mass of black holes & 14 TeV \\ MPLNCK & Planck scale & 1 TeV \\ MSSDEF & Convention for Planck scale & 2 \\ TOTDIM & Total number of dimensions & 6 \\ NBODY & Number of particles in remnant decay & 2 \\ GTSCA & Black hole mass used as PDF momentum scale & True \\ TIMVAR & Allow \(T_{H}\) to change with time & True \\ MSSDEC & Use all Standard Model particles as decay products & True \\ GRYBDY & Include grey-body effects & True \\ KINCUT & Use a kinematic cut-off on the decay & True \\ \hline \hline \end{tabular}
\end{table}
Table 1: Default parameters used in the CHARYBDIS generator.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline \(n\) & \(m_{\rm BH}\)( TeV) & \(\sigma\)( pb) & Note \\ \hline
2 & 5-14 & 40.7 & \\
2 & 8-14 & 0.34 & \\
4 & 5-14 & 24.3 & \\
7 & 5-14 & 22.3 & \\ \hline
2 & 5-14 & 6.4 & MPLNCK=2 \\
3 & 5-14 & 28.5 & \\
5 & 5-14 & 22.7 & \\
2 & 5-14 & 40.7 & KINCUT=0 \\
7 & 5-14 & 22.3 & KINCUT=0 \\
2 & 5-14 & 40.7 & TIMEVAR=0 \\
7 & 5-14 & 22.3 & TIMEVAR=0 \\
2 & 5-14 & 40.7 & NBODY=4 \\
7 & 5-14 & 22.3 & NBODY=4 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Monte Carlo datasets and their respective cross-sections used in this analysis. The first four samples were simulated using both full and fast simulations; the lower nine samples were simulated using the fast simulation ATLFAST. The final column shows the CHARYBDIS parameter that was changed with respect to the reference set shown in Table 1.

* \(t\bar{t}\) leptonic and hadronic decay modes. This process yields the largest contribution to the background due to its large cross-section at the LHC and the large branching ratio to hadronic final states. The matrix element calculation is done with MC@NLO [37] and HERWIG is used to perform the parton shower evolution, parton decay and their hadronisation.
* QCD dijet production. The requirements placed on the hadronic part of the signal events reduces the contribution from low-\(p_{T}\) QCD jets. This background is generated using Pythia 6.4[38]. Note that the complete QCD inclusive jet production is not fully modelled by the Pythia dijet simulation due to the lack of higher-order QCD contributions. Very low-statistics samples of multijet samples generated by ALPGEN [39] were also used.
* \(W\rightarrow\ell\nu+\text{jets}\) production. These backgrounds, though coming from the hard process, have cross-sections that rapidly become small compared to the signal as more jets are added. Vector boson plus jets samples were generated using ALPGEN.
* \(Z\rightarrow\ell\ell+\text{jets}\) production.
* \(\gamma(\gamma)+\text{jets}\) production.

### Detector Simulation

The detector simulation and reconstruction of both signal and background Monte Carlo events were performed within the ATLAS offline framework.

Fast simulation (ATLFAST) was used to widen the range of signal samples studied. The primary advantage of this is one of processing rate: since no detector interactions are modelled it requires less than one second per event. In contrast, full simulation requires approximately 15 minutes for typical Standard Model events, and over 30 minutes per black hole event. Despite this advantage, there are drawbacks: the fast simulation does not include a complete treatment of lepton isolation and misidentification nor of photon conversion.

The same generator signal samples were passed through the full and fast simulations in order to understand the differences in black hole events. Variables ranging from simple multiplicities to event shapes were compared. Sample distributions of particle multiplicity and \(\not{E}_{T}\)are shown in Figure 1. The multiplicity difference is due to differing default jet algorithms; we use a cone algorithm with radius \(\Delta R=0.4\) in the full simulation, whereas ATLFAST uses a \(k_{\perp}\) algorithm. Using the same algorithm for both samples gives very close agreement, nonetheless this discrepancy will have an effect in analyses

\begin{table}
\begin{tabular}{c|c} \hline \hline Process & \(\sigma\,\text{(pb)}\) \\ \hline Semi/Fully Leptonic \(t\bar{t}\) & 463 \\ Hadronic \(t\bar{t}\) & 370 \\ QCD dijets & \(12.84\times 10^{3}\) \\ \(W\to e\nu_{e}+\text{jets}\) & 281 \\ \(W\rightarrow\mu\nu_{\mu}+\text{jets}\) & 279 \\ \(Z\to ee+\text{jets}\) & 25.8 \\ \(Z\rightarrow\mu\mu+\text{jets}\) & 26.0 \\ \(\gamma+\text{jets}\) & \(5.00\times 10^{3}\) \\ \(\gamma\gamma+\text{jets}\) & 67.6 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Background Monte Carlo datasets and their respective branching ratio times cross-sections.

dependent purely upon multiplicity information. All other variables investigated showed concordant results.

## 4 Event Properties

The high mass scale, and the thermal nature of the decay process, result in black hole events being characterised by a large number of high-\(p_{T}\)final state particles, including all the Standard Model fields. Graviton emission is also expected, but is not simulated in CHARYBDIS. Of the final state particles, the detector can measure jets, electrons, muons and photons well, and will be able to reconstruct some of the \(Z\) and \(W\) bosons. The missing transverse energy, produced mainly by neutrino and graviton emission, can also be measured. In this section, the data sample with two extra dimensions and black hole masses above 5 TeV is used as the reference signal sample.

A key feature of black hole decays is that the Hawking temperature is higher for larger \(n\), for a given black hole mass. A higher temperature produces higher energy emissions, with the consequence that the energy is shared between fewer particles. This has a significant effect on the multiplicity and event shape distributions. Similarly, the samples with a higher black hole low-mass cutoff produce more high energy final state particles.

### Particle Types and Multiplicities

Figure 2 shows the types of particles produced directly by black hole decay. The vertical axis shows the average number of particles per black hole decay. From this figure, we see that a heavier black hole has more decay products. The particle-antiparticle balance is broken by the initial state of two protons colliding. Moreover, due to conservation of energy and momentum, colour connection etc., a perfect democratic decay cannot be achieved, e.g., the number of top quarks is smaller than that of ligher quarks. The possibility of identifying fermions and bosons and determining their branching ratios in black hole decays was studied in [40].

Figure 3 shows \(p_{T}\) and pseudorapidity (\(\eta\)) distributions of particles produced directly from black hole decays. As expected, the shape depends little on particle type. Figure 4 shows the reconstructed multiplicity of final-state jets, leptons and photons. Four signal samples are shown for \(n=2\), 4 and 7 with a minimum black hole mass of 5 TeV, and for \(n=2\) with a minimum black hole mass of 8 TeV. The figure also compares the reference signal to the backgrounds. The multiplicity in the signal falls as

Figure 1: Total particle multiplicity and missing transverse energy distributions for signal samples from fast and full simulation.

Figure 3: Generator \(p_{T}\) distributions (top row): leptons (left) and \(Z\) bosons (right) emitted from the black hole. The bottom row shows \(p_{T}\) and \(\eta\) spectra for all particles emitted from the black hole.

Figure 2: PDG code of particles emitted from black hole decay for a minimum black hole mass of \(5\,\mathrm{TeV}\) and \(n=2\), \(4\) and \(7\) and for a minimum black hole mass of \(8\,\mathrm{TeV}\) and \(n=2\) (\(|PdgId|=1-6\) are quarks, \(11-16\) leptons, and \(21-25\) gauge and Higgs bosons). The vertical axis shows multiplicity per black hole decay.

rises, because the black holes decay at a higher temperature.

### Event Shape

At first sight, one would expect black hole events to be very different from the background in event shape variables [41, 38, 42] such as sphericity, because of the high multiplicity thermal decay. However, the event shape of the black hole events varies considerably with \(n\), making such variables less useful than could be hoped. Though the background distributions show less variation, when these are scaled by their large cross sections, there is a large degree of overlap, disfavouring their use as a cut variable. Additionally, our ignorance of the decay modes of the final black hole remnant introduces a significant systematic effect. In our version of CHARYBDIS, once the mass of the black hole has dropped below the Planck scale, the remnant decays to either 2 or 4 bodies. We have selected the two-body option for our standard samples. This means that at high \(n\), where events can reach this stage after few emissions, the circularity of the events is reduced, and the thrust increased.

The distinguishing power between signal and backgrounds of a selection of event shape variables was studied; Figure 5 shows the circularity distribution for the same samples as Figure 4; similarly Figs. 6 and 7 show their thrust distributions, sphericity and aplanarity. The expected bias towards more "jet-like" events is clearly seen at high \(n\). For this reason, we choose not to use event shape variables as a discriminant in this analysis.

## 5 Trigger

The ATLAS trigger and data-acquisition system consists of three levels (L1, L2, EF) of online event selection [43]. Each subsequent trigger level refines the decisions made at the previous level and may apply additional selection criteria. The ATLAS trigger is described in detail in ref. [44].

### Triggering on Black Holes

Each black hole produces multiple decay products, including hadronic jets, leptons and photons, as described in Section 4. The jets typically carry a dominant fraction of the visible decay energy and hence provide the best option for triggering black hole events.

The response of the jet trigger, as simulated in the current version of the ATLAS detector simulation, is demonstrated in Figure 8. The plots show the trigger efficiencies for various \(p_{T}\)-thresholds as functions

Figure 4: Multiplicities of reconstructed objects for (left) black hole samples and (right) backgrounds. They are normalised to the integrated luminosity of \(1\,\mathrm{fb}^{-1}\).

Figure 5: Circularity calculated from reconstructed objects for (left) black hole samples and (right) backgrounds.

Figure 6: Thrust calculated from reconstructed objects for (left) black hole samples and (right) QCD dijet and \(t\bar{t}\) backgrounds.

Figure 7: Different event shape variables for black hole samples and backgrounds. They are normalised to the integrated luminosity of 1 fb\({}^{-1}\).

of the jet \(p_{T}\) reconstructed offline. For these plots, a match between the jet reconstructed offline and at the respective trigger level is required. The matching consists of searching for the closest offline jet in \(\Delta R=\sqrt{(\Delta\eta)^{2}+(\Delta\phi)^{2}}\), where \(\Delta\eta\) and \(\Delta\phi\) are the distances between the reconstructed jet and the trigger jet in pseudorapidity \(\eta\) and azimuth \(\phi\), respectively. To avoid incorrect matching for L1 jets, a modified criterion is applied: the L1 jet closest in energy to the reconstructed jet is chosen among the jets found within the \(\Delta R=0.5\) distance around the reconstructed jet. The shape of the L1 efficiency distribution for the 800 GeV threshold is due to the saturation of the L1 trigger tower energies at 255 GeV. Events in which the transverse energy in one trigger tower exceeds 255 GeV are automatically accepted, as larger values fill up the memory of the L1 trigger analog-to-digital converters.

The efficiency at each trigger level is determined independently of the decisions at the other levels. Were the trigger chain to have the same threshold on all levels, the total efficiency would be the convolution of the respective functions. The L2 algorithms are based on regions of interest provided by L1, hence it is not possible to determine their efficiency completely independently of the L1 decisions. The L2 algorithms were run on all L1 jet RoI starting from the lowest L1 jet \(p_{T}\)threshold of 35 GeV. This is much lower than the thresholds studied, making the L2 decision (shown in Figure 8b) virtually independent of the L1 efficiency.

The total trigger efficiencies are listed in Table 4 for three signal samples and demonstrated in Figure 9 for the signal sample with \(n=2\) and \(m>5\) TeV. The highest efficiency is provided by the single-jet trigger, which we consider to be the master trigger for the black hole events. The presence of multiple high-\(p_{T}\) jets per event, each of which is likely to pass the trigger, results in very high total efficiencies. Setting this trigger threshold at 400 GeV will provide greater than 99% efficiency at all trigger levels. The Standard Model process rate at this threshold is expected to be less than 0.1 Hz at an instantaneous

Figure 8: Simulated jet trigger efficiencies as functions of the offline reconstructed jet \(p_{T}\)for a) L1, b) L2 and c) EF. The efficiencies are determined for different \(p_{T}\)-thresholds: 150 GeV (black), 300 GeV (red), 400 GeV (blue), 600 GeV (magenta) and 800 GeV (cyan).

luminosity of \(10^{31}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\), which should allow this trigger to run at this threshold without prescaling for the first few years of LHC data taking. The rate of black hole events is expected to be less than \(5\,\mathrm{mHz}\) at the \(10^{31}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\) luminosity. For the start-up running at the luminosity of \(10^{31}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\), it is planned to set the highest threshold for the single-jet trigger at \(120\,\mathrm{GeV}\), guaranteeing an efficiency of almost \(100\%\) for black hole events.

Alternatively, a trigger based on the scalar sum of transverse energies of all recorded decay products ("sum-\(E_{T}\) trigger") can be used. No simulation of this trigger is available in the samples used in this study. Looking at this sum in the offline reconstruction suggests that this trigger would collect nearly \(100\%\) of black hole events for Planck scales above \(1\,\mathrm{TeV}\). It is foreseen to run this trigger in the start-up data taking, unprescaled at the threshold of \(650\,\mathrm{GeV}\).

Based on experience from previous collider experiments, one may expect detector hardware problems at the beginning of data taking. In particular, noisy channels in the calorimeter or trigger electronics may cause high trigger rates for the single-jet trigger and for the sum-\(E_{T}\) trigger, such that even the highest threshold triggers have to be prescaled. In such cases, a multijet (3- or 4-jet) trigger is considered for use until the detector problems are resolved. The efficiencies of such triggers are listed in Table 4.

In the present study, the minimum mass of a black hole is set at \(5\,\mathrm{TeV}\) or more in order to be safely above the Planck scale. At lower masses one may expect an increased rate of dijet events described by a contact interaction. The single-jet trigger or the sum-\(E_{T}\) trigger at the thresholds considered above are well suited for detecting such signatures. Such events may not, however, be selected by multijet triggers.

The trigger efficiencies, studied here in the simulation, have to be determined from data. An unbiased determination requires an "orthogonal" trigger, e.g. a trigger based on fully independent information

Figure 9: Simulated jet trigger efficiencies for black hole events from the signal sample with \(n=2\) and \(m>5\) TeV as functions of the jet \(p_{T}\)threshold for a) single-jet trigger, b) 3-jet trigger and c) 4-jet trigger. The efficiencies are determined for L1 (black), L2 (red) and EF (blue).

from that used by the master trigger. A muon trigger which is based solely on signals in the muon detector should be well suited for such studies.

## 6 Signal Selection and Background Rejection

### Event Selection

Since all types of Standard Model particles are produced from black hole decay, we make full use of particle identification information (PID) from our detectors. First we select muons, electrons, photons and jets, which are called _objects_ in this section. Table 5 shows the details of their selection criteria.

The identification of objects is sometimes ambiguous: e.g., an electron could be simultaneously reconstructed as a jet. To resolve this, we apply PID to each object, selecting muons, electrons, photons and jets in that order of priority. Once an object passes the PID criteria in a given category, any remaining ambiguous assignments are removed if they match the chosen object within a \(\Delta R\) of less than 0.1.

Next we select black hole events using these objects as described below. Then we reconstruct a black hole from all the identified objects for the selected event. The mass of the black hole in an event

\begin{table}
\begin{tabular}{r|r r r} \hline \hline Trigger & L1 & L2 & EF \\ \hline j100 & 1 & 1 & 1 \\ j400 & 0.997 & 0.997 & 0.997 \\ jj100 & 0.998 & 0.998 & 0.998 \\ j250 & 0.972 & 0.971 & 0.971 \\ j100 & 0.985 & 0.985 & 0.985 \\ j250 & 0.865 & 0.862 & 0.862 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Simulated jet trigger efficiencies for black hole events as functions of the jet-\(p_{T}\)threshold for different simulation samples.

\begin{table}
\begin{tabular}{r|r r} \hline \hline Trigger & L1 & L2 & EF \\ \hline j100 & 1 & 1 & 1 \\ j400 & 0.990 & 0.987 & 0.985 \\ j100 & 0.807 & 0.806 & 0.805 \\ j1250 & 0.710 & 0.704 & 0.704 \\ j100 & 0.525 & 0.522 & 0.522 \\ j1250 & 0.343 & 0.341 & 0.341 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Simulated jet trigger efficiencies for black hole events as functions of the jet-\(p_{T}\)threshold for different simulation samples.

is calculated from the four-momenta of the reconstructed final state objects and missing \(E_{T}\), which is included in the calculation to improve the reconstructed mass resolution:

\[p_{\rm BH} =\sum_{i=\text{objects}}p_{i}+(\not{E}_{T},\not{E}_{T\,x},\not{E}_{T \,y},0)\, \tag{2}\] \[m_{\rm BH} =\sqrt{p_{\rm BH}^{2}}. \tag{3}\]

We present two methods to select black hole events. One is based on the scalar summation of \(p_{T}\) and the other on the multiplicity of high-\(p_{T}\) objects. Both make use of the characteristic of a black hole having large mass. After that, we require a high-\(p_{T}\) lepton to reject backgrounds further.

Figure 10 shows the scalar summation of the \(p_{T}\) of each object, \(\sum|p_{T}|\), which demonstrates good background discrimination and high signal efficiency for all black hole samples. We require \(\sum|p_{T}|\) to be larger than \(2.5\,\text{TeV}\) to reject backgrounds. This requirement is relatively unaffected by changes in the model, in particular by changes to the number of extra dimensions \(n\). Figure 11 shows \(m_{\rm BH}\) distributions after this requirement. The QCD dijet background is already well suppressed, but we also investigated the effect of a further selection, requiring a lepton with a \(p_{T}>50\,\text{GeV}\). This resulted in the QCD dijet background being rejected by a factor greater than \(10^{6}\) as shown in Table 6, which summarises the event numbers for an integrated luminosity of \(1\,\text{fb}^{-1}\). Though the high statistics QCD samples used were generated with Pythia, a leading order generator, there were also \(p_{T}\)-sliced small ALPGEN multijet samples available. When investigated using the \(\sum|p_{T}|\) and lepton cut method, a very similar, marginally lower number of background events was predicted according to the very limited statistics available. Larger scale studies would be needed to conclude anything more concrete. Poisson confidence limits are used for samples where fewer than 20 events passed the requirements. Signal cross-section errors are statistical only; the theoretical uncertainties are large as discussed in Section 2. 8

Footnote 8: In the case of two hadronic subsamples (\(t\overline{t}\) and dijets) where very few events passed the \(\sum|p_{T}|\) requirement, the lepton requirement rejection factor was applied to the \(\sum|p_{T}|\) requirement’s Poisson bound to estimate the background distribution error.

An alternative selection procedure was also used. Figure 12 shows the \(p_{T}\) distributions of the leading, 2nd-, 3rd- and 4th-leading objects out of all the selected objects. The 4th-leading object still has larger \(p_{T}\) in the signal events than in the background events. We require the number of objects with \(p_{T}>200\,\text{GeV}\) to be equal to or greater than four. Figure 14 (left) shows \(m_{\rm BH}\) distributions after this requirement. Since

Figure 10: \(\sum|p_{T}|\) distributions for (left) black hole samples and (right) backgrounds (QCD dijet, \(t\overline{t}\) and vector boson plus jets), along with one signal sample for reference. They are normalised to an integrated luminosity of \(1\,\text{fb}^{-1}\).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Before selection & \(\sum|p_{T}|>2.5\) TeV & After requiring a lepton & acceptance \\  & (fb) & (fb) & (fb) & \\ \hline \(n=2,m>5\) TeV & \(40.7\pm 0.1\times 10^{3}\) & \(39.2\pm 0.3\times 10^{3}\) & \(18.6\pm 0.2\times 10^{3}\) & 0.46 \\ \(n=4,m>5\) TeV & \(24.3\pm 0.1\times 10^{3}\) & \(22.6\pm 0.2\times 10^{3}\) & \(6668\pm 83\) & 0.27 \\ \(n=7,m>5\) TeV & \(22.3\pm 0.1\times 10^{3}\) & \(20.1\pm 0.2\times 10^{3}\) & \(3574\pm 60\) & 0.17 \\ \(n=2,m>8\) TeV & \(338.2\pm 1\) & \(338.1\pm 2.5\) & \(212\pm 16\) & 0.63 \\ \(t\bar{t}\) & \(833\pm 100\times 10^{3}\) & \(23.6^{+12.2}_{-6.7}\) & \(8.2^{+2.43}_{-2.43}\) & \(9.8\times 10^{-6}\) \\ QCD dijets & \(12.8\pm 3.7\times 10^{6}\) & \(5899^{+1773}_{-1771}\) & \(5.37^{+3.25}_{-2.02}\) & \(4.3\times 10^{-7}\) \\ \(W_{\ell\nu}+\geq 2\) jets & \(1.9\pm 0.04\times 10^{6}\) & \(12.3^{+9.0}_{-1.8}\) & \(4.67^{+8.75}_{-0.93}\) & \(2.4\times 10^{-6}\) \\ \(Z_{\ell\ell}+\geq 3\) jets & \(51.8\pm 1\times 10^{3}\) & \(2.75^{+2.02}_{-2.01}\) & \(2.57^{+0.95}_{-0.64}\) & \(5.0\times 10^{-5}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Acceptance for each signal and background dataset in fb after requiring \(\sum|p_{T}|>\)2.5 TeV, and a lepton with \(p_{T}>50\) GeV.

Figure 11: Black hole mass distribution with a requirement \(\sum|p_{T}|>\)2.5 TeV (left), and black hole mass distribution with an additional requirement on the lepton-\(p_{T}\) of \(p_{T}>50\) GeV (right). The signal sample with \(n=2\) and \(m>5\) TeV and backgrounds are shown.

QCD processes still remain large, a lepton requirement is again used to decrease it. Figure 13 shows the distribution of the highest \(p_{T}\,\) lepton (muon or electron). As expected, the number of leptons from QCD processes is small. Requiring the number of leptons (muons or electrons) with \(p_{T}>200\,\)GeV to be equal to or greater than one results in the \(m_{\rm BH}\) distributions shown in Figure 14(right).

The shape of the background in the region of high \(m_{\rm BH}\) was fitted with a gaussian plus an asymmetric gaussian (Figure 15) and that function is used to estimate the number of background events.

CHARYBDIS does not include graviton emission. In practice this, and the energy lost in gravitational interactions during the balding phase, would be another source of \(\not{E}_{T}\). Consequently we expect CHARYBDIS to underestimate this for black hole events. Nonetheless, each of the black hole samples studied in this analysis often have very wide distributions of \(\not{E}_{T}\), with tails extending out to several TeV. This property of models with black holes is most unusual and hard to reproduce in other new physics scenarios, and should make it possible to distinguish between Black Holes and the majority of SUSY models for example.

A requirement on \(\not{E}_{T}\,\) above \(\sim 500-600\,\)GeV was studied as an alternative to a lepton requirement for black hole signal selection. Figure 16 shows the potential of this method, and contrasts these models with three common supersymmetric models of different cross-section and mass scale. Despite the possibility of early evidence for the presence of black holes, there are disadvantages to relying on such a selection. Firstly, our ability to reconstruct the black hole's mass is aided by limiting \(\not{E}_{T}\,\) to be under \(100\,\)GeV (Figure 22). Such a signal from high \(\not{E}_{T}\,\) events would be dominated by those events reconstructed most poorly, limiting their use for cross-section measurement and discovery. The theoretical uncertainties are large and difficult to quantify, and finally there are experimental difficulties in calibrating and accurately measuring this variable across a wide energy range.

Figure 12: \(p_{T}\,\) distributions of leading (top left), 2nd- (top right), 3rd- (bottom left) and 4th-leading (bottom right) objects out of all the selected objects for the signal sample with \(n=2\) and \(m>5\) TeV and backgrounds (see Table 7).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Before selection & After multi-object & After lepton requirement & Acceptance \\  & (fb) & requirement (fb) & (fb) & \\ \hline \(n=2,m>5\) TeV & \(40.7\times 10^{3}\) & \(38.9\pm 0.4\times 10^{3}\) & \(14.0\pm 0.2\times 10^{3}\) & 0.34 \\ \(n=4,m>5\) TeV & \(24.3\times 10^{3}\) & \(17.9\pm 0.3\times 10^{3}\) & \(4521\pm 126\) & 0.19 \\ \(n=7,m>5\) TeV & \(22.3\times 10^{3}\) & \(9953\pm 185\) & \(1956\pm 82\) & 0.087 \\ \(n=2,m>8\) TeV & \(338\) & \(338\pm 4\) & \(164\pm 3\) & 0.49 \\ \(t\bar{t}\) & \(833\times 10^{3}\) & \(129\pm 27\) & \(36^{+12}_{-9}\) & \(4.3\times 10^{-5}\) \\ QCD dijets & \(12.8\times 10^{6}\) & \(38.9\pm 1.9\times 10^{3}\) & \(6^{+107}_{-3}\) & \(5.6\times 10^{-7}\) \\ W+jets & \(560\times 10^{3}\) & \(99^{+28}_{-2}\) & \(56^{+24}_{-13}\) & \(1\times 10^{-3}\) \\ Z+jets & \(51.8\times 10^{3}\) & \(29^{+90}_{-4}\) & \(19^{+90}_{-3}\) & \(4\times 10^{-4}\) \\ \(\gamma(\gamma)\)+jets & \(5.1\times 10^{6}\) & \(285^{+87}_{-76}\) & \(0^{+40}_{-0}\) & \(<10^{-5}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Acceptance of the 4-object requirements for each dataset in fb. 90% confidence limits are used when no events passed the requirements.

Figure 14: Black hole mass distribution for the signal sample with \(n=2\) and \(m>5\) TeV and backgrounds (see Table 7) after multiplicity requirement of at least 4 objects with \(p_{T}>200\) GeV (left plot) and an additional requirement of a lepton (electron or muon) with \(p_{T}>200\) GeV (right plot).

Figure 13: \(p_{T}\) distributions of the leading lepton (electron or muon) after requiring the number of objects (electron, muon, photon or jet) with \(p_{T}>200\) GeV to be larger than 3 for the signal sample with \(n=2\) and \(m>5\) TeV and backgrounds (see Table 7).

Figure 16: The left hand plot shows the missing transverse energy distributions after a \(\sum|p_{T}|>2.5\) TeV requirement. A requirement of \(\not{E}_{T}\)\(>500\) GeV would leave negligible background and a large number of signal events for all samples. The right hand plot compares two black hole samples with three supersymmetric models with a range of mass scales; the two classes of models can easily be distinguished by their differing cross-sections and the extent of the \(\not{E}_{T}\) tail.

Figure 15: The background shape after the 4-object and lepton requirements is shown (data points). The points were fitted by the sum (black line) of a gaussian (red line) and an asymmetric gaussian (green line).

### Discovery Reach

Making a robust discovery potential for black hole events is difficult, because the semi classical assumptions used to model them are only valid well above the Planck scale. Close to the Planck scale, events may occur due to gravitational effects with lower multiplicities, but without the signatures anticipated by our event selections. As the energy rises above the threshold needed for black hole creation, our requirements should become more efficient. Lack of theoretical understanding makes it impossible to model this threshold region.

To account for this, we impose a lower requirement on the true mass of black holes created in our simulated samples, \(BH_{thresh}\), normally set at 5 TeV, and we do not attempt to account for any additional signal from lower masses. In order to estimate the discovery potential, two methods have been considered:

1. we keep our signal selection requirements constant, and increase the value of \(BH_{thresh}\). Since the analysis requirements are unchanged, the background remains constant, while the signal drops as the production of events occurs at higher mass. We then evaluate the luminosity required to detect a minimum of 10 signal events, with \(S/\sqrt{B}>5\), assuming the production cross-section is as high as predicted. Such a study is shown in Figure 17, using the \(\Sigma|p_{T}|\) and lepton requirements. This method produces conservative limits, taking some account of the uncertainty in the production cross-section near the threshold.
2. We keep the production model unchanged with \(BH_{thresh}=5\) TeV, but apply an additional requirement on the reconstructed black hole mass. This requirement reduces substantially background events, while allowing the higher mass signal to pass unchanged. This is less conservative, since it allows black hole signal events to be produced at low mass, but to migrate above the reconstructed mass requirement because of the detector mass resolution, hence increasing the signal. As before, we use the nominal value of the production cross-section, and evaluate the luminosity needed to meet our discovery criteria, this time as a function of reconstructed mass. A study using this method is shown in Figure 18 using the 4-object and lepton requirements.

The two approaches are complementary and illustrate the uncertainties in different ways. We observe that the search reach is limited eventually at high mass by the falling production cross-section, reflecting the falling parton luminosity and the limited energy of the LHC. We conclude that, if the semi classical cross-section estimates are valid, black holes can be discovered above a 5 TeV threshold with a few pb\({}^{-1}\) of data, while 1 fb\({}^{-1}\) would allow a discovery to be made even if the production threshold was at 8 TeV.

## 7 Systematic Uncertainties

### Signal uncertainties

We have investigated the systematic uncertainties using fast simulation runs, having checked that the full and fast simulations agree well for this purpose.

There are a number of theoretical parameters associated with CHARYBDIS which can generate systematic errors in the estimates of the acceptance for signal events. These are:

* The kinematic cutoff. This parameter is normally true, and causes the generator to end thermal emission if an unphysical emission is randomly selected. The generator moves immediately to the final remnant decay phase. This approximation deteriorates at high numbers of extra dimensions because of the high temperature and emitted particle energies. We have investigated the alternative, where a new emission is selected until a physical one is chosen. In this case, thermal emission will continue until the black hole mass falls below \(M_{\rm DL}\).

Figure 17: Discovery potential using \(\sum|p_{T}|\) and lepton selections: required luminosity as a function of black hole mass threshold. Error bars reflect statistical uncertainties only.

Figure 18: Discovery potential for black holes using four-object and lepton requirements. The required luminosity is shown as a function of the requirement on the reconstructed black hole mass. The error bars correspond to experimental systematic uncertainties. (See text for constraints.)

* Temperature variation. The Hawking temperature of the black hole is normally allowed to increase as its mass decreases, as expected if the black hole has time to equilibrate between decays. We have investigated the alternative of keeping the temperature fixed at the initial value, as would be the case if the black hole decayed very quickly or "suddenly" (see Table 8).
* Number of extra dimensions. In addition to our full simulation samples with \(n=2\), 4 and 7, we have simulated \(n=3\) and 5 with the fast simulation. As noted above, the events become more jet-like at high \(n\) and the particle multiplicity drops (see Figure 20), due to the increased Hawking temperature. Our signal selection remains robust, as shown in Table 8.
* Planck scale. We have investigated changing the Planck scale from its default value of \(M_{\rm DL}=1\,\)TeV to \(2\,\)TeV. We note that, since the model is only valid for black hole masses much larger than the Planck scale, this scenario is not well modelled in the range of masses accessible at the LHC.
* Remnant decay. We have investigated changing the remnant decay model from a two-body to a four-body mode (see Figure 20).

Figure 19 shows the effect of changing the kinematic cutoff on the particle multiplicity and \(\sum|p_{T}|\) distributions. Since the black hole is forced to decay thermally until it falls below \(M_{\rm DL}\), the multiplicity is higher, and the events have lower energy emissions. The total energy remains constant, however so the \(\sum|p_{T}|\) distribution is relatively stable.

The acceptance of the signal for various parameter choices are shown in Table 8. Compared to the standard full simulation, the largest effect is observed for high \(n\), when the kinematic cut-off parameter is changed. This is as expected, since this parameter has a large effect on the evolution of the black hole during its decay. Similarly changing the remnant decay from 2 to 4 bodies has a large effect at high \(n\); the effect of this, and of changing number of extra dimensions is shown in Figure 20.

### Uncertainties on detector performance

Two kinds of systematic uncertainties on detector performance were studied. One is an uncertainty on lepton identification efficiency. To estimate this effect, we loosened and tightened particle identification selections, by changing the hadronic leakage for electrons and the isolation cut for muons. The changes in signal efficiencies are around 2%.

The effect of a 5% error in the jet energy scale (JES) was also considered. The effect of these uncertainties on the discovery potential is shown in Figure 18.

Figure 19: Particle multiplicity and \(\sum|p_{T}|\) distributions, showing the variation when the kinematic cut-off parameter is changed.

## 8 Results

### Search Reach for Black Hole Production at the LHC

The studies presented show that the ATLAS detector is capable of discovering the production of black holes up to the kinematic limit of the LHC, assuming that the signal is correctly modelled. This conclusion is largely based on the predicted huge production cross-section, and the small background from QCD dijets at very high-\(p_{T}\), especially when the presence of a high energy lepton is required. However, both of these assumptions are suspect. The high production cross-section is subject to considerable discussion in the literature, as discussed in Section 2. Moreover, until the LHC has measured the QCD cross-section at 14 TeV, we cannot be certain of the tails of the QCD distributions. The Monte Carlo simulations of these tails are working at the limit of their validity, given the high energies and large multiplicities involved.

For these reasons, we prefer not to place too much weight on the detailed search reach limits. Instead, we confine ourselves to the statement that, with current understanding, the black hole signature considered should be clearly visible if it exists.

### Determination of Model Parameters

We have considered the possibility of extracting model parameters from the data, should a signal be observed. There are two key parameters: the Planck scale \(M_{D}\) (or \(M_{\rm DL}\) depending on the convention) and the number of extra dimensions \(n\). In Ref. [46, 47] a method was proposed to extract \(M_{D}\) from the cross-section data, which fixes the Planck scale (within the model assumptions), and from events with high energy emissions.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \(n\) & Full Sim & Fast Sim & Kin. Cut off & \(T_{H}\)-variation off & 4-body remnant \\ \hline
2 & 45.8 & 42.9 & 47.2 & 48.7 & 47.9 \\
3 & - & 33.2 & - & - & - \\
4 & 27.4 & 26.6 & - & - & - \\
5 & - & 21.7 & - & - & - \\
7 & 16.1 & 15.9 & 29.2 & 16.6 & 27.4 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Signal acceptance (%) for different model assumptions.

Figure 20: Particle multiplicity distributions, showing the variation produced by a change in the number of extra dimensions, or the number of particles produced by remnant decay.

The Hawking temperature \(T_{H}\) of the black hole depends on \(n\). If we detect events with emissions near \(m_{\rm BH}/2\), the energy of those emissions is a measure of the initial \(T_{H}\). Hence, over the sample of black holes, the probability of such emissions is a measure of the characteristic temperature, and can be used to extract \(n\). This method was first put forward in Ref. [46], and here is made compatible with the need for background rejection requirements. The requirements described in Section 6 are not appropriate: the lepton requirement biases the selected events in favour of final states with many particles, and hence against those events with a single high energy emission. A suitable requirement was found to be \(\sum|p_{T}|>3.5\) TeV; this removes the background without biasing the signal events selected.

Figure 21 shows the probability of a hard emission for two samples, compared to the predictions. The method requires accurate mass resolution, and so an additional requirement on \(\not{E}_{T}<100\) GeV is applied. The addition of this requirement lowers the efficiency noticeably, but does improve the black hole mass reconstruction; details of the resolution and efficiency can be found in Figure 22. The data are consistent with the expected value of \(n\), but due to this reduction in signal efficiency, more data would be required to make a definitive measurement. It should be noted that this measurement requires the Planck scale to be known. If this cannot be determined from the production cross-section, it is likely that the threshold behaviour near the Planck scale would provide an indication of its value. At present, no theoretical model exists to allow us to make predictions in this region.

## 9 Summary

The search for black holes in the first \(100\,{\rm pb}^{-1}\) of LHC data with the ATLAS detector and software framework was simulated.

We summarised the current experimental limits on black hole production and studied, with the help of the black hole event generator CHARYBDIS and Standard Model Monte Carlo data sets, the basic event properties, trigger and selection efficiencies, theoretical and experimental uncertainties of black hole production at the LHC for a flat ADD extra dimension scenario with the Planck scale \(M_{\rm DL}=1\,{\rm TeV}\). We have explored the uncertainties inherent in the theoretical modelling and our understanding of the detector. We conclude that, if the semi-classical cross section estimates are valid, black holes above a \(5\,{\rm TeV}\) threshold can be discovered with a few \({\rm pb}^{-1}\) of data, while \(1\ {\rm fb}^{-1}\) would allow a discovery to be made even if the production threshold was \(8\) TeV.

Figure 21: The probability of a hard emission near \(m_{\rm BH}/2\) for \(n=7\) and \(n=4\). The bands show the expected range for \(n=7\) and \(n=6\), and for \(n=5\) and \(n=4\), respectively, for a luminosity of approximately \(0.75\ {\rm fb}^{-1}\).

## References

* [1] N. Arkani-Hamed, S. Dimopoulos, and G. R. Dvali, _Phys. Lett._**B429** (1998) 263-272, arXiv:hep-ph/9803315.
* [2] I. Antoniadis, N. Arkani-Hamed, S. Dimopoulos, and G. R. Dvali, _Phys. Lett._**B436** (1998) 257-263, arXiv:hep-ph/9804398.
* [3] N. Arkani-Hamed, S. Dimopoulos, and G. R. Dvali, _Phys. Rev._**D59** (1999) 086004, arXiv:hep-ph/9807344.
* [4] L. Randall and R. Sundrum, _Phys. Rev. Lett._**83** (1999) 3370-3373, arXiv:hep-ph/9905221.
* [5] L. Randall and R. Sundrum, _Phys. Rev. Lett._**83** (1999) 4690-4693, arXiv:hep-th/9906064.
* [6] G. F. Giudice, R. Rattazzi, and J. D. Wells, _Nucl. Phys._**B544** (1999) 3-38, arXiv:hep-ph/9811291.
* [7]**Particle Data Group** Collaboration, W. M. Yao _et al._, _J. Phys._**G33** (2006) 1-1232.
* [8] S. Dimopoulos and G. L. Landsberg, _Phys. Rev. Lett._**87** (2001) 161602, arXiv:hep-ph/0106295.
* [9] S. Hossenfelder, arXiv:hep-ph/0412265.
* [10] P. Kanti, _Int. J. Mod. Phys._**A19** (2004) 4899-4951, arXiv:hep-ph/0402168.
* [11] X. Calmet and S. D. H. Hsu, arXiv:0711.2306 [hep-ph].

Figure 22: Black hole mass resolution distributions and their fits to double Gaussian functions, using a \(\sum|p_{T}|\) and lepton requirement (\(1\,\mathrm{fb}^{-1}\)). The upper curve is without a \(\not{E}_{T}\)cut. The lower curve has an additional requirement on \(\not{E}_{T}\)\(<100\) GeV.