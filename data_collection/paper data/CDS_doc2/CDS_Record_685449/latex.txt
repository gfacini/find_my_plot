**Confidence Level Calculations for**

\(H\to W^{+}W^{-}\to l^{+}l^{-}/\!\!\!\!/p_{T}\) **for**

\(115<M_{H}<130\,\mbox{GeV}\)

**Using Vector Boson Fusion**

Kyle Cranmer, Peter McNamara, Bruce Mellado

William Quayle, Sau Lan Wu

_University of Wisconsin - Madison_

_Department of Physics_

This note considers three methods used to anticipate the statistical significance of an observation of the Higgs boson at the LHC with the ATLAS detector. We compare these methods in the case of a Higgs produced via Vector Boson Fusion in the decay channel \(H\to W^{+}W^{-}\to l^{+}l^{-}/\!\!\!\!/p_{T}\) for \(115<M_{H}<130\,\mbox{GeV}\). We also address the technical challenges involved in evaluating the ATLAS combined significance as a function of the Higgs mass. The impact of systematic errors on the significance is addressed.

Introduction

The purpose of a search, of course, is to find evidence of the existence of a new particle. Statistics provides a framework in which the strength of this evidence can be quantified. In recent years there have been conferences devoted solely to advanced statistical methods and their interpretation in the context of particle physics [1, 2]. By applying the techniques developed at LEP, we have found new technical challenges not present at LEP. These technical challenges must be overcome before ATLAS begins data taking.

In this note we consider the production of Higgs via vector boson fusion (VBF). An early analysis performed at the parton level with the decay \(H\to W^{+}W^{-}\) indicated that this process is a powerful discovery mode in the range of the Higgs mass, \(M_{H}\), \(115<M_{H}<200\,\mbox{GeV}\)[3]. Our group has developed an improved cut analysis [4] and neural network analysis [5] which are the basis of this note.

At the time of the ATLAS Technical Proposal [6] and ATLAS Detector Performance and Physics Technical Design Report (TDR) [7] several approximate statistical methods were used. Rigorously speaking, the approximation used for the TDR is correct at the 10-15% level in the vicinity of the 5\(\sigma\) discovery region. This uncertainty was considered at the time of the TDR to be negligible with respect to the uncertainties on signal and background production cross-sections and selection efficiencies. To be explicit, the statistical guidelines were as follows [7, 8]:

* If the expected number of signal and background events greater than 25, then the heuristic \(s/\sqrt{b}\) was used.
* If either signal or background was below 25 events, then the Poisson significance has been used.
* The combined significance of several channels was performed by adding significances for the individual channels in quadrature.
* Whenever considered relevant, the systematic uncertaintity on the background at the level of 5-10% has been included. In that case the significance was estimated as \(s/\sqrt{b(1+\alpha^{2}b)}\), with \(\alpha\) being systematic error on the background and the denominator the result of the convolution of two Gaussians. When \(\alpha^{2}b\gg 1\), it lead to the significance estimate \(s/\alpha b\).

For individual channels, we will compare three different measures of significance in this note: 1) the heuristic \(s/\sqrt{b}\); 2) the Poisson significance, \(\sigma_{P}\), a more accurate form of the previous; and 3) the significance, \(\sigma_{L}\), obtained from using the likelihood ratio together with a discriminating variable. We will demonstrate three main points. First, the heuristic \(s/\sqrt{b}\) systematically overestimates the significance to the presence of a signal - this is most serious in analyses with small numbers of background events. Second, by restricting ourselves to a pure event counting statistical analysis, we are simply throwing out useful information and underestimating our sensitivity to signal. Third, using the statistical techniques described in this note, optimization points will be seen to be qualitatively different from those computed using the \(s/\sqrt{b}\). The effect on optimization can have a significant impact on the search strategy; this is illustrated in Section 6. For the combination of channels, we outline technical difficulties encountered by applying the techniques used at LEP to the ATLAS Standard Model Higgs combination. In Section 8 we compare the various solutions to the problems encountered. By adopting the statistical techniques presented below, more correct estimations of search sensitivity can be made.

## 2 Confidence Levels and Significance

Current estimates of analysis sensitivity are based on the statistical guidelines outlined above. The heuristic \(s/\sqrt{b}\) has its origin in the observation that for a large number of expected background events, \(b\), one obtains a distribution of observed background events which is roughly Gaussian with standard deviation \(\sqrt{b}\). In the presence of signal, one expects \(s+b\) events, which is separated from the background-only expectation by \(s/\sqrt{b}\) (in units of the standard deviations of the background-only distribution).

In more general cases, the signal and background distributions are not Gaussian, though the expression of the sensitivity in terms of Gaussian significance is intuitive. It would therefore be nice to adapt this definition to a more general distribution. To do so, first we define the background confidence level,

\[CL_{b}=\int_{N}^{\infty}\rho_{b}(n)dn \tag{1}\]

which is the probability of observing \(N\) or more events if the background-only hypothesis is true. In this equation, \(\rho_{b}\) is the probability density function (pdf) for observations given the background-only hypothesis.

This background confidence level is computed on a one-sided confidence interval, meaning that the interval is bounded by \(N\) on one side, and infinity on the other. Such a quantity can be converted into an equivalent number of Gaussian standard deviations in a straightforward way. Taking a Gaussian distribution with mean 0 and standard deviation 1, find the boundary, \(x\), of a one-sided confidence interval for which the confidence level equals the confidence level of interest. In particular, we want the value of \(x\) which satisfies

\[CL_{b}=\frac{1-\mathrm{erf}(x/\sqrt{2})}{2}. \tag{2}\]

where \(\mathrm{erf}(x)=(2/\sqrt{\pi})\int_{0}^{x}\exp(-y^{2})dy\) is a function readily available in most numerical libraries.

A significance of \(0\sigma\) corresponds to \(x=0\) and a confidence level of \(0.5\), a significance of \(1\sigma\) corresponds to \(x=1\) and a confidence level of \((1-0.6\,8)/2)=0.16\), and a significance of \(5\sigma\) corresponds1to \(x=5\) and a confidence level of \(2.85\times 10^{-7}\).

Footnote 1: This is a purely conventional conversion from confidence level to significance. Sometimes this equivalence is quoted as \(5.8\times 10^{-7}\), but careful consideration will yield the conclusion that this is the probability of being 5 or more standard deviations _away_ from the background expectation, not 5 or more standard deviations _above_ the background expectation. Equation 2 is the definition consistent with the heuristic.

It should further be noted that this type of generalization of significance measures distance not from the mean of the background distribution, but from the median. The transformation from number of events to confidence level and the transformation from confidence level to significance are quite non-linear. Thus the mean of the signal-plus-background distribution will be different if computed in terms of number of events, confidence level or significance. To avoid this ambiguity, using the median of the signal-plus-background curve when estimating the significance is preferable to the mean, as this provides a single, well-defined value.

The first main goal of this note is clarify the regions in which the heuristic \(s/\sqrt{b}\) is trustworthy and emphasize that it is not accurate at the level of 10 background events. In fact, for \(\approx 10\) background events deviations from the heuristic to the significance obtained from Poisson confidence levels are at the level of 15-50%. Table 1 demonstrates the systematic overestimate of the significance obtained from the \(s/\sqrt{b}\).

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Signal & Background & \(s/\sqrt{b}\) & Poisson Significance & ratio \\ \hline \hline
1 & 1 & 1.00 & 0.63 & 1.59 \\
1 & 5 & 0.45 & 0.29 & 1.52 \\
1 & 10 & 0.32 & 0.21 & 1.51 \\ \hline
5 & 1 & 5.00 & 3.24 & 1.54 \\
5 & 5 & 2.24 & 1.85 & 1.21 \\
5 & 10 & 1.58 & 1.38 & 1.14 \\ \hline
10 & 5 & 4.47 & 3.51 & 1.28 \\
10 & 10 & 3.16 & 2.70 & 1.17 \\
10 & 25 & 2.00 & 1.82 & 1.09 \\ \hline
25 & 25 & 5.00 & 4.34 & 1.15 \\
25 & 50 & 3.54 & 3.25 & 1.09 \\
25 & 100 & 2.50 & 2.38 & 1.05 \\ \hline
50 & 50 & 7.07 & 6.18 & 1.14 \\
50 & 100 & 5.00 & 4.62 & 1.08 \\
50 & 250 & 3.16 & 3.05 & 1.04 \\ \hline \end{tabular}
\end{table}
Table 1: Heuristic and Poisson significance calculations for several signal and background estimates, and ratio between them.

Combining Channels and the Likelihood Ratio

In principle, the simplest way to combine two channels is to add the signal expectations of the channels together, and to add the background expectations of the channels together. Except in trivial cases such as when the signal and background expectations for the channels are the same, this is not the best way to combine the channels. In fact, there is a significant risk that if this technique is used, one would be better off selecting the better channel and throwing away the weaker channel.

A more optimal way would be to add the sensitivities in quadrature. This assures that a weak channel's performance will not degrade a strong channel's performance. Unfortunately, while this may suffice when discussing sensitivity in the abstract, it offers no prescription for combining actual observations and does not have a solid statistical motivation.

An efficient test statistic that can be used to combine channels is the likelihood ratio [9]. Given an observation of \(N\) events in a particular channel, the likelihood is the Poisson probability for observing \(N\) events given the hypothesis. The likelihood ratio is simply the ratio of the likelihood for the signal-plus-background hypothesis to the background-only hypothesis,

\[Q=\frac{\mathcal{L}_{s+b}}{\mathcal{L}_{b}}=\frac{e^{-(s+b)}(s+b)^{N}/N!}{e^{- b}\,b^{N}/N!}=e^{-s}\left(1+\frac{s}{b}\right)^{N}. \tag{3}\]

For convenience, the natural logarithm of this expression,

\[q=\ln Q=-s+N\ln\left(1+\frac{s}{b}\right) \tag{4}\]

is often taken. It can immediately be seen that this expression consists of an offset \((-s)\) and a term proportional to the number of events observed. This proportionality factor can be considered to be an event weight, though in this simple example, all events are given the same weight. Given this fact, the conversion to likelihood ratio has no impact on the significance of a simple counting analysis.

To combine two channels, one simply multiplies the likelihood ratios together (or adds the log-likelihood ratios). For \(N_{ch}\) channels, this becomes

\[q=\ln Q=-\sum_{i=1}^{N_{c}\hbar}s_{i}+\sum_{i=1}^{N_{c}\hbar}N_{i}\ln\left(1+ \frac{s_{i}}{b_{i}}\right) \tag{5}\]

where \(s_{i}\), \(b_{i}\) and \(N_{i}\) are the signal expectation, background expectation and number of events observed for the \(i^{th}\) channel. This can be seen to consist of an offset which is the total signal expectation for all channels and a sum over candidates, where each candidate is given a weight dependent on its channel's purity.

As in the single channel case, the confidence level can be computed using the Poisson probabilities for observing various numbers of events. With multiple channels,however, this is much more complicated, as the probability density function (pdf) for two channels is the convolution of the single channel pdf with itself.

\[\rho_{A\,B}(q)=\int_{-\,\infty}^{\,\infty}\rho_{A}(q^{\prime})\rho_{B}(q-q^{ \prime})dq^{\prime}. \tag{6}\]

As a result, the multi-channel probability distribution is usually computed with Monte Carlo techniques. Monte Carlo techniques, however, have the drawback that it is quite time consuming to generate a sufficiently large sample when computing significances larger than a few standard deviations and the number of expected events is quite large. Fortunately, one can make use of analytic methods, which perform the convolution via fast Fourier Transform (FFT), to compute the multi-channel probability distribution quickly and accurately [10]. This will be expounded upon in the next section. It is worth noting that the confidence level is obtained from a numerical integration, and that for significance values well above \(5\sigma\) the numerical precision becomes an issue.

## 4 Discriminating Variables

This section will expand on the previous methods slightly by introducing techniques which were quite common in LEP Higgs searches [11], but have not yet been incorporated into the analyses at ATLAS. The conceptual jump to be introduced comes from the consideration of a discriminant variable, \(x\), chosen to discriminate between signal and background. In LEP Higgs searches, the discriminant variable was typically the reconstructed Higgs mass, a neural network output, or a _b_-tagging variable. From Monte Carlo, it is possible to construct estimates of the signal and background pdf's \(f_{s}(x)\) and \(f_{b}(x)\), respectively. While a histogram will suffice, kernel estimation techniques offer an unbinned and non-parametric estimate of the pdf [12].

From a statistical point of view, calculating the likelihood with a discriminating variable is the continuous limit of combining multiple channels (see Equation 5). Just as there were channels with low and high purity, there are regions in the discriminating variable with low and high purity. For a single event with \(x=x_{i}\), the log-likelihood ratio generalizes in a straight forward manner,

\[q=\ln Q=-s+\ln\left(1+\frac{sf_{s}(x_{i})}{bf_{b}(x_{i})}\right). \tag{7}\]

In this way, \(f_{s}(x)\) and \(f_{b}(x)\) are mapped into an expected distribution of \(q\). For the background-only hypothesis, \(f_{b}(x)\) provides the probability of corresponding values of \(q\) needed to define the single event pdf \(\rho_{1}\). For multiple events, the distribution of the log-likelihood ratio must be obtained from repeated convolutions of the single event distribution [10]. In the Fourier domain, denoted with a bar, the distribution of the log-likelihood for \(n\) particles is

\[\overline{\rho_{n}}=\overline{\rho_{1}}^{n} \tag{8}\]Thus the expected log-likelihood distribution for background takes the form

\[\rho_{b}(q)=\sum_{n\,=0}^{\infty}\frac{e^{-b}\,b^{n}}{n!}\rho_{n}(q) \tag{9}\]

which in the Fourier domain is simply

\[\overline{\rho_{b}(q)}=\,e^{b\,[\overline{\rho_{1}\,[q]}-1]}. \tag{10}\]

An analogous calculation is used for the signal-plus-background hypothesis.

Interestingly, when discriminant variables are used in the significance calculation, it is beneficial to loosen the cuts relative to the number-counting analysis. This behavior is analogous to the combination of a low and high purity sample yielding a higher significance than the high purity sample alone.

Figure 1: Distributions of the transverse mass for signal-plus-background (solid line) and background only (dashed line) for the channel \(H\to W^{+}W^{-}\to e^{\pm}\mu^{\mp}\not{p}_{T}\) after applying the cuts specified in [4]. Plots are shown for four different values of \(M_{H}\) and have been normalized according to cross-sections.

Choice of Discriminating Variables

When choosing a discriminant variable, one clearly seeks a quantity that separates the signal-like regions from the background-like regions. Because the final result is obtained directly from the discriminant variable distributions, it is imperative that the distribution be infra-red safe, affected little by higher-order corrections, and well understood with respect to the detector response.

For the Higgs channel \(H\to W^{+}W^{-}\to l^{\pm}l^{\mp}\not{p}_{T}\), the transverse mass is expected to be a well-understood distribution and simulations point to it as a powerful discriminating variable. The transverse mass is defined as

\[M_{T}=\sqrt{\left(E_{T}^{ll}+E_{T}^{\nu\overline{\nu}}\right)^{2}-\left( \overrightarrow{P}_{T}^{ll}+\overrightarrow{p}_{T}^{\overline{\nu}}\right)^{2}} \tag{11}\]

where

\[E_{T}^{ll}=\sqrt{\left(\overrightarrow{P}_{T}^{ll}\right)^{2}+\mathit{M}_{ll }^{2}}\hskip 28.452756pt\mbox{and}\hskip 28.452756ptE_{T}^{\nu\overline{\nu}}= \sqrt{\left(\overrightarrow{p}_{T}^{\overline{\nu}}\right)^{2}+\mathit{M}_{ll }^{2}}. \tag{12}\]

After the application of the cuts described in [4], the transverse mass still has discrimination power (see Figure 1). The impact on the significance from using the transverse mass as a discriminating variable is reported in Section 7.

In multivariate analyses, the choice of a discriminant variable may not be so straightforward. Ideally, the likelihood distribution would be constructed out of multiple discriminant variables. One must be very careful in this case, because correlations among the variables are likely to exist. One really must use an estimate of the joint probability distribution to properly treat the correlated variables. Luckily, kernel estimation techniques generalize to arbitrary dimensionality [12]. Unfortunately, it can be difficult to generate enough samples to reliably estimate a high dimensional probability distribution. For that reason, multivariate techniques such as neural networks and support vector regression, which produce a single discriminant variable, are attractive.

Figure 2 illustrates the discriminating power of a neural network output trained at a Higgs mass of 120. Clearly, the signal is concentrated near 1, while the background peaks near 0. The neural network, described in Ref. [5], can be considered as creating a single, powerful discriminating variable from seven input variables:

* the pseudo-rapidity difference between the two leptons,
* the azimuthal angle difference between the two leptons,
* the invariant mass of the two leptons,
* the pseudo-rapidity difference between the two tagging jets,
* the azimuthal angle difference between the two tagging jets,
- the invariant mass of the two tagging jets,
* the transverse mass.

In addition to the improvements from such a multivariate analysis, the impact on the significance from using neural network output as a discriminating variable is reported in Section 7.

## 6 Effect on Optimization

The optimization of an analysis with respect to a set of cuts is an important task in order to accurately assess the experiment's sensitivity. While the heuristic \(s/\sqrt{b}\) is very convenient, we have established in Section 2 that it is not very accurate for moderate numbers of events. This inaccuracy manifests itself in two ways. First, for a given set of cuts and a given number of signal, \(s\), and background, \(b\), satisfying those cuts the significance is systematically too large when calculated with the heuristic. Secondly, the heuristic and the correct Poisson calculation produce qualitatively different optimal cut values. Figure 3 illustrates this behavior.

Let us briefly examine the behavior seen in Figure 3. First we write the number of signal and background events accepted after a series of cuts as \(s=L\sigma_{s}\epsilon_{s}\) and \(b=L\sigma_{b}\epsilon_{b}\), respectively. Then the heuristic takes the form:

\[\frac{s}{\sqrt{b}}=\frac{L\sigma_{s}\epsilon_{s}}{\sqrt{L\sigma_{b}\epsilon_{b} }}=\sqrt{L}\frac{\sigma_{s}}{\sqrt{\sigma_{b}}}\frac{\epsilon_{s}}{\sqrt{ \epsilon_{b}}}. \tag{13}\]

Because the cross-sections, \(\sigma_{s}\) and \(\sigma_{b}\), and integrated luminosity, \(L\), are independent of the cut value, optimization with respect to the heuristic only depends on the efficiencies \(\epsilon_{s}\) and \(\epsilon_{b}\). Thus, the optimal cut value, with respect to the heuristic, is independent of integrated luminosity and production cross-sections. In reality, the optimal cut value _is_ dependent on the overall number of events in addition to the ratio \(\epsilon_{s}/\sqrt{\epsilon_{b}}\). The deviation of the log-likelihood ratio distribution from the Gaussian form assumed by the heuristic is responsible for the disparity between the two optimization points. Furthermore, the cuts optimized with Poisson statistics will be looser, in general; thus we rely on the Monte Carlo less heavily to identify the optimal selection region.

As was mentioned in Section 4, when one uses a discriminating variable in the likelihood ratio calculation, it is beneficial to loosen the cuts on that variable. In principle, additional phase space (independent of its purity) will only improve the significance of the analysis when computed with a discriminant variable. In practice, the

Figure 3: The the left plot shows the signal (blue solid line) and background (green dashed line) remaining as a function of the cut value on a neural network output variable for \(M_{H}=120\) GeV and \(H\to W^{+}W^{-}\to e^{\pm}\mu^{\mp}\nu\bar{\nu}\) for an integrated luminosity of \(30\) fb\({}^{-1}\). The different optimization points obtained using the heuristic \(s/\sqrt{b}\) (black solid arrow) and the significance obtained from the correct Poisson calculation (red dashed arrow) are shown in both plots. The right plot shows the heuristic \(s/\sqrt{b}\) (black solid line) and the significance obtained from the Poisson calculation (red dashed line) as a function of the cut value.

entirety of the sensitivity comes from a well defined region and the technical challenges in parametrizing the pdf's in a substantially larger region are not worth the effort. As a rule of thumb, the largest region of the discriminant variable for which the pdf can be reliably parametrized should be used.

## 7 Results

We have implemented three methods of significance calculation for a cut-based analysis using the transverse mass as a discriminating variable [4] and a neural network analysis using the neural network output as a discriminating variable [5]. The three types of significance calculations are:

* the heuristic significance,
* the significance obtained from Poisson confidence level,
* the significance obtained from a confidence level on the log-likelihood ratio distribution derived from a discriminating variable.

A comparison of these significance calculations is presented in Table 2 and Table 3. The expected number of events expected in 30 fb\({}^{-1}\) of data has been obtained for different channels and Higgs masses. The different channels are denoted by the leptons present from the \(W\)-boson decays (e.g. \(H\to W^{+}W^{-}\to e^{\pm}\mu^{\mp}\not{p}_{T}\) is denoted \(e\mu\)). \(s\) and \(b\) correspond to the number of expected signal and background events, respectively. Combined results for \(s/\sqrt{b}\) are obtained by adding components in quadrature. For \(\sigma_{P}\) and \(\sigma_{L}\), combined results are obtained from the procedure outlined in Section 3.

In both the cut analysis and the neural network analysis we see that the heuristic overestimates the significance2 around 15-20%. Furthermore, we see a marked improvement in the significance if we include discriminating variables in the likelihood ratio. The neural network analysis improves the significance with respect to the cut analysis by taking into account correlations between the input variables and intelligently producing a composite discriminating variable. The improvement as a function of \(M_{H}\) is presented in Figure 4.

Footnote 2: It is a misleading coincidence that the heuristic is often close to the significance calculated with a discriminating variable, \(\sigma_{L}\).

It is now possible to produce the familiar LEP "discovery" plots, which show \(-\,2\ln(Q)\) vs. \(M_{H}\) for background and signal simulation. The median of the background-only distribution (surrounded by 1\(\sigma\) - 5\(\sigma\) bands) can be compared to the median of the expected signal-plus-background curve. Figure 5 shows these results for the cut-based and neural network-based analyses.

[MISSING_PAGE_EMPTY:12]

Figure 4: The improvement in the combined significance for VBF \(H\to WW\) as a function of the Higgs mass, \(M_{H}\). The two curves show the improvement which results from using discriminant variables in the likelihood ratio calculation relative to pure number counting (Poisson Statistics). The black (solid) line shows the improvement in the cut analysis where the transverse mass is used as a discriminating variable. The blue (dot-dashed) line shows the improvement in the neural network based analysis where the neural network output is used as a discriminating variable. This figure illustrates that the neural network output combines the discriminating power of the transverse mass with the discriminating power of the other input variables of the network.

## 8 Case Study: ATLAS Combined Significance

Finally, we would like to address the technical challenges encountered when evaluating the ATLAS combined significance to the Standard Model Higgs as a function of \(M_{H}\) including the most recent VBF results [13] and the results presented in the Technical Design Report (TDR) [7]. First, we will give a brief review of what was done for the cover of Volume II of the TDR. Then for the techniques described in this note we will discuss the technical challenges encountered, a temporary (hybrid) solution, and propose a more internally consistent one.

For the plot found in the TDR, if the number of signal or background events were less than 25, the significance was calculated with Poisson statistics, else the heuristic was used. As discussed in Section 2, deviations between the heuristic and the Poisson significance are found to be 10 - 15% at the transition threshold of 25 events. This transition between statistical treatment has an effect on the shape of the curve as a function of \(M_{H}\) because individual channels may be treated with different methods in different mass regions.

The first change to the TDR method was to consistently use Poisson statistics throughout the calculation. Naive attempts to calculate the Poisson probability \(P(x;\mu)\) of \(x\) events with an expectation of \(\mu\) events suffers from numerical precision problems.

Figure 5: The left and right plots correspond, respectively, to the cut-based analysis and the neural network-based analyses of the \(e\mu\) channel with 30 fb\({}^{-1}\) of data. The signal-plus-background expectation (dashed line) can be seen to be separated from the background-only expectation (solid line). The \(3\sigma\) (green) and \(5\sigma\) (yellow) bands surround the median of the background-only expectation.

Thus, the first challenge is to cast the Poisson calculation in such a way that numerical precision problems are avoided. This is achieved by the identity

\[P(x;\mu)=e^{-\mu}\mu^{x}/x!\equiv e^{-\mu+x\log(\mu)-\log(x!)} \tag{14}\]

and \(\log(x!)\equiv\sum_{i}\log(i)\).

The ability to calculate Poisson significance also for large numbers of events allows the individual channels to be treated in a consistent way. However, it does not, in itself, provide a well defined prescription to combine channels with the likelihood ratio. The second change to the TDR method was to use the likelihood ratio to combine channels. Here two technical challenges arise.

The first is the amount of memory required to store the \(\rho\) distributions. The essence of the problem is that the \(\rho_{1}\) and \(\rho_{b}\) distributions have a very different mean and standard deviation (see Equations 8 and 9). The exponentiation of Equation 10 is performed in-place, which requires the distribution be both broad and finely sampled. The size of the necessary array is roughly proportional to the number of expected signal and background events.

The second challenge is related to the evaluation of the confidence level \(CL_{b}\). The \(CL_{b}\) is found by directly integrating the log-likelihood ratio distribution for the background-only hypothesis (see Equation 1). Experimentation with simple toy experiments shows that this confidence level does not go below about \(10^{-16}\) which corresponds to about \(8\sigma\). This is a combination of numerical precision limitations in the numerical integral and in the FFT. Fortunately, past the \(5\sigma\) level the precise value of the significance ceases to be particularly important. Recent work has showed that it is possible to solve these problems by implementing the FFT with an arbitrary precision library, and this will be the focus of a future communication.

As a temporary solution to these challenges, one can resort to a Hybrid method which combines as many channels as possible with the likelihood ratio and the remaining channels in quadrature. A more internally consistent solution is to approximate the \(CL_{b}\) by fitting the \(\rho_{b}\) distribution to a functional form. The first method of extrapolation studied was a simple Gaussian fit to the \(\rho_{b}\) distribution. In that case, we quote the significance as the distance from the median of the signal-plus-background distribution to the mean of the background-only distribution in units of the standard deviation of the background-only distribution. This method works fairly well, but tends to overestimate the significance because the Gaussian may have a significant portion of it's probability below the hard limit, \(q\geq-s\), imposed by Equation 4. The second method we studied was based on a Poisson fit to the \(\rho_{b}\) distribution. The Poisson distribution has the desirable properties that it will have no probability below the hard limit and that its shape is more appropriate. However, the Poisson distribution is a discrete distribution thus we must find some affine transformation between the space of the log-likelihood ratio and the space of the Poisson distribution. This is accomplished as follows: First we use the identity that for a Poisson distribution the \(P(x;\mu)\) the mean is given by \(\mu\) and the variance is given by \(\mu\). Next we assume that our distribution \(\rho_{b}(q)\) takes the form of a Poisson with \(q=\alpha x\), which forces \(\text{mean}(\rho_{b})=\alpha\mu\) and \(\text{var}(\rho_{b})=\alpha^{2}\mu\). This gives us two equations which we can use to solve for \(\mu\) and \(\alpha\). With those parameters, the median of the signal-plus-background distribution and the mean of the background-only distribution can be transformed via \(\alpha\) to produce the corresponding Poisson significance.

In Figure 6 we compare the different methods of combined significance. The green dotted line corresponds to adding the individual channel's Poisson significance in quadrature. For small numbers of events, it can be demonstrated that this method underestimates the significance. For large numbers of events, however, we expect this curve to be fairly trustworthy. The red dashed line corresponds to the likelihood ratio combination with no modifications. It is clearly seen that the numerical integration is not able to produce significance levels above about \(8\sigma\). The blue dash-dotted line corresponds to the Gaussian extrapolation technique. In the region that the unmodified likelihood ratio combination is reliable, we see that the Gaussian extrapolation tech

Figure 6: Comparison of the combined significance obtained from adding in quadrature (green dotted line) and various likelihood ratio combinations. The red dashed line corresponds to the unmodified likelihood ratio which can not produce significance values above about \(8\sigma\) (see text). To solve this problem Gaussian (blue dash-dotted line) and Poisson (black solid line) extrapolation techniques have been developed. The Gaussian extrapolation technique tends to overestimate the significance, while the Poisson extrapolation is well behaved across the entire mass range. The VBF channels and the channels discussed in [7] are used for this combination. This figure is meant to demonstrate the different methods of combination and does not include updated numbers for non-VBF analyses. No systematic errors on background normalization have been included.

nique overestimates the significance. Finally, the black solid line corresponds to the Poisson extrapolation technique (which has the nice property that for single channels, it reproduces the confidence level calculation exactly). The Poisson extrapolation technique also overestimates. However, it agrees fairly well with the quadrature method in the higher mass regions where the approximation is most valid. The Poisson extrapolation technique provides a consistent method to combine channels and offers the best approximation over the entire mass range.

The Cousins-Highland formalism for including systematic errors on the normalization of the signal and background is provided in [14] and generalized in [10, 15]. Reference [10] provides a analytic expression for the log-likelihood ratio distribution including a correlated error matrix; however, this equation was obtained with an integration over negative numbers of expected events and does not hold. Instead, a numerical integration over the positive semi-definite region has been adopted.

The estimation of systematic errors and the correlations among channels is an area of ongoing study. In [5] estimations of theoretical uncertainty due to the parton shower model and matrix element have been evaluated for the dominant \(t\overline{t}\) background in both the neural network and cut-based analyses. These uncertainties are of the level of 35%. The large theoretical uncertainties motivate background normalizations based on the data. For the VBF channels studied in [13] an experimental uncertainty in the background normalization was taken to be 10%. In a typical VBF channel, the

Figure 7: Effect of systematic error on the combined significance. Both methods use the Poisson extrapolation technique (see text). The VBF channels have a 10% systematic error on the background normalization [13] and the other channels have the same systematic errors presented in the Technical Design Report (TDR) [7].

background is of the order of \(1\) fb - for which a \(10\,\%\) systematic error is relatively small in comparison to the natural statistical fluctuations expected in \(10\) fb\({}^{-1}\) of data. Thus, the impact of systematic errors on VBF channels is fairly small. For the channels with lower purity and higher numbers of events, systematic error has more of an impact (in particular the non-VBF \(H\to WW\to l\nu l\nu\) analysis). From Figure 7 it can be seen that the presence of systematic errors does not appear to threaten discovery. However, the presence of systematics may have a noticeable impact to the integrated luminosity required to reach the discovery threshold. So far little has been done to address the correlations in these uncertainties among different channels.

## 9 Conclusion

In conclusion, we have implemented Poisson and likelihood ratio techniques in order to calculate the background confidence level, \(CL_{b}\). We have presented a method to convert this confidence level into the more familiar notion of significance in terms of Gaussian "sigma" and compared these results with the heuristic \(s/\sqrt{b}\). We find that the heuristic systematically overestimates the significance of an analysis and results in qualitatively different optimization criteria for an analysis.

In relationship to VBF \(H\to W^{+}W^{-}\to l^{+}l^{-}/\!\!\!\!\!\!/_{T}\) for \(115<M_{H}<130\) GeV, we find that the transverse mass is indeed a good discriminating variable. The discrimination power can be improved by producing a composite discriminating variable with the use of a neural network.

We have compared the combination of multiple channels by adding significance in quadrature with the likelihood ratio technique. For VBF the presence of several channels with small numbers of events results in an underestimation of the combined significance when added in quadrature. We have found that the Poisson extrapolation technique allows for pure likelihood combinations to be performed over the entire mass range.

It is our conclusion that while approximate statistical methods have their place, they show substantial deviations from more precise calculations. These qualitative differences impact our approach to the Higgs analysis and cannot be neglected. Furthermore, there are technical challenges that need to be solved and digested within ATLAS such as the combination of channels with small and large numbers of events or channels with sizable systematic uncertainties. Experience at LEP has shown that the adoption of a statistical framework is a time consuming and highly debated topic. This note is meant to add to that debate and assess the techniques used at LEP in the context of the LHC.

## References

* [1] F. James (ed.), Y. Perrin Y.(ed.) and L. Lyons (ed. ), Confidence limits. Proceedings, 1st Workshop, Geneva, Switzerland, January 17-18, 2000, CERN-2000-005.
* [2] M.R. Whalley (ed.) and L. Lyons (ed. ), Advanced statistical techniques in particle physics. Proceedings, Conference, Durham, UK, March 18-22, 2002.
* [3] D. Rainwater and D. Zeppenfeld, Phys. Rev. **D60** (1999) 113004; N. Kauer et al., \(H\to WW\) as the Discovery Mode for a Light Higgs Boson, hep-ph/0012351 (2000).
* [4] K. Cranmer, P. McNamara, B. Mellado, W. Quayle, Sau Lan Wu, Search for Higgs Bosons Decay \(H\to W^{+}W^{-}\to l^{+}l^{-}/\!\!\!\!/_{T}\) for \(115<M_{H}<130\) GeV Using Vector Boson Fusion, ATLAS internal note ATL-PHYS-2003-002 (2002).
* [5] K. Cranmer, P. McNamara, B. Mellado, Y. Pan, W. Quayle, Sau Lan Wu, Neural Network Based Search for Higgs Boson Produced via VBF with \(H\to W^{+}W^{-}\to l^{+}l^{-}/\!\!\!\!/_{T}\) for \(115<M_{H}<130\) GeV, ATLAS internal note ATL-PHYS-2003-007 (2003).
* [6] ATLAS Collaboration, ATLAS Technical Proposal, CERN-LHCC/94-43 (1994).
* [7] ATLAS Collaboration, Detector and Physics Performance Technical Design Report (Volume II), CERN-LHCC/99-15 (1999).
* [8] D. Cavalli, _et. al._, Minimal Supersymmetric Standard Model Higgs rates and backgrounds in ATLAS, ATLAS Internal Note ATL-PHYS-96-074.
* [9] A.L. Read, Modified frequentist analysis of search results (The CL(s) method), "Workshop on Confidence Limits", Eds. F. James, L. Lyons and Y. Perrin, CERN 2000-005 (2000), p. 81".
* [10] H. Hu, J. Nielsen, Analytic Confidence Level Calculations Using the Likelihood Ratio and Fourier Transform, "Workshop on Confidence Limits", Eds. F. James, L. Lyons and Y. Perrin, CERN 2000-005 (2000), p. 109.
* [11] LEP Higgs Working Group, Search for the Standard Model Higgs Boson at LEP, LHWG Note/2002-01 (2002).
* [12] K. Cranmer, Comput. Phys. Commun. **136** (2001) 198-207.
* [13] S. Asai et al., Prospects for the search of a Standard Model Higgs Boson in ATLAS using Vector Boson Fusion, ATLAS Scientific Note SN-ATLAS-2003-024.
* [14] R.D. Cousins and V.L. Highland, Nucl. Instrum. Meth. **A320** (1992) 331-335.
* [15] T. Junk, Nucl. Instrum. Meth. **A434** (1999) 435-443.