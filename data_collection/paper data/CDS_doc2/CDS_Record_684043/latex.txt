Evaluation of Commercial SCI Components and Low-level SCI Software for the ATLAS Second Level Trigger

A. Belias

A. Bogaerts

D. Boterill

F. Giacomini

R. Hauser

R. Middleton

F. Wickens

P. Werner

RAL, Chilton, Didcot, UK CERN, Geneva, Switzerland

RAL, Chilton, Didcot, UK CERN, Geneva, Switzerland

Michigan State University, East Lansing, USA RAL, Chilton, Didcot, UK CERN, Geneva, Switzerland

###### Abstract

This note describes the work that has been carried out over a period of two years on the evaluation of hardware and software components for the Scalable Coherent Interface (SCI) [1]. These components include two generations of link controllers, several PCI-SCI interfaces and switches, a number of PCs and workstations, low-level Application Programming Interfaces and simple message passing libraries.

Scalable Coherent Interface, SCI, SISCI API, Switch

## 1 Introduction

We report on the evaluation of the performance of commercial components and the low-level SCI software. This work has been carried out over a period of two years and covers:

* two generations of link controllers (LC1 and LC2 running at 200 MB/s and 400 MB/s respectively) from Dolphin Interconnect Solutions [2];
* PCI-SCI interfaces and switches based on these technologies, from Dolphin;
* a range of workstations and PCs from various manufacturers with a large variety of PCI interface chips;
* a low-level API;
* a simple message passing software;
* simple communications benchmarks.

Measurements reported in Section 2 were carried out with first generation workstations equipped with PCI and LC1-based PCI-SCI interfaces from Dolphin. Elementary performance measurements show very acceptable results, bandwidth up to \(\sim\)50 MB/s and very little overhead introduced by the message passing layer. However, more application-oriented software (the communications benchmark suite) showed serious degradation of the overall system performance. This was due to an uncontrollable amount of retransmission traffic generated by the LC1 when the destination is overloaded. This has later been overcome with a new generation of hardware from Dolphin.

Section 3 contains a large set of detailed measurements carried out on LC2-based PCI-SCI interfaces D310 and D321 and switches from Dolphin. As the measurements show, there are many parameters that influence the performance drastically. Some of these are different settings of operational modes of the host's PCI-memory bridge chipset, some are settings on Dolphin's PCI-SCI interface card, and others are related to the way the low-level API transfers data (4 or 8 byte integers, flushing of on-chip buffers). The overall quality of the PCI hostbridge varies and some require elaborate software checking and resetting to recover from lock-ups and lost data. The fairly recent BX chipset offers an acceptable level of both performance and reliability; Dolphin's latest D321 PCI-SCI interface is more intelligent and reduces drastically the choice of operational modes; the low-level API defined within the SISCI project [3] will now choose the best data transfer size and flush when required. We use exclusively BX chipsets and have changed motherboards when necessary; all PCI-SCI interfaces have been graciously changed to D321 by Dolphin; the software techniques for coping with spurious hardware glitches learned with older PCI chipsets have been very useful for the operation of a large Demonstrator with the introduction of the switch.

Additional measurements on a 16-port switch are presented in Section 4, where the aim is to characterise switch in presence of a high traffic load.

## 2 Communication benchmarks

A suite of communications benchmarks [4], shown in Figure 1, have been defined to study traffic patterns which are closer to real applications than elementary bandwidth and latency measurements:

\begin{tabular}{l l} Ping-Pong & one node sends data to another node and waits for the data to be \\  & returned. Half the round-trip time is used to calculate the latency; \\ Out-farming (Broadcast) & one node sends data to several receivers. SCI does not support \\  & hardware broadcast which is thus simulated in software; \\ Funnel-in & several data sources send data to a single receiver; \\ All-to-All & all nodes send data to each other simultaneously. In an ideal \\  & uncongested system, increasing the number of nodes in the system \\  & increases the overall system throughput. \\ \end{tabular}

The benchmarks were designed to test scenarios which are typical for the ATLAS

Demonstrator but they turned out to be fairly general. These tests put stress on the system as they cause congestion in various parts. For instance, Out-farming tests the output capacity of a data source, Funnel-in the input bandwidth of a destination and All-to-All is a severe test for the entire network.

### Hardware setup

The benchmarks were executed on a small system of single board computers equipped with DEC Alpha processors, running at 166 MHz. Each node was equipped with 32 MB of memory, Ethernet (for control) and a PCI-SCI adapter from Dolphin (based on their link controller LC1 running at a link speed of 200 MB/s). Four nodes were arranged in a simple SCI ring. It is possible to operate the PCI-SCI adapter in modes with 1, 2, 4 or 8 requests outstanding. Unless otherwise stated, the mode with up to 4 outstanding requests was used as this gave optimum performance and minimised degradations owing to internal host CPU and I/O bottlenecks.

### Software

The software consists of three layers: low-level API, message passing and application.

The raw throughput achievable between two nodes on the ring using only the low-level software is approximately 45 MB/s (for 64 KB messages) and is limited only by host node CPU and I/O bus effects. The latency for 64 byte messages is about 4 us.

The message passing library implements a simple synchronisation protocol between source and destination:

* the sender writes the data at a specified remote address on the receiver;
* the sender writes a control word, representing the size of the message, at an agreed location on the receiver;
* the receiver sends back to the sender an acknowledgement, representing the next address to use for sending data.

The message passing layer introduces a fixed overhead of a few microseconds, which becomes negligible for large data transfers.

The benchmarks were constructed on top of the message passing library.

Figure 1: Communication benchmarks.

### Results

The benchmarks have been executed for varying message sizes and numbers of nodes and used the message-passing library mentioned above.

Figure 2 shows the overall system throughput as a function of message size for Ping-Pong, All-to-All, Funnel-in and Broadcast.

For Ping-Pong data is shown for one and two pairs. The overall system throughput is clearly doubled by the addition of a second pair to the ring.

The Broadcast plot shows little variation as more data receivers are added to the system. The small visible variations are due to minor differences in processor motherboard versions.

In the case of Funnel-in, overall system throughput increases as nodes are added to the system, though it is clear that the addition of the third sender causes significant reductions in throughput for larger message sizes. This is caused by the receiving node's inability to

Figure 2: Communication benchmark results.

keep up with three senders. In particular, the host PCI bus appears to be a bottleneck and incoming SCI traffic is 'busied', with a consequential immediate retry on SCI [5].

In the All-to-All test, the increase from 2 to 3 nodes appears to give almost 3 times the performance boost, as expected, at least for smaller message sizes. However, above 1 KB in the 3-node system, throughput degrades as the receiving node is no longer able to cope.

In the 4-node system each node has to process data from three sources, in addition to sending to 3 destinations and is likely to be CPU limited. Thus there is little performance enhancement.

Overall the tests indicate reasonable performance of the individual components, both hardware and software, but serious degradation of system performance when destinations are overloaded. This effect has also been shown in previous modelling work and has been remedied in the next generation of LC2-based PCI-SCI interfaces and switches.

The 4-node, single-ring system used was too small to test scaling properties.

## 3 Evaluation of components using the SISCI API

In this section we present the results of the measurements we have made in order to characterise more recent hardware and to understand under which conditions we could get the best performance out of it.

Our tests consist exclusively of data transfers based on Programmed Input/Output (PIO) whereby it is the CPU that actively transfers data from local memory to the network interface. Several hardware components may affect the efficiency of the communication: the CPU, the host bus, the I/O bus (in our case PCI), the SCI card, the SCI network and all the interfaces between them. In particular we have used two types of CPUs (Pentium II and Pentium III), two different chipsets (LX and BX) and two kinds of SCI interfaces (named D310 and D321). The hardware components are described in more detail in Section 3.1.

Section 3.2 gives an overview of the software infrastructure we have used: operating systems, compilers and test programs, together with a list of system parameters we have studied.

The real evaluation of the hardware components at our disposal is finally presented in detail in Sections 3.3 and 3.4. Since not all the combinations of hardware components made sense or were available at a given time we have collected the measurements in two main sections: Section 3.3 contains the results obtained using machines with an LX chipset and a D310 card whereas Section 3.4 presents the results achieved for machines with a BX chipset and a D321 card. In this latter case we have also been able to run with different CPUs with different clock speeds.

### Hardware

#### 3.1.1 Processor

We have used two kinds of CPUs: Pentium II running at 300 MHz and Pentium III running at 450 MHz. While the differences between PII and PIII do not influence the system performance, at least without software that exploits them, the difference in speed may have some importance if the CPU happens to be a too slow data source.

#### 3.1.2 Chipset

The chipset controls the flow of data between the CPU, the main memory and the system bus and is therefore fundamental for having efficient data transfers between the CPU and the I/O system. We have run tests with two types of chipsets: 440LX and 440BX, the latter being a more recent product.

#### 3.1.3 SCI interface

We evaluated two models of SCI cards, code-named D310 and D321. An SCI card consists of two components: a link controller, connected to the SCI links, and a PCI-SCI bridge (PSB), connected to the host I/O bus. Both D310 and D321 use the same version of the link controller (LC2, with a link speed of 400 MB/s) but D321 uses an improved version of the PSB which can interface to a 64 bit PCI bus and allows a more efficient data streaming to a remote node.

#### 3.1.4 Switch

SCI nodes can be connected together using either rings or switches. The introduction of a switch adds some latency to a data transfer. This has been measured using a 16-port switch built up from six D515 modules.

### Software

#### 3.2.1 Operating system

There are several Operating Systems one may want to run on a PC. The most straightforward choice is to have a dual-boot system with Windows NT and Linux. Since Dolphin provides the low-level software (i.e. drivers and a S3CI API implementation) for both of them we could easily switch between the two. Since SCI transfers using transparent operations directly access the hardware, the OS impact on the performance of data transfers using SCI is considered negligible.

#### 3.2.2 Compiler

For the compilation and linking of our software we have used Visual C++ when operating under Windows NT and GCC when operating under Linux. Provided that the compiler generates efficient code for data transfers, the impact of the compiler on the performance is considered negligible.

#### 3.2.3 Test programs

We are interested in measuring:

* maximum bandwidth between two nodes
* maximum output from a node
* maximum input into a node
* minimum latency between two nodes
* maximum output from a single switch port
* maximum input into a single switch port
* maximum throughput of the switch
* scalability of the switch

All the tests are based on the remote shared memory paradigm, whereby a data transfer essentially consists in a memory copy, driven by the CPU, between local and remote memory, possibly with some error-checking code to guarantee the correct delivery of the data.

The maximum bandwidth between two nodes is measured transferring data from local to remote memory as fast as possible and dividing the total amount of transferred data by the duration of the test.

The maximum bandwidth between two nodes may be limited either by the sender or by the receiver. To know which one is the bottleneck we have measured the maximum output from a node (running multiple senders on the same machine and one receiver per remote node) and the maximum input into a node (running multiple receivers on the same machine and one sender per remote node). These tests are also useful to see how fairly the available bandwidth is shared among all the senders.

Latency is usually defined as the ping-pong time divided by two. A ping-pong consists in a node sending a message to another one and waiting for the reply to come back. In some of our tests, presented in Section 3.3, "LX chipset, D310 PCI-SCI card", we have used a different method, consisting in a node transferring data and then doing a barrier operation to guarantee that the data have arrived on the other side. Practically it has been observed that a latency measured in this way is approximately 1 \(\upmu\)s longer that the ping-pong time divided by two.

Since an SCI system may consist of ringlets connected to switch ports it is interesting to measure the input and output load of a port. This is done by having all senders (receivers) on a ring connected to that port and all the receivers (senders) on other switch ports.

The maximum load through the switch is measured running concurrently multiple pairs of nodes, one acting as a data source and the other as a sink, making sure that the traffic flows through the whole switch and is not confined within a single module.

The switch scalability is defined as the way the load on the switch varies as the number of send-receive pairs running concurrently increases.

#### 3.2.4 Parameters

On a system there are several parameters which can affect the efficiency of SCI data transfers:

* number of stream buffers used on the PCI-SCI card;
* write-combining on the CPU, whereby the CPU collects some data before flushing it out;
* write-gathering on the SCI card, whereby the PSB tries to build and send out full 64-byte packets;
* 4-byte or 8-byte transfer instructions in the program;
* message size.

When possible we have tried to scan over all the possible combinations of reasonable values of the above parameters in order to have a better understanding of their influence.

### LX chipset, D310 PCI-SCI card

For these tests we used dual-Pentium II machines running at 300 MHz. The aim was to measure the maximum bandwidth and the minimum latency between two nodes achievable with a certain parameter configuration, as described in Section 3.2.4.

In the following tables the header rows indicate the number of stream buffers (denoted by "# streams") and the type of store instruction (4 or 8 bytes, denoted by "PIO") used to measure the point for a specific message size, as indicated by the corresponding entry in the first column.

Tables 1 to 4 collect the results for bandwidth tests between two nodes connected back to back.

Tables 5 to 8 collect the results for latency tests between two nodes connected back to back. In these tests latency is defined as the time it takes to transfer the data plus a barrier operation to guarantee its delivery.

The measurements allow to draw some conclusions and provide valuable clues about the most efficient combination of parameters:

* enabling write-combining is always effective;
* if write-combining is enabled enabling write-gathering has a negative effect on short messages;
* if write-combining is enabled 4-byte and 8-byte stores give the same results;
* if write-combining is disabled enabling write-gathering becomes important; in this case 8-byte store operations should be used;
* the best results are always obtained maximizing the number of stream buffers in use, i.e. the number of outstanding SCI request packets. However we observe that the main limitation to the performance is due to the internal PCI bus.

Unfortunately the LX chipset does not allow a safe use of write-combining, that must therefore be disabled.

### BX chipset, D321 PCI-SCI card

Using the experience gained with the LX chipset and considering the characteristics of the new PCI-SCI card we could run tests on the BX chipset without scanning all the possible parameter configurations. In particular in these tests:

* write-combining is always enabled;
* the number of stream buffers is out of the control of the user and it is always set to 16;
* write-gathering is left under the control of the driver.

The variable parameters in these tests are then:

* CPU type (Pentium II at 300 MHz and 400 MHz, Pentium III at 450 MHz);
* type of store operation (4-byte or 8-byte);
* message size (as before, from 8 bytes to 4 KB).

\begin{table}
\begin{tabular}{|r|c|c|c|c|c|c|c|} \hline
**\# streams** & \multicolumn{2}{c|}{**1**} & \multicolumn{2}{c|}{**2**} & \multicolumn{2}{c|}{**4**} & \multicolumn{2}{c|}{**8**} \\ \hline
**PIO** & **4** & **8** & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline \hline
**8** & 2.5 & 3.0 & 5.0 & 5.1 & 9.3 & 9.4 & 13 & 13 \\
**16** & 4.8 & 4.8 & 9.5 & 9.5 & 17 & 18 & 17 & 26 \\
**32** & 5.1 & 5.1 & 5.0 & 10 & 5.0 & 19 & 5.0 & 26 \\
**64** & 4.7 & 14 & 7.6 & 27 & 7.9 & 50 & 7.9 & 63 \\
**128** & 4.7 & 14 & 7.6 & 26 & 7.8 & 49 & 7.8 & 63 \\
**256** & 4.7 & 14 & 7.6 & 27 & 7.9 & 50 & 7.8 & 63 \\
**512** & 4.7 & 14 & 7.6 & 27 & 7.8 & 50 & 7.8 & 63 \\
**1024** & 4.7 & 14 & 7.6 & 27 & 7.8 & 50 & 7.8 & 63 \\
**2048** & 4.7 & 14 & 7.6 & 27 & 7.8 & 51 & 7.8 & 63 \\
**4096** & 4.7 & 14 & 7.6 & 27 & 7.8 & 50 & 7.8 & 63 \\ \hline \end{tabular}
\end{table}
Table 1: Node-to-node bandwidth with an LX chipset and a D310 card. Write-gathering and write-combining are both disabled (figures are expressed in MB/s)

\begin{table}
\begin{tabular}{|r|c|c|c|c|c|c|c|c|} \hline
**\# streams** & \multicolumn{2}{c|}{**1**} & \multicolumn{2}{c|}{**2**} & \multicolumn{2}{c|}{**4**} & \multicolumn{2}{c|}{**8**} \\ \hline
**PIO** & **4** & **8** & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline \hline
**8** & 2.5 & 2.9 & 2.5 & 3.0 & 2.5 & 3.2 & 2.5 & 3.1 \\
**16** & 4.8 & 4.8 & 4.8 & 4.8 & 4.8 & 4.8 & 4.8 \\
**32** & 5.1 & 5.1 & 5.1 & 5.1 & 5.1 & 5.1 & 7.9 & 7.9 \\
**64** & 13 & 14 & 25 & 27 & 32 & 50 & 32 & 63 \\
**128** & 12 & 14 & 25 & 27 & 32 & 50 & 32 & 63 \\
**256** & 12 & 14 & 24 & 27 & 32 & 50 & 32 & 63 \\
**512** & 12 & 14 & 24 & 27 & 32 & 50 & 32 & 63 \\
**1024** & 12 & 14 & 24 & 27 & 32 & 51 & 32 & 63 \\
**2048** & 12 & 14 & 24 & 27 & 32 & 51 & 32 & 63 \\
**4096** & 12 & 14 & 24 & 27 & 32 & 50 & 21 & 63 \\ \hline \end{tabular}
\end{table}
Table 2: Node-to-node bandwidth with LX chipset and D310 card. Write-gathering is enabled and write-combining is disabled (figures are expressed in MB/s)Tables 9 and 10 collect the results for bandwidth and latency tests between two nodes connected back to back.

What emerges from these tests is:

* for message sizes greater than 256 bytes both bandwidth and latency are independent of the CPU speed;
* for shorter messages there is some dependency on the CPU speed. However this dependency disappears already at 400 MHz;
* using 4-byte or 8-byte store instructions makes sense only for short messages and on a slow machine; otherwise the effect of write-combining is certainly more important.

With the same node configuration we have also measured the maximum output from a node and the maximum input into a node.

\begin{table}
\begin{tabular}{|r|c|c|c|c|c|c|c|} \hline
**\# streams** & \multicolumn{2}{c|}{**1**} & \multicolumn{2}{c|}{**2**} & \multicolumn{2}{c|}{**4**} & \multicolumn{2}{c|}{**8**} \\ \hline PIO & **4** & **8** & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline \hline
**8** & 5.0 & 5.0 & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 & 2.5 \\
**16** & 4.8 & 4.8 & 4.8 & 4.8 & 4.8 & 4.8 & 4.8 \\
**32** & 5.1 & 5.1 & 5.1 & 5.1 & 5.1 & 5.1 & 8.0 & 8.0 \\
**64** & 14 & 14 & 27 & 27 & 53 & 53 & 81 & 81 \\
**128** & 14 & 14 & 27 & 27 & 53 & 53 & 81 & 81 \\
**256** & 14 & 14 & 27 & 27 & 53 & 53 & 81 & 81 \\
**512** & 14 & 14 & 27 & 27 & 53 & 53 & 81 & 81 \\
**1024** & 14 & 14 & 27 & 27 & 53 & 53 & 81 & 81 \\
**2048** & 14 & 14 & 27 & 27 & 53 & 53 & 81 & 81 \\
**4096** & 14 & 14 & 27 & 27 & 53 & 53 & 81 & 81 \\ \hline \end{tabular}
\end{table}
Table 4: Node-to-node bandwidth with an LX chipset and a D310 card. Write-gathering and write-combining are both enabled (figures are expressed in MB/s).

\begin{table}
\begin{tabular}{|r|c|c|c|c|c|c|c|c|} \hline
**\# streams** & \multicolumn{2}{c|}{**1**} & \multicolumn{2}{c|}{**2**} & \multicolumn{2}{c|}{**4**} & \multicolumn{2}{c|}{**8**} \\ \hline PIO & **4** & **8** & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline \hline
**8** & 2.5 & 2.5 & 2.5 & 2.5 & 9.5 & 9.5 & 13 & 13 \\
**16** & 4.8 & 4.8 & 9.5 & 9.5 & 18 & 18 & 26 & 26 \\
**32** & 5.1 & 5.1 & 10 & 10 & 19 & 19 & 26 & 26 \\
**64** & 14 & 14 & 27 & 27 & 52 & 52 & 81 & 80 \\
**128** & 14 & 14 & 27 & 27 & 52 & 52 & 80 & 80 \\
**256** & 14 & 14 & 27 & 27 & 53 & 53 & 80 & 80 \\
**512** & 14 & 14 & 27 & 27 & 53 & 52 & 81 & 81 \\
**1024** & 14 & 14 & 27 & 27 & 52 & 52 & 81 & 80 \\
**2048** & 14 & 14 & 27 & 27 & 53 & 53 & 81 & 80 \\
**4096** & 14 & 14 & 27 & 27 & 53 & 53 & 81 & 81 \\ \hline \end{tabular}
\end{table}
Table 3: Node-to-node bandwidth with an LX chipset and a D310 card. Write-gathering is disabled and write-combining is enabled (figures are expressed in MB/s)To measure the maximum output from a node we have run up to four instances of a send-receive program with all the sources on one node and different destination nodes.

Similarly, to measure the maximum input into a node we have run up to six instances of the same send-receive program with the senders on different nodes and all the receivers on one node.

Table 11 shows that the maximum output from a node is 86 MB/s and, more important, that the bandwidth is fairly distributed among all the senders. The output from a single sender is less than what is reported in Table 9 (77 MB/s instead of 81 MB/s) because of the penalty due to the error check that here is done after each data transfer.

Table 12 shows how the input into a node varies with the number of senders, reaching a maximum of 82 MB/s. Unlike the previous case the bandwidth is not equally shared by all senders, in particular one of them is almost starving. However there is no evidence of a drop

\begin{table}
\begin{tabular}{|r|c|c|c|c|c|c|c|c|} \hline \multicolumn{1}{|c|}{**\# streams**} & \multicolumn{2}{c|}{**1**} & \multicolumn{2}{c|}{**2**} & \multicolumn{2}{c|}{**4**} & \multicolumn{2}{c|}{**8**} \\ \hline \multicolumn{1}{|c|}{PIO} & **4** & **8** & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline \hline
**8** & 4.5 & 4.4 & 4.5 & 4.4 & 4.6 & 4.3 & 4.5 & 4.4 \\
**16** & 4.6 & 4.4 & 4.7 & 4.4 & 4.7 & 4.4 & 4.6 & 4.4 \\
**32** & 7.8 & 7.3 & 7.9 & 7.3 & 7.9 & 7.2 & 7.9 & 7.3 \\
**64** & 6.9 & 5.8 & 6.9 & 5.8 & 6.9 & 5.7 & 6.8 & 5.8 \\
**128** & 12 & 10 & 8.8 & 6.8 & 8.8 & 6.7 & 8.8 & 6.7 \\
**256** & 21 & 19 & 14 & 11 & 13 & 8.8 & 13 & 8.8 \\
**512** & 41 & 37 & 24 & 21 & 20 & 14 & 20 & 13 \\
**1024** & 80 & 72 & 44 & 39 & 36 & 24 & 36 & 20 \\
**2048** & 158 & 144 & 80 & 75 & 67 & 44 & 67 & 36 \\
**4096** & 314 & 256 & 166 & 148 & 128 & 83 & 128 & 67 \\ \hline \end{tabular}
\end{table}
Table 6: Node-to-node latency with an LX chipset and a D310 card. Write-gathering is enabled and write-combining is disabled (figures are expressed in \(\mu\)s).

\begin{table}
\begin{tabular}{|r|c|c|c|c|c|c|c|c|} \hline \multicolumn{1}{|c|}{**\# streams**} & \multicolumn{2}{c|}{**1**} & \multicolumn{2}{c|}{**2**} & \multicolumn{2}{c|}{**4**} & \multicolumn{2}{c|}{**8**} \\ \hline \multicolumn{1}{|c|}{PIO} & **4** & **8** & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline \hline
**8** & 6.7 & 4.1 & 6.7 & 4.2 & 6.7 & 4.2 & 6.7 & 4.2 \\
**16** & 5.8 & 4.2 & 5.8 & 4.3 & 5.9 & 4.3 & 5.8 & 5.3 \\
**32** & 10 & 7.2 & 9.9 & 7.2 & 10 & 7.2 & 10 & 7.2 \\
**64** & 16 & 5.8 & 16 & 5.8 & 16 & 5.8 & 16 & 5.7 \\
**128** & 28 & 10 & 25 & 6.9 & 25 & 6.8 & 25 & 6.8 \\
**256** & 52 & 19 & 45 & 11 & 44 & 8.8 & 45 & 8.8 \\
**512** & 100 & 37 & 76 & 21 & 74 & 14 & 74 & 13 \\
**1024** & 80 & 72 & 44 & 39 & 36 & 24 & 36 & 20 \\
**4096** & 314 & 256 & 166 & 148 & 128 & 83 & 128 & 67 \\ \hline \end{tabular}
\end{table}
Table 5: Node-to-node latency with an LX chipset and a D310 card. Write-gathering and write-combining are both disabled (figures are expressed in \(\mu\)s).

in the performance due to SCI retry traffic as it used to be with the previous generation of link controllers.

### Switch

As already mentioned in Section 3.2.3 we have characterised the 16-port D515 modular switch from Dolphin with the following parameters:

* minimum latency between two nodes connected to different switch ports;
* maximum output from a single switch port;
* maximum input into a single switch port.

The results concerning switch throughput and switch scalability are presented in the next section.

\begin{table}
\begin{tabular}{|r|r|r|r|r|r|r|r|r|} \hline
**\# streams** & \multicolumn{2}{c|}{**1**} & \multicolumn{2}{c|}{**2**} & \multicolumn{2}{c|}{**4**} & \multicolumn{2}{c|}{**8**} \\ \hline
**PIO** & **4** & **8** & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline \hline
**8** & 4.2 & 4.2 & 4.2 & 4.2 & 4.2 & 4.2 & 4.2 & 4.2 \\
**16** & 4.3 & 4.3 & 4.3 & 4.3 & 4.3 & 4.3 & 4.3 & 4.3 \\
**32** & 7.2 & 7.2 & 7.2 & 7.2 & 7.2 & 7.2 & 7.2 & 7.1 \\
**64** & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 \\
**128** & 9.8 & 9.8 & 6.4 & 6.4 & 6.4 & 6.4 & 6.4 & 6.4 \\
**256** & 19 & 19 & 11 & 11 & 7.9 & 7.8 & 7.9 & 7.8 \\
**512** & 36 & 36 & 20 & 19 & 12 & 12 & 11 & 11 \\
**1024** & 71 & 71 & 37 & 37 & 21 & 21 & 17 & 17 \\
**2048** & 141 & 141 & 73 & 73 & 40 & 40 & 29 & 29 \\
**4096** & 281 & 281 & 145 & 145 & 77 & 77 & 53 & 53 \\ \hline \end{tabular}
\end{table}
Table 7: Node-to-node latency with an LX chipset and a D310 card. Write-gathering is disabled and write-combining is enabled (figures are expressed in \(\mu\)s).

\begin{table}
\begin{tabular}{|r|r|r|r|r|r|r|r|} \hline
**\# streams** & \multicolumn{2}{c|}{**1**} & \multicolumn{2}{c|}{**2**} & \multicolumn{2}{c|}{**4**} & \multicolumn{2}{c|}{**8**} \\ \hline
**PIO** & **4** & **8** & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline \hline
**8** & 4.4 & 4.4 & 4.4 & 4.4 & 4.4 & 4.4 & 4.4 \\
**16** & 4.5 & 4.5 & 4.5 & 4.5 & 4.5 & 4.5 & 4.5 \\
**32** & 7.4 & 7.3 & 7.4 & 7.3 & 7.4 & 7.3 & 7.4 & 7.3 \\
**64** & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 \\
**128** & 9.9 & 9.8 & 6.4 & 6.4 & 6.4 & 6.4 & 6.4 & 6.3 \\
**256** & 18 & 18 & 11 & 11 & 7.9 & 7.8 & 7.8 & 7.8 \\
**512** & 36 & 36 & 19 & 19 & 12 & 12 & 11 & 11 \\
**1024** & 71 & 71 & 37 & 37 & 21 & 21 & 17 & 17 \\
**2048** & 140 & 140 & 73 & 73 & 40 & 40 & 28 & 29 \\
**4096** & 280 & 280 & 144 & 144 & 77 & 77 & 53 & 53 \\ \hline \end{tabular}
\end{table}
Table 8: Node-to-node latency with an LX chipset and a D310 card. Write-gathering and write-combining are both enabled (figures are expressed in \(\mu\)s).

The corresponding tests have been executed using nodes with the BX chipset and a D321 card.

\begin{table}
\begin{tabular}{|r|c|c|c|c|c|c|} \hline
**CPU speed** & \multicolumn{2}{c|}{**300**} & \multicolumn{2}{c|}{**400**} & \multicolumn{2}{c|}{**450**} \\ \hline PIO & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline \hline
**8** & 2.7 & 2.7 & 2.4 & 2.3 & 2.3 & 2.3 \\
**16** & 2.8 & 2.8 & 2.4 & 2.4 & 2.4 & 2.4 \\
**32** & 4.5 & 4.5 & 4.3 & 4.3 & 4.3 & 4.3 \\
**64** & 3.8 & 3.8 & 3.4 & 3.4 & 3.4 & 3.4 \\
**128** & 6.8 & 6.7 & 6.2 & 6.2 & 6.3 & 6.2 \\
**256** & 8.2 & 8.1 & 7.5 & 7.5 & 7.5 & 7.4 \\
**512** & 12 & 11 & 10 & 10 & 10 & 10 \\
**1024** & 18 & 18 & 16 & 16 & 16 & 16 \\
**2048** & 29 & 30 & 29 & 29 & 28 & 28 \\
**4096** & 52 & 52 & 52 & 52 & 52 & 52 \\ \hline \end{tabular}
\end{table}
Table 10: Node-to-node latency with a BX chipset and a D321 card.

\begin{table}
\begin{tabular}{|r|c|c|c|c|c|c|} \hline
**CPU speed** & \multicolumn{2}{c|}{**300**} & \multicolumn{2}{c|}{**400**} & \multicolumn{2}{c|}{**450**} \\ \hline PIO & **4** & **8** & **4** & **8** & **4** & **8** \\ \hline
**16** & 6.9 & 6.9 & 9.4 & 8.5 & 8.6 & 8.5 \\
**32** & 12 & 12 & 12 & 12 & 12 & 12 \\
**64** & 66 & 70 & 81 & 81 & 81 & 81 \\
**128** & 75 & 81 & 81 & 81 & 81 & 81 \\
**256** & 81 & 81 & 81 & 81 & 81 & 81 \\
**512** & 81 & 81 & 81 & 81 & 81 & 81 \\
**1024** & 81 & 81 & 81 & 81 & 81 & 81 \\
**2048** & 81 & 81 & 81 & 81 & 81 & 81 \\
**4096** & 81 & 81 & 81 & 81 & 81 & 81 \\ \hline \end{tabular}
\end{table}
Table 9: Node-to-node bandwidth with a BX chipset and a D321 card. (figures are expressed in MB/s.)Table 13 shows the latency for node-to-node communication depending on the number of switch modules SCI packets have to traverse to reach the remote node. If compared with the results obtained for a direct connection between two nodes going through the switch introduces a delay that is of the order of 1 us per module.

For the characterisation of a switch port we have used the system configuration illustrated in Figure 3. The switch port studied is the one connected to a 4-node ringlet (with nodes A, B, C and D). Other 5 nodes are involved in the test: 4 on the same module, of which 2 on a port by themselves (E and F) and 2 on a ringlet (G and H), and 1 on another module (I). The basic idea to measure the input load that a switch port can carry is to start a number of send-receive pairs with up to four nodes chosen among A to D acting as receivers and an equal number of nodes among E to I acting as senders. To measure the output load the test is run exchanging senders and receivers.

Tables 14 and 15 show how the input load on a switch port varies with the number of senders (they are started staggered by a few seconds). The two tables differ in that in the former all senders are on the same module, hence two senders are on the same ringlet (G and H), whereas in the latter one sender is allocated on another switch module. Not only the overall performance is different but also the contribution from the different senders varies in the two cases.

\begin{table}
\begin{tabular}{|r|c|c|c|c|} \hline
**type of connec-** & **across** & **across** & & \\
**tion** & **1 module** & **3 modules** & **direct** \\ \hline \hline
**8** & 2.7 & 3.4 & 2.3 \\
**16** & 2.8 & 3.5 & 2.4 \\
**32** & 5.3 & 7.1 & 4.3 \\
**64** & 4.0 & 5.0 & 3.4 \\
**128** & 7.7 & 10 & 6.2 \\
**256** & 8.9 & 11 & 7.4 \\
**512** & 12 & 14 & 10 \\
**1024** & 17 & 20 & 16 \\
**2048** & 29 & 31 & 28 \\
**4096** & 52 & 56 & 52 \\ \hline \end{tabular}
\end{table}
Table 13: Node-to-node latency through the switch, depending on the message size (figures are expressed in \(\mu\)s).

Table 16 illustrates the maximum output load of a switch port and the single contributions of nodes A to D when sending data to nodes sitting on other switch ports. The two columns differ in that the fourth receiver is either on the same module as the senders, and hence in a ringlet with another receiver, or on a different module. What changes between the two cases is not the overall performance but rather how the bandwidth is allocated to a sender according to where the corresponding receiver is.

## 4 Throughput measurements on a 16-port SCI switch

This section contains additional measurements on a 16-port SCI switch comprising six D515 expandable switch modules from Dolphin, which forms a central part of the SCI-based

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|} \hline
**input from E** & 76 & 75 & 55 & 30 & - & - \\
**input from F** & - & 75 & 55 & 30 & 42 & - \\
**input from G** & - & - & 55 & 30 & 40 & 76 \\
**input from I** & - & - & - & 52 & 67 & 72 \\ \hline \hline
**Total input** & **76** & **150** & **165** & **142** & **149** & **148** \\ \hline \end{tabular}
\end{table}
Table 15: Maximum input load on a switch port with one sending node on a different switch module (figures are expressed in MB/s).

Figure 3: System configuration used to characterise a switch port.

ATLAS Level-2 Trigger Testbed. The measurements presented here aim at characterising the switch in presence of a high traffic load and, unlike the basic measurements shown in the previous section, have been executed using a message passing protocol to guarantee reliable communication.

### Technical description

The 16-port SCI switch is obtained by cascading six D515 expandable switch modules. Each module contains 4 user ports running at a nominal speed of 400 MB/s and 4 expansion ports running at 500 MB/s. The 8 ports are internally connected by two buses (B-link) running at 640 MB/s each, as shown in Figure 4. Internally the switch contains separate circuitry (Link Controllers and B-links) for request and response packets. The maximum payload is 425 MB/s for requests only and 640 MB/s for interleaved traffic. The actual logic for routing data on/off the SCI rings and B-link is contained in the LC2 link controller which contains two lookup tables. This allows packets to be switched between the SCI ring and B-link based on a combination of SCI node identifier, transaction identifier and packet type (request/response). A module contains 12 link controllers. The port-to-port latency is specified as 250 ns.

\begin{table}
\begin{tabular}{|c|c|c|} \hline
**output to E** & 56 & 38 \\
**output to F** & 62 & 40 \\
**output to G** & 35 & 39 \\
**output to H** & 35 & NA \\
**output to I** & NA & 68 \\ \hline \hline
**Total output** & **188** & **185** \\ \hline \end{tabular}
\end{table}
Table 16: Maximum output load on a switch port (figures are expressed in MB/s).

Figure 4: D515 switch module showing 4 user ports, 4 expansion ports and 2 internal B-links.

The 16-port switch is obtained by interconnecting 4 user modules with 2 additional expansion modules configured as in Figure 5. This gives a nominal maximum bandwidth of \(4\times 425\) MB/s for bi-directional traffic and a worst-case port-to-port latency of 500 ns when traversing the entire switch fabric (two layers). It should be noted that each port may connect a ring of maximum 15 nodes with a maximum payload of 244 MB/s for a uni-directional and 176 MB/s for a bi-directional nwrite64 operation respectively.

Figure 5: 16-port SCI switch obtained by interconnecting six D515 4-port expandable switch modules.

### Switch scalability

Low-level measurements are reported in Section 3. The message passing layer of the Demonstrator software based directly on the SICSI API uses exclusively the SCI nwrite64 transaction.The maximum usable payload in the switch is as follows:

* uni-directional nwrite64 into a user port: 292 MB/s
* bi-directional nwrite64 into/from a user port: 292 MB/s
* uni-directional nwrite64 inside a module: 425 MB/s
* bi-directional nwrite64 inside a module: 425 MB/s
* uni-directional nwrite64 between modules: 850 MB/s
* bi-directional nwrite64 between modules: 850 MB/s

The most crucial test involves inter-module traffic. We have loaded one half of the switch (two modules) with data generators and the opposite two modules with receivers ensuring that all traffic has to cross the two interconnecting modules. This will load all the internal B-links of all modules to the maximum of 425 MB/s. The maximum theoretical send-receive bandwidth is thus 425 MB/s per module or a total of 850 MB/s.

A fast Pentium processor equipped with Dolphin's PCI-SCI adapter is capable of sending or receiving ~80 MB/s. 22 such processors (11 pairs) are then sufficient to saturate the switch. We have therefore used commodity Pentium-based processor boards rather than special purpose hardware data generators which are of limited use. It turned out that to guarantee safe data transmission a robust protocol was necessary which was easy to implement in software but would have been difficult with traffic generators.

The test consisted of running 11 send-receive pairs starting with a delay. As each pair comes up adding ~80 MB/s to the switch throughput it is expected that saturation effects will become visible when the throughput builds up. We have also measured the throughput of each pair alone on the switch without contention to determine the offered load. Figure 6 shows the offered and achieved load as a function of the number of send-receive pairs. The system saturates at 610 MB/s or 72% of the nominal maximum capacity, well in agreement with earlier simulations. Figure 7 shows how the throughput varies over time for two pairs of nodes, one that joins the test early, when the switch is not much loaded, and one that joins the test when the switch is already saturated. In both cases the throughput grows again when other pairs stop running.

## 5 Conclusion

To achieve good overall system performance it is important to have low error rates in the components. The older LX PCI chipset is a source of frequent errors which propagate through the switch and seriously degrade the performance. We have upgraded 8 motherboards from LX to the BX chipset. We have otherwise successfully used a mixture of boards from different vendors with different clock frequency, memory speed and single or dual processors.

Faultly switch components are hard to diagnose. We had to change one switch module. The Dolphin SCI driver monitors the switch constantly for errors and closes faulty routes temporarily. The error rate is relatively low, at a rate of a few per second at high loads. ThisFigure 6: Offered (pairs alone) and achieved (pairs simultaneous) bandwidth obtained with 11 send-receive pairs

Figure 7: Throughput behaviour over time for an “early” (top) and “late” (bottom) send-receive pairs.

is too high for normal operation and requires a safe protocol with systematic error checking, a technique already available in the SISCI API and initially developed to cope with occasional PCI chipset errors. To facilitate error recovery, whose procedure is somewhat dependant on the version of the SCI PCI interface card, Dolphin has graciously upgraded all D310 to the newer D321 cards.

The Dolphin driver is derived from a common source for WNT and Linux. RAL has collaborated with Dolphin to adapt the driver to support a multi-processor Linux kernel. With the existing expertise we found it easier to operate a large cluster with Linux.

We have shown that it is possible to build a large system from commodity components. In addition it is possible to purchase large integrated systems, for example from Scali AS [6].

## Acknowledgements

This work has been partially funded by the European Union through the SISCI Project (Standard Software Infrastructures for SCI-based Parallel Systems), Contract 23174.

## References

* [1] IEEE Computer Society, _IEEE Standard for the Scalable Coherent Interface (SCI)_, IEEE Std 1596-1992 (1993).
* [2] Dolphin Interconnect Solutions AS. [http://www.dolphinics.no/](http://www.dolphinics.no/)
* [3] F. Giacomini et al., _Low-level SCI Software: Requirements, Analysis and Functional Specification_, Deliverable 1.1.1 of the SISCI Project (1998). [http://www.cern.ch/SCI/WP1/](http://www.cern.ch/SCI/WP1/)
* [4] J. Apostolakis et al., _Abstract Communication Benchmarks in Parallel Systems for Real-time Applications_, CHEP '97, Berlin (1997).
* [5] A. Bogaerts et al., _Studies of SCI for the ATLAS Level-2 Trigger System_, X IEEE Real Time Conference, Beaune (1997).
* Scalable Linux Systems AS. [http://www.scali.com/](http://www.scali.com/)