**Proposal for a discussion on the global software management issues.**

_Christian ARNAULT_

LAL - Orsay, France. arnault@lal.in2p3.fr

June 29, 1995

## 1 Purpose of this note.

This is an informal note which attempts to identify and define a set of topics that should be discussed within the domain of the Atlas global software management.

This is in particular a view of the general architecture design centered around the problem of designing first the data sets that are exploited and manipulated in the Atlas environment. The point is to consider that many technical and design choices in the software environment are strongly dependent on the data set definitions (this is true for most of the applications but also for the database systems or the management utilities in charge of the version control for instance).

The main questions envisaged here are :

* How to produce the specification of the various data sets generally needed in the Atlas software environments.
* How to configure, maintain and distribute these specifications.

It should be clear that this note deals only with the particular problem of specifications' (and associated tools) but not with the construction of the application software, such as the simulation, the reconstruction etc... And even more precisely, we speak here about how the specification of data sets can apply to the Atlas software.

By specification of data sets, I mean the formalisms that are used to define and document the various data sets that will be produced, exchanged and exploited by the various components of the software chain of the Atlas experiment, within both the online and offline context. The primary data sets concerned with such a study are (among others) the raw data produced in the near future by the simulation (and of course eventually by the detector), and the detector description.

A specification yields a form of description of the data structures and their semantics which can be both understood by humans and translated into computer forms. The role of such a description is to define completely the information manipulated in the experiment outside of any computer implementation constraint. For example details of the exchange medium, ie Zebra and the format of the banks are not generally described in this kind of specification, but are rather taken into account by associated software tools. This specification can serve as the source for documentation and standardization in the community and also the production of software tools (such as database management products or data access packages). Specification languages and tools are frequent in the industry with in particular the Data Definition Languages (DDL) in the database environments.

Before one attempts to actually work on these topics, or even propose any technical solution, it is essential to define a kind of long-term strategy that would define some milestones to be eventually fulfilled, as well as likely paths towards them.

Having these milestones in mind would considerably help us to select the possible short-term choices.

On the other hand, many people are now willing to develop software or exploit data that are used or produced by others. It is therefore urgent to propose and provide means of specifying, producing, accessing datasets in a way that sharing is possible, and also set up the basis for evolution.

I think that only an open discussion managed by a working group formed by a few people involved in the different software domains (simulation, trigger, online, etc...) can achieve this kind of goals.

The roles of such a working group could be :* to identify and adopt a set of criteria that would characterize what is the 'Atlas standard behaviour' for the topics mentioned here.
* to organize a documentation, management and distribution scheme for primary data-sets.
* to verify that the candidates proposed by contributors satisfy the validity criteria.

## 2 The data sets.

Here, the problems are :

* To identify the various domains where data sets are required such as
* The detector description
* The raw data (typically the output of the simulation and input to reconstruction and trigger applications.)
* The magnetic field
* The calibration data
* etc... Among these examples, only the first two are required in a quite short term (and could be qualified to be 'primary datasets').
* To specify the definition of these data sets using (a) (possibly specialized) language(s) so that its structure and its characteristics can be entirely described, documented, propagated to the collaboration. Examples of such specification languages are well known :
* AGE (DICE95)
* STEP/Express for geometry
* ODL for object-oriented data
* Adamo's DDL
* C structures It is important to consider the capability of a given language to be qualified as a'specification language'. This aspect needs some study, and for some of the examples above (C structures), is dependent on the way it is used.
* To provide for each of these data sets a computer readable description and a set of utility operators for producing or accessing the data. Sometimes the specification languages already provide these tools (or packages) and for some others, we might need to develop them.
* To allow coexistence of the various specification languages that we might select, since one cannot ensure (nor hope!) that any such unique language will ever fit the whole range of data sets we need. Thus one should expect that each data domain will be given its own specialized language (eg Express for Geometry due to the CAD constraints). One important consequence of this is that one has to identify the places where inter-language communication must be supported.

The configuration and management aspects.

In order to be effective, the data definitions as well as the associated tools must be managed using quite robust and flexible systems. These should obey at least the following criteria :

* The management of the data set specifications and of their supporting tools must be kept independent of the management of the client software (eg the reconstruction algorithms) that users will provide.
* Control of the degree of compatibility across successive versions must be managed, as well as how to specify the configurations required to assemble a complete system composed of applications that access several data sets. Schemes for version identification independent of the management tools must be provided.
* The management of data specifications must be both centralized for ensuring a consistent and reliable definition and capable to handle the easy distribution ofversions to developers.

## 4 Example of a possible solution.

This section is a very rough collection of ideas that come out from the analysis of the current situation and tries to make use of the existing packages and tools that could form the first examples we could work on.

Clearly a quite short term general solution does not exist yet, especially when people need to exchange "non standard" data sets (as this is often the case between the trigger application and the simulation or reconstruction environment). Instead, exchange formats are simply described or even rely on conventional packages (such as DZDoc). This is not enough to become a specification, nor to form a general documentation for the data structures.

However various tools such as AGE and Adamo's DDL can be thought as potential first steps towards the goals proposed above, although they must be considered as temporary solutions. Cooperation must be established with other teams (such as Geant4 for STEP/Express, MOOSE for ODL) in order to understand what should be the next iterations.

On the management area, quite simple conventions dealing with :

* The directory management (eg. the structure, organization and access controls based around AFS)
* The version identification
* The choice of the version control tool (such as CVS)

could provide efficient startup.

In longer term perpective, one should provide to users a set of guide-lines and conventions (ie procedures) to allow introducing into the standard software the specification of new data-sets.