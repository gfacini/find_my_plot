ATLAS-CONF-2003-003

June 26, 2003

**An Overview of Algorithms for the ATLAS High Level Trigger**

S. Armstrong,\({}^{\dagger}\) J.T. Baines, C.P. Bee, M. Biglietti, A. Bogaerts, V. Boisvert, M. Bosman, S. Brandt, B. Caron, P. Casado, G. Cataldi, D. Cavalli, M. Cervetto, G. Comune, A. Corso-Radu, A. Di Mattia, M. Diaz Gomez, A. dos Anjos, J. Drohan, N. Ellis, M. Elsing, B. Epp, F. Etienne, S. Falciano, A. Farilla, S. George, V. Ghete, S. Gonzalez, M. Grothe, A. Kaczmarska, K. Karr, A. Khomich, N. Konstantinidis, W. Krasny, W. Li, A. Lowe, L. Luminari, C. Meessen, A.G. Mello, G. Merino, P. Morettini, E. Moyse, A. Nairz, A. Negri, N. Nikitin, A. Nisati, C. Padilla, F. Parodi, V. Perez-Reale, J.L. Pinfold, P. Pinto, G. Polesello, Z. Qian, S. Resconi, S. Rosati, D.A. Scannicchio, C. Schiavi, T. Schoerner-Sadenius, E. Segura, T. Shears, S. Sivoklokov, M. Smizanska, R. Soluk, C. Stanescu, S. Tapprogge, F. Touchard, V. Vercesi, A. Watson, T. Wengler, P. Werner, S. Wheeler, F.J. Wickens, W. Wiedenmann, M. Wielers, H. Zobernig

###### Abstract

Following rigorous software design and analysis methods, an object-based architecture has been developed to derive the second- and third-level trigger decisions for the future ATLAS detector at the LHC. The functional components within this system responsible for generating elements of the trigger decisions are algorithms running within the software architecture. Relevant aspects of the architecture are reviewed along with concrete examples of specific algorithms.

\(\dagger\)Presented by S. Armstrong on behalf of the ATLAS High Level Trigger Group [1] at the 13th IEEE-NPSS Real Time Conference 18-23 May 2003, Montreal, Quebec, Canada

_To be published in The Proceedings of the IEEE Real Time 2003 Conference and IEEE Transactions on Nuclear Science_Introduction

### Overview of the Large Hadron Collider

The future Large Hadron Collider (LHC) project at CERN in Geneva, Switzerland will be a colliding proton synchrotron with a center-of-mass energy of 14 TeV. It is anticipated to deliver an initial luminosity of \(2\times 10^{33}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\) to two large general purpose detector experiments (ATLAS and CMS) and scale up to the full design luminosity is \(10^{34}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\). The high final state particle multiplicities from the proton-proton collisions necessitate highly granular and large scale detector systems leading to 1 to 2 MByte event sizes involving on the order of \(10^{8}\) electronic channels. The bunch crossing rate of 40 MHz implies fast trigger and data acquisition. When these factors are considered along with the high radiation environment in which the detectors and their electronics must function for the decade-long lifetime of the experiment, the LHC program places unprecedented and extreme demands on detectors and trigger/data acquisition systems.

### ATLAS Detector

The ATLAS detector [2] consists of several highly granular and hermetic concentric subdetector systems oriented coaxially with respect to the LHC beamline1 and centered around the nominal proton-proton collision point. This subsection briefly summarizes details of the detector relevant to the discussion below.

Footnote 1: The LHC beamline is referred to as the \(z\) axis in the text that follows. Kinematic variables such as transverse momentum \(p_{\mathrm{T}}\) are defined from this axis as is the polar angle \(\theta\) from which pseudorapidity \(\eta=-\mathrm{logtan}(\theta/2)\) is derived.

Three subdetector systems at the innermost radii constitute the Inner Detector (InDet) tracking system: the Pixel detector, the Semiconductor Tracker (SCT), and the Transition Radiation Tracker (TRT). These detectors are immersed in a 2 T axial magnetic field generated by a superconducting solenoid magnet outside the TRT. By reconstructing hits left from charged particles traversing the fiducial tracking volume, high efficiency tracking with good impact parameter resolution can be achieved down to a pseudorapidity of 2.5. The InDet provides a reconstruction efficiency greater than 90% over a broad \(p_{\mathrm{T}}\) spectrum for isolated tracks with a resolutions of \(\sigma(1/p_{\mathrm{T}})=0.36\oplus 13/(p_{\mathrm{T}}\sqrt{\mathrm{sin}\theta})\) (\(\mathrm{TeV}^{-1}\)) and \(\sigma(d_{0})=11\oplus 73/(p_{\mathrm{T}}\sqrt{\mathrm{sin}\theta})\) (\(\mu\)m).

A liquid Argon calorimeter (LAr) with a accordion-shaped electrode design is situated outside the solenoid magnet and provides electromagnetic calorimetry within \(|\eta|<3.2\) with a resolution of \(\sigma_{\mathrm{E}}/E=10\%/\sqrt{E}\oplus 0.7\%\oplus 0.27/E\) and a position resolution of \(\sigma_{\theta}=40\,\mathrm{mrad}/\sqrt{E}\). Outside the LAr is a hadron calorimeter with a novel radial scintillating tile geometry (Tile) providing a resolution of \(\sigma_{\mathrm{E}}/E=50\%/\sqrt{E}\oplus 3\%\). LAr technology is also used for hadron calorimetery in the endcap regions as well as special forward calorimeters extending the coverage to \(|\eta|=4.9\).

Outside the calorimetry system is an air-core Barrel Toroid (BT) and Endcap Toroid (ECT) magnet system interleaved by and surrounded with several types of tracking chambers comprising the Muon Spectrometer. The peak field intensity is 3.9 and 4.1 T in the BT and ECT respectively. There are two types of fast response chambers contributory to the Level-1 Trigger decision:the Thin Gap Chambers (TGC) and Resistive Plate Chambers (RPC). There are also two types of precision tracking chambers: Monitored Drift Tubes (MDT) and Cathode Strip Chambers (CSC). The muon spectrometer provides a good standalone momentum resolution of roughly 2% allowing it to identify muons up to 1 TeV/\(c\).

The physical size of the detector is defined by the outermost chambers of the muon spectrometer: 46 m long, with a diameter of 22 m. The overall mass of the detector is roughly \(7\times 10^{6}\) kg.

### Trigger System

The ATLAS trigger system must accept the high 40 MHz bunch crossing frequency and reduce it to a manageable rate of roughly 200 Hz. It is comprised of a three-level system. The first-level hardware-based trigger (Level-1) quickly analyzes data from the calorimeter and muon spectrometer systems to derive an accept or reject decision within 2 \(\mu\)s. Events are passed on to a second-level software-based trigger (Level-2) at a rate of 75 kHz which must derive a decision within an average latency of 10 ms. Level-2 accepted events are passed on to the third-level software-based Event Filter (EF) at a rate of roughly 3 kHz which has a more generous latency of roughly 1 s to pass the event on to offline mass storage with a rate of roughly 200 Hz. It is axiomatic that only events surviving this three-stage triggering system can be part of subsequent physics analysis. Together, the Level-2 and EF are referred to as the High Level Trigger (HLT).

To achieve the required rejection power while retaining sensitivity to signal events within the broad ATLAS physics program, components of physics analysis traditionally deferred to offline environments must be embedded within the trigger system. Hence, the first stages of ATLAS physics analysis reside and must be understood within the online trigger system. Table 1 provides examples of physics objects, the corresponding trigger nomenclature for them, and the area of physics analysis reliant upon them.

## 2 Architecture of the High Level Trigger Selection Software

Figure 1 provides a disaggregated view of the key components of the High Level Trigger Selection Software (HLTSSW). The HLTSSW runs on dedicated hardware consisting of farms of PCs interfaced to the ATLAS Data Collection Systems. For Level-2, the

components of the HLTSSW must behave in a multithreaded-safe manner.

### High Level Trigger Algorithm Strategy

Algorithms reconstruct objects and extract features from event data; these features are used to derive the trigger decision. At Level-2, highly specialized algorithms use a restricted portion of event data usually defined in terms of Region-of-Interests (RoI) derived from the Level-1 decision. Modified algorithms from the offline software are used as Event Filter algorithms and have potential full access to event data. In both cases, algorithms must be capable of being seeded from results derived at a previous stage of the trigger chain. Furthermore, they may be called multiple times per event (_e.g._, in the case of multiple RoIs found at Level-1 in a single event). Hence they do not operate in a general purpose mode as in the offline software, but rather must work in a Bayesian-like environment by validating only specific hypotheses given a certain seed.

### Event Data Model

To facilitate the importation of algorithms from the offline software as well as to permit a configurable continuum of selection in the HLT by means of interchangeable Level-2 and EF algorithms, a common Event Data Model (EDM) is essential. The EDM is the common language within and between algorithms defining the obj

\begin{table}
\begin{tabular}{|c|c|c|} \hline Object & Nomenclature & Coverage \\ \hline electrons & e25i, 2e15i & Higgs (SM, MSSM), new gauge bosons, extra dimensions, SUSY, W, top \\ \hline photons & \(\gamma\)60i, 2\(\gamma\)20i & Higgs (SM, MSSM), extra dimensions, SUSY \\ \hline muons & \(\mu\)20i, 2\(\mu\)10 & Higgs (SM, MSSM), new gauge bosons extra dimensions, SUSY, W, top \\ \hline jets & j360, 3j150, 4j100 & SUSY, compositeness, resonances \\ \hline jets + \(\not\!\!E_{\rm T}\) & j60 + xE60 & SUSY, leptoquarks \\ \hline tau + \(\not\!\!E_{\rm T}\) & \(\tau\)30 + xE40 & extended Higgs models, SUSY \\ \hline \end{tabular}
\end{table}
Table 1: Examples of physics objects, the corresponding trigger nomenclature, and areas of physics coverage.

of event data; these include clusters of hits within the tracking system, three-dimensional space points derived from clusters, a track defining the trajectory of a charged particle, _etc_. The establishment of a common EDM within the ATLAS offline and online software environments is an on-going effort [3].

### Importation of the Offline Software Framework

In addition to algorithms, the HLTSSW imports and uses key components of the ATLAS object-oriented C++ Offline Software framework, referred to as Athena [4]. Within this framework, there is a separation of algorithmic objects from data objects, the EDM specifies the nature and content of data objects which are passed between algorithms and software packages via a Transient Event Store (TES) to communicate information about the event. A corollary to this approach is that, contrary to canonical object-oriented programming, data object classes contain minimal algorithmic content (_e.g._, algorithms for finding, following, or fitting tracks are separated from methods in the class definition of the Track itself).

Figure 1: A package diagram illustrating the key components of the High Level Trigger Selection Software.

### Restricted Data Access

At Level-2, algorithms actively request portions of event data from the Data Collection System. The relevant data are defined by RoIs based on information from the decision from Level-1 or a previous result in Level-2 processing. For each RoI, the total data volume with respect to the whole detector is roughly a few percent. Hence, this restricted data access strategy represents a significant reduction in the required HLT processing and networking resources.

For a given RoI, typically defined by an extent in \(\eta\) and \(\phi\) within the physical detector volume, a _Region Selector_[6] translates the physical volume into a set of offline identifiers [5]. These identifiers are translated at a subsequent stage (see Section 2.5) into online identifiers which may then be used to request the data themselves.

It may seem counterintuitive to use such a scheme (_i.e._, conversion into a geometrical region which requires translation into Offline identifiers which then require translation into Online identifiers). There are, however, a variety of motivations for the Region Selector. The prime motivation is to gain access in a uniform and rapid way to event data from sub-detectors which do not participate in the Level-1 trigger decision (_e.g._, event data from InDet tracking information given a Level-1 trigger based on an energy deposit in the Calorimeters). An additional motivation includes allowing for possible secondary RoIs as needed by an algorithm which may lie outside the primary RoI defined by Level-1.

Figure 2: A simplified schematic diagram of the sequence by which HLT algorithms request and receive event data.

### Paradigm for Realistic Raw Data Access

Raw data from the ATLAS detector will be delivered in terms of a _ByteStream_ of data consisting of hierarchically arranged fragments formatted in a sub-detector-dependent way. This ByteStream data must be converted into objects which can then be used by algorithms. Modeling this flow and conversion of ByteStream data in a realistic way is vital to an accurate modeling of the HLT performance and subsequent estimation of required network and computing resources.

The HLTSSW adopts a scheme whereby the interaction of HLT algorithms with the Data Collection System is hidden behind a call to the TES. Figure 2 illustrates this scheme. An algorithm requests data within a certain region by first feeding the parameters of the region to the RegionSelector. The RegionSelector returns a set of Offline Identifiers which the algorithms then uses to request collections of relevant data objects from the TES. If the TES does not contain the data objects, it requests these data from a RawDataConverter. The Offline Identifiers are translated into Online or Read-out Buffer (ROB) Identifiers which are used to request the data from the Data Collection System. Table 2 gives the current granularity of the ATLAS detector in terms of the number of Offline and ROB Identifiers used for each sub-detector system. The raw data returned from the Data Collection System are in ByteStream format and are converted into data objects and stored in the TES in collections tagged with Offline Identifiers. The TES then returns the collections of data objects the algorithm originally requested.

\begin{table}
\begin{tabular}{|c||c|c|c|} \hline  & Unit & Off. Id. & ROB Id. \\ \hline Pixels & module & 1744 & 81 \\ SCT & side of module & 8176 & 256 \\ TRT & straw layer & 19008 & 256 \\ LAr & Trigger Tower & 7168 & 768 \\ Tile & module & 256 & 32 \\ MDT & chamber & 1168 & 192 \\ CSC & chamber & 32 & 32 \\ RPC & chamber & 574 & 32 \\ TGC & chamber & 1584 & 32 \\ \hline \end{tabular}
\end{table}
Table 2: The current granularity of the ATLAS detector for each sub-detector system in terms of the definition of the Offline unit of granularity (Unit), number of Offline Identifiers (Off. Id.), and Online ROB Identifiers ROB Id.

### Guidance and Seeding of Algorithms

The component of the HLTSSW which guides and steers algorithms is referred to as the _Steering_[7]. The primary motivating factor for the Steering component of the HLTSSW is the need for a fast and early rejection of uninteresting events in a flexible and configurable manner.2 This is realized in a way that permits full control of the algorithms executing within the HLT processing flow with the simple modification of XML configuration files.

Footnote 2: The Steering is also motivated by a need for a pre-scale/forced-accept of some events.

The Step Controller (SC) of the Steering software replaces the Athena Event Loop Manager and has the responsibility of calling algorithms. Two XML files encode _Sequences_ and _Signatures_ that in turn instruct the Steering on when and how to run an algorithm and if a physics signature is fulfilled. Signatures and Sequences are built upon Trigger Elements (TE). The TEs characterize abstract physics objects with a succinct label (_e.g._, "e20i" for an isolated 20 GeV electron). This decouples the Steering from details of the EDM. A _Navigation_ scheme relates TEs to each other and to underlying concrete event data objects. Hence a TE can be thought of as the entry point for an algorithm into an event.

The HLT processing flow is disaggregated into _Steps_. Input TEs provide seeds to algorithms executing in each step. The decision to go further in the process is taken at every new Step by the comparison between active TEs in the TES and the corresponding configuration Signature. An event is accepted if all its constituent Sequences have been executed and at least one of the corresponding Configuration Signatures has been satisfied.

## 3 High Level Trigger Algorithms

This section summarizes algorithms intended to be used within the HLTSSW. An emphasis is placed on the more optimized and specialized algorithms developed for Level-2 which must cope with more restrictive data access and latency. At the time of this writing, most algorithms have already been incorporated into the HLTSSW framework. Results will become available for inclusion in the Summer 2003 ATLAS Technical Design Report for the Trigger and Data Acquisition System to be submitted to the LHCC.

In the case of Level-2 track reconstruction involving the precision Pixel and SCT sub-detectors, two parallel algorithms have been developed: IDSCAN (described in Section 3.1) and SiTrack (described in Section 3.2). Likewise, in the case of Level-2 track reconstruction in the TRT sub-detector, two parallel algorithms have also been developed: TRTxKalman (described in Section 3.3) and TRT-LUT(described in Section 3.4). This dual algorithm approach has proved beneficial in terms of allowing cross-checks, rapid development, and redundancy.

### Level-2 Track Reconstruction: IDSCAN

Taking as input SpacePoints found in the Pixel and SCT sub-detectors, the Level-2 tracking algorithm IDSCAN consists of a series of sub-algorithms: ZFinder, HitFilter, GroupCleaner, and TrackFitter.

The ZFinder finds the \(z\)-position of the primary interaction vertex, which has a spread of \(\sigma=\pm 5.6\,\mathrm{cm}\) due to the LHC bunch size. The algorithm puts all hits into narrow \(\phi\)-bins and extrapolates pairs of hits in each bin back to the beam-line, storing the \(z\) of intersection in a histogram. It takes as the \(z\)-position the histogram region with the most entries. The HitFilter finds groups of hits compatible with Tracks from the \(z\) position found by ZFinder. It puts all hits into a histogram binned in \(\phi\) and \(\eta\). It then finds clusters of hits within this histogram. It creates a _group_ of hits if such a cluster has hits in more than a given number of layers. The group of hits found by HitFilter is used by GroupCleaner which splits groups into Tracks and removes noise hits from groups. Each triplet of hits forms a potential track for which \(p_{\mathrm{T}}\), \(\phi_{0}\), and \(d_{0}\) are calculated. It forms groups from these triplets with similar parameters, applying certain quality cuts. It accepts a track candidate if a group contains enough hits. Finally, the TrackFitter verifies track candidates and finds the track parameters by using a standard Kalman-filter-type fitting algorithm. It returns a list of SpacePoints on the Track, the Track parameters, and an error matrix.

### Level-2 Track Reconstruction: SiTrack

SiTrack takes Pixel and SCT SpacePoints as input and outputs fitted reconstructed Tracks, each storing pointers to the SpacePoints used to build it. SiTrack is implemented as a single main algorithm SiTrack which instances and executes a user defined list of sub-algorithms: STSpacePointSorting, STMonVertex, STTrackSeeding, and STThrreePointFit.

STSpacePointSorting collects pointers to SpacePoints coming from the Pixel and SCT detectors and sorts them by module address, storing the result in a Standard Template Library (STL) map. This processing step is performed in order to speed-up data access for the other reconstruction sub-algorithms.

In the case of a trigger with a high \(p_{\mathrm{T}}\) muon signature, STMonVertex is a primary vertex identification algorithm mostly suitable for low luminosity events. It is based on track reconstruction inside the Level-1 muon RoI: the most impulsive track is assumed to be the muon candidate and its \(z\) impact parameter is taken as the primary vertex position along \(z\).

STTrackSeeding, using the sorted SpacePoint map and a Look-Up Table (LUT) linking each module within the inner most Pixel layer (B-layer) to the ones belonging to other logical layers, builds track seeds formed by two SpacePoints and fits them with a straight line; one or more logical layers can be linked to the B-layer, the latter optionbeing particularly useful if robustness to detector inefficiencies must be improved. If the primary vertex has already been reconstructed by STMuonVertex, a fraction of fake track seeds can be rejected during their formation, applying a cut on their \(z\) distance from the primary vertex. Otherwise, if no vertex information is available, a histogram whose resolution depends on the number of seeds found is filled with the \(z\) impact parameter of each seed; its maximum is then taken as \(z\) position for the primary vertex. This vertexing algorithm is most suitable for high luminosity events containing many high \(p_{\mathrm{T}}\) tracks (_e.g._, b-tagging). Independent cuts on \(r-\phi\) and \(z\) impact parameters are eventually applied to the reconstructed seeds to further reduce the fake fraction.

STThreePointFit extends track seeds with a third SpacePoint; it uses a map associating to each seed a set of module roads3 the track could have hit passing through the Pixel or SCT detectors. A subset of modules is extracted from each road according to a user defined parameter relating to their "depth" inside it (_e.g._, the user can decide to use modules at the beginning or in the middle of each road, _etc._). SpacePoints from the selected modules are then used to extend the seed and candidate tracks are fitted with a circle; ambiguities (_e.g._, tracks sharing at least one SpacePoint) can be solved on the basis of the track quality, leading to an independent set of tracks that can be used for trigger selection or as a seed for further extrapolation.

Footnote 3: A road is a list of modules ordered according to the radius at which they are placed starting from the innermost one

### Level-2 Track Reconstruction: TRTxKalman

TRTxKalman utilizes hits from the TRT sub-detector. The core of algorithm is a set of utilities from the offline reconstruction package xKalman [8] for reconstruction of tracks in the TRT detector. It is based on a Hough-transform (histogramming) method. Upon initialization, a set of trajectories in the \(\phi\)-\(r(z)\) space is calculated for the barrel and endcap parts of TRT. The real value of magnetic field is taken into account at each straw position coordinates when calculating trajectories. The lower bound of \(p_{\mathrm{T}}\) used is \(0.5\,\mathrm{GeV}/c\). Then the histogram (with a binning of 500 in \(\phi\) and 70 in curvature) is filled with hit positions determined from drift circles in each straw. Each hit will populate many cells in the histogram, but for each track in an RoI there will be a bin where all hits on the track have an entry. Thus, track candidates can be identified from peaks in the histogram. Bins with more than 7 contributing cluster are considered as a track candidates. Track candidates must also satisfy quality criteria on the number of unique hits and ratio of hits to number of straws crossed by a track.

### Level-2 Track Reconstruction: TRT-LUT

TRT-LUT [9] is an algorithm for track reconstruction in the TRT.

The algorithm takes as input hits in the TRT. The algorithmic processing consists of Initial Track Finding, Local Maximum Finding, Track Splitting, and Track Fitting and Final Selection. It outputs the Hits used and Tracks with their parameters.

During the Initial Track Finding, every hit in a three-dimensional image of the TRT detector is allowed to belong to a number of possible predefined tracks characterized by different parameters. All such tracks are stores in a Look-Up Table (LUT). Every hit increases the probability that a track is a genuine candidate by one unit.

During Local Maximum Finding, a two-dimensional histogram is filled with bins in \(\phi\) and \(1/p_{\mathrm{T}}\). A histogram for a single track would consists of a "bow-tie" shaped region of bins with entries at a peak in the center of the region. The bin at the peak of the histogram will, in an ideal case, contain all the hits from the Track. The roads corresponding to other filled bins share straws with the peak bin, and thus contain sub-sets of the hits from the track. A histogram for a more complex event would consist of a superposition of entries from individual tracks. Hence, bins containing a complete set of points from each track can be identified as local maxima in the histogram.

The Track Splitting stage of the algorithm analyzes the pattern of hits associated to a track candidate. By rejecting fake candidates composed of hits from several low-\(p_{\mathrm{T}}\) tracks, the track splitting step results in an overall reduction by a factor of roughly two in the number of track candidates. For roads containing a good track candidate, it identifies and rejects any additional hits from one or more other tracks. The result of the overall Track Splitting step is a candidate that consists of a sub-set of the straws within a road.

The final step of TRT-LUT, Track Fitting and Final Selection, performs a fit in the \(r-\phi\)\((z-\phi)\) plane for the barrel (end-caps) using a third order polynomial to improve the measurement of \(\phi\) and \(p_{\mathrm{T}}\). The track is assumed to come from the nominal origin. After the fit, a reconstructed \(p_{\mathrm{T}}\) threshold of \(0.5\,\mathrm{GeV}/c\) is applied.

### Level-2 Calorimetry: T2Calo

T2Calo [10] is a clustering algorithm for electromagnetic (EM) showers, seeded by the Level-1 electromagnetic (EM) trigger RoI positions. This algorithm can separate isolated EM objects from jets using the cluster \(E_{\mathrm{T}}\) and certain shower-shape quantities.

Calibrated calorimeter cells are taken as input to the algorithm. The first step in T2Calo is to refine the Level-1 position from the cell with highest energy in the second sampling of the EM calorimeter. This position \((\eta_{1},\phi_{1})\) is later refined in the second sampling by calculating the energy weighted position \((\eta_{c},\phi_{c})\) in a window of \(3\times 7\) cells (_N.B._, a cell is \(0.025\times 0.025\) in \(\eta\times\phi\)) centered in \((\eta_{1},\phi_{1})\). The algorithm proceeds in step-wise manner, calculating each of the following variables and could permit at each stage the application of selection criteria.

* In sampling 2, \(R_{\eta}^{\mathrm{shape}}=E_{3\times 7}/E_{7\times 7}\) is calculated. The expression \(E_{n\times m}\) stands for the energy deposited in a window of \(n\times m\) around \((\eta_{1},\phi_{1})\). This shape variable takes into account that most of the energy of EM showers is deposited in the second sampling of the EM calorimeter.
* In sampling 1, \(R_{\eta}^{\rm strip}=(E_{\rm 1st}-E_{\rm 2nd})/(E_{\rm 1st}+E_{\rm 2nd})\) is obtained in a window of \(\Delta\eta\times\Delta\phi=0.125\times 0.2\) around \((\eta_{c},\phi_{c})\). \(E_{\rm 1st}\) and \(E_{\rm 2nd}\) are the energies of the two highest local maxima found, obtained in a strip-by-strip basis. The two \(\phi\)-bins are summed and only the scan in \(\eta\) is considered. A local maximum is defined as a single strip with energy greater than its two adjacent strips.
* The total transverse energy \(E_{\rm T}\) deposited in the EM calorimeter is calculated in a window of \(3\times 7\) cells around \((\eta_{1},\phi_{1})\).
* Finally, the energy that leaks into the hadron calorimeter \(E_{\rm T}^{\rm had}\) is calculated in a window of size \(\Delta\eta\times\Delta\phi=0.2\times 0.2\) around \((\eta_{c},\phi_{c})\).

### Level-2 Muon Reconstruction: muFast

The muFast [11] algorithm is a Level-2 track reconstruction algorithm for the Muon Spectrometer.

The program is steered by the RoI given by the Level-1 Muon Trigger and uses both RPC and MDT measurements. At present this algorithm is limited to the barrel region, and it functions in sequential steps:

* Pattern recognition is performed using the RPC hits that induced the Level-1 trigger accept decision to define a road in the MDT chambers around the putative muon trajectory. MDT tubes lying within the road are selected and a contiguity algorithm is applied to remove background hits not associated with the muon trajectory;
* A straight-line track fit is made to the selected tubes (one per each tube monolayer) within each MDT station. For this procedure the drift-time measurements is used to fully exploit the high measurement accuracy of the muon tracking system. The track sagitta is then evaluated;
* A fast \(p_{\rm T}\) estimate is made using Look-Up-Tables (LUT). The LUT encodes the linear relationship between the measured sagitta and the \(Q/p_{\rm T}\), as a function of \(\eta\) and \(\phi\).

### Event Filter Algorithms

Event Filter algorithms consist of algorithms imported directly from those developed in the offline Software and are described in detail elsewhere. Two complementary InDet track reconstruction packages are used: xKalman++ [8] and iPatRec [12]. The package egammaRec is designed to combine tracking information with information from calorimeter clusters found by the LArClusterRec and TileRec packages. A track reconstruction package for the Muon Spectrometer, Moore(Muon Object Oriented REconstruction) uses collections of digits or cluster from the MDT chambers in the Muon Spectrometer to find and fit tracks [13].

## 4 Summary and Conclusions

The future Large Hadron Collider at CERN offers unprecedented challenges to the design and construction of detectors and trigger/data acquisition systems. For ATLAS, a three-level trigger system has been developed to extract interesting physics signatures with a \(10^{6}\) rate reduction. To accomplish this, components of physics analysis traditionally deferred to offline physics analysis will be embedded within the online trigger system.

A dedicated and specialized selection software framework has been designed for the High Level Trigger. Algorithms will operate within this framework and must cope with data access and latency limitations. Tests of this model along with a broad suite of algorithms using a realistic raw data access paradigm are underway. Results will be part of the Summer 2003 ATLAS Trigger/DAQ Technical Design report.

## Acknowledgments

We wish to acknowledge and thank the ATLAS Offline Software community for their many contributions to the High Level Trigger Selection Software framework as well as High Level Trigger Algorithms. Crucial efforts to the establishment of a realistic raw data access paradigm for the various sub-detectors were made by M. Cobal, H. Ma, L. Santi, J. Schieck, A. Solodkov, A. Zalite, and Y. Zalite. The realization of the region selection mechanism was made in collaboration with K. Assamagan, G. Gorfine, F. Luehring, H.Ma, A. Solodkov.

## References

* [1] The ATLAS High Level Trigger group [Online] [http://atlas.web.cern.ch/Atlas/GROUPS/DAQTRIG/HLT/AUTHORLISTS/rt2003.pdf](http://atlas.web.cern.ch/Atlas/GROUPS/DAQTRIG/HLT/AUTHORLISTS/rt2003.pdf)
* [2] ATLAS Collaboration, _ATLAS Detector and Physics Performance Technical Design Report_, CERN/LHCC/99-14.
* [3] S. Armstrong _et al._, _Requirements for an Inner Detector Event Data Model_, Atlas Internal Note ATL-DAQ-2002-011.
* [4] ATLAS Offline Software group, _Athena: The ATLAS Common Framework User Guide and Tutorial_, [Online] [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/Development](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/Development)
* [5] A. Schaffer and C. Arnault, _Use of a Generic Identification Scheme Connecting Events and Detector Description in ATLAS_, Proceedings of the 2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California.
* [6] S. Armstrong _et al._, _A New Implementation of the Region-of-Interest Strategy for the ATLAS Second Level Trigger_, Proceedings of the 2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California.
* [7] G. Commune _et al._, _The Algorithm Steering and Trigger Decision mechanism of the ATLAS High Level Trigger_, Proceedings of the 2003 Conference for Computing in High Energy and Nuclear Physics, La Jolla, California.
* [8] I. Gavrilenko, _Description of Global Pattern Recognition Program (xLalman)_, ATLAS Internal Note ATLAS-INDET-97-165.
* [9] J. Baines _et al._, _Global Pattern Recognition in the TRT for B-Physics in the ATLAS Trigger_, ATLAS Internal Note ATLAS-TDAQ-99-012; M. Sessler and M. Smizanska, _Global Pattern Recognition in the TRT for the ATLAS LVL2 Trigger_, ATLAS Internal Note ATLAS-TDAQ-98-120.
* [10] S. Gonzalez, T. Hansl-Kozanecka, and M. Wielers, _Selection of high-\(p_{T}\) electromagnetic clusters by the level-2 trigger of ATLAS_, ATLAS Internal Note ATLAS-TDAQ-2000-002; S. Gonzalez, B. Gonzalez Pineiro, and T. Shears, _First implementation of calorimeter FEX algorithms in the LVL2 reference software_, ATLAS Internal Note ATLAS-TDAQ-2000-020; S. Gonzalez and T. Shears, _Further studies and optimization of the level-2 trigger electron/photon FEX algorithm_, ATLAS Internal Note ATLAS-TDAQ-2000-042.
* [11] A. di Mattia _et al._, _A Muon Trigger Algorithm for Level-2 Feature Extraction_, ATLAS Internal Note ATLAS-DAQ-2000-036.