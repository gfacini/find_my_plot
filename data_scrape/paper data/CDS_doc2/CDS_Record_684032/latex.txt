**Ptolemy simulation of the ATLAS level-2 trigger**

**Table of Contents**

**Ptolemy simulation of the ATLAS level-2 trigger**

\begin{tabular}{c c} Abstract & 1 \\ Contents : & 1 \\
1 Introduction & 1 \\
2 Overview of Ptolemy & 2 \\
3 Ptolemy implementation of the ATLAS level-2 trigger simulation stars & 4 \\
3.1 Implementation of the Message Library & 5 \\
3.2 Implementation of the nodes & 5 \\
3.3 Supporting Software & 5 \\
3.4 Implementation of Amessage-Ethernet Interface & 9 \\
3.5 Implementation of the Ethernet Switch & 9 \\
4 Using the level-2 stars in simulations & 11 \\
5 Characterisation of nodes on Ethernet testbed & 13 \\
5.1 Supervisor Characterisation & 15 \\
5.2 ROB Emulator Characterisation & 16 \\
5.3 Steering Characterisation & 16 \\
5.4 Ethernet Interfaces and Switches & 18 \\
6 Comparison of model with small Ethernet testbed configurations & 18 \\
7 Full System Simulation & 20 \\
8 Conclusions & 25 \\
9 References & 25 \\
10 Appendix & 26 \\
10.1 Calculating the parameters for the Supervisor & 26 \\
10.2 Calculating the parameters for the ROB & 28 \\
10.3 Calculating the parameters for the Steering & 29 \\ \end{tabular}

[

###### Abstract

The ATLAS level-2 trigger has been modelled using the Ptolemy simulation package. The model has been tuned and compared with results from the Pilot Project Ethernet Testbed Program. Results from the comparison are presented. A full system model, using large Ethernet switches, has been developed.

P.Clarke]P.Clarke]G.Crone]M.Dobson1]R.Hughes-Jones3]K.Korcyl2,1]S.Wheeler4

1.CERN, Geneva, Switzerland

2.Henryk Niewodniczanski Institute of Nuclear Physics, Cracow, Poland

3.Manchester University, UK

4.University College London, UK

###### Abstract

The ATLAS level-2 trigger has been modelled using the Ptolemy simulation package. The model has been tuned and compared with results from the Pilot Project Ethernet Testbed Program. Results from the comparison are presented. A full system model, using large Ethernet switches, has been developed.

Version : _1.0_

Date : 14th April 2000

Reference : ATL-COM-DAQ-2000-020

###### Contents :

* 1 Introduction
* 2 Overview of Ptolemy
* 3 Ptolemy Implementation of the ATLAS level-2 trigger simulation stars
* 4 Using the level-2 stars in simulations
* 5 Characterisation of nodes on Ethernet testbed
* 6 Comparison of model with small Ethernet testbed configurations
* 7 Full System Simulation
* 8 Conclusions
* 9 References
* 10 Appendix

## 1 Introduction

An initiative has been taken to develop a high-level model of the ATLAS level-2 trigger. This model is based on the Ptolemy[1] simulation package and is intended to be complementary to the work on SIMDAQ++[2]. Effort has been concentrated upon simulation of the Pilot Project Ethernet Testbed Program[3], although work is also underway (on a best-efforts basis) to produce a full system model. It is

Ptolemy simulation of the ATLAS level-2 triggerintended to keep the simulation as uncomplicated as possible. Each node in the system is seen as a black box which is only allowed to receive simple messages and, as a result, send out other simple messages at a later time according to transfer functions (which may be nothing more than a fixed delay). In addition, all switches in the system present a clear interface to nodes, with the actual details of internal delays, queuing, etc. hidden from the nodes.

## 2 Overview of Ptolemy

Ptolemy is a software environment developed at the University of California at Berkeley that supports heterogeneous system simulation and design using several different models of computation, each implemented in a different domain. For the level-2 trigger simulation, which involves nodes communicating over a network, only the Discrete Event (DE) domain is important. In this domain messages, representing events are passed among the lowest level simulation blocks called _stars_. Ptolemy maintains a chronologically sorted list of events and executes the star where the oldest event in that list resides. Stars communicate with each other by passing the messages through one or more input or output ports.

Models built within the Ptolemy environment are hierarchically organised, so that stars can be grouped together into _galaxies_. Galaxies can communicate with other galaxies or with stars. A _universe_ is a combination of stars and galaxies with no external ports and represents a whole, executable model. The hierarchical structure allows complex objects to be split into a number of simpler objects and where necessary simple objects to be connected together into more complex ones. All of Ptolemy has been developed in C\(++\) using object-oriented software which supports extensibility and modularity. Extensions to

## 2 Overview of Ptolemy

Figure 1: Ptolemy Hierarchy

stars and new stars can be written easily with the aid of various objects and classes provided by Ptolemy.

Stars and galaxies have _states_ associated with them. These are data structures which are used to remember data values from one simulation run to the next. For example, the network address of a node in the level-2 trigger system is a state of that node. The term _parameter_ is used to refer to the initial value of a state. Initial values for star states are defined in the source code for the star. These can be changed before and between simulation runs using the graphical user interface, described below. The graphical user interface can also be used to assign states to galaxies and subsequently change their values.

An interactive graphical interface (pigi) is provided with the Ptolemy package. It is rather out-of-date, but provides a user-friendly way of building simple simulations using both standard Ptolemy and user-written stars. This is useful for testing the individual components of a larger system. For large simulations, however, the use of the graphical user interface becomes impractical and in such cases the configurations are specified using the Ptolemy scripting language (ptcl).

The decision to use Ptolemy as the simulation environment was based, firstly on its modularity. This is essential for a community of independent code writers working on a common project in widely dispersed locations. Secondly, the user-friendliness of the GUI allows novices to get off to a quick start.

2 Overview of Ptolemy

Figure 2: Schematic diagram of levelâ€“2 trigger

## 3 Ptolemy implementation of the ATLAS level-2 trigger simulation stars

The ATLAS level-2 trigger system can be represented by a number of "nodes" connected via a network for exchanging messages as shown in Figure 2.

For events selected by the level-1 trigger, the detector data are transferred to the ROBs where they remain until the level-2 decision is available. The Supervisor will tell one of the processors to perform the level-2 processing for a particular event. The RoI data for the event are accessed from the ROBs by the Processor over the network. The Processor analyses the RoI data and decides whether the event is to be accepted. The trigger decision is passed back to the Supervisor, which informs the data-acquisition system whether or not the event data are to be passed to the event filter or discarded. The last job of the Supervisor is to send a message to the ROBs telling them to clear the data for particular events.

For the Ptolemy simulation, the system has been factorised into a number of nodes with well-defined external boundaries. One possible way of implementing the nodes was to characterise them using very simple probability density functions (PDFs). However, this could not be done if the simulation was to be calibrated against the Ethernet testbed since a very large and time-consuming number of measurements would have to be made. In order that a more limited number of testbed measurements would be required, a more precise understanding and simulation of the software, used to implement the node in the testbed, was needed. In the case of the level-2 trigger system this software is the level-2 Reference Software[4]. This approach has involved the use of queues and threads in the node simulations. Instead of using PDFs a number of fixed, tuneable delays are used to calibrate each node.

The delay mechanism used in all the nodes is based on that used in the standard Ptolemy DE domain server star. A code extract from which is shown below:

```
go{ //Nooverlappedexecution.setthetime. if(arrivalTime)completionTime)  completionTime=arrivalTime+double(serviceTime); else completionTime+=double(serviceTime); Particlespp=input.get(); output.put(completionTime)=pp; }
```

When a message arrives at the star it is time-stamped with _arrivalTime_. The star keeps a record of the time it will next be free in the _completionTime_ variable. If the message above is the first to arrive then arrivalTime will be greater than completionTime (which will be 0). The star is therefore not busy and can process the message immediately. It simulates the processing by setting completionTime (i.e. the time at which the star will next be free) to a _serviceTime_ plus the arrivalTime. If another message now arrives before completionTime then completionTime is greater than arrivalTime and the star is busy and unable to service the message immediately. The message will only be serviced once the current completionTime is reached. So the new completionTime is calculated as the current completionTime plus the serviceTime.

A feedback mechanism has also been implemented to enable particular functions to be executed at a particular time. For example, the star may need to perform some action at the actual time a message is sent, _not_ at the time it was _scheduled_ to be sent. When the completionTime for the message is scheduled a method is also called which will cause the star to "refire" at that given time. In effect the star sends a feedback message to itself to arrive at the given time. An integer value is assigned to the feedback message. When the feedback message arrives the star decides what action to take based on the value of the message.
All the level-2 nodes use similar mechanisms augmented in some cases with explicit queues for incoming and/or outgoing messages.

The boundaries between nodes are defined by the messages a node can send and receive. A document[5] has been written to describe the message passing scheme and the scheme has been implemented as a library in C++. A summary of the message library implementation is given in section 3.1.

High-level specifications have been written for each node and Ptolemy code has been written based on these specifications. Summaries of the node implementations are given in the sections 3.2.1, 3.2.2 and 3.2.3.

The nodes communicate over an Ethernet network via a switch. A Ptolemy star has been written to present a uniform interface to the network for all the nodes, converting Ptolemy messages to "Ethernet packets" understood by the switch. An Ethernet switch has been implemented in Ptolemy to connect the nodes together. Summaries of the interface and switch implementations are given in sections 3.5 and 3.6.

### Implementation of the Message Library

In the Ptolemy high-level model framework, the boundaries between nodes are defined by messages each node can send and receive. A base ATLAS message class has been defined (Amessage) which inherits from the Ptolemy Message class. Amessage provides all the common services needed to define and pass a message between 2 nodes. The specific messages sent (e.g. _RoiDataRequest_) inherit from Amessage and add any extra information as required. The following messages have been derived from Amessage their influence on the various nodes (on sending and receiving) are described in the following sections:

* RoiData
* RoiDataRequest
* DeleteData
* SteerEvent
* SteerComplete

More information on the Message library can be found in the implementation document[5].

### Implementation of the nodes

#### 3.2.1 Supervisor Emulator

The various messages a supervisor node can send and receive are summarised in Figure 3.

evid is a simple integer event ID (not an Amessage) corresponding to an event selected by the level-1 trigger. Each time the Supervisor receives a new evid message it is put into the event queue. When a SteerComplete message is sent back to the Supervisor, indicating that a Processor has completed analysing an event, it will put the message on the receive queue.

If the Supervisor is not busy it will service the event queue provided the number of outstanding events has not reached the limit (defined in the state _outstanding_in_this_super_). The number of outstanding events is defined as the number of events the Supervisor can send to the Processors without receiving a reply. When the event queue is serviced a SteerEvent message is scheduled to be sent out after a time characterised by the delay defined by the state _time_to_send_. The _SteerEvent_ message is sent to a Processor in a round-robin scheme, therefore all Processors have the same priority. The Supervisor uses the _nprocs_ (total number of Processors) and _first_proc_ (address of first Processor) states to address the Processors. The size of the generated _SteerEvent_ message is controlled by the state _st_packet_size_. After the _SteerEvent_ message has been sent, the Supervisor creates another one after time _time_to_produce_.

In the testbed system the Supervisor is an emulator. which does not necessarily have an RoI Builder input (level-1) and therefore can be run in such a way that it produces a fake level-1 identifier. The _time_to_produce_ state characterises the time to produce this identifier and is used in the comparison of the model with the testbed measurements. In the final system the time to read in the event identifier from the RoI Builder will replace this _time_to_produce_. The Supervisor will repeat the following actions: produce a new event, check to see if the event can be sent, send the event - until the _outstanding_in_this_super_ limit is reached at which point, events continue to be produced but cannot be sent and are queued instead.

If the _outstanding_in_this_super_ limit has been reached the Supervisor will first check to see if a

DeleteData message needs to be sent to the ROBs. This is not currently used because broadcast messages have not yet been implemented in the switch. The proposed implementation is described in the following paragraph. The receive queue is then checked. It takes a certain time, defined in the state _time_to_receive_ for the Supervisor to receive the _SteerComplete_ message from the receive queue. Total latency for the event can then be calculated by the Supervisor. Once an event is completed the number of outstanding events is decremented and the Supervisor can inject another event into the system from the queue of produced events.

The _DeleteData_ message is implemented in the following manner. After receiving a given number of

_SteerComplete_ messages, specified by the _packing_ state, the supervisor will generate aDeleteData message which will be sent to all ROBs which will cause them to delete all data for the given events. The time taken to add an event to the list of events to be deleted is given by the _id_process_time_ state. As stated previously the _DeleteData_ message is currently not dispatched.

Output from the Supervisor node consists of a histogram giving the total latency for events in the system, i.e. the time between sending a _SteerEvent_ message and receiving the corresponding _SteerComplete_ message. The event rate is also calculated as the total number of events processed in a simulation run divided by the total amount of simulated run-time.

### Implementation of the Message Library

Figure 3: Input and Output messages for Supervisor

#### 3.2.2 Processor Emulator

_N.B. In this document the Processor is also referred to by its alternative name of "Steering"_.

The various messages a Processor node can send and receive are summarised in Figure 4.

On receiving a SteerEvent message from a Supervisor or an ROData message from a ROB the Processor queues the message.

If the Processor is not busy the queue is serviced by the receive thread. It takes the thread a certain time, defined by the state _receive_event_time_, to receive a _SteerEvent_ message from the queue. The thread takes a certain time defined by the state _receive_roi_time_ plus a data-size dependent time defined by the state _ProcessorRoiReceiveFloat_ to receive an _ROIData_ message. If a _SteerEvent_ message is received from the queue a check is made to see if there is an available worker thread to process the event. If all worker threads are busy the _SteerEvent_ message is put in a queue for later analysis when a worker thread becomes free.

If a worker thread is free it requests the current event from the Event Generator object. For testbed studies a dummy event is provided, however for the full system simulation described in section 7 a fully engineered physics event is provided. More details on the physics event generator can be found in section 3.3.1. The worker thread traverses the event structure and for each feature generates RoiDataRequest messages to be sent to the ROBs to gather the necessary RoI data to reconstruct the feature. The Processor uses a lookup table to determine which ROBs are intersected by the RoI. _RoIDataRequest_ messages are only generated for these ROBs. The lookup table is generated from a configuration file which specifies, for each sub-detector, the eta-phi regions covered by each ROB. A more detailed description of the detector configuration files is given in section 3.3.2. For the testbed simple configuration files, not corresponding to a sub-detector, are used.

Each _RoIDataRequest_ message is scheduled to be output after a time defined by the state

_ROBroiDataRequest_prepare_time_. After sending all the _RoIDataRequest_ messages required for a particular

Figure 4: Input and Output messages for ProcessorRoI the worker thread goes to sleep until it is sent the corresponding _RoIData_ messages.

When the receive thread finds an _ROIData_ message in the input queue it checks to see if it has received all the _ROIData_ it was expecting for the feature. If this is the case the relevant worker thread is woken up. The worker thread adds the time taken to run the algorithm to process the feature, defined in the _fex_process_time_ state, to the total time needed to process the event. If there are other features to be reconstructed in the event, the ROI data collection sequence is repeated until all features for all steps in the event have been obtained. The _SteerComplete_ message is then sent back to the Supervisor. The _SteerComplete_ message is scheduled to be sent after the cumulative algorithm time needed to reconstruct all the features.

The number of worker threads in the Processor is defined by the state _nTreads_. RoI data collection can be switched on and off using the _dataCollection_ parameter.

Currently the Processor outputs two graphs. The first is a plot giving the length of the input event queue as a function of time. The second is a histogram showing the Processor latency. This is defined as the time between the Processor receiving the _SteerEvent_ message and sending the _SteerComplete_ message.

Further information on the Processor star can be found in the implementation documentation[6].

#### 3.2.3 ROB Emulator

The various messages a ROB node can send and receive are summarised in Figure 5.

Receipt of a RoIDataRequest message causes an RoIData message to be generated and scheduled for output at the time taken for the request to be prepared after the ROB next becomes available (after time, time_to_reply_on_RoIDataRequest). This time is split into two parts: a constant part defined by the state variable _request_prepare_time_ plus a data-size dependent part characterised by the state variable _ROBFloat_. The _fragment_size_ parameter is dependent on the type of detector the ROB is serving.

Receipt of a DeleteData message only affects the scheduling of _RoiData_ messages by adding a delete handling time (defined in the _delete_handling_time_ state variable) to the time at which the ROB will be available for each event to be deleted. Note that _DeleteData_ messages are not currently used.

### Implementation of the nodes

Figure 5: Input and Output messages for ROB

## 3.3 Supporting Software

### 3.3.1 Implementation of the Event Generator

In the level-2 trigger system it is foreseen that the Supervisor will send _SteerEvent_ messages containing identifiers of the level-1 RoIs to be processed for each particular event. In the Ptolemy simulation the _SteerEvent_ message only contains the event number of the event to be processed. On receipt of the _SteerEvent_ message, the Processor obtains the list of RoIs in the event from a separate "Event Generator" object. This is considered equivalent and has allowed the Event Generator to be written as a separate module.

The Event Generator module is responsible for providing a fully engineered physics event to the simulation program. As input it uses the level-1 trigger menu[7] for high-luminosity. The menu item is chosen at random according to the frequency at which it occurs and the correct number of RoIs generated in random directions. These are then supplied to the Processor node on request.

Currently the event generator is only used to simulate the first step in the processing of an event at the stage of the level-2 trigger. This is the verification step, where data from RoIs supplied by the level-1 trigger are analysed from the muon chambers and calorimeters only. It is foreseen to upgrade the event generator to provide information to allow the full event processing to be simulated.

### 3.3.2 Detector Configuration Files

The configuration files describe the eta-phi range covered by each ROB in a particular detector. They are used by the Processor nodes to determine which ROBs have been intersected by a particular RoI. So far, for the full system simulation, only the configuration files for the calorimeters and muon detectors are used. The format for the files is simple. Each line corresponds to 1 ROB giving the phi range (minimum phi, maximum phi) and the eta range (minimum eta, maximum eta). The configuration files correspond to the ROB-detector mappings determined in previous studies[8]. A configuration file is also available for the SCT and will be used when additional trigger steps are simulated. The format here is more complicated because it is important to take the z spread of the interaction region into account for the inner tracking detectors. Instead of an eta range for the ROB it is necessary to give the r and z coordinates of the SCT modules to which the ROB corresponds. The phi range is given as before. Configuration files for the remaining detectors are still to be generated.

### 3.4 Implementation of Amessage-Ethernet Interface

A clear aim of the modelling effort is that the level-2 trigger nodes should communicate via simple _Amessages_. They should not need to know any details of the hardware, transport and network components of the system. For example, they should not have to worry about network timing and packet re-tries. Similarly, the network should not need to know the contents of the _Amessages_ sent between nodes. A network interface has been implemented as a Ptolemy star which converts _Amessages_ into _GigEPackets_ (a class representing Ethernet packets derived from the Ptolemy Message class). This is part of a project[9] to simulate network components of the level-2 trigger. Each of the nodes described in the previous sections is connected to the network via one of these interfaces. A schematic showing the internal workings of the _GigEIntFace_ star is shown below in Figure 6.

On receipt of an _Amessage_ from a node, a set of simulated network packets is sent to the specified destination over the network using the _GigELink_ object. The number and length of the Ethernet packets match the length of the _Amessage_. The _GigELink_ object has an output queue in case it is still sending a packet. Although "real" data need not be sent the network must be kept busy for the correct period of time.

If an incoming Ethernet packet is a data packet, it is stored until all the packets for an _Amessage_ have been received. The _Amessage_ is then sent to the associated node after the time taken to pass through the interface. The _GigElink_ object has an input queue, in case packets arrive before the interface has given the message to the local system.

If reception of a packet from the Ethernet causes the _GigELink_ input buffer to pass the "high water mark", an Ethernet PAUSE packet is sent to the originator. If the incoming Ethernet packet is a PAUSE packet, sending of further packets is delayed by the time given. If the time is set to 0 the interface can send again.

The bandwidth or data transfer rate of the link connecting the interface to the network and the interface to the node can be specified. Histograms and statistical output is provided to show:

\(\bullet\) Bandwidth utilisation

\(\bullet\) Number of data and control packets

\(\bullet\) Number of data and control bytes

\(\bullet\) Queue Lengths

\(\bullet\) Packet transfer times over the link

The following network standards are implemented:

\(\bullet\) IEEE 802.3z for 100 Mbit and Gigabit Ethernet

\(\bullet\) IEEE 802.3x for Ethernet flow control

It is foreseen to allow extensions to allow flow control between the network interface and node.

### Supporting Software

Figure 6: Schematic of GigEIntFace Star internals

## 3.5 Implementation of the Ethernet Switch

Two Ethernet switch models have been developed and are described in the following sections.

#### 3.5.1 Simple ("Manchester") switch model

A switch has been developed in the context of the same project referred to in the previous section using the same classes. A schematic of this switch is shown in Figure 7.

The switch does not contain Amessage-Ethernet interface components as described in the previous section as it operates only at the the Ethernet packet level.

On receipt of a packet, the output port to which the packet is to be sent is determined. The time of arrival of packets from the network must be given to the switch as that of the start of the packet to allow for "worm hole" routing. A range of addresses is allowed on the output ports to allow switches to be cascaded. The switch is able to "learn" which network interface addresses are accessible through each switch port. The switch must inform all output ports when a new address is learnt (except the port that provided the information). The switch allows for different switch architectures to be implemented e.g. extra queues or backplanes inside the switch. The bandwidth of the network connection can be specified. Similar histograms and statistics to those provided for the _GigEIntFace_ star are available. The switch implements the same network standards as the _GigEIntFace_ star.

This switch model can be used in its own right or as a framework for a more sophisticated switch model. Such a model has been developed and is described in the next section. The model replaces the _GigERouter_ object, but still uses the _GigELink_ object to implement the Ethernet protocol.

#### 3.5.2 Parameterized switch model

The parameterized model of the switch[10] assumes a modular architecture. The switch is assembled out of modules with an arbitrary number of ports per module (this is one of the configuration parameters). Frames can be transferred either between ports on the same module (intra-module communication) or between ports on different modules (inter-module communication). For intra-module communication the switch uses

### 3.5 Implementation of the Ethernet Switch

Figure 7: Ethernet Switchshared memory within the module. For the inter-module transfers the switch uses a model of a backplane transfer medium (usually modelling the trunking crossbar switch, but other models can be implemented and incorporated).

The parameterized model of the switch assumes that there is a model of the low-level communication layer, which handles traffic on the communication medium according to the Ethernet protocol and delivers frames to the parameterized model. This is provided by the _GigElink_ class used in the implementation of the _GigESwitch_ stars referred to in the previous section. Frames delivered to the parameterized model should allow access to the Ethernet control information: source address, destination address. The parameterized model holds a frame for the time necessary to model the transfer through the internals of the switch. At the end of that time the frame is put into the output queue. The switch checks with the _GigELink_ module whether the outgoing link is busy. If the link is idle, the switch pulls the frame out from the output queue and requests the _GigELink_ to send it. If the outgoing link is busy, the switch holds the frame in the output queue and awaits the signal from the _GigELink_ to indicate that the transfer has completed. At that moment, the _GigELink_ is given the frame from the output queue to send it out.

The basic principle of the operation of the parameterized model assumes that any limitations in resources in

### Implementation of the Ethernet Switch

Figure 8: Parameterized model of the switch

the switch can be modelled as input queuing. When a frame arrives at the switch a check is made as to whether there are enough resources to transfer the frame to the output queue of the destination port. When the check is positive the frame is inserted into the transfer queue. In the transfer queue the frame waits for a time corresponding to the time it would take a frame to traverse the internals of the switch without any congestion. When the check is negative the frame is put into the wait queue where it stays until resources become available (usually when another frame leaves the switch). If resources become available the frame is pulled out from the wait queue and inserted into the transfer queue. When the transfer time elapses the frame is pulled out from the transfer queue and inserted at the end of output queue of the destination port. In such a scenario, the backplane is a fully nonblocking medium for frame transfer between modules. Frames may experience limitations in getting to the backplane, but once they get access there is nothing that may affect their transfer to the destination output queue.

The decision as to whether a frame can be inserted into the traffic queue or into the wait queue is based on the switch resource calculations. The resources are characterized by the following parameters:

1. _Max Loads_ (streaming measurements) are maximum loads (in MB/s) the switch can handle without dropping frames or activating a flow control mechanism at its inputs.
2. _Transfer Bandwidth_ is based on the measurement of the time it takes for the frame to be transferred through the switch (Ping-Pong).

Each frame inserted into the transfer queue reserves a portion of the _Max Load_ proportional to the _Transfer Bandwidth_. When _Max Load_ has been fully reserved, the frame has to go into the wait queue.

On most switches the _MaxLoad_ gradually decreases as the frame size shrinks (the drop is due to the maximum number of frames the switch can handle and that number changes with the size of frames). On the other hand, the bandwidth assigned to frame transfer is fixed (_Transfer Bandwidth_). To calculate resource availability (using fixed _Transfer Bandwidth_ compared to smooth _MaxLoad_) the concept of _Bandwidth_ _Filler Frames_ was invented and adopted. The decrease in _MaxLoad_ due to shorter frames is modelled by inserting artificial frames, which only exist internally in the switch. Artificial frames reserve the switch's bandwidth, making the switch togher to get through for the real frames. This results in a drop of the _MaxLoad_ for real, shorter frames (what is actually measured). This feature is currently not used in the model described in section 7 (Full System Simulation).

## 4 Using the level-2 stars in simulations

The Ptolemy GUI has been used to create a number of simulation schematics. Firstly, each node has been turned into a galaxy by attaching a _GigEIntFace_ star to the corresponding node star. This closely matches the physical world. An example showing the Processor galaxy is shown in Figure 9.

A simple configuration consisting of one Supervisor, one Processor, two ROBs and a 4 port Manchester switch star (_GigE4Sw_) is shown in Figure 10.

Studies described in the following sections have been made to tune the various delays and to model a large-scale system (see section 7, Full System Simulation).

## 4 Using the level-2 stars in simulations

Figure 10: **Ptolemy GUI display showing simple configuration**

Figure 9: **Ptolemy GUI display showing Processor galaxy**

## 5 Characterisation of nodes on Ethernet testbed

In the Pilot Project Ethernet Program, the required functionality of the different components of the ATLAS level-2 system has been implemented using COTS (commodity off-the-shelf) processors/computers. The computers run the LINUX operating system with the Reference Software[4]. Nodes communicate over an Ethernet network using low-level Ethernet communication software together with the low-level thread switching scheme[11].

In modelling the ATLAS level-2 system we want to characterise the functionality of each component with simple transfer functions defining the time it takes for the node to respond to an incoming message. The transfer functions may be defined using PDFs (Probability Density Functions) or by simple timing equations. The latter was selected for our modelling, as collecting a huge number of measurement points necessary for the PDFs was found to be impractical.

Using the computers in our tests we found that the performance of a node depends mainly on the speed of the CPU. Therefore, in models of all the nodes (apart from the switches and the Ethernet interface stars) we use an additional state _cpuSpeed_. This state is used to scale the performance of the node depending on the actual CPU speed used. All timing parameters used in the nodes are calibrated to a 300 MHz CPU. Nodes use the _cpuSpeed_ state internally to rescale their response time.

A number of simple configurations have been run on the Ethernet testbed in order to characterise the delays in the various nodes. Tests have been run in accordance with the Pilot Project Measurements Plan[121].

A TITAN4 switch from BATM was used for the Ethernet testbed measurements. The TITAN4 switch is a store-and-forward switch with a modular architecture. The chassis can house up to 4 modules. A module either has 8 ports with Fast Ethernet links or a single port with a Gigabit Ethernet link. For the Ethernet testbed measurements, the 8 ports Fast-Ethernet modules were used. As the traffic on the network was very low (usually only 8 ports on the same module were used), a simplified model of the switch was used in the modelling of the Ethernet testbed.

### 5.1 Supervisor Characterisation

A simplified setup was used in the measurements for the Supervisor characterisation. In this setup there were no ROBs and simplified Steering software was used. The simplified Steering software returned the _steerComplete_ message as soon as the message _SteerEvent_ was received from the Supervisor.

To saturate the Supervisor a number of simplified Steerings were used. In such a situation the measured rate was limited by the Supervisor and was used to obtain values for the Supervisor parameters.

Three parameters were used to describe the performance of the Supervisor (see section 3.2.1 for a description of the parameters). The following values for the parameters were calculated (for a Supervisor CPU speed of 300 MHz):

\(\bullet\)_time_to_produce_: 37 \(\upmu\)s

\(\bullet\)_time_to_send_: 65 \(\upmu\)s

\(\bullet\)_time_to_receive_: 64 \(\upmu\)s

(see the Appendix in section 10 for more details on how these values were calculated).

In the measurements designed to saturate the Supervisor, no _DeleteData_ messages were sent by the

5 Characterisation of nodes on Ethernet testbedSupervisor to the ROBs. Since the frequency of these messages is low (one message per 100 events - resulting in 1% of events affected by the extra message), they only have a minor effect on the overall behaviour of the Supervisor.

## 5.2 ROB Emulator Characterisation

We have assumed that the performance of the ROB emulator model can be summarised by a linear equation of the following form:

time_to_reply_on_RoIDataRequest = ROBconst + ROBFloat * RoIDataSize

(see section 3.2.3 for a description of the parameters, _ROBconst_ is equivalent to the parameter

_request_prepare_time_). The equation can be used to calculate the time it takes for the model to reply with the message _RoiData_. The equation contains two parameters: _ROBconst_ and _ROBfloat_. _ROBconst_ does not depend on the size of the _RoiData_message. It only depends on the CPU speed of the processor used to run the ROB emulator software. _ROBfloat_ also depends on the CPU and is the parameter used to characterise the contribution of the size of the _RoiData_ message to the response latency.

To calculate the value of the _ROBfloat_ parameter we used measurements with a single event in the system. We based our calculations on measurements where the only change was to alter the CPU speed for the processor running the ROB software.

To calculate the _ROBconst_ parameter we used measurements taken with the testbed setups designed to saturate the ROB. In these measurements, a number of Supervisors were communicating with a number of Steerings. The latter were requesting data from a single ROB. In this case the ROB was the system bottleneck and the rate of the system was determined by the ROB performance. Data taken with different RoI data sizes and different CPU speeds (for the processor running the ROB emulator software), were used in the calculation.

Substituting the values calculated for _ROBconst_ and _ROBfloat_ the equation used to model the ROB emulator now has the form:

time_to_reply_on_RoIDataRequest [\(\mathrm{\SIUnitSymbolMicro s}\)] = 82 [\(\mathrm{\SIUnitSymbolMicro s}\)] + 0.117 [\(\mathrm{\SIUnitSymbolMicro s}\)/byte] * RoIDataSize [bytes]

The values presented are for a 300 MHz CPU running the ROB emulator software. The parameters are scaled internally by the node when a different CPU speed is used. For more details on the calculations consult the Appendix in section 10.

## 5.3 Steering Characterisation

The Steering is a much more complicated component of the model. It receives two types of message and responds with another two types of message (see section 3.2.2). In the list of proposed testbed measurements [12], none were focussed on the identification of the parts of the Steering performance which could be attributed to its response to different messages. In addition, we did not want to alter the Steering Reference Software to measure times for replies. Therefore, we were only able to measure the overall performance of the Steering. First the overall performance of the Steering was parameterised using the measurements. Subsequently, this overall time was split between the various activities of the Steering in such a way as to match the performance of the model as closely as possible to the measurements.

In a similar way to the parametrisation of the ROB emulator, we assume that the overall performance of the Steering can be simplified by the linear equation:

\[\text{time\_to\_reply}=\text{SteerConst}+\text{SteerFloat * RoIDataSize}\]

(see section 3.2.2 for a description of the parameters). The _SteerConst_ parameter does not depend on the size of the _RoiData_ message (the only message the Steering receives with variable size). It only depends on the CPU speed. _SteerFloat_ also depends on the CPU speed and is the parameter used to calculate the contribution of the size of the _RoiData_ message to the overall performance of the Steering.

To calculate the _SteerFloat_ parameter we used measurements with a single event in the system. We based our calculations on measurements where the only change was to alter the CPU speed of the processor running the Steering software.

To calculate the _SteerConst_ parameter we used measurements taken with setups designed to saturate the Steering. In these measurements, a single Supervisor was sending requests to a single Steering. The Steering was requesting data from a single ROB and once the _RoiData_ message was received it sent the _SteerComplete_ message back to the Supervisor. The Supervisor was configured to keep three outstanding events in the system. The Steering was allowed to run only one worker thread. The events were queued at the Steering and it became the system bottleneck. The measured rate was determined by the performance of the Steering. We used measurements with different RoI data sizes and different CPU speeds for the Steering to calculate the equation parameters.

Substituting the values calculated for _SteerConst_ and _SteerFloat_, the equation used to model the overall Steering performance now has the form:

\[\text{time\_to\_process\_message}\text{ [$\mu$s]}=179\text{ [$\mu$s]}+0.117\text{ [$\mu$s/byte] * RoIDataSize [bytes]}\]

The values presented are for a 300 MHz CPU running the Steering software. For more details on the calculations consult the Appendix in section 10.

The Steering software can be partitioned into 4 activities:

\(\bullet\) reception of the _SteerEvent_ message

\(\bullet\) reception of the _RoiData_ message

\(\bullet\) production of the _RoiDataRequest_ message

\(\bullet\) production of the _SteerComplete_ message

(N.B. the production of the _SteerComplete_ message in the Reference Software includes the FEX processing. However, the FEX processing was not used in the testbed measurements).

Only the second activity is dependent on the variable size of the message. Therefore, the _SteerFloat_ parameter has been assigned to the reception of the _RoiData_ message. The _SteerConst_ parameter has been divided and included in all four activities. A number of trial divisions of the _SteerConst_ parameter between different activities, were used in the model and compared with measurements from the corresponding testbed setups. The division of _SteerConst_ chosen was that which minimised the divergence between the model and the measurements.

The current division of the _SteerConst_ between the various activities is as follows:

\(\bullet\)_receive\_event\_time_ = 80 \(\mu\)s

\(\bullet\)_receive\_roi\_time_ = 20 [$\mu$s] + 0.117 [$\mu$s/byte] * RoIDataSize [bytes]

5.2 ROB Emulator Characterisation\(\bullet\)_ROBroiDataRequest_prepare_time_\(=40\)\(\upmu\)s

\(\bullet\)_fex_process_time_ (includes sending of _SteerComplete_) \(=39\)\(\upmu\)s

Note that here the _fex_process_time_ is being used as a _time_to_send_ parameter (as FEX processing is currently not modelled). In the next version of the model, the _fex_process_time_ and _time_to_send_ should be implemented as two separate states.

### Ethernet Interfaces and Switches

During the characterisation of the testbed nodes a simplified (NULL) model of the switch was used. The simplification was justified by the fact that all nodes were communicating using FastEthernet technology. The internal bandwidth of the TITAN4 switch used is bigger than 1 Gigabit/sec, making its impact on rates and latencies negligible within the accuracy of the measurements.

The simplified model of the switch allows the nodes to communicate directly without the need to use Ethernet Interface stars to convert messages to Ethernet packets and vice versa. All the latencies introduced by the Ethernet technology including the time the Ethernet packets spend on the network and the time the packets spend inside the switch were taken care of by setting the appropriate delays inside the simplified model.

## 6 Comparison of model with small Ethernet testbed configurations

In the preceding sections we presented the way in which the parameters for the individual nodes were determined. We verified these settings by modelling some other testbed configurations, not used for the node characterisation. Results showing how measured (labelled "atb") and modelled rates and latencies compare are presented in Figures 11 and 12 respectively. For both plots a system comprising 1 Supervisor, 1 Steering and 1 ROB was used. For all points shown the Supervisor and ROB were run on 400 MHz processors. The Steering was run on either a 300 MHz or 400 MHz processor as indicated in the key. The number of Steering Worker threads was varied as indicated by the key and also the ROB Data sizes as indicated. The numbers used to create the plots can be found in an Excel spreadsheet[13], the table below indicates where the numbers can be found in the spreadsheet.

Figure 11 shows how the event rate measured on the testbed compares with the rate measured with the model. Each pair of lines correspond to a set of testbed/model rate measurements for a particular configuration as a function of number of outstanding events in the system. Figure 12 shows how the latency measured on the testbed compares with the rate measured with the model. Each pair of lines correspond to a set of testbed/model latency measurements for a particular configuration as a function of number of outstanding

events in the system. (To ease interpretation, the line pairs for both plots appear in the same vertical order as the entries in the key.) These plots only represent a subset of the testbed and model measurements. All Ethernet testbed measurements and the corresponding modelling measurements (where available) can be found in the spreadsheet.

From the plots it can be seen that, for the limited testbed systems studied, the model agrees with measurement to within 5-10%. This gives us confidence that we are using a valid modelling technique. However, there is still concern that even slight discrepancies in a small system may result in a large divergence between model and reality when the model is extrapolated to a full system. For this reason further work is ongoing to tune the node parameters as finely as possible. For instance, the characterisation of the nodes was performed on a very simple configuration using only a single worker thread in the Steering. Further studies are underway to understand the possible impact of the number of worker threads on Steering performance.

### Ethernet Interfaces and Switches

Figure 11: Comparison of Measured and Modelled Event Rate

## 7 Full System Simulation

In parallel to the work currently underway to model the Ethernet testbed, work has also been carried out on the development of a full-scale model of the ATLAS level-2 trigger (at2sim) [14]. To build such a large system the Ptolemy GUI is impractical and the Ptolemy scripting language (Ptcl) has been used instead.

Ptcl is based on John Ousterhout's Tc1[15] (tool command language), which is an extensible interpreted language. All the commands of Tc1 are available in ptcl. This interface is more convenient than the graphical interface when large complex universes are being created. Ptcl extends the Tc1 interpreter language by adding new commands, such as those to load and instantiate new stars and define and instantiate new galaxies. The underlying grammar and control structure of Tc1 are not altered.

A possible architecture for the ATLAS level-2 system already proposed in [10] has been implemented and is shown in Figure 12.

## 7 Full System Simulation

Figure 12: Comparison of Measured and Modelled LatencyIn this "Central Switch" architecture, ROBs are grouped into sub-detectors (also called partitions) and are connected to the network via Fast Ethernet links. The ROBs are grouped in sub-detectors because in general they should all generate data with the same frequency and data size. The links are attached to a layer of concentrating switches. From each concentrating switch there is at least one Gigabit Ethernet up-link which is connected to the central Gigabit Ethernet switch. In the upper part of the diagram there is another layer of concentrating switches for connecting the Processor nodes. All processing nodes lie within the same partition as the expected data sizes and frequencies should be approximately equal for each Processor. Similarly, all Supervisors lie within the same partition (left-hand part of diagram). It should be noted that at the the time of writing, Ptolemy simulations of large scale Ethernet switches have only just become available and have not been fully tested. Therefore, in the simulations described in this section, we are only using the switch implementations we currently have at hand.

In the ptcl script the number of Processors to be instantiated and the number of ports on each of the concentrating switches are given by the global variables _nproc_ and _portsPerswitch_ respectively and must be initialised before creating the universe. Concentrating switches with different numbers of ports are created for each different type of sub-detector under consideration. This is because data rates per ROB differ between sub-detectors. The sum of the loads produced by the ROBs connected to the concentrating switch by the Fast Ethernet links should not exceed the maximum load on the Gigabit Ethernet up-link.

The number of ROBs is obtained by reading a list of sub-detectors from a global configuration file containing the names of all the sub-detectors currently under consideration. The number of ROBs are then read from each detector's own configuration file. A more detailed description of configuration files is given section 3.3.2.

The number of concentrating switches to be instantiated is determined from the number of ROBs in each

Figure 12: Central switch architecture

sub-detector and the size of the corresponding concentrating switch. A similar calculation is made to find the number of concentrating switches needed in the Processor and Supervisor layers. The central switch is instantiated with one port for each concentrating switch.

The full-scale model currently simulates the first step of the trigger using a multi-stage Ethernet switch in a three sub-detector (ECAL, HCAL, Muon) system. The latest configuration uses a 16 port parameterised switch (see section 3.5.2) for the Supervisors, the Processors and the ECAL ROBs. A 24 port 100 Mbit Manchester switch (see section 3.5.1) is used for the Muon ROBs and a group of Manchester 32 port Gigabit switches are used for the "central switch". In that we do not yet have a model of a sufficiently large Gigabit switch, the "central switch" is made of 3, 32 port switches. The speed of the links interconnecting these 32 port switches were set to 10 Gb/sec in order to avoid them becoming bottlenecks. The concentrating switches were attached to the central switch so that the ROB switches and the Processor switches were interleaved rather than having all the ROB switches connected to one 32 port switch and the Processors attached to the other(s). Flow control was activated for all the switches.

The plots presented in this section were produced with a system consisting of a total of 10 Supervisors, 400 Processors, 952 ROBs, 82 concentrating switches and 3 central switches. The Supervisors, Processors and ROBs use the parameters as laid out in section 5. Each Processor runs with 3 worker threads and the number of outstanding events allowed for each Supervisor (_outstanding_in_this_super_) is set equal to the number of Processors i.e. 400. The _cpuSpeed_ parameter was set to 400 MHz for all Supervisors, Processors and ROBs. This configuration has been successfully instantiated and run. An event rate of 75 kHz was achieved.

The overall level-2 latency obtained from running the above simulation is shown in Figure 14. The latency here is calculated as the time between the Supervisor sending a _SteerEvent_ to a Processor and receiving the corresponding _SteerComplete_ from its receive queue. The plot shows the combined latency for all 10 Supervisors. The x-axis is in units of 1 millisecond. The plot corresponds to 11080 completed events in 0.2 seconds of simulated running time. 0.5 seconds of simulated time is spent to initialise the switches. In total, the model took 30 minutes of real time to run on a 400 MHz PC and it allocates 60 MB of RAM.

From the plot it can be seen that the average latency is long, of the order of 50 ms. We believe this is due to the way the Supervisor has been implemented in the Reference Software. If the _outstanding_in_this_super_ limit for the Supervisor has not been reached and the Supervisor receives a _SteerComplete_ message, it is not dealt with immediately, it will be added to a receive queue. This queue will only be serviced when the _outstanding_in_this_super_ limit has been reached. In the current simulation, each Supervisor sends 400 events before checking its receive queue after which time most of the 400

Figure 14: Levelâ€“2 latency for first trigger step

_SteerComplete_ messages will be waiting. After reading one _SteerComplete_ from the receive queue the Supervisor sends another event into the system, produces a new event and checks the receive queue again where it finds another _SteerComplete_ and so on. Each Supervisor is driven at a rate of 7.5 kHz and therefore can only service the receive queue every 133 \(\upmu\)s. However, because there are already 400 responses in the receive queue, the last _SteerComplete_ message will have to wait \(\sim\)400 * 133 \(\upmu\)s = \(\sim\)53 ms before it is received from the queue.

The above hypothesis was confirmed by producing another latency plot. This time, the latency is measured as the time between the Supervisor sending a _SteerEvent_ to a Processor and the time at which the corresponding _SteerComplete_ message _arrives_ back in the Supervisor's receive queue. The plot obtained is shown in Figure 15. From the Figure it can be seen that the average latency is now significantly less (of the order of 15 ms). This is because the latency now no longer depends on the length of the receive queue in the Supervisor and hence the amount of time the Supervisor takes to service its receive queue. These results would suggest that the implementation of the Supervisor in the Reference Software needs to be reviewed.

The average latency in Figure 15 is still rather long in that FEX processing times have so far not been taken into account in the model. The distribution is also broad. A probable explanation is the round-robin scheduling used by the Supervisors to allocate events to Processors. Large event queues build up on each Processor in turn, as all 10 Supervisors send their first event to the same Processor and then their second event to the next Processor etc. This will result in longer latencies. This explanation was confirmed by making another simulation run with a different scheduling mechanism in the Supervisor. Here the first Supervisor assigns the first event to the first Processor, the second Supervisor assigns the second event to the second Processor and so on. In this way no significant queues build up on the Processors.

The latency plot for this run is shown in Figure 16. It can be seen that the distribution is now much narrower than that presented in Figure 15 and the average latency has been reduced to \(\sim\)2.5 ms. The plot shows significant structure which we still have to understand. One possibility is that each peak corresponds to an integer increase in the number of ROBs accessed for RoI data. However, this needs to be investigated further. Although this scheduling scheme is not currently used by the Reference Software the simulation clearly demonstrates how the latency can change with different scheduling. Future work will investigate the effect of other scheduling schemes on the overall latency.

## 7 Full System Simulation

Figure 15: "Realâ€ levelâ€“2 latency for first trigger step

Figure 17 shows the hit frequency (in Hz ) of the ROBs in the ECAL. The plot was obtained running the same configuration used to produce the latency plots. The x-axis is the ROB index number. This plot compares favourably with that produced by the Level-2 Paper Model [16].

## 7 Full System Simulation

Figure 16: â€Realâ€ levelâ€“2 latency for first trigger step â€“ modified scheduling scheme

Figure 17: ROB hit rate for ECAL

## 8 Conclusions

The Ptolemy modelling group have produced a large-scale environment which can either be used for modelling the Ethernet testbed or the full ATLAS system. The parameters for the model have been tuned using measurements made with the Ethernet testbed. The current simulation does not include all the detail that is taken into account in SIMDAQ, however, there is now a framework in place which allows large-scale Ethernet simulations and some cross-checks of SIMDAQ to be performed. The framework will allow, where required, the study of different possible level-2 configurations. The use of simple ROBs in this first version is an example of such a configuration.

The results shown in this document are still preliminary and further work needs to be done to refine the model. Initially this work will consist of checking and re-tuning where necessary the node parameters using measurements from the Ethernet testbed. Future work will also include the extension of the model to include all the trigger steps and detectors. Initial studies to simulate the first step of the level-2 trigger in a full system have shown that the current implementation of the Supervisor in the Reference Software and the way in which the Supervisor allocates new events to the Processors may need to be reviewed.

## 9 References

[1]"The Ptolemy Project", Department of EECS, UC Berkeley, [http://ptolemy.berkeley.edu/](http://ptolemy.berkeley.edu/)

[2]"Computer modelling of the ATLAS LVL2 trigger system", J.Vermeulen, ATLAS Internal Note, ATL-COM-DAQ-2000-031, March 2000.

[3]"The Use of Commodity Products in the ATLAS Level-2 Trigger", F.J.Wickens et al, presented by M. Dobson at the International Conference on Computing

in High-Energy Physics : CHEP 2000, Padova, Italy; 7-11 Feb 2000,

[http://home.cern.ch/mdobson/docs.html](http://home.cern.ch/mdobson/docs.html)

[4]"The Atlas Level-2 Reference Software", R.Hauser, ATLAS Internal Note,

ATL-COM-DAQ-2000-032, March 2000

[5]"Overview of message passing scheme for the ATLAS trigger Ptolemy modelling",

P.Clarke, March 1999,

[http://www.hep.ucl.ac.uk/atlas/simulation/MessageLibrary/messagepassing.pdf](http://www.hep.ucl.ac.uk/atlas/simulation/MessageLibrary/messagepassing.pdf)

[6]"Implementation of Processor Star for Ptolemy simulation", S.Wheeler, July 1999,

[http://www.hep.ucl.ac.uk/atlas/simulation/processor/proc_implementation.html](http://www.hep.ucl.ac.uk/atlas/simulation/processor/proc_implementation.html)

[7]"RoI-based event descriptions for modelling the ATLAS second level trigger",

S.George, T.Hansl-Kozanecka, K.Mahboubi, A.Watson, ATLAS Internal Note,

ATL-COM-DAQ-99-010, April 1999

[8]"Detector and Read-Out Specification and Buffer-RoI Relations for Level-2 Studies",

P.Clarke et al, ATL-DAQ-99-014, September 1999

[9]"Discussion and Specification of the "Network" Components: Network Interface,

Link Interconnect and Switch", R.Hughes-Jones, October 1999,

[http://www.hep.man.ac.uk/](http://www.hep.man.ac.uk/) rich/ptolemy/new_simulation_v06.pdf

[10]"Modeling Ethernet switches for the ATLAS LVL2 system", K.Korcyl, F.Saka, R.W.Dobinson,

ATLAS Internal Note, ATL-COM-DAQ-2000-021, March 2000

[11]"MESH: MESaging and ScHeduling for Fine-Grain Parallel Processing on Commodity

Platforms", M.Boosten et al, presented by M.Boosten at the International Conference

on Parallel + Distributed Processing Techniques and Applications : PDPTA99, Las Vegas,

USA: 28 June - 1 July 1999,

[http://home.cern.ch/mboosten/](http://home.cern.ch/mboosten/)

[12]"Pilot Project Measurements Plan", B.Blair, September 1999,