ATLAS Internal Note

LARG-NO-71

10 March 1997

The Data Acquisition System of the Liquid Argon Calorimeter Testbeams 1996

A.Bazan, M. Caprini, P.Y. Duval, P. Fassnacht, V. Kozlov, T.Leflour,

A. Le Van Suu, B. Lund-Jensen, Z. Qian, I. Seez-Wingerter

## 1 Introduction

The aim of this document is to describe the data acquisition system used during the 1996 test period (April and June) by the Liquid Argon calorimeter group when testing the 2m barrel propotype [1]. The April 96 run was a combined test bringing three detectors (Liquid Argon calorimeter, Tile calorimeter and TRT) together. The June 96 run was a stand-alone Liquid Argon calorimeter test.

During the combined run of April 1996, the three detectors were exposed to a variety of beams of different particles (electrons, pions and muons) from 10 to 300 GeV. The aim of this common ATLAS run was to study the combined response of both calorimeters, the electromagnetic and the hadronic as well as to study reconstruction properties of the tracking device. The scalability of the DAQ system and the easy integration of the three DAQ sub-systems into a common event building, allowed to obtain good quality data.

During the four weeks of data taking in June 1996, the collaboration has used standard running conditions (electrons from 50 to 200 GeV) at fixed positions on the calorimeter in order to test four different pipeline read-out systems. Owing to the flexibility of the RD13 system, a fast adaptation to each of the read-out system was easily done making possible a successful data taking.

The data acquisition system is based on the RD13 developments [2] and adapted to the specific needs of the testbeam environment [3] and of the Liquid Argon calorimeter [4].

[MISSING_PAGE_FAIL:2]

[MISSING_PAGE_FAIL:3]

## 3 Trigger and Busy Logic

The trigger and busy logic used for the combined run is described in detail in Ref [5]. It mainly uses signals which are generated at the level of the LArg subdetector and which we describe in what follows.

Trigger logic in stand-alone mode:

They are two different types of triggers: the Physic and the Calibration trigger.

The first is built during the burst, i.e. between the Start and the End of ejection signals, using the information provided by four scintillator counters.The fast coincidence also takes into account the physic validation bit set by software and is vetoed by a 5\(\upmu\)s gate (dead time) to avoid overlapping triggers. In standard running conditions, rates of typically 200 events per burst have been obtained. Randomly a so-called RNDM pulse was sent during burst. It was used to measure Pedestal values during the burst.

The second is generated out of burst and is validated by software to run pedestal, voltage or current calibration, timing and beam chamber calibration. An external clock, running at a speed of 1 kHz and vetoed by the busy, fixes the rates. Typically rates of a few 100 of events were obtained.

Both triggers exist at two different levels: at Level 1 they provide a fast gate signal (typically within 30 ns) to the T&Hs where the calorimeter signals are sampled at the maximum and at Level 2, i.e. at the level of the ADCs, where typically 40 ns later the calorimeter signals are digitized and read-out as described in the preceeding section.

During the june run we had the choice between a synchronous and an asynchronous running mode for the four different pipelines. The synchronization between the trigger and the read-out internal 40 MHz clock was obtained by a standard coincidence unit. The rates were typically four times lower compared to the asynchronous mode.

## 4 Run Control

The RD13 run control [2] was adapted to be used for this test beam DAQ. It is based on commercially available tools for message passing (ISIS) and database management system (QUID).

The run control system has been designed in order to be able to work either in a standalone mode (Liquid Argon detector alone) or in a combined mode getting data from several detectors at the same time. The corresponding (standalone or combined) configurations were stored in different databases which were read by the run control system in order to start at run time the necessary (different) processes.

[MISSING_PAGE_EMPTY:5]

#### 4.2.1 Main DAQ users control interface

This process runs on the DAQ main control workstation (HP in standalone and SUN in combined) and displays a window (see fig. 5) from which the user can:

* have a summary of the current run parameters and a dynamic status;
* open a window to have an overview of the run control modules and acquisition modules status;
* send commands to start, stop and switch the DAQ system in its various global states;
* start event monitoring tasks and open detailed DAQ status display windows;
* access to the various databases to modify or select them.

Figure 4: Run Control system in combined modeThis process sends the commands selected by the user to the global DAQ run control module. It updates the displayed informations according to messages it receives from the run control modules.

#### 4.2.2 The global Run Control module

This process runs on the DAQ main control workstation and is responsible for the coordination of local and remote actions to be done in order to switch properly the whole DAQ system from one state to another.

Figure 5: Main DAQ users control interface window

It is responsible for the VIC initialisation, opening automatically the windows to query information from the user when needed, sending the commands to and receiving state informations from the local control modules.

In the standalone configuration (June test), the global run control module was also in charge of selecting automatically the proper software database according to the run parameters selected by the user. The association between the run type (Physics or one of the five different calibration tasks) and the database was stored in a separate configuration file.

#### 4.2.3 The local Data Flow Protocol Control (DFPCRL) module

This local process runs on the RAID VME board and executes the commands it receives from the global run control module. It is free to automatically switch into a new state in case local events occur. In this case it sends an information message to its master module.

It uses the software database to get information in order to:

* initialize the front end share memory;
* create (and kill) the acquisition, calibration and recording processes in the suitable order.

It is also responsible to route towards the error message facilities the error messages that are sent by the acquisition tasks.

#### 4.2.4 The local detector control modules

These processes run on the RAID VME board which reads the electronic channels out. They execute the command they receive from the global run control module. They are free to automatically switch into a new state in case local events occur. In this case they send an information message to the master module.

It uses the software database to get information in order to:

* initialize the local event storage memory,
* create (and kill) the readout related processes.

### Message reporting system

The run control has a message reporting system which is a process running on the DAQ main control workstation to collect, filter and distribute error messages emitted by all the acquisition or control processes. These messages can be stored in a file and/or received online in a window that can be opened from the main DAQ user control interface.

## 5 Readout

### General descripton

In order to use as much as possible the same code for the "combined" and "standalone" versions of the readout module, both are based on the same Finite State Machine (FSM) [6], presented in a simplified version in figure 6.

At the initialization stage the run and detector parameters are read from the Run Control Parameters database. Depending on the run mode type (PHYSICS or CALIBRATION) a software signal (START_DAQ or START_DAQ_CALIB) is sent, provoking the transition to the appropiate state. Then the changes of state are driven by three interrupt signals which are produced by the hardware and transformed into interrupts by the CORBO module (Start of Burst, Trigger and End-of-Burst).

The FSM assumes a detector readout which takes place on an event-by-event basis. After each interrupt the corresponding event is built and other specific actions are performed. Calibration data are taken outside the burst, so the module enters the IN_CALIB state after an End-of-Burst signal (but the actions performed in the transition are the ones which are normally performed on receipt of the Start-of-Burst signal). In Calibration Mode, at each Trigger signal the event is built and then the conditions for the next calibration trigger are prepared (DAC value, delay, pulser pattern, etc).

Figure 6: **Finite State Machine diagram of the readout module**

### Data Format

For the 1996 runs the same data format was used for the combined and for the standalone modes. There are 5 event types (Start-of-Run Event, End-of-Run Event, Start-of-Burst Event, End-of-Burst Event, Normal Event). The events have a tree like structure [7] with the following levels:

* experiment
* detector (LARGON)
* sub-detectors (BEAM and CALO, only for normal events)
* data blocks (only for normal events)

At the experiment, detector and sub-detectors levels there are fixed headers and user headers. The data blocks contain a size word, a block identifier word and then the raw data. The data read from CAMAC modules are inserted in the event as short words (16 bits).

### Calibration Control

There are five calibration types (Pedestal, Test Pulse, Timing, Beam Chamber and Pipeline). Receiving the START_DAQ_CALIB signal the readout module read the calibration parameters from the detector and run parameter (RCP) database and initialize the calibration control (initial setting of DAC value, delay, pulser pattern, calculation of number of events). After each trigger the conditions for the next trigger are prepared. For pedestal calibration the maximum number of events is selected by the user. For test pulse, timing and pipeline calibrations done in automatic mode and for beam chamber calibration the run must finish after a calculated number of events. In order to inform the user about the calculated number of events, this value is calculated and then written in the RCP database. When the systems enters the RUNNING state the DAQ Main Window read the RCP database and displays the correct value for the calculated number of events.

For the automatic Test Pulse and Pipeline calibration procedures, up to three different DAC ranges can be used to perform the task. For each range the initial DAC value, the final DAQ value and the number of steps can be selected. The pulser patterns are changed automatically (12 combinations). For the Pipeline calibration for each DAC value the delay is changed between selectable initial and final delay values in a programmable number of delay steps. For Test Pulse in manual mode it is possible to introduce in the RCP database a given pulser pattern, essentially used for debugging the cabling.

### Pipeline Readout

The June 96 testbeam data have been taken using four different pipeline types (OSAM, FERMI, ALBERTA and NEVIS). In normal running conditions, the system reads only one pipeline at the time even if the readout architecture would allow to take data from all the four simultaneously.

For the OSAM, FERMI and ALBERTA pipelines, reading out the internal buffer memory of the VIC 8250 module of the pipeline crate is used to transfer data and to communicate between the central DAQ readout and the pipeline readout.

For NEVIS readout, the central DAQ controls the VME modules located in the remote pipeline crate. The parameters for the initialization can be read from the RCP database. During the June 96 testbeam the initialization (booting) of the NEVIS crate was done locally before the start of the run and at the beginning of the run the central readout checked only the booting. After each trigger the data have been read accessing the pipeline modules through VICBus.

## 6 Monitoring

The monitoring programmes are organised so that they can be used both online and offline. The purpose of the monitoring is to assure the data quality both concerning the beam itself and the proper functioning of the detector and readout electronics. The are two kinds of monitoring: calibration and physics.

The programmes are written in C; they use the HBOOK package for histogramming. The information needed by the monitoring software about the calorimeter geometry, correspondance between electronics channels and calorimeter cells as well as calibration constants were stored in a database using a ZEBRA RZ file. For some of the pipelines an ASCII file replaced the database.

### Calibration

The calibration calculations are mostly made online and the results are used immediately for the physics monitoring but also for the offline analysis. The computed constants are stored in the database which is later transfered to the atlas cluster. The calibration monitoring programmmes include parts to detect and report suspicious or faulty electronics channels to the operator. As mentioned in the previous section, there are four types of calibrations:

* Pedestal: This programme analyses data taken between bursts of particles when no signal is sent to the electronic chain; the averages and sigma are computed for each channel. These results which will be later used either to subtract pedestal or to measure the noise are stored in the RZ database.
* TestPulse: This programme analyses data taken between bursts of particles when signals from the pulsers are sent to the electronic chain. Using the relevant information concerning the pulsing pattern and the DAC value which is stored in the event, the TP programme computes averages for each configuration. Knowing the equivalence between the DAC value (Volts) and the energy, one can compute the gain of the chain. For the Track and Hold analysis, a third order polynomial fit was applied. The 3 fitted slope parameters were then stored in the RZ database. For the pipeline analysis, a LUT (look-up table) was used.

Both pedestal and TP data are regularly taken during the test beam period; typically every two hours.

* Timing: This programme analyses data taken between bursts of particles when a signals from the pulser with constant amplitude but varying delays are sent to the electronic chains. This programme was used to measure time shift between groups of cells. It was essentially used online at the begining of data taking periods to adjust the calorimeter timing.
* Pipeline: This programme was forseen to study the variation of the pulse shape with amplitude when using multiple samples data such as with the pipelines. During the June beam period, the pipeline monitoring programme was used also for the test-pulse and timing calibrations which are subsets of a pipeline calibration.

For the last three programmes it was mandatory to analyse 100% of events to be sure that all patterns, DAC or delay values were analysed. The only way to ensure this constraint with the present DAQ system was to run these programmes on the RAID processor. The tasks on the RAID must stop at the end of run, and are thus not suitable for display the obtained results and possible warning messages. The solution was to let the monitoring programme on the RAID start a display task on the HP, at the end of the run, to which the collected statistics was transfered.

### Physics monitoring

The physics monitoring has to show that the calorimeter behaves as expected i.e. that the energy resolution is reasonable. This is why the online calibration is important. The physics monitoring programme was run on the HP. Knowing the beam impact point, only cells around this point were reconstructed. The cluster (which size could be adjusted with parameters) was reconstructed and its parameters stored both in histograms and ntuple. The histograms could be accessed during the run and the ntuple was analysed at the end of the run. This technique allowed to follow very efficiently and with good enough flexibility the calorimeter behaviour. For instance the energy resolution for a 300 GeV beam was typically 1% when is was 0.7% after a more detailed offline analysis. LARG event dump

LARG Event Dump is a monitoring task which can be used online and offline to obtain informations about the event. It gives some global event information and the contents of ADC and pipeline blocks in a short word (16 bits) format. Decoded ADC values from these blocks are also available (using the corresponding conversion factor given by the database and the pipeline event formats). The online program monitors part of the events coming through the dataflow during either the physic or the calibration run. The offline version of the program allows to check events written to a file, starting from the pointed event.

### Histogram presenter

The RD3 histogram presenter was adapted to be used for this test beam DAQ. It is an online hplot system. The final hplot looks like figure 7.

The architecture is shown in figure 8. It is composed by Hserver, Hviewer and Hloader. With this structure the user can do online monitoring or offline view:

* Online monitoring : Hserver receives monitoring data from either the physic monitoring task (with sampling data) or the calibration task (with all the data) and send it to Hviewer. Many Hviewer can be run at the same time to see different plots.
* Offline view : In the same way the user can see all the histograms stored in a HBOOK format file. Hloader reads from such a file and runs the corresponding part of the online monitoring task. The data are transfered from Hloader to Hserver, then they are seen by Hviewer.

This architecture has two separate communication packages, one for the Hviewer to Hserver link, another for the monitoring_task to Hserver link. The first one is a socket-based communication originally made by RD3. The second one, an adaptation of the RD13 amsg.c module, was adapted to the testbeam needs. This link is established at first SOB signal, in the USER_SOB function, and kept permanently during the run.

Figure 8: Histogram presenter architecture

Figure 7: Example of a typical online hplot spectrum

## 7 Calibration

There is a DAQ software database file for each type of calibration. The selection of the calibration type (or the selection of the database file) is done during the start-of-calibration. Each type of calibration task is divided into two programs, the monitoring part and the display part. The monitoring part is running on the RAID processor (in the main data flow) and collects information from all the events of the calibration run. At the end of run this program writes the data into a file on disk and starts the corresponding display program on the HP workstation.

The display program reads the file, displays the collected information on the HP workstation screen, asks for storing and updating the data into the LAr database and finally for sending the output to a printer. This program creates an output file (the copy of the screen output) and prints it when it gets a positive answer on the request for printing. Immediatly after it connects to the Histogram Server allowing to look different histograms using the Hviewer.

## 8 Slow control

There is a program to control the LAr cryostat movements in the \(\phi\) and \(\eta\) directions. It uses the RS232 port on the RAID processor to communicate with the state machine which controls the electrical gears to move the cryostat.

The program opens a control window on the HP workstation (see fig. 9) and shows the current positions of the cryostat (numerical values and graphic insets). To move the cryostat, the user enters new angle values and uses the button "MOVE" to start the rotation.

Figure 9: User interface controlling cryostatThe program reflects the process of movement, changing angles numbers and pictures. A new control window will appear giving the possibility to stop the movement before arriving in the final position. The movement can be continued pressing button "MOVE" again.

## 9 Recording

Two data recording modes were available:

* Local recording on tape using an exabyte or an IBM 3480 recording device connected to the RAID processor;
* Remote recording mode using central data recording (CDR) system [8] provided by CN/PDP group. CDR lays on a Client-Server mechanism which fetchs the files locally written onto the local DAQ disk and send them over ethernet via TCP/IP protocol to the stageing machine at CN where the files are copied on the tape streams (mainly on DLT tapes).

During the 1996 testbeam runs only the remote recording was used. The CDR system gives means to check if local files have been copied to the CN tapes. The user can obtain a window displaying these informations (files copied on shift directory in CN and tapes written). In order to read data written on tapes, tools are available on the Atlas workstations to get files [9]. These tools make links between the user's directory and the stage disk at CN.

## 10 Some results

As already mentioned the described DAQ system was used in a combined (April 96) and in a stand-alone (June 96) mode. It was tested successfully in different run conditions, reading out different electronics with different trigger and busy logics.

We briefly mention here some of the nice results we obtained during these two periods.

During the first period the aim was to measure the global response of both the EM and the hadronic (Tile) calorimeters to different particles (electrons, pions and muons) in the energy range 10 to 300 GeV. The obtained resolution as a function of the pion energy is shown in FIGURE 10., together with some simulation results. There is a good agreement between the simulation and the data. Further results concerning linearity, longitudinal leakage and e/\(\pi\) separation can be found elsewhere [10].

During the second period in June 1996, the aim of the run was to test four different pipeline systems. Three out of four were analog systems when the fourth was digital. With all the four systems we have taken calibration and physics data in a few reference regions of the Liquid Argon calorimeter. The Front-End electronic as well as the shapers were identical for all the systems. In the energy range 20 to 200 GeV, we have measured the resolution performances in order to compare the intrinsic performances of the pipelines. As already mentioned, the DAQ system was flexible enough to cope with the various conditions, essentially for what concerns the calibration facilities (including online monitoring,updating of the databases,...). This permits to get already online a first idea about the quality of the system. A detailed offline analysis was performed by the different groups involved in this activity and the main conclusions of these analysis can be found in the Technical Design Report, Chapter2. To illustrate this, figure 11 shows the energy spectrum for 200 GeV electrons as obtained with the OSAM (analog pipeline version) setup and the energy spectrum for 150 GeV electrons as measured with the Fermi (digital pipeline version) setup.

## 11 Appendix

A user's guide for the people on shift describes all the necessary steps and parameters to run and control the data acquisition system [9].

Figure 11: Energy spectrum for 200 GeV (left) and 150 GeV (right) electrons obtained with an analog (OSAM) and a digital pipeline readout system, respectively.

Figure 10: Energy resolution for pions measured with the combined Larg-Tile set-up compared to simulation results.

## References

* [1] D.M Gingrich, Nucl. Instrum. Methods, A364 (1995) 290.
* [2] RD13 Collaboration, Status Report of a Scalable Data Taking System at a Testbeam for LHC, CERN/LHCC 95-47, 1995.
* A Proposal for a Flexible and Adaptable System, RD13/TN124, March 1994.
* A Test Beam Data Acquisition System for the ATLAS Liquid Argon Calorimeters
- Draft-1, 15 May 1995.
* [5] M. Caprini et al, The Data Acquisition System of the ATLAS Combined Testbeam April 1996, ATLAS T/DAQ Note 58.
* [6] G. Mornacchi, G. Polesello, M. Niculescu, The DAQ System for the Testbeam of ATLAS Subdetectors, RD13/TN169, February 1996.
* [7] G. Ambrosini, G.Fumagalli, M.Huet, Event Format Library, RD13/TN109, August 1994.
* [8] B. Panzer, M. Niculescu, Central Data Recording, RD13/TN160, November 1995.
* [9] M. Caprini, V. Kozlov, A. Le Van Suu, Z. Qian, DAQ Shift Guide for LARGON Runs, June 1996, [http://atlasinfo.cern.ch/Atlas/GROUPS/LIQARGON/TESTBEAM/](http://atlasinfo.cern.ch/Atlas/GROUPS/LIQARGON/TESTBEAM/) TBDAQ.
* [10] Z.Ajaltouni et al.,CERN-PPE/96-178.