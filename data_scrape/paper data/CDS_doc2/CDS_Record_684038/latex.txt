**Further Studies and Optimisation of the Level 2 electron/photon FEX Algorithm**

**Tara Shears**

**University of Liverpool**

**Saul Gonzalez**

**University of Wisconsin**

**Abstract**

Investigations into data transfer volume, execution time and background rejection have been performed on the level 2 electron/photon feature extraction algorithm, as implemented in version 01-04-00 of the level 2 trigger reference software. Two approaches have been used: applying an energy threshold to calorimetric cells; reducing the RoI size. Benchmarking and performance for both studies are given, including dataflow studies using realistic ROB maps. Further possibilities for improving performance with sequential processing are included.

[MISSING_PAGE_EMPTY:2]

## 1 Introduction

This note summarises studies to improve feature extraction algorithm execution time and reduce data transfer overheads, without harming the efficiency for finding electrons with the second level trigger. Three approaches have been employed, which reduce the amount of data that needs to be considered in the level two trigger RoI. The first approach uses only the cells in the RoI that have a non-negligible energy deposit. The second approach reduces the extent of the RoI. The third approach takes advantage of the modularity of the algorithms to reject an event as soon as it is possible. As the algorithm execution time is a strong function of the number of calorimetric cells considered, and the average energy of an RoI cell is tiny, these approaches offer a way to maintain performance whilst reducing data transfer volume and execution time.

The datasets used for the measurements, and general definitions of the default RoI size, benchmarking conditions and performance, are given in Section 2. The performance of the existing electron/photon calorimetric feature extraction algorithm, against which all results will be compared, is described in Section 3. The studies to improve this performance follow; threshold energy studies in Section 4, and RoI reduction studies in Section 5, ROB-RoI studies in Section 6, and sequential selection studies in Section 7. Finally, conclusions are given in Section 8.

## 2 Definitions and Datasets

### Benchmarking

All benchmarking results have been obtained using a 300 MHz Pentium II PC, running Linux. Optimisation of code is turned on in compilation. The wallclock time, rather than process time, has been used to determine execution speed as this has greater accuracy. Unfortunately this also implies that other processes running on the CPU will contribute to the time taken for the algorithm to run. It should therefore be noted that the intrinsic performance of the algorithm may be slightly better than the results quoted here.

It should also be noted that only the time taken for the trigger quantities to be calculated in the feature extraction algorithm is quoted. No times for data preparation are included in the results. It is assumed that this step will take place, and be optimised and measured, elsewhere.

### Performance

The performance of the electron algorithm is defined as the background rejection for a given signal efficiency. In order to determine the best rejection factor for each efficiency the four electron discriminant variables are scanned to obtain the optimal set of cuts. This procedure is described in [9-3]. A plot of efficiency versus jet rejection is given in each case to quantify this. Note that no such plots are given for photons. The background for photons is the same at the same threshold; we have shown electrons as the thresholds are generally lower.

### Rol Size

The standard level two RoI size is \(0.4\times 0.4\) in \(\Delta\eta\times\Delta\phi\). The number of calorimeter (presampler, electromagnetic, hadronic and crack-scintillator) cells contained in the RoI depends on the \(\eta\) coordinate of the RoI centre and the nature of the particles passing through the calorimeters. In general, at low luminosity, an RoI contains of order 250 cells for a single electron, and nearer 400 for an electron in a jet environment.

### Datasets

All measurements contained in this note were obtained using ASCII files produced by ATRIG. Each file contains both RoI and digitisation information. An event is written out if it passes the level one trigger criteria. Samples of about 7400 single electron RoIs (\(p_{\mathrm{T}}=20\) GeV) and 4800 electron RoIs in a jet environment were used for the low luminosity (ie. without pileup) measurements. Samples of approximately 1000 events for both single electrons (\(p_{\mathrm{T}}=30\) GeV) and electron RoIs in a jet environment, which include the effects of pileup, were used for the high luminosity measurements. Further information on all of these samples can be found in **[http://www-wisconsin.cern.ch/](http://www-wisconsin.cern.ch/)\(\sim\)atsaul/egamma/data/atrig/**.

## 3 Performance of the Existing Algorithm

The electron/photon feature extraction algorithm in the CVS, version 01-04-00, has been documented elsewhere [1]. At present the algorithm execution time is 0.14 (0.26) ms for single electrons and 0.21 (0.50) ms for electron RoIs in a jet environment at low (high) luminosity. All cells in the RoI are transferred. In terms of performance, a level 2 accept rate of approximately 1.1 kHz is obtained for an electron efficiency of about 90%. The studies detailed in this note aim to improve the algorithm execution time by reducing the number of cells that need to be transferred to level 2, without significantly degrading this performance.

## 4 Studies of Applying Energy Threshold Cuts to Cells

### Description

The average energy of an RoI cell, shown in Figure 4-1, is very small indeed.

Calculations of electron discriminants are governed by a few tens of cells which contain the majority of the energy deposit. It seems plausible that if cells of below a certain energy are suppressed from the calculation, little difference would be made to the eventual performance. In this investigation the thresholds tested (given in multiples of the expected electronic noise), are 0.0, 0.1, 0.2, 0.5, 1.0, 2.0, 3.0, and 5.0 sigma. The particular values for the expected noise have been taken from ATRIG.

In order to prepare for the data for these investigations the entire RoI is looped over, cell by cell. The expected electronic noise, which is a function of calorimeter type and \(\eta\) position, is calculated for each cell. If the cell fails the threshold cut it is discarded. In this way a smaller sample of RoI cells is obtained for input to the feature extraction algorithm.

The algorithm execution time is a strong function of the number of cells contained in the RoI, due to the number of loops over RoI cells in the feature extraction calculations. Figure 4-2 shows the algorithm execution time as a function of threshold cut (and hence the number of RoI cells). The lines shown on the plot are one dimensional polynomial fits to each set of data (single electrons at low luminosity, electrons in a jet environment at high luminosity, etc). It can be seen that the execution time decreases linearly as the number of cells to be considered are decreased. The fit shows some scatter, which is thought to be due to using wallclock time to measure

Figure 4-1: Cell transverse energy for high luminosity electrons.

the algorithm execution time. However, it is clear that reducing the number of cells considered in the feature extraction calculations reduces the execution time.

### Performance

The algorithm performance as a function of threshold cut is shown in Figures 4-3 and 4-4, for low and high luminosity respectively. Note that a finite number of bins are used in the performance calculation, hence the curves shown in the figure are not smooth. Note also that when pileup is added negative energies may be stored in the RoIs; applying a threshold cut automatically rejects those cells with negative energy.

It can be seen that in order to maintain a reasonable performance at low luminosity (ie. close to 90% efficiency for an event rate of 1.5 kHz), a maximum threshold cut of 1 sigma should be applied. At high luminosity an efficiency of only \(\sim\)50% is achievable with an event rate of 1.5 kHz. If one wishes to retain an efficiency of between 80 and 90%, an event rate of 2.5 - 3.5 kHz results. Again, at high luminosity, a maximum threshold cut of 1 sigma should be applied.

Figure 4-2: (Left) Number of calorimetric cells contained in the RoI versus algorithm execution time as different cell energy threshold cuts are applied for (solid circles) single electrons, (squares) electron RoIs in jets, both at low luminosity, and (triangles) single electrons and (open circles) electron RoIs in jets, both at high luminosity. The threshold cuts applied, in terms of multiples of expected electronic noise, are 0.0 (top rightmost point),0.1, 0.2, 0.5, 1.0, 2.0, 3.0, 5.0 (bottom leftmost point) in each case. (Right) Number of calorimetric cells contained in the RoI versus threshold cut applied to the RoI cell energies. The same key as before applies.

Figure 4: Performance as a function of cell threshold cut at high luminosity. Each curve is labelled by the corresponding algorithm execution time.

Figure 4: Performance as a function of cell threshold cut at high luminosity. Each curve is labelled by the corresponding algorithm execution time.

### Implications

Although applying a threshold cut can reduce execution time, it need not necessarily reduce data transfer volume. The "quanta" of data transferred is governed by Read-Out Buffers (ROBs), and varies according to position in the detector. However, recent proposals with "active ROBs" [9-4] may allow preprocessed data to be transferred instead of the full ROB data. In order to evaluate the impact on the amount of data that needs to be transferred a further study of the ROB layout must be performed (see Section 6).

## 5 Studies of Reducing the LVL2 Rol Size

### Description

The motivation for this study is the same as before: reducing the number of cells can reduce the volume of data to be transferred to level 2 and reduce the time taken for the algorithm to run. This study considers whether the full RoI size of \(0.4\times 0.4\) is needed. Different RoI sizes (\(0.3\times 0.3\), \(0.2\times 0.2\), \(0.1\times 0.1\)) are used to examine the effect on performance and algorithm execution time.

In order to prepare the data for this study, all cells in the RoI are examined. If the cell falls within the desired RoI size it is retained, otherwise it is discarded. The centre of the RoI is taken from level 1 information; Figure 5-1 shows that this position is usually close to the level 2 cluster centre, which is calculated with the full granularity information from the calorimetry. In this way a smaller sample of RoI cells are passed as input to the feature extraction algorithm.

The algorithm execution time is shown as a function of the number of cells contained in the RoI under investigation in Figure 5-2, together with the number of cells contained in the RoI as a function of RoI size. The lines shown on the former plot are one degree polynomial fits. It can be seen that cells are distributed almost uniformly over the RoI, and that the algorithm execution time is a strong function of cell mulitplicity, as before.

### Performance

The performance of the algorithm is shown in Figure 5-3. It can be seen that approximately 100 to 200 Hz of event rate are added, at 90% efficiency, when the RoI size is decreased. At 95% efficiency the difference in event rate is negligible. The slightly better preformance of an \(0.1\times 0.1\) RoI size compared to \(0.2\times 0.2\) is not yet investigated; the effect is small and will be examined later.

Figure 5-1: Difference between the level 1 Rol centre and the level 2 cluster centre for eta(left) and phi (right), for a single electron with no pileup.

Figure 5-2: (Left) Number of calorimetric cells contained in the Rol versus algorithm execution time as different Rol sizes are examined for (solid circles) single electrons, (squares) electron Rols in jets, both at low luminosity, and (triangles) single electrons and (open circles) electron Rols in jets, both at high luminosity. The Rol sizes examined are \(0.1\times 0.1\) (bottom leftmost point), \(0.2\times 0.2\), \(0.3\times 0.3\), \(0.4\times 0.4\) (top rightmost point). The lines shown are one-dimensional fits. (Right) Number of calorimetric cells contained in the Rol versus Rol size examined. The same key as before applies.

At high luminosity, as shown in Figure 5-4, performance degrades for all RoI sizes.

Unlike the cell threshold study, no requirements are placed on non-negative cell energies and so signal separation is reduced. Now reducing the RoI size increases the event rate by approximately 500 Hz (\(0.3\times 0.3\) and \(0.2\times 0.2\)) to 1 kHz (\(0.1\times 0.1\)).

Figure 5-4: Performance as a function of RoI size, at high luminosity.

Figure 5-3: Performance as a function of RoI size, at low luminosity.

The reason that there is little difference between \(0.4\times 0.4\) and \(0.3\times 0.3\) is straightforward. All feature extraction algorithms cover an area of \(0.2\times 0.2\), at most, around the cluster centre. We have already seen that the RoI centre used in the calculations can be offset by as much as 0.05 in eta or phi, therefore an RoI of extent \(0.3\times 0.3\) will contain all information and an RoI of extent \(0.2\times 0.2\) may not; some of the discrimination power of the shape variables will be lost. This effect is magnified with an RoI size of \(0.1\times 0.1\).

### Implications

Reducing the size of the RoI rapidly reduces algorithm execution time and data transfer volume, and can degrade performance. The default RoI size was chosen to be \(0.4\times 0.4\) so that the isolation of an electromagnetic cluster could be calculated as it is in the level 1 trigger. If the RoI size were cut to \(0.2\times 0.2\) isolation would not be available as an additional feature extraction variable. Preliminary studies [9-5] suggest that although isolation can help to reject background for a low luminosity electron sample for some physics channels, it can reduce efficiency for others. Applying an isolation cut makes little difference at high luminosity.

These results imply that one can safely reduce the RoI size, depending on the extra allowable trigger rate. Halving the RoI size (ie. \(0.2\times 0.2\)) reduces the execution time by a factor of four and adds 100 (500) Hz at low (high) luminosity. It could be possible to reduce this rate. It is foreseen that the ROBs can perform some data preprocessing before sending data to level 2. If so, then the centre of the RoI would be equivalent to the cluster centre, and all feature extraction information would be included in an RoI of size \(0.2\times 0.2\), which could improve performance.

## 6 ROB-Rol Studies

### Description

Since RoIs in general can cover different detector readout regions, one or more ROBs must be readout by the LVL2 system in order to build a single RoI. Larger RoIs require more ROBs and thus contribute more traffic to the LVL2 trigger system.

The present view of the detector-ROD-ROB mappings is given in detail in [9-2]. In order to understand the effect of RoI sizes on dataflow, the implications of the mappings presented in [9-2] have been studied (for \(\left|\eta\right|\)\(<2.5\)) with a sample of fully simulated di-jet events (see Section 2.4). The LVL1 RoIs, built with the full LVL1 simulation using ATRIG [9-6], were used to define the \(\eta\) and \(\phi\) boundaries to be read out. Only electromagnetic RoIs were considered; an average of 1.2 RoIs/event were contained in the di-jet files at low luminosity. The RoI boundaries were then used, together with the detector-ROB map, to determine the multiplicity and location of the ROBs needed to assemble the requested RoI at LVL2. A sample detector-ROB map, as taken from [9-2], is shown in Figure 6-1 for the second sampling of the EM calorimeter.

### Performance

The performance effects of the ROB-RoI mappings were characterized in terms of the numbers of ROBs required to build a single RoI as a function of the RoI size. Figure 6-2 shows1 the number of ROBs for all calorimeter layers as a function of \(|\eta|\) for three RoI sizes: \(0.2\times 0.2\), \(0.4\times 0.4\), and \(0.8\times 0.8\). As expected, the ROB multiplicity increases with RoI size and the number of ROBs required for an RoI is more pronounced in the barrel-endcap transition region, where there are some ROB overlaps.

Footnote 1: The region \(|\eta|<\)0.5 has been omitted from the figures since this study was performed only for \(\eta\)-0.

An important consideration when designing the detector to ROB mappings is the fraction of time (or fraction of the LVL1 rate) that a single ROB is requested to build an RoI. This is shown in Figure 6-3 for different RoI sizes and integrated over the number of ROBs. For example, for 50% of ROBs, the maximum rate of requests for a single ROB is 3.5% times the LVL1 rate for an RoI size of \(0.4\times 0.4\). Also in this example, 5% of ROBs have a request fraction of 8.5% or more.

Figure 6-1: Mapping of ROBs in the second sampling of the EM Calorimeter for \(|\eta|<2.5\) as taken from [9-2]

### Implications

As expected, the larger the area of an RoI the more ROBs readouts are required to build the RoI. This effect from data transfer must be optimized together with the latency and physics performance effects presented in Section 5.2. In addition, the possible repercussions of large request rates from single ROBs due to large RoI sizes must also be taken into account in this optimization.

## 7 Studies of Sequential Processing

### Description

One possible strategy in reducing the overall bandwidth and latency in the LVL2 trigger is to use sequential processing. In this scheme, algorithms are modularized into logical components that work on detector elements or even readout elements. Each one of these components realizes a part of the trigger decision on its own. By carefully ordering these decision elements so that uninteresting RoIs are rejected as soon as possible, both the amount of data needed for a decision and the amount of time spent processing the decision can be reduced (w.r.t. a full algorithm execution).

In the case of the calorimeter trigger, a layer-by-layer ordered readout lends itself to a sequential selection scheme because the algorithm can approximately be factorized on a layer-by-layer basis. In this study, we only consider as modular the first two layers of the calorimeter ("Em1" and "Em2") since they provide most of the LVL2 calorimeter rejection [9-1][9-3].

### Performance

The performance implications of sequential processing have been characterized in terms of ROBs required to form an RoI and the total latency1 needed to form a LVL2 calorimeter decision. Figure 7-1 (top) shows the average number of ROBs needed to form a full e.m. RoI in the case of an all layer readout. The lower Figures show the same in the case of a sequential readout; as can be seen the average number of required ROBs is reduced by a factor of two. Reading out the Em1 layer first yields a slightly better result than reading the Em2 layer first. This can be understood from the fact that the first sampling contributes with a larger rejection than the second sampling [9-3], however, the latter has a coarser ROB granularity.

Footnote 1: The physics performance, of course, remains unchanged in this scheme.

The effect of sequential selection on overall algorithm latency is shown in Figure 7-2 for the case where Em1 is processed first (dotted) or th case of full processing (solid). The figure shows the fraction of events processed as a function of latency for di-jet events at low luminosity. As can be seen, the median processing time is four times smaller in the case of sequential selection. However, events with long processing times still contribute in both cases.

### Implications

The use of sequential selection in the LVL2 calorimeter trigger can reduce the amount of data that needs to be transferred from the ROBs to the LVL2 processors by a factor of two. This method may then free up resources to perform more sophisticated triggers, e.g. by using larger RoIs. In addition to reduced data transfers, the e.m. algorithm latency is significantly reduced, although events "in the tails" remain. These improvements do not take into account any other possible overheads that may be due to (effectively) more algorithms running in the LVL2 system.

## 8 Conclusions

Reducing the number of calorimetric cells in the LVL2 RoI reduces the time taken to calculate feature extraction quantities and the amount of data that needs to be transferred. Two methods of achieving this have been tested: applying an energy threshold to cell energies; reducing the extent of the RoI. The former method looks most promising; algorithm execution time can be reduced by a third to three-quarters for electron RoIs in a jet environment (at high and low luminosity respectively), with little degradation in algorithm performance if a one sigma threshold is applied. The latter method is not so successful, mainly because the larger cell granularity at level-1 does not allow a precise calculation of an electromagnetic cluster centre to base the RoI around, thus information is easily lost and discrimination reduced with any offset. The way that ROBs are mapped onto the detector has an important impact on the LVL2 system and must be optimized together with other system parameters. Finally, sequential processing within the LVL2 calorimeter trigger may bring relatively large reductions in both latency and data transfers.

## References

* [1] S. Gonzalez, B. Gonzalez-Pineiro, T. Shears, First Implementation of Calorimeter FEX Algorithms in the Level-2 Reference Software, ATL-DAQ-2000-020 (Mar. 2000)
* [2] P. Clarke, et al., Detector and Read-Out Specification. and Buffer-RoI Relations, for Level-2 Studies, ATL-DAQ-99-014 (Oct. 1999).
* [3] S. Gonzalez, T. Hansl-Kozanecka, M. Wielers, Selection of high-\(p_{T}\) electromagnetic clusters by the level-2 trigger of ATLAS, ATL-DAQ-2000-002 (Feb. 2000)
* [4] R.K. Bock et al., The Active ROB Complex, ATL-DAQ-2000-022 (Nov. 1999)* [9] M. Wielers, Isolation of Electrons and Photons on the Second Level Trigger, ATL-DAQ-2000-026 (Feb. 2000)
* [10] Trigger Performance Status Report, CERN/LHCC/98-15 (Aug. 1998)