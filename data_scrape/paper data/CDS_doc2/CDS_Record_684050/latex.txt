###### Abstract

The ATLAS level-2 trigger has to offer an event rate reduction of approximately 100, from an input rate of up to 100 kHz. An RoI guided scheme and a sequential selection strategy reduces the resources required. Testbeds of up to 48 nodes have been constructed using different networking technologies for the inter-node communication. This note summarises measurements obtained on these Testbeds using the Atlas Level-2 Reference Software.

**Results from the Level-2 Pilot Project Testbeds**

_A. Belias\({}^{8}\), R. Blair\({}^{1}\), R. Bock\({}^{2}\), J.A.C. Bogaerts\({}^{2}\), M. Boosten\({}^{2}\), D.R. Botterill\({}^{8}\), D. Calver\({}^{3}\), P. Clarke\({}^{10}\), G. Crone\({}^{10}\), J. Dawson\({}^{1}\), R.W. Dobinson\({}^{2}\), M. Dobson\({}^{2}\), F. Giacomini\({}^{2}\), S. Gonzalez\({}^{12}\), J.R. Hansen\({}^{6}\), R. Hauser\({}^{5}\), C. Hinkelbein\({}^{9}\), M. Huet\({}^{3}\), R. Hughes-Jones\({}^{11}\), K. Korcyl\({}^{4,2}\), A. Kugel\({}^{9}\), I. Mandjavidze\({}^{3}\), B. Martin\({}^{2}\), R.P. Middleton\({}^{8}\), S. Qian\({}^{13}\), F. Saka\({}^{2,7}\), J. Schlereth\({}^{1}\), M. Sessler\({}^{9}\), T. Shears\({}^{2}\), H. Singpiel\({}^{9}\), S. Tapprogge\({}^{2}\), A. Waananen\({}^{6}\), P. Werner\({}^{2}\), S. Wheeler\({}^{10}\), F.J. Wickens\({}^{8}\), H. Zobernig\({}^{12}\)_

\({}^{1}\) Argonne National Laboratory, USA

\({}^{2}\) CERN, Geneva, Switzerland

\({}^{3}\) DAPNIA, CEA Saclay, France

\({}^{4}\) Henryk Niewodniczanski Institute of Nuclear Physics, Cracow, Poland

\({}^{5}\) Michigan State University, East Lansing, MI, USA

\({}^{6}\) Niels Bohr Institute, Copenhagen, Denmark

\({}^{7}\) Royal Holloway, University of London, U.K.

\({}^{8}\) Rutherford Appleton Laboratory, U.K.

\({}^{9}\) Universitat Mannheim, Germany

\({}^{10}\) University College London, U.K.

\({}^{11}\) University of Manchester, U.K.

\({}^{12}\) University of Wisconsin, USA

ATLAS, Level-2, Trigger, Commodity

## 1 Introduction

The high rate of interactions in future LHC experiments places stringent demands on the trigger and data acquisition systems. The ATLAS experiment uses a three-tier trigger [1]. Level-1 [2] is based on custom hardware to reduce the trigger rate from the 40 MHz bunch-crossing rate to below 100 kHz. After a level-1 ACCEPT decision, all data for that event (1-2 MByte per event) are sent to readout buffers (ROBs) for temporary storage. The higher-level triggers (level-2 and Event Filter, EF) are required to reduce the event rate for permanent storage to \(\sim 100\) Hz.

The level-2 trigger uses Regions of Interest (RoI) guidance from the level-1 trigger to reduce the amount of data requested from the ROBs and to reduce the requirements on processing power and network bandwidth. Further computing-power and data-volume reduction is possible by using a sequential selection strategy. This strategy involves processing the data in the RoIs initially from only one or two subdetectors, and taking a decision to continue or abandon the event before processing the data in the RoIs from the other subdetectors. After each step, the event rate is reduced leading to a cut in total data volume and processing power required.

Initially small scale Testbeds were installed at various locations principally to integrate the technologies into the ATLAS Level-2 prototype software called the Reference Software [3]. Subsequently equipment from these small scale Testbeds were merged at CERN to provide Testbeds of up to 48 nodes.

This paper presents a representative set of results from these Pilot Project Testbeds, which use standard PCs and networking technologies wherever possible but also using custom items in areas where the performance required is very demanding.

## 2 The Level-2 Trigger and the Testbeds

### The Level-2 Trigger

The level-2 trigger architecture has four different components, the ROBs, the processors, the RoI builder and Supervisor, and the network. The implementation of the level-2 architecture on the Testbeds is shown in Figure 1.

The RoI builder [4] is custom hardware designed to combine RoI fragments from the level-1 processors into one event record, which it passes on to the supervisor farm. The latter consists of a small farm of general-purpose processors each with a simple custom input card for receiving the event records. The supervisors assign each event to one of the processors. The processor, possibly helped by a co-processor, performs one or more steps of data collection and analysis from relevant ROBs. The trigger decision can be issued at any step. It is returned to the supervisor which distributes it to the ROBs. Rejected events are discarded from the ROBs; accepted events are passed on to the EF for further analysis (path to EF not shown in Figure 1).

The Level-2 event network can be divided into two functional units, the control network and the data network. The former carries RoI request, ROB clear, Level-1 event and Level-2 decision messages, whereas the latter carries the ROB data reply messages. The two networks could be physically separate but are combined in the Testbeds. A third low performance network is needed for the configuration, initialisation and monitoring of the system and is represented in Figure 1 by the dashed line labelled "Ethernet".

Figure 1: The level-2 Testbed system.

### The Level-2 Requirements

In this section, the requirements of the level-2 trigger are presented. These indicative performance requirements are given by the Paper Models [5, 6] and are summarised in table 1.

### The Testbeds

Prototypes referred to as Testbeds have been set up to check that individual components (commodity items wherever possible) meet the required performance; provide information on scaling up to moderately sized systems; and to provide data for the full system computer models.

The Testbed systems vary in size from 25 to 48 nodes, however, a 96 node commercial cluster at Paderborn University 1 was also used. These systems correspond to a few percent of the final ATLAS system. Ethernet (Fast and Gigabit), ATM and SCI technologies have been studied for the network. All the Testbeds follow the Level-2 trigger architecture shown in Figure 1. In general the Testbeds used emulations for the supervisor and ROBs. Tests with a co-processor were only performed on the ATM Testbed.

Footnote 1: [http://www.uni-paderborn.de/pc2/systems/psc/index.htm](http://www.uni-paderborn.de/pc2/systems/psc/index.htm)

The Ethernet and ATM Testbeds (see Figure 2) share the same PCs, which are single or dual-processor machines with processor speeds from 200-450 MHz. The ATM Testbed also uses ten PowerPC single-board computers running LynxOS. The network equipment for ATM is a 48-port, 155 Mbit/s per port, FORE switch. For Ethernet it is three BATM Titan 4 Fast or Gigabit switches with up to 32 Fast ports or 4 Gigabit ports per switch.

The SCI Testbed has 23 single or dual-processor PCs with processor speeds of 300-450 MHz. The SCI switch is a 16 port Dolphin switch. The results from this system are reported in [7]. The Siemens cluster at Paderborn University (see Figure 3) with 96 nodes, interconnected with SCI technology has also been used. The results obtained on this cluster are reported in [8].

The various configurations that were envisaged to test the different elements in the system were collected together [9] so that measurements across Testbeds could be compared with ease.

### The Reference Software and OS

The Reference Software 2[3] is the OO C++ prototype level-2 software and has been run on all Testbeds under Linux and Windows NT, and on Solaris at Paderborn. Note that the Reference

\begin{table}
\begin{tabular}{l l l} \hline
**Parameter** & **Value** & **Comment** \\ \hline Number of ROBs & 1600 & \\ Number of Processors & \(\sim 450\) & 1000 MIPS with \\  & & B-Physics \\ Total bandwidth in LVL2 network & \(\sim 5\) GByte/s & \\ Max RoI request rate per ROB & \(\sim 14\) kHz & \\ Max output bandwidth to LVL2 per ROB & \(\sim 9\) MByte/s & \\ Max RoI Builder/Supervisor rate & 75 kHz & \\ Average number of ROBs required per RoI & 10â€“30 & Depends on RoI type \\ Number of ROBs per data request & \(\sim 4\) & If EM Calo. layers \\ Data for TRT full scan & \(\sim 80\) kByte from \(\sim 256\) ROBs & \\ Average number of sequential steps & \(\sim 2\) & \\ Event rate per LVL2 processor & \(>0.1\) kHz & \\ \hline \end{tabular}
\end{table}
Table 1: Indicative performance requirements for Level-2 components.

Software is not yet optimised. On the ATM Testbed, earlier optimised C based software [10, 11] has also been run, under Windows NT, Linux and LynxOS.

The Reference Software has three parameters of interest for the latter discussions in this document. The first is the data size returned by the ROBs. The second is the number of Worker Threads in the processors, which are the number of concurrent events that are allowed to be processed in each processor. The third is the number of outstanding events in the supervisor. The supervisor will continue to assign events to the Level-2 processors until this limit for the number of Level-2 decisions still pending is reached.

## 3 Results

### Uni-processor versus multi-processor

Tests were undertaken on the SCI Testbed to compare the processor occupancies between single processor and dual processor machines. All the machines used were identical dual processor machines, some running a uni-processor Linux kernel and some running the multi-processor Linux kernel. The configuration used one supervisor, one processor and one ROB, allowed 5 outstanding events and had a ROB data reply size of 64 bytes. The results are presented in Figure 4.

All nodes have one thread for input and were polling for incoming information. In the case where no information was ready, the input thread returned control to the other threads. The rate in the system is limited by the processor and its idle time is given in Figure 5. The other nodes all have idle times of \(\geq 20\%\). In the case of a uni-processor node, all spare CPU time is taken up by the polling loop. In the case of a multi-processor node, all spare time on one CPU is taken up by the polling loop and the idle time given in the table refers to the other CPU. The idle time is only approximate but gives an indication of the usage of the processors. Clearly the best performance seen in Figure 4 yields a rate increase of \(\sim 2\) between a uni-processor and a multi-processor node. In the setup for Figure 4, where the processor is CPU bound and where control is relinquished by the polling thread when no data has been received, the rate sustained by a dual-processor node is twice the rate sustained by a uni-processor node. In the final system the processors will be CPU bound because of the running of algorithms. However it has also been observed that if the polling thread does not relinquish control when no data has been received, little or no benefit is obtained from using a dual processor node.

### Supervisor Performance

The supervisor tasks are to get an event from the RoI builder, allocate it to a processor, receive the decision back, update the statistics, pack the decisions and send them to all the ROBs. To study the supervisor performance, the system is configured so that it saturates the supervisor.

The rate achieved as a function of the number of RoIs in the event record is shown in Figure 6. With a single RoI, a rate of \(\sim 11\) kHz per supervisor emulator is reached. In the figure the performance obtained on the ATM platform is lower than the other measurements because the supervisor emulator was running on 300 MHz whereas in the Ethernet Testbed this was 400 MHz and in the SCI Testbed it was 450 MHz. The performance across network technologies is very similar because the supervisor emulator is mainly limited by the computing task of producing randomly distributed RoIs for the events.

One of the tasks of the supervisor is to send the decisions to all the ROBs in the system (up to approximately 1600). Should the supervisor send a message to each one sequentially the event rate achieved by the supervisor will be strongly dependent on the number of ROBs in the system. However the rate of the supervisor would be independent of the number of ROBs if a hardware multicast is used when sending the event decisions to all ROBs. For the final system, the availability of multicast/broadcast capability in the network is important. Currently only ATM and Ethernet have this capability, but it's use has only been tested in the context of the ATM Testbed running the earlier C based software. The grouping or packing of the decisions into one message will also be an important means of reducing the message rate from the supervisor.

The rate versus the number of level-2 processors has been shown to be proportional to, and increase linearly with the number of processors up to supervisor saturation.

The results also show that the system rate scales with the number of supervisors. This is shown in Figure 7 for supervisor emulators on the three Testbeds and also with the RoI Builder and four supervisors on the ATM Testbed [10, 11]. A rate of 120 kHz was also achieved with 12 supervisor emulators (no RoI builder) on the Paderborn cluster (see Section 3.6, Figure 14).

### ROB Access Performance

Within the Testbeds, investigations were made into the rate at which the ROB could provide event data to the processors.

ROB performance tests use a ROB emulator running on a PC. The system is configured to saturate the ROB, and the rate of requests for many processors which can be met by a single ROB is shown in Figure 8. The performance is consistent with that expected for real ROBs and used in system models [5, 6]. For a typical data size of 1 kByte, the request rate is of the order of 10-13 kHz. The reason for the low results in the Ethernet Testbed are not yet understood and investigations are ongoing to find the cause.

Figure 8: ROB request rate versus ROB fragment size.

In addition to these tests, a prototype of the Saclay ROBin [12] was integrated into a ROB Complex (grouping of buffers into one unit) on the ATM Testbed. A ROB Complex composed of one to three ROBins was tested and the average rate of requests from the processors that can be serviced is shown in Figure 10. The ROB Complex controller was a 400 MHz Linux PC serving requests from approximately 10 processors via one ATM link. For a typical fragment size of 1-2 kByte, the service rate almost reached the bandwidth limit for the ROB controller link (16 MByte/s).

### Processor Performance

The first task of the processor is to collect data from many sources, which can be very demanding, and the second task is to process the data received. The emphasis in the Testbeds has been on the first task and the results are presented here.

The system was configured to saturate a processor using up to 16 ROBs. When collecting 64 Bytes from each ROB (see Figure 10), the processor can sustain a rate of 5-7 kHz for 1 ROB, down to 1 kHz for 16 ROBs. For a typical RoI request involving 4 ROBs, and a total level-1 rate of 100 kHz, the data collection overhead requires CPU cycles equivalent to \(\sim 30\) of todays processors. This is acceptable in view of the total farm size that is envisaged (several hundred, see table I).

The rate of events is also dependent on the fragment size collected from the ROBs. This is in turn dependent on the RoI type and the subdetector concerned. For fragment sizes of 64 Bytes to 4 kByte, the event rate sustained is given in Figure 11. The system consists of one Supervisor, one ROB and one Processor with enough worker threads and outstanding events to saturate the processor (_e.g._ two worker threads and five outstanding for the Ethernet results). For the collection of typical fragment sizes of 1 kByte and 4 kByte, from one ROB, a rate of 4 kHz and 2.5 kHz respectively is achieved.

For the RoI data collection or for the data collection for a subdetector scan (see later) it is interesting to look at the bandwidth of data that can be input into a processor when collecting from many ROBs. The setup consists of one Supervisor, one Processor and 1-32 ROBs. The fragment size collected from each ROB is 8 kByte. The bandwidth achieved into the processor on the Paderborn cluster (using MPI over SCI) with this setup is shown in Figure 13. It is clear that there is a catastrophic breakdown in the bandwidth achievable into the processor when the collection is from more than \(\sim 20\) ROBs. This might be due to the overloading of the input port of the processor. A possible cause for this overloading could be the retries in the underlying low level protocol for this technology (MPI over SCI). In this case the number of retries far outnumber the packets that get to the destination and therefore the throughput drops drastically. However, further investigation will be carried out to better understand this effect. Many technologies do not have low level retries (_e.g._ Ethernet, ATM) and congestion on a link will result in lost packets. To avoid data loss from dropped packets, other technologies either use high level protocols, like TCP/IP, to guarantee delivery of packets, or low level protocols such as hardware flow control (available on Ethernet) or constant bit rate streams (available for ATM). The low level protocols like hardware flow control would lead to the bandwidth on the link saturating at or near to the link bandwidth. This test will be undertaken on the Ethernet Testbed in the near future to verify this hypothesis. An example of tests using constant bit rate streams in ATM are shown next.

Some algorithms need to scan a complete subdetector for tracks. This data collection was simulated on the ATM Testbed by collecting data from 20 ROBs (see Figure 13). The event rate scales linearly with the number of processors and a bandwidth of 260 MByte/s and 328 MByte/s was achieved respectively for 2 kByte and 4 kByte data fragments. These high bandwidths, respectively 85% and 97% of the ATM link bandwidth, can be achieved on ATM through the use of mechanisms such as Constant Bit Rate channels, without loosing any data (note this mechanism is only applicable to pre-defined groups of ROBs). Such features are not currently available in the other technologies tested and therefore such high link utilisation is not thought possible at this time. Some algorithms will require data collection from up to several hundred ROBs (_e.g._ the TRT scan). FPGA co-processors [13] are being investigated to handle the associated computing intensive algorithms, and results from these systems are shown in the following section.

### FPGA Performance

The current Level-2 B-Physics trigger strategy is built upon a non RoI-guided scan of the full volume of the TRT detector. This involves collecting data from several hundred ROBs into one processor. FPGA co-processors connected to the processors are used to speed up the algorithms used for the TRT full scan [14]. The current FPGA system is the ATLANTIS system, based on compact PCI. The processor is a standard Intel based PC on compact PCI. The operating system used was Windows NT. The ATLANTIS system was successfully integrated into the Reference Software framework and physically into the Level-2 pilot project Testbed using the ATM network technology. This integration was possible without any changes to the Reference Software framework: the ATLANTIS system is a standard processor node to the Testbed system. The integration has shown that the FPGA algorithm quality is the same as the CPU-only implementation of this same algorithm. However the overall setup at the time of the integration did not allow any performance tests to be performed on the system.

### System Performance

The system performance with the Reference Software has been evaluated mainly on the large commercial cluster at Paderborn University. Measurements have shown the linear scaling of the system rate with the number of supervisors present (see Figure 15) and have shown linear scaling of the system rate with the number of ROB/Processor pairs added for various data sizes when collecting from one ROB (see Figure 15). For the latter setup the processors collected data from one ROB each. These measurements have also shown stable performance of the supervisor versus the number of processors, and correct operation of the Reference Software on a moderately large system.

The use of sequential selection reduces the network and processor requirements and allows more complex algorithms to be run at lower rates. To verify this and to check the effect of sequential selection, a test selection strategy was implemented with the ATM software [10, 11]. The test selection strategy is shown in Figure 15. The test setup consisted of one Supervisor, 14 ROBs, 14 processors and ran at a rate of 8 kHz with 50 % link and CPU load. The corresponding latencyplot obtained on this test setup is shown in Figure 18. The structure in the plot reveals the different algorithm steps in the sequential selection strategy. Two distinct peaks are visible; the first below 5 ms which actually is composed of two maxima corresponding to the first two sequential stages; and one very wide peak above 5 ms corresponding to the third sequential step, much suppressed due to the high rejection achieved by the first two steps. The shape of the latency curve is characteristic of a multi-stage sequential selection algorithm. The average latency is reduced compared to a single stage selection running the three same algorithms. The effect of rejection at early stages in the sequential selection is clear: 50% of events finish within 1.9 ms, 95% of events finish within 11.3 ms, 99% of events finish within 16.5 ms and the average latency is 3.7 ms. A similar test with real algorithms running at the different steps, has been tried within the Reference Software framework on the Paderborn cluster and is presented in the following section.

### Algorithms

To validate the principle of multi-step data transfers and processing a Testbed run was made on the cluster at the University of Paderborn with the Reference Software including algorithms for three detectors: Calorimeter electro-magnetic clustering [15], TRT (Hough transformation) tracking [16] and SCT Precision tracking [17]. The setup consisted of one supervisor, one ROB and one processor. Furthermore there was only one worker thread on the processor due to the fact that the algorithms are not multi-thread safe. The Supervisor and RoB emulators were pre-loaded with a data file containing 3000 jet events with no pile-up, preselected to contain at least one Level-1 EM RoI. Approximately 15% of the events contained two RoIs. The menu consisted of three consecutive steps (Calorimeter alone, Calorimeter followed by TRT and finally Calorimeter \(+\) TRT \(+\) SCT/Pixel) each selecting 20 GeV electrons. The fraction of events accepted after each step was 0.19, 0.05 and 0.02.

The latency is measured in the supervisor as the time interval between allocating an event to a processor and receiving the decision back from that processor. It includes communication, data preparation and actual processing time (steering and feature extraction). Communication delays contribute \(\sim\)500 \(\mu\)s. The RoI data size is \(\sim\)10-20 kByte, contributing another \(\sim\)300 \(\mu\)s for each RoI/detector combination. The distribution in Figure 19 reveals the sequential execution of the three algorithms: Calorimeter electro-magnetic clustering predominantly below 4 ms, subsequent TRT tracking 4-7 ms and finally SCT Precision tracking extending beyond 7 ms. The average and median values are 3.7 and 3.1 ms respectively. The effect of rejection at early stages in the sequential process is shown explicitly in Figure 19: 50% of the events finish within 3.1 ms, 95% within 7.2 ms and 99% within 10.8 ms. Had there not been the sequential aspect, all events would have run all three algorithms, thereby wasting computing power in the processors and increasing the average latency to somewhere around the third peak in the latency plot.

## 4 Conclusions

The test using a dual-processor node has demonstrated that through careful use of the thread scheduling, the performance doubles with respect to that obtained on a uni-processor node.

A prototype RoI Builder has been built using FPGAs in a highly parallel architecture. It has been integrated with a supervisor into the ATM and Ethernet Testbeds and operation with a small supervisor farm has been shown to satisfy the requirements of the Level-2 trigger (a rate of 100 kHz).

The performance of a ROB emulator has been shown to be consistent with that used in paper models and gives confidence that the network connection from the ROB Complex to the Level-2 system is attainable over the network technologies studied. The results obtained with a prototype ROBin are compatible with the current estimates of requirements, but further investigations on a more complete system are needed.

The processors have shown the ability to collect data within an RoI from a single detector at an acceptable rate. Furthermore large amounts of data, corresponding to all the data from a single detector, has also been gathered in to the processors as would be required for a full scan in one of the tracking detectors. Sequential selection strategy in the processor has been demonstrated with prototype algorithms running on a multi-node Testbed with the processor requesting data from ROB emulator nodes.

FPGA technology has shown the ability to accelerate the execution of the TRT full scan by a factor of approximately six, from the CPU only implementation. This technology has also been integrated into the Reference Software and the ATM Testbed with success, and appears to the rest of the network as a standard node.

The level-2 strategy and architecture have been successfully implemented on moderately large Testbed systems. The performance of the different components has been measured and it is now clear that commodity products can be used for the majority of the level-2 components (OS, supervisor, processors and network technology). It may, however, be necessary to use custom items for the drivers and software, and for the co-processors. The RoI builder is implemented in custom hardware because of the need to combine seven high-rate data streams.

## References

* [1] ATLAS Collaboration, "ATLAS Trigger Performance Status Report", CERN/LHCC 98-15, CERN, June 1998.
* [2] ATLAS Collaboration, "Level-1 Trigger Technical Design Report", CERN/LHCC 98-14, CERN, June 1998.
* [3] R. Hauser, "The Atlas Level-2 Reference Software", ATLAS Internal Note, ATL-COM-DAQ-2000-032, March 2000.
* [4] R. Blair et al., "A Prototype RoI Builder for the Second Level Trigger of ATLAS Implemented in FPGA's", LEB'99, Snowmass, September 20-24 1999.
* [5] M. Dobson _et al._, "Paper Models of the ATLAS Second Level Trigger", ATLAS Note, ATL-DAQ-98-113, June 1998.
* [6] J. Bystricky and J.C. Vermeulen, "Paper modelling of the ATLAS LVL2 trigger system", ATLAS Note, ATL-DAQ-2000-030, March 2000.
* [7] A. Bogaerts _et al._, "ATLAS Level-2 Trigger SCI Demonstrator Evaluation Report", ATLAS Internal Note, ATL-COM-DAQ-2000-037, October 1999.
* [8] A. Bogaerts _et al._, "Running the ATLAS Second Level Trigger Software on a Large Commercial Cluster", ATLAS Internal Note, ATL-COM-DAQ-2000-027, March 2000.
* [9] The ATLAS Second Level Trigger Community, "A Minimum Set of Measurements to be made on the Application Testbeds", ATLAS Note, ATL-DAQ-99-005, March 1999.
* [10] D. Calvet _et al._, "Operation and Performance of an ATM based Demonstrator for the Sequential Option of the ATLAS Trigger", IEEE TNS vol. 45 pp 1793-1798, August 1998.
* [11] J. Bystricky _et al._, "An integrated system for the ATLAS High Level Triggers: Concept, General Conclusions on Architecture Studies, Final Results of Prototyping with ATM", ATLAS Note, ATL-DAQ-2000-011, March 2000.
* [12] D. Calvet _et al._, "A Scheme of Read-Out Organisation for the ATLAS High-Level Triggers and DAQ based on ROB Complexes", ATLAS Note, ATL-DAQ-2000-014, November 1999.
* [13] C. Hinkelbein _et al._, "Prospects of FPGAs for the ATLAS LVL2 Trigger", ATLAS Note, ATL-DAQ-2000-006, December 1999.
* [14] C. Hinkelbein _et al._, "LVL2 Full TRT Scan Feature Extraction Algorithm for B Physics Performed on the Hybrid FPGA/CPU Processor System ATLANTIS: Measurement Results", ATLAS Note, ATL-DAQ-2000-012, March 2000.
* [15] S. Gonzalez _et al._, "First Implementation of Calorimeter FEX Algorithms in the LVL2 Reference Software", ATLAS Internal Note, ATL-COM-DAQ-2000-013, March 2000.
* [16] M. Sessler and M. Smizanska, "Global Pattern Recognition in the TRT for the ATLAS LVL2 Trigger", ATLAS Note, ATL-DAQ-98-120, June 1998.