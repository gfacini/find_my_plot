Report from the ATLAS Architecture TaskForce

**Version 2.7 (16/12/99)**

**Abstract**

In this report, the activities and conclusions of the ATLAS Architecture TaskForce (ATF) are summarised. A key part of the ATF's work has been the first attempt at a design of the global architecture for the ATLAS Off-line Software. This is contained in this document and an auxiliary report [1] and should lead to the realisation of the ATLAS Framework.

**TaskForce**

* Katsuya Amako (KEK)
* Laurent Chevalier (Saclay)
* Andrea Dell'Acqua (CERN)
* Fabiola Gianotti (CERN)

* Norman McCubbin (RAL)
* Helge Meinhard (CERN)
* David Quarrie (LBNL)
* RD Schaffer (CERN+LAL)
* Marjorie Shapiro (LBNL)
* Valerio Vercesi (INFN)
* ATLAS Management, ex officio

Associated with this Report are two **auxiliary reports**:

1. "Use-case Model and Supplementary Requirements of the ATLAS Common Framework" [2].
2. "First Attempt at Analysis and Design of the ATLAS Common Framework" [1].

## Current Status
Table of Contents

* 1 Introduction
	* 1.1 Structure of the Report
	* 1.2 Mandate
	* 1.3 General Comments
		* 1.3.1 Scope
		* 1.3.2 The Gaudi Example.
		* 1.3.3 Time-scales
		* 1.3.4 Definitions of "Architecture" and "Framework".
		* 1.3.5 The Complete ATLAS Software System
* 2 Workings of ATF
	* 2.1 Meetings
	* 2.2 Short Term Frameworks for Simulation and Reconstruction
	* 2.3 Control Paradigms
* 3 Introduction to the Architecture Design Work
	* 3.1 Approach Taken
	* 3.2 General Design Choices
* 4 Use-case based Architecture Design
	* 4.1 Development Process.
		* 4.1.1 Overview
		* 4.1.2 Application to ATLAS
	* 4.2 Use-cases
		* 4.2.1 Use-cases and Requirements
		* 4.2.2 Use-case Matrix
	* 4.3 ATLAS Common Framework Environment
		* 4.3.1 Brief Description of the Use-case
		* 4.3.2 Analysis Model
			* 4.3.2.1 Analysis Package
		* 4.3.3 Design Model
			* 4.3.3.1 Class Diagram
			* 4.3.3.2 Interface Definition
			* 4.3.3.3 Sequence Diagram
* 5 Components Expected in the ATLAS Common Framework
	* 5.1 Components Responsible for Application Configuration and Execution
	* 5.2 Components associated with the Application and Framework Managers
	* 5.3 Data Components
	* 5.4 Additional Services and Components
* 6 Detailed Discussion of Some Components
	* 6.1 Introduction
		* 6.1.1 An Example
		* 6.1.2 Granularity of Modules and Data Objects
	* 6.2 The Control Domain

\begin{tabular}{r l}
6.2.1 & Data Flow Requirements. \\  & 6.2.2 & Control Flow Requirements. \\  & 6.2.3 & Control Paradigms \\
6.3 & Event \\  & 6.3.1 & Design Choices \\  & 6.3.2 & Role of the Transient Event Data Store \\  & 6.3.3 & Ownership and Modifiability of Event Data Objects \\  & 6.3.4 & Interface to Event Data Objects. \\
6.4 & Detector Description \\  & 6.4.1 & Design Choices \\  & 6.4.2 & Issues and Requirements \\  & 6.4.2.1 & Basic Detector Description Capabilities. \\  & 6.4.2.2 & Detector Description Tagging \\  & 6.4.2.3 & Links between Event and Detector Description Data \\
6.5 & Conditions Data \\  & 6.5.1 & Design Choices \\  & 6.6 & Transient and Persistent Issues \\  & 6.6.1 & Design Choices \\  & 6.6.2 & Issues and Requirements. \\  & 6.7 & Visualisation \\  & 6.7.1 & Design Choices \\
7 & Physical Design \\
8 & Recommended Continuation of Work \\  & 8.1 & Architecture Team \\  & 8.2 & Development Process \\  & 8.3 & Work Packages and Time-table \\
9 & Conclusions \\  & 9.1 & General Remarks \\  & 9.2 & Open Issues \\  & 9.3 & Major Decisions \\
10 & References \\
11 & Appendix A - Glossary \\
12 & Appendix B - An Example of Scripting \\
13 & Appendix C - Example Solution for On-demand Access to Persistent Objects \\
14 & Appendix D - Associated Proposals \\  & 14.1 Libraries \\  & 14.1.1 Proposals \\  & 14.1.2 & Open Issues \\
14.2 & Modelling Language \\ \end{tabular}
\end{table}
Table 1: Data Flow Requirements.

[MISSING_PAGE_EMPTY:5]

"The taskforce will have a composition taken from a **large base in the collaboration** so as to ensure that the architecture will be one with a **broad support**."

The ATF believe that it is important that this Report sets out an **initial direction** for the ATLAS Off-line Software work. To first order, this should be achieved by providing an outline for the Global Architecture. There are many important **strategic decisions** which need to be made for the Architecture and its implementation. Therefore we have tried to make the best decisions possible within the time allotted to us.

### General Comments

#### 1.3.1 Scope

The ATF considers its work to encompass all Off-line Software, including those parts of the On-line which do not involve communication with detector hardware, such as: the Event Filter, event displays, physics data monitoring, high-level calibration.

#### 1.3.2 The Gaudi Example

The ATF, along with many others, have been impressed by the approach taken by the Gaudi Team which is developing the design of the LHCb Architecture [5]. While the ATF does not necessarily agree with all of the decisions made by the Gaudi Team, we recognise that the way in which they have gone about their work and, above all, the level of their documentation provide a good example.

#### 1.3.3 Time-scales

With the completion of this Report, it is appropriate to move into a phase of more detailed design and then implementation. It is hoped that a first implementation of the ATLAS Framework can be made available to the collaboration around **May 2000**. Our proposal for how the work should proceed from now is set out in Section 8.

#### 1.3.4 Definitions of "Architecture" and "Framework"

We define:

1. The **Architecture** to be the _high-level design of the software_ - something which can appear on paper. In particular, the **Global Architecture** defines the general structure of the software, with special attention to common aspects (services) required by all Domains.
2. The **Framework** to be a collection of classes that provide a set of services for a particular application domain (e.g. the reconstruction). The Framework enables the realisation of the core features of the Architecture in code. (Note the UML definition is somewhat larger.)

Detailed definitions of other key terms can be found in the Glossary.

#### The Complete ATLAS Software System

The complete ATLAS software will consist of a set of libraries (see Figure 1-1):

1. Basic C++ utility libraries, such as STL, CLHEP, histogramming and persistency libraries, etc..
2. The ATLAS Common Framework, which defines such things as access to event data and the detector description. **This is the focus of the work of the ATF.**
3. Libraries for well defined applications such as Event Filter, reconstruction, simulation, analysis and visualisation (includes event displays, histogramming etc.).

Using the ATLAS Software System, users will be able to build applications (executable code) to perform standard tasks or combinations of them.

This report does not contain the design relating to specific applications, such as the reconstruction. The development of this level of the Architecture will follow in a subsequent phase and is outlined Chapter 8.

Figure 1-1 Different sets of libraries comprising the ATLAS Software System.

## 2 Workings of ATF

### Meetings

The ATF Meetings have been closed meetings and the ATF mailing list was restricted. Nevertheless, the ATF has attempted to solicit the views of members of the collaboration and to listen to the feedback. To facilitate an interchange, a large amount of information has been placed on the ATF Web pages [6] and drafts of this Report have been made public at the earliest sensible time.

The early meetings concentrated on understanding what other experiments are doing: this was particular useful in educating some of the ATF members less familiar with C++ and OO Analysis and Design. In particular, we interacted with representatives from LHCb (the Gaudi Framework) [5], BaBar and CDF (the AC++ Framework) [7] and D0 (the D0 EDM) [8]. Later meetings were devoted to consideration of our own design ideas. From these meetings, we noted several interesting principles, including:

* Data Objects are passed between Algorithms. (See Section 3.2.)
* follows from Mandate. (See Section 6.6.)
* The **Event Class** is vitally important, as are the **Detector Model** and **Persistency Mechanism**. (See Section 6.3 and Section 6.4.)
* **Use-cases** are essential to understand Community's needs, stimulate the Design process and test it. (See Section 4.1.)
* It is essential in all development to consider the needs of the **Trigger/DAQ** and, in particular, those of the **Event Filter**.
* We should not exclude a possible migration to **Java**.
* A good **Scripting Language** is essential for reliable job preparation. (See Section 12.)
* It is essential to form an **Architecture Team** with a **Chief Architect** to pursue the detailed design work. (See Section 8.1.)
* We should be able to **test** parts of the software (e.g. simulation, reconstruction) with test beam data and to compare the Geant4 simulation with Geant3 simulation.
* The new software should give **physics performance** comparable to that presented in the Physics TDR [9] and validated by test beam data.
* The **speed** of the code is an important consideration.
* Code should be able to run both in **interactive** and **batch** modes.

These principles have shaped our own thinking.

### Short Term Frameworks for Simulation and Reconstruction

The Architecture outlined in this Report will allow the implementation of the ATLAS Framework. However, the time-scales for this are of the order of half a year. In the meantime, the Sub-systems need to start development and should not have to wait this long. Therefore the ATF has identified some short term strategies to enable prototyping of algorithms and detector descriptions. This work is distinct from prototyping of the ATLAS Framework, and, although it requires some code to provide basic infrastructure, the programs provided for these tasks should not be considered as _the_ ATLAS Framework. This approach should work well, provided the code used is modular. Obviously, lessons learnt from the use of the temporary programs may be useful for the implementation of the "final" framework.

For short-term Simulation work, people are encouraged to work with the Geant4 program [10] (and Chaos when it becomes available). For short-term Reconstruction work, the Paso program [11] has been developed. More information about short-term work can be found from the Web [12].

### Control Paradigms

A control paradigm is a method by which the execution of modules is scheduled and the data which they use as input and subsequently create as output is handled.

One outcome of the presentations to the ATF on existing software for control (Gaudi, AC++, Object Networks) was the recognition that there are alternative paradigms that warranted further study since they have a major impact on the overall design. A team lead by Craig Tull from LBNL was therefore asked to perform more detailed evaluations in this area, based on issues raised by ATF team members. These evaluations are discussed in more detail in Section 6.2.3 of this report.

## 3 Introduction to the Architecture Design Work

### Approach Taken

The Common Framework for a project on the scale of ATLAS is sufficiently important that the ATF felt it was essential to tackle the problem **ab initio** in order to ensure that the ATLAS-specific needs and requirements are not missed at this early stage. The methodology followed for this approach is based on the Unified Software Development Process (USDP) [14] which starts with "use-cases" and proceeds from there to identify the essential requirements on the code. Our work along these lines, which is by no means a complete design for the whole architecture, is described in Chapter 4 and the auxiliary reports [1][2].

However, the ATF also recognised that the Common Framework for ATLAS is bound to have certain generic features which are present in all solutions to this problem, and therefore which can be found in the solutions implemented or planned in other experiments. To put it another way, we _know_ that we will have to read and write vast amounts of event data, and that we will have to handle calibration data, etc., etc.. Therefore, we have also examined carefully the work done in several other experiments and, based on our findings and experience, developed a preliminary specification for the components which we expect will feature in the Common Framework. This work is described in Chapter 5. These components provide a suitable vocabulary to discuss the work which is on-going in ATLAS, and which is described in more detail for the Event and Detector Description in Chapter 6. We note that an examination of the approaches followed by other experiments was also an explicit part of the mandate to the ATF.

The ATF wishes to emphasise that we view these two approaches as **complementary** and that, as the work proceeds beyond the ATF, the design of the components which emerge from the USDP process can and should benefit from past and present work done in ATLAS and other experiments. In other words, the goal is the best of both worlds: consider the problem anew, and benefit from experience.

### General Design Choices

Before proceeding to describe our work along the lines indicated above, we highlight some choices which influence the design work in a very general way. Later in this report, the choices related to specific domains such as control, persistency, event, conditions and visualisation are listed. Some, but not all, of the choices correspond to proposals made in a report from the former DIG Working Group [13].

Every effort has been to make optimal choices. In some cases, we briefly discuss alternatives, and the main reasons for our choices. Some choices may need to be re-considered in the light of subsequent experience.

**Object-oriented Paradigm**

_The design of the software system follows the object-oriented paradigm._

This choice is stipulated by the result of the ATLAS Computing Review, and the resulting action plan by the ATLAS management. The benefits of OO are: Abstraction, Encapsulation, Modularity and Hierarchy.

#### Language

_For infrastructure packages, C++ is the default implementation language. All exceptions from this rule will need to be justified, and will require the authorisation of the ATLAS computing management that will take issues such as the software integration, support, long-term maintenance and code performance into account._

At the time of writing, Java looks like the only serious long-term alternative to C++. We suggest that the **Quality Control Group** provide recommendations for how to use C++ in order to be prepared for a potential future migration.

#### Domain Decomposition

_The framework architecture is decomposed into infrastructure domains with well defined interfaces and functionality._

This choice aims at defining domains of the infrastructure which are as loosely coupled as possible; domains using the services of a particular domain should require as little knowledge as possible of the serving domain. Examples of domains in that sense are control, user interface, event collection etc. This domain decomposition helps defining work packages which can be implemented rather independently. It must reflect in the physical design of the software system as well.

#### Distinction between Data and Algorithms

_Data and algorithms are implemented as different objects._

Algorithms are significant, non-trivial chunks of data processing, with well defined input and output objects, for example track-finding code. The aim of this separation is to allow for an easy exchange of algorithms operating on the same input types, and producing the same output types. Some simpler computations, for example transformations of reference frames or coordinate systems, are not considered algorithms, and should be implemented as methods of the corresponding data objects.

#### The Role of Modules

_All modules (see Glossary) must be capable of being used in different applications._

Modules must not make any assumption as to which application they are being used for. For example, any module must be able to migrate from private analysis code to a general analysis package, the reconstruction program or the Event Filter application etc..

## 4 Use-case based Architecture Design

### Development Process

#### Overview

As one approach to come up with a first design for the ATLAS global architecture, the ATF has adapted the **Unified Software Development Process** (USDP) of Jacobson, Booch and Rumbaugh [14]. A similar approach was used by the Geant4 Collaboration. This was presented to the ATF by Katsuya Amako [15]. For the diagrams, the **Unified Modelling Language** (UML) [16] has been used. The interested reader should look at these various references.

The development is **Use-case driven** - the use-cases are based on the wishes and requirements of the user community, in particular, the Detector and Physics Communities. The process consists of a set of steps:

**Requirements**: Obtain _Feature List_ (a "wish" list) from Users, develop _Use-cases_ and _non-functional Requirements_. Identify _Actors_ (people or perhaps the programs they run which "do things").
**Analysis**: Analyse Use-cases to identify _Objects_ and _Classes_ at the top level.
**Design**: Design the Classes, adding more detail and identifying their _Data Members_ and _Methods_.
**Implementation**: Write the code.
**Testing**: Test the code, ensuring that it works technically, satisfies the Use-cases and gives the required performance for Physics.

The process is **incremental** in so far as it does not attempt to develop all the software in one pass; rather, one starts off with critical items and other items can be added in subsequent cycles. The process is **iterative** in so far as it allows improvements to be made as a result of what is learnt from subsequent steps. The use-cases are used to initiate the work and then verify the development at each stage. This process allows code to be implemented and tested at a very early stage of the development.

#### Application to ATLAS

The process outlined above is a very **pragmatic** approach, because once the analysis and design phases of the first iteration are finished, users (in particular those developing detector code) can see the definition of interfaces (for example to access geometrical data, the way to make data persistent, etc.) and start to write code which will fit with the Framework.

This methodology does not seek to define the documentation to accompany the development nor define the procedure by which code will be accepted. This area comes under the mandate of the **Data Quality Group**.

The emphasis should be on **adaptation** rather than **adoption**: the USDP methodology should be adapted to help ATLAS.

The work of the ATF has come to a conclusion with the completion of this Report. We have collected requirements and use-cases, attempted the higher-level analysis and made a start on the design. Clearly, there is much more to do - this is discussed in Chapter 8.

### Use-cases

#### 4.2.1 Use-cases and Requirements

The use-cases and requirements which pertain to the Global Architecture are contained the auxiliary report [2].

#### 4.2.2 Use-case Matrix

From the use-cases collected in [2], the association has been made with the corresponding _applications_ (programs which people run to do different tasks) which are denoted as "actors". The associations are illustrated in Table 4.1. We do not claim that every use-case and every actor is represented by the table, however we believe that it contains the most important ones and can be used as a good starting point for the first iteration under the USDP. For example, one can envisage a use-case "Use Particle Properties" which is not included.

\begin{table}
\begin{tabular}{l|c c c c c c c c c} \hline \hline  & & & & & & & & & \\ \hline \multirow{2}{*}{Actors} & \multirow{2}{*}{**Detector Description**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\  & & & & & & & & & & \\ \hline \multirow{2}{*}{**Magnetic Field Provider**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\  & & & & & & & & & \\ \hline \multirow{2}{*}{**Online Calibration and Alignment**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\  & & & & & & & & & \\ \hline \multirow{2}{*}{**Event Filter**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\  & & & & & & & & & \\ \hline \multirow{2}{*}{**Offline Calibration and Alignment**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\  & & & & & & & & & \\ \hline \multirow{2}{*}{**Offline Event Streaming**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\  & & & & & & & & & \\ \hline \multirow{2}{*}{**Physics Analysis**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\  & & & & & & & & \\ \hline \multirow{2}{*}{**MC Event Generator (4-vectors)**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\  & & & & & & & & \\ \hline \multirow{2}{*}{**Detector Simulator**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\  & & & & & & & & \\ \hline \multirow{2}{*}{**Visualisation\({}^{\text{d}}\)**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} & \multirow{2}{*}{**X**} \\ \hline \hline \end{tabular}

* Setup and Use ATLAS Common Framework Environment: the top-level activity needed to make the application execute.
* The Event will include MC information for simulated events, along the same lines as GENZ and KINE were included in Zebra events. What is not clear is where the 4-vectors output from the event generators will go.
* Exchange Messages: communication of information between Modules.
* this then involves a second actor.

\end{table}
Table 4: Matrix of which Actors use which Use-cases.

### ATLAS Common Framework Environment

The use-case **UC-00 "(Setup + Use) ATLAS Common Framework Environment"** is presented in this section to illustrate the progress with the analysis and design along the lines foreseen by the USDP. More detail for this use-case, along with detail for UC-01 ("Store + Retrieve Detector Description") and UC-03 ("Store + Retrieve Event Collection") is provided in the auxiliary report "Analysis and Design of the ATLAS Common Framework" [1].

What is shown in this section should be considered as "**work in progress**". The designs shown are **EXAMPLES** and are not yet definitive - they may change after further development. This is particularly true of class names, methods etc..

Throughout this chapter, we use the notation and nomenclature of USDP - in some places, this is slightly different from that used by other authors.

#### Brief Description of the Use-case

The actor sets up the initial environment of the ATLAS Common Framework before he uses the services provided by the framework. Setting up the initial environment includes specifying, for example:

* the input source of an event collection,
* the input source of the detector description,
* the input source of the magnetic field,
* other initial conditions related to use-cases which appear in the ATLAS Common Framework.

After setting up the initial conditions for the necessary services, the actor starts to use them.

The actor may also change environment during the data processing. This includes modification of input sources specified at the initialisation stage.

#### Analysis1 Model

To initiate the design of the system, we first need to map the requirements in the language of the system developer - a process called analysis. Since our software development process is object-oriented, this step can be referred to as _object-oriented analysis_.

The analysis model is represented by a diagram representing the top-level analysis package. Using other analysis packages provides a way of organising the analysis model into more manageable pieces that represent abstractions of partial or possibly complete layers in the system's design. Analysis classes represent abstractions of classes and possibly sub-components in the system's design. Within the analysis model, use-cases are realised by analysis classes and their objects.

##### Analysis Package

Figure 4-1 is a realisation of the use-case number UC-00 in the analysis process. The oval at the top of the diagram represents the use-case. From the developer's point of view, it is the class which manages all actions to set up the initial environment to use the ATLAS Common Framework. It also manages an actor's request to modify the environment. The box represents an analysis package which contains such a management class. The analysis package is called "Atlas Common Framework Management". We may need a collection of implementation classes which work closely together to realise the use-case. The analysis package represents all of these classes.

The analysis package has an interface which is graphically shown by a small circle. All actors' requests are handled through this interface. From the user's point of view, this interface definition provides everything he needs to set up and use the ATLAS Common Framework.

Figure 4: Analysis Package Diagram for ATLAS Common Framework Environment.

#### Design Model

The design model is an object model that describes the physical realisation of use-cases by focusing on how functional requirements, together with other constraints related to the implementation environment, impact the system under consideration.

Within the design model, use-cases are realised by design classes and their objects.

##### 4.3.3.1 Class Diagram

Figure 4-2 shows the ATLCommonFrameworkManager design class which is a physical realisation of the use-case UC-00.

In UML, a class is shown as a rectangle. The class name is put in the top compartment of the rectangle. The number "1" appearing in the right most part of the top compartment means that the class ATLCommonFrameworkManager is a singleton, i.e. only one object can be instantiated from this class. This is a natural design choice because the initialisation and modification of the ATLAS Common Framework should be handled through a single channel.

Major attributes of the class should be shown in the second compartment of the class rectangle. The class should have pointers to various interface objects, which would be represented by private attributes. This is a matter of detailed design, although some illustrations are given in [1].

The third compartment contains the operations (methods) of the ATLCommonFrameworkManager class. The names and the return types of major methods are defined in it. Because ATLCommonFrameworkManager is an interface class, which will be seen by all actors in the ATLAS Common Framework, a more detailed description is helpful. This is provided by another diagram called the "Design Model: Interface Definition".

The fourth compartment contains comments describing the basic characteristics of the class.

Figure 4: EXAMPLE Class Diagram for ATLAS Common Framework Environment.

#### 4.3.3.2 Interface Definition

This describes in more detail the definition of the public methods. This deliverable is not defined in UML or USDP, but we have introduced it because the interface definition is one of the most important products from the design process. Once the interfaces have been defined, the actor has all he needs to know how to implement what is needed to be able to use the class. In our case, the C++ language is used to define the interface of a class.

Figure 4-3 shows a few illustrative examples of the interfaces for ATLCommonFrameworkManager.

[MISSING_PAGE_EMPTY:21]

#### 4.3.3.3 Sequence Diagram

A sequence diagram shows the interaction between a set of objects, including the messages that may be dispatched among them. Graphically, a sequence diagram is a table that shows objects arranged along the x-axis and messages1, ordered in increasing time, along the y-axis.

Footnote 1: “Messages” are synonymous with “method calls”, which are shown as solid arrows; “return values” are shown as dashed arrows.

Figure 4-4 shows a simplified sequence diagram to realise the use-case UC-00 when two actors "Offline Event Reconstruction" and "Visualisation" share resources in the ATLAS Common Framework. The interaction of objects in this diagram is a typical pattern applicable to interactive data processing. For example, a user may want to compose a chain of processes of something like

Event Generation \(\rightarrow\) Detector Simulation \(\rightarrow\) Event Reconstruction \(\rightarrow\) Physics Analysis

to process event-by-event. The pattern shown in Figure 4-4 can be used for this kind of chained application.

The objects which may participate in this realisation are:

* an ATLCommonFrameworkManager object,
* an ATLDetDescrManager object,
* an ATLMagFieldManager object,
* an ATLEventCollectionManager object,
* an ATLEvent object,

where the definition of the classes can be found in the auxiliary report "Analysis and Design for Top-level Use-cases" [1].

The realisation of the use-case can be understood by following the sequence diagram from the top to bottom. Because the ATLCommonFrameworkManager is the interface object between an actor and the ATLAS Common Framework, the actor sends all requests related to the initialisation and changes to the framework environment through this interface object. The sequence starts when the actor "Offline Event Reconstruction" sends a message to the ATLCommonFrameworkManager specifying the detector description it wants to use. Receiving this message, the ATLCommonFrameworkManager passes the specification to the top-level object of the ATLDetDescrManager. ATLDetDescrManager then returns the pointer to the specified detector description to the ATLCommonFrameworkManager, and then the manager passes the pointer to the actor.

The actor "Offline Event Reconstruction" passes a message object to the actor "Visualisation" when it completes processing one event. After receiving the message object, the actor "Visualisation" processes the event (a typical process might be to display the event passed by the "Offline Event Reconstruction") and then sends another message object to the actor "Offline Event Reconstruction". Note that the message objects exchanged between the two actors are different from the event they are sharing.

Figure 4.4: Simplified Sequence Diagram appropriate to interactive Offline Event Reconstruction for the ATLAS Common Framework Environment.

To realise this pattern, it is necessary to have a mechanism of synchronisation between the two actors to process each event. There are several ways to do this synchronisation, the realisation of which is a matter for the implementation phase.

## 5 Components Expected in the ATLAS Common Framework

The work illustrated in the previous chapter was based on the use-case approach. Based on our own experience (including that from DIG) and recent experience from AC++ (CDF and BaBar), the components listed in the Gaudi Document and the ATLAS requirements, we have derived the following list of components which we expect to be important for ATLAS. Although this list has been derived from an analysis of the ATLAS requirements, it contains many similarities with the component list developed by the Gaudi team for LHCb. We have therefore chosen, when appropriate, to adopt the Gaudi terminology. One exception to this rule is the component Gaudi calls "Algorithm Interface". Because the word "algorithm" is widely used in ATLAS with a meaning different from that of Gaudi, we have chosen to replace the name "Algorithm" with "Module" (see Glossary).

In providing a list of components, we have attempted to be as complete as possible. Nevertheless, it is inevitable that some will have been forgotten, while others may need to be merged or split. Table 5-1 gives a list of the main components which are explained in more detail in the following text.

### Components Responsible for Application Configuration and Execution

Most of the following components correspond to what ATLAS has traditionally called the "Control Domain" (the **Job Options Service** corresponds to what was called the "User Interface"). These components collaborate to setup and execute a software application.

#### Framework Manager

The Framework Manager is responsible for building and configuring the application. It creates all the required services and modules and initialises them. The Framework Manager uses information provided by the **Job Options Service** to determine what services and modules should be created and to appropriately configure these components.

The Framework Manager corresponds to what in the past would have been identified as the

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Responsible for Application Configuration and Execution** & **Associated with the Application and Framework Managers** & **Associated with the** & **Additional Services and Components** \\ \hline Framework Manager & Event Input & Event & User Interface \\ Application Manager & Event Output & Detector Description & Message Service \\ Job Options Service & Data Item Selector & Conditions Data & Bookkeeping \\  & Event Collection Mgr & Statistics Data & History \\  & Event Merge & Magnetic Field & Particle Properties \\  & Module Interface & & \\ \hline \hline \end{tabular}
\end{table}
Table 5-1: The main components which we would expect to be important for ATLAS.

"main program", and in particular that part of the main program associated with the initialisation of the job, for example initialisation of zebra or databases.

This component is closely related to the designs associated with use-case **UC-00 "(Setup + Use) ATLAS Common Framework Environment"**.

**Application Manager1**

The Application Manager is responsible for sequencing the application. It uses information from the **Job Options Service** to determine the desired order of execution of the **modules** and implements the **event loop**. The Application Manager can terminate the processing of a specific event based on "filter" decisions provided by the **modules** (if this feature is activated via the **Job Options Service**).

In the past, new jobs were created by editing the main program, especially the subroutines called within the event loop. In the ATLAS Common Framework, the Application Manager will decide which modules are called and in which order.

**Job Options Service**

The Job Options Service is used to specify run-time control and configuration information to other components of the system. This service uses the **User Interface** to read commands from the terminal, from a file or from a graphical interface. It parses these commands and passes the appropriate information to the components.

In the past, this functionality would have been achieved by reading in data cards or using a script (as in ATLASIM[17]). See Appendix B for an example.

### Components associated with the Application and Framework Managers

In this section, those components which are configured and executed by the Application and Framework Managers are discussed. The components listed below are abstract base classes that define objects that can be bound to the application library. The interfaces defined in the base classes tell the control domain classes defined above how to create properly, initialise and execute these components. The ATLAS Software System will contain multiple concrete classes that obey these interfaces and provide the functionality needed by the various applications.

**Event Input Component**

The Event Input is responsible for providing events for the **event loop**. Logically, the Event Input acts like a server that delivers events when they are requested. The Event Input may use an **Event Collection Manager** to ensure that the events it delivers satisfy a specific set of requirements or conditions. Some concrete examples of Event Input classes that ATLAS is likely to develop are:

* **Database Event Input**: Delivers events that are stored in the ODBMS.
* **Zebra Event Input**: Delivers events from a zebra file.
* **Shared Memory Input**: Delivers events from shared memory (for example in a test beam).
* **Monte Carlo Input**: Generates an event using a Monte Carlo generator.

In all cases above, each resulting event is made available for further processing using the **Transient Event Data Store**.

#### Event Output Component

The Event Output is responsible for writing events back from the **Transient Event Data Store** to the **Persistent Event Data Store**. This component may use **Event Collection Manager** to ensure that only events meeting a specified set of conditions are written out. It may also use **Data Item Selectors** to specify that only a subset of the objects in the **Transient Event Data Store** should be made persistent. The Event Output may also use information provided by the **Application Manager** to ensure that events only be stored if they have successfully completed a given analysis path or if a specific module filter has accepted the event.

#### Data Item Selector

A **Data Item Selector** is a class used by the **Event Output** component to specify what classes and class instances should be written out. For example, suppose in an analysis job a user has run a private version of the jet clustering and a private version of the silicon pattern recognition and track reconstruction. The user may decide to write out the results of the silicon pattern recognition and track reconstruction to a private area of the database (that he owns) so that he can work with these tracks at a later time. But, he may not want to write out the new jet clusters. The user would then specify to the **Event Output** component (via the **Job Options Service**) "Write SiliconTrack Collection and SiliconTrackHitLinks created with algo-rithm=MyAlgorithm" and "Don't Write JetCluster Collection created with algo-rithm=MyJetAlgorithm". The **Event Output** component will put these commands into a form that the **Data Item Selector** can understand and pass the request to the **Data Item Selector**. The **Data Item Selector** then interacts with the **Event Persistency Service** to make sure only the correct objects are written out.

#### Event Collection Manager

Both the **Event Input** and **Event Output** components will need to use a facility that selects subsets of events that pass specific criteria. For example, a user might want to tell the **Event Input** component to only pass to the event loop those events that have the "High pT Electron" trigger-bit set. Similarly, the user might want to tell the **Event Output** component that it should write out only events that have passed the "Good Higgs Candidate" event selection. The **Event Collection Manager** is the component of the system that takes such selection criteria and creates a list (collection) of events meeting the criteria. The **Event Collection** is not a file of events; it is merely the list of event identifiers (together with a set of interfaces that specify how to retrieve the events from the identifiers). Functionally the **Event Input** operation described above would be performed as follows:

The **Event Input** component receives a command (from the **Job Options Service**) saying "Select Events with HighPtElectronTrigger=True". The **Event Input** component puts this command into a form that the **Event Collection Manager** can interpret and passes the command to the **Event Collection Manager**. The **Event Collection Manager** queries the database to get the list of events meeting the specified requirement. Then, each time the **Event Input** component needs a new event it asks the **Event Collection Manager** to pass it a pointer to the next event in the list. (Note: the word pointer here is used in a general sense to stand for whatever information the system needs to access the next event. Depending upon the implementation, this might be a pointer, an identifier or the header for the event itself.) **Event Output** will work in a similar manner.

This component is closely related to the designs associated with use-case **UC-03 "(Store + **Retrieve) Event Collection"** and Section 6.3.

#### Event Merge Component

This component can be used to merge information from several events into a single instance of the Transient Event Data Store. Examples of applications where such a component would be useful include

* Merging several Monte Carlo events to model multiple interactions.
* Merging a Monte Carlo track into a real data event to study pattern recognition efficiency.

#### Module Interface

The majority of code written by ATLAS physicists will take information from the event and additional services, process that data and produce output in the form of objects to be added to the event, information to be displayed to the screen, histograms, etc.. The Module Interface is the class used to bind such components to the application library. Instances of classes that implement the Module Interface, called **modules**, are the smallest units of computation that can be dynamically controlled by the application. The **Framework Manager** controls their creation and initialisation, and whether they are executed. The **Application Manager** controls the sequencing of execution of modules within the event loop. In general, a module will have properties or parameters that should be settable at run-time. The Module Interface uses the **Job Options Service** to implement such run-time configuration.

### Data Components

Data components are used to manage data that is passed between components of the system. Each service provides a source and a sink for data objects that serve a common purpose and have similar lifetimes. The various data components all follow a common pattern. They include a transient store responsible for caching data used in the application, a data service responsible for managing this transient store and a persistency service responsible for reading these data (on demand) from a persistency manager (ODBMS, file-system manager, zebra, etc.) and for writing data back to that external source. In all cases we assume that a data converter will be used to take the numbers provided by the persistency service and transform them into a transient object of a type appropriate for the application.

#### Event

An event corresponds to the information associated with a single triggered bunch-crossing recorded by the ATLAS detector. Components used to access the event are:

* **Transient Event Data Store**: This is used to cache data objects used during the execution of the modules. Objects are put into this store and fetched from this store using the **Event Data Service**. Note that in this context "put" and "fetch" refer to the passing of a (smart) pointer and _not_ the copying of an object to another location in memory.
* **Event Data Service**: This service is responsible for managing the **Transient Event Data Store**. Modules query this service to locate objects in the Data Store and to insert new items into the store.
* **Event Persistency Service**: This service is responsible for obtaining event data objects from the **Persistency Manager** and for writing event data objects back to that external source. This service uses the implementation-specific code associated with the given persistency choice.
* **Event Data Converters**: Data converters are used to map between the representations of an object in the transient and the persistent stores. Use of such converters decouples thetwo representations. For example, it would allow applications to switch between different persistent representations of the data (namely which entities are stored) without recompiling the application specific code1. A unique converter is needed for each data type in the event.

Footnote 1: This is referred to as “schema evolution”. In the past, this was encountered when the definition of zebra banks changed.

These classes need to understand the structure of the event, in contrast to **Event Input** and **Event Output** (responsible for reading in and writing out complete events) which do not.

For discussion on Event see Section 6.3, and see Section 6.6 for transient and persistent issues.

#### Detector Description

The Detector Description contains the set of classes describing the geometry, materials and read-out information for the detector. It uses the **Conditions Data Service** (see below) to access time dependent information, such as geometrical data from different configurations of the detector. Classes used in the detector description include:

* this provides the transient view of the detector. Objects are put into this store and fetched from this store using the **Detector Description Data Service**.
* **Detector Description Data Service**: This service manages the **Detector Description Data Store**. Modules query this service to locate objects in the Data Store. In particular, this service provides access to the description of the detector corresponding to the application (reconstruction, simulation, visualisation etc.).
* **Detector Description Persistency Service**: This service is used to fetch (or put) detector description data from (or into) an external file or database.
* **Detector Description Data Converters**: These converters build the transient detector description from the persistent representation.

This component is closely related to the designs associated with use-case **UC-01 "(Store + Retrieve) Detector Description"** and Section 6.4.

#### Conditions Data

The term Conditions Data is used to include data objects that are stored in databases other than the event store and that have a time dependence. Examples of conditions data are slow control information, accelerator information, calibration information and alignment constants.

The **Detector Description** could be considered to be Conditions Data in that both are time-dependent. Hence it may be desirable to treat them as part of the same component.

Classes used to access conditions data are:

* **Conditions Data Store**: This is used to cache data objects used by the application. Objects are put into this store and fetched from this store using the **Conditions Data Service**. Objects in the Conditions Data Store have as data members a "validity range" which specifies the time interval for which the data is valid.
* **Conditions Data Service**: This service manages the **Conditions Data Store**. Modules query this service to locate objects in the Data Store and to insert new items into the store. This service is responsible for synchronising the detector information with the current event being processed.

* **Conditions Persistency Service**: This service is used to fetch (or put) conditions data from (or into) an external file or database.
* **Conditions Data Converters**: This component converts between the persistent and transient representations of the conditions data.

**Statistics Data**

The term Statistics Data refers to histograms, ntuples and other objects that are collated during the processing job. Classes used to access these data are:

* **Statistics Data Store**: This is the mechanism that is used to store the data within the application. Examples of such Data Stores are the Zebra common block used by Hbook and the TTree used by Root.
* **Statistics Data Service**: This is the mechanism used by the application programmer to book and fill histograms and ntuples. The **Framework Manager** and **Application Manager** may collaborate with the Statistics Data Service to ensure that all Modules within a single application use the Statistics Data Service in a consistent way (for example, by providing a mechanism for globally managing histogram identifiers or for placing the histograms for a each module its own directory).
* **Statistics Persistency Service**: This is the service used to write (or read) histograms or ntuples to (or from) external files.

**Magnetic Field**

This class defines the ATLAS magnetic field.

It may be useful to consider whether the magnetic field description should be included as part of the **Detector Description**. However, this cannot be determined until a detailed design has been developed.

### Additional Services and Components

**User Interface**

The User Interface is the code that accepts input from a terminal, file or graphical interface and passes it to the **Job Options Service**. It is likely that the User Interface will include an interface to a standard scripting language (e.g. Tcl, Python, Java) but the architectural design of the system does not require this to be the case.

**Message Service**

The Message Service is responsible for passing messages created by the other components of the system to the outside world. These messages may be segregated according to their severity or according to the source of the message. Messages are informational and provide a journaling function. The Message Server is not an exceptions handler.

For example, the Message Service might be used to print out warning messages for different levels of severity.

**Bookkeeping**

Bookkeeping is responsible for keeping job statistics. It is used both by the control components of the application library and by the modules.

#### History

The history service stores the configuration information for the job. This service could be used in conjunction with a persistency service to write production configuration information to a database for permanent storage.

#### Particle Properties

The Particle Properties service is a set of classes that define the standard elementary particles and their properties.

## 6 Detailed Discussion of Some Components

In this chapter, more details are provided on some of the major components identified in the previous chapter. Clearly as the work progresses (see Chapter 8), this will need to be harmonised with the design work described in Chapter 4 and the auxiliary report [1].

Sections 6.3 (Event) and 6.4 (Detector Description) contain ideas associated with work which was started before the ATF came into existence and is continuing. The Paso program [11] provides access to what has been implemented so far.

### Introduction

#### An Example

Figure 6-1 shows a _greatly simplified example_ of a reconstruction application, designed to illustrate the roles and interactions of some of the **components** and to clarify the _nomenclature_ used in Chapter 5 and Chapter 6. We suppose that part of the reconstruction consists of building track segments from raw data separately for TRT, SCT and Pixel and the muon reconstruction builds

Figure 6-1: Simplified example of a reconstruction application, designed to illustrate the roles and interactions of some of the components and to clarify the nomenclature used in Chapter 5 and Chapter 6. See text for more detail.

a muon track list from muon raw data. This illustration is _not_ meant to reflect any of the actual algorithms used in the Inner Detector. The reconstruction developers will put their algorithmic code for the different steps into separated **modules**, which typically would be instances of classes which inherit from an abstract **Module Interface** base class. Closely related module classes are grouped into the same **package** and the **domain** is made up of the associated packages. The modules will run in a certain order orchestrated by the **Application Manager** (not shown). The modules will take as input and produce as output **data objects** which logically reside in the **Transient Event Data Store**. The modules actually communicate with the **Event Data Service** to access the data objects (Figure 6-1 should have communication lines to each of the modules), and typically it is only pointers to objects that are being passed around. Similarly, the modules have access to the detector description and the alignment, calibration and other conditions data which reside in either the **Transient Detector Description** or **Transient Conditions Data Stores**. Again, access to this information is via communication with the respective data services, which are not shown. The actual decomposition of the problem into different modules and data objects is for the developers themselves undertake.

#### Granularity of Modules and Data Objects

A module is the smallest unit of computation that can be controlled dynamically by the application code. Modules can vary in size and complexity from the very simple to the very complex (e.g. "perform pattern recognition in the SCT" or even "reconstruct the event"). Modules can undertake different tasks (e.g. different parts of the reconstruction) or provide alternatives algorithms (e.g. different ways of cluster finding). The appropriate size for a module should be determined by the application developer on a case-by-case basis and is not specified by the architecture.

In Section 6.2, we recommend that the "data-flow" from module to module should happen via the Transient Event Data Store. There is an issue of _granularity_ here which affects the size of the modules and the data objects they exchange. It is clearly the intention that the data-flow between, for example, the calorimeter shower reconstruction and the Inner Detector track-finding should pass via the Transient Event Data Store. Equally, it would be completely inappropriate to use the Transient Event Data Store for communication of (x,y,z) co-ordinates to a method which will calculate (R,0,0). A good understanding of the appropriate granularity will only come with experience.

### The Control Domain

The control domain comprises those components used to create, configure and execute the services, modules and event input and output components of the system (see Section 5.1). This section describes in more detail the requirements on these components and the rationale for these requirements.

We differentiate between the data flow and control flows associated with the control domain. Data flow describes the mechanism whereby data is accessed from the persistent stores (via transient stores) or is created by computational modules, and is made available for use by other such modules, or for eventual storing in the persistent stores. Control flow describes the way in which the execution of modules is sequenced. This is conceptually independent of the data flow, although in practice they are highly correlated since the relative sequencing of modules is usually constrained by their data flow requirements.

#### Data Flow Requirements

Access on Demand

_Access to information from an input event should be "on demand" (also called "deferred" access). This means that only the data which is actually required is read into memory._

A common scenario is that a subset of input events be selected on the basis of some filter criteria, this selected sample being reprocessed in some manner. Important performance benefits can be gained if only the relevant information necessary for performing the selection is first accessed, further access being deferred until the selection has taken place.

Output Subsetting

_It should be possible to specify a subset of the information of the event for output._

Another important scenario is that a subset of events is selected and that only condensed summary information for each event is written to persistent storage. This allows the efficient data distribution of such samples for processing at remote sites, and also improves access performance for subsequent local processing of this summary information.

Multiple Output Streams

_It should be possible for there to be multiple output streams, a subset of the events being directed to each stream._

Events are typically categorised by the underlying physics signatures, and further processing of the event of each category is performed by different analysis groups. The binding of events to different output streams allows for the efficient processing of such samples, especially when combined with output subsetting. Each stream might not only have different event selection criteria, they might also have different subsetting criteria applied to them.

Access to Event Data

_All access to event data information by modules is via the Transient Event Data Store. Internally within a module, direct communication of information via C++ objects is both allowed and recommended._

The Transient Event Data Store is the only repository of transient event data information. This is necessary to avoid ambiguities of ownership and garbage collection.

Inter-module Communication

_There shall be no direct communication of data objects between modules._

Exchange of data objects is done via the Transient Event Data Store. But this is not meant to preclude a distribution of the control flow where a composite module can control the execution of other sub-modules, similar to what is done in Gaudi [5].

#### Control Flow Requirements

Compile-time Dependence

_The Framework Manager shall have no compile-time dependence on the services and modules used by the application. It is desirable but not required that there be no link-time dependence either._

The ATLAS Common Framework will be used for a variety of applications. These applications will need many different modules and many different services. Some applications may require special purpose services and modules that are not easily exportable or are not relevant for the general ATLAS user. This requirement avoids massive recompilations for cases where services must be added or removed. It makes it possible for developers to add new services.

Dynamic Binding

The Framework Manager shall support both dynamic binding of the modules and services to the application, (where loading is done via shared libraries) and static binding and loading.

Dynamic loading allows for a rapid turnaround during the development phase. It permits users to add their own modules to pre-built binaries, decreasing the number of cases where users need to link their own executables. For applications such as Production and the Event Filter, however, where binaries may be run for many months, static linking is essential to ensure long term stability and reproducibility of the code. Static binding may also be necessary in cases where binaries are run on systems that do not have access to the compiler and/or shared libraries.

Different Event Input and Event Output Components

An application user should be able to switch among different **Event Input** components and among different **Event Output** components without rebuilding the executable.

For example, one should be able to read from different sources (eg. Zebra, Objectivity) without rebuilding. This allows the pre-building of binaries that can be used for a variety of applications.

Run-time Specification of Ordering

The Application Manager shall allow run-time specification of the order (sequence) in which application modules run, for example within the event loop. We will refer to such a sequence as a "Path".

This requirement allows the user to configure the executable to meet his needs.

Stopping Execution

The Application Manager shall provide a mechanism to stop execution in the middle of a path for a given event based on a decision (true or false) provided by a module in the path. We will refer to such a decision as a "Module-provided filter".

This feature provides a mechanism for stopping event processing for events that fail a simple filter before cpu-intensive analysis is done.

Multiple Independent Paths

The Application Manager shall support the execution of multiple independent paths for a given event in the event loop. It must be possible to specify independent filter requirements for each path.

This feature allows the user to specify different execution procedures for different categories of events (for example different triggers).

Nesting of Paths

The Application Manager shall support the nesting of paths.

This requirement allows the user to "shrink wrap" a sequence of modules into a single "super-module". For example, suppose the full ATLAS tracking software consists of a number of modules that need to be run in a specific order (eg. to do TRT tracking, matching SCT and Pixel hits to TRT, silicon tracking on hits left after matching). Creating a pre-packaged mini-path that contains the list of these modules in the right order would allow users creating a full analysis job to specify the full tracking path by a single name (eg. "Tracking").

Processing History

The event-by-event processing history shall be accessible to other components by querying the Application Framework.

This requirement is necessary to allow the **Event Output** component to stream different eventsto different files based on the module provided filters. It is also necessary for the implementation of the **History Service**.

**Settable Parameters**

_The **Job Options Service** shall provide a simple mechanism whereby other components of the system can declare certain data members of their classes "settable parameters." It is the responsibility of the **Job Options Service** to parse run-time input and use that input to set these data members._

This function provides a general mechanism for implementing a modern equivalent of run cards.

**Application Commands**

_The **Job Options Service** shall provide a well defined interface that services and application modules can use to provide more complex run-time configuration and execution. This interface shall allow components to define "commands" for the application._

This feature will be used extensively by the components of the ATLAS Common Framework. It will also be used by application modules that require more complex run-time interactions with the user than provided by the straightforward parameter setting described above.

For example, a user wants to be able to interactively examine track data in the event store. He writes an application module that will print information about the tracks for each event. To specify how much detail he wants, he gives the following set of commands to his module before starting the event loop:

List AllData Print Silicon Tracks -level verbose Print Silicon Hits -level terse Print TRT Tracks -level verbose These commands would tell the module to print a list of all data objects in the Transient Event Data Store and then print a detailed list of all silicon tracks, a terse list of all silicon hits and a detailed list of all TRT tracks.

**Resolution of Inconsistent Sequencing**

_It is highly desirable that inconsistencies between the control and data flows be detected prior to execution._

For example, if one module requires information that is created by another module, then it is desirable that the control flow should constrain their sequencing when the sequencing is being specified, rather than when the modules are being executed.

**Modules Implement a Common Interface**

_All modules implement a well-defined common interface. All communication with modules passes through that interface._

This is required for advanced features, e.g. building the list of modules to run at run time, to work. It is expected that the common interface will be enforced by an abstract module base class. Module designers are encouraged to use the same abstract interface for major algorithms within their modules (in order words, a module hierarchy should be established; modules with access to event data can be packaged as a new module with larger functionality).

**Control Flow Independence**

_The Framework and **Application Managers** shall not depend on any specific data type used in other components._

For example, the **Application Manager** shall implement an iteration over objects, without knowing that these objects are events.

#### Garbage Collection

In particular, this relates to "ownership", namely who is responsible for cleaning up memory. This is important and will need to be addressed.

#### Control Paradigms

The ATF has discussed in some detail alternative control paradigms, in particular those where the control flow is determined by explicit data flows between computational components (modules), and those where the control flow is determined by a scripting language, independently of the data flow. An example of the former is the Object Networks prototype of Lassi Tuura [18], and of the latter is the initial implementation of the Gaudi Framework from LHCb [5]. To allow the detailed design and implementation to begin as rapidly as possible in order to meet the required time-scales, the ATF felt it was crucial to reach a decision on which paradigm should be pursued. Furthermore, this decision has an impact on several others.

In an attempt at understanding better the implications of these alternatives, several issues were subjected to detailed evaluations by a team lead by Craig Tull from LBNL using the corresponding framework software. Their report is contained in [19]. The issues that they evaluated were:

* Number of Data Types
* Number of Data Items
* Number of Modules
* Filtering
* Multiple Output Streams
* Adjustable Parameters
* Sharable Object Instances
* Ownership and Side Effects
* Connection Object Persistence
* Pass-through Data Objects

A summary of the advantages and disadvantages of each approach was also made (more details can be found in [19]). Although there are significant differences in the two approaches, the conclusion was that there was no over-riding problem that precluded either. However, there were concerns raised about scaling, ownership and filtering within the Object Networks approach. In the light of these concerns and with no overwhelming advantages offered by Object Networks, it was decided to recommend that a traditional control flow scheme be adopted for the ATLAS Framework. In considering Object Networks, the ATF recognised that the design has several good features (e.g. use of the observer pattern, smart pointers), and that it may prove valuable to incorporate some of these in the ATLAS implementation.

### Event

We discuss here some of the issues and requirements of the event components that were introduced in Section 5.3.

#### Design Choices

There are two important design choices which have been taken for Event:

1. A Transient Event Data Store is used to communicate data objects between modules.
2. Modules do not access persistent objects in the persistent store directly.

There is also a important policy choice which has been taken, and may need to be refined, but will have important implications on the eventual design of the interface to the event, or the Event Data Service:

1. Once put into the Transient Event Data Store, data objects should not be modified.

Design choice 2 above is a direct result of design choice 3 in Section 6.6 which treats more generally the transient and persistent issues. We therefore refer the reader to Section 6.6 for further understanding of this design choice. The other choices are discussed further in the following.

#### Role of the Transient Event Data Store

The Transient Event Data Store provides the role of a central repository for data objects which are exchanged between different modules during event processing. This can be viewed in Figure 6-2.

The _apparent_ data flow is from one module to the next. In this example, Module A accepts a data object of type T1 as input and produces two data objects of type T2 and T3 as output. The outputs T2 and T3 themselves are inputs to Module B and C, respectively. Finally, Module C receives as well a data object of type T4 from Module B and outputs a data object of type T5.

In contrast to the apparent data flow, the _real_ data flow is via a central repository, i.e. the Transient Event Data Store. This means that each module is actually fetching (pointers to) its input

Figure 6-2: The Transient Event Data Store serves as a central repository for data objects produced by modules (copied from the Gaudi Design Document [5]).

data objects from the transient store, creating output objects and then storing (pointers to) them in the transient store. Note that each module interacts with the Event Data Service to fetch and store data objects in the Transient Event Data Store, and that normally it is only pointers to data objects which are being passed around - there is no significant copying of the data.

The apparent and the real data flows provide two complementary views of the interaction of the modules and the Transient Event Data Store. With the apparent flow one can concentrate on the decomposition of a problem into a sequence of algorithmic steps, identifying the task of each module as well as who needs what from whom. The real data flow allows a more exploratory "what if" perspective where different modules put their "ideas" up on a "blackboard" and any other module downstream can examine them. Eventually, one keeps the good ideas and erases the uninteresting ones.

One consequence of this architecture is that not all data objects in the Transient Event Data Store are intended to be written out to the persistent event. For example, a pattern recognition module may output a large number of track candidates and it may only be the final tracks which one wants to save in the persistent event. It is the role of the Event Output component to organise the selection of what is written out from the data store.

#### 6.3.3 Ownership and Modifiability of Event Data Objects

It is the modules that create the data objects that go into the Transient Event Data Store. Until a module finishes execution, it may be considered as the owner of the data objects that it creates, and thus may delete ones that it may not want to put into the data store. However, when a data object is put into the data store (typically one of the last tasks a module does), the ownership is transferred to the Event Data Service. It is then the Event Data Service which is responsible for deleting the data objects in Transient Event Data Store at the end of each event loop, whether written out or not.

An important issue is the **modifiability** of data objects which have been put into the Transient Event Data Store. One consequence of having a data-centred repository is that all modules have access to all data objects. In order to avoid total chaos where data objects are changing when one does not expect them to change, we propose the general policy:

* Once put into the Transient Event Data Store, data objects should not be modified.

A consequence of this policy is that one module may not modify those data objects produced by another - it would have to create new ones and then modify them. This may not be the best policy for all use-cases, for example one may want to have several modules that have read/write access to the same set of data objects. An example of the "softening" of this policy can be found in CDF where they allow any module to extend collections of objects in the Transient Event Data Store and add new objects, but the existing objects in the collection cannot be modified.

Note that the _enforcement_ of a non-modification policy can be done in different ways. As in BaBar, enforcement can be done by simply making developers aware of the policy. One can be more strict and enforce non-modification with the const mechanism in C++, which is discussed in the next section. Finally, at the risk of adding more complexity, one can add some sort of access control where certain modules are given read/write access to specific sets of data objects.

#### Interface to Event Data Objects

It is the Event Data Service which provides modules with access to data objects in the Transient Event Data Store. We present here a list of some of the desirable aspects for the Event Data Service and provide some examples as to how these have been implemented elsewhere.

**Objects to be put in the Transient Event Data Store**

It is common to provide base classes for sets of data objects1, for example Figure 6-3 presents an example where a track set and hit set are the objects put into the Transient Event Data Store and each set contains a number of simple data objects which can refer to each other. Similar examples can be found in Gaudi [5] and the D0 Event Data Model (EDM) [8], which is used by both CDF and D0.

Footnote 1: We use the term “set” in a general way to refer to some type of collection, generally STL-like, and _not_ that it necessarily has set semantics, e.g. union and intersection.

It should be pointed out that the term "data object" refers to objects that primarily concentrate on their state, i.e. the data that they contain or represent. In Figure 6-3, _all_ of the classes can be considered "data objects", i.e. Track, Hit, TrackSet, HitSet. However, one generally only provides base classes for the larger collection-like objects. The exact design for base classes, etc. is part of the overall design of the Event Data Service and how it manages the Transient Event Data Store.

**Accessing Data Objects via a Logical Description**

What is meant by "logical description" is that some sort of identification scheme is used to specify which data object is desired. One example would be a simple flat organisation where the type name and a possible secondary identifying key are used to locate data object. A second example, as in Gaudi, would organise data objects in a logical directory structure. The ATF felt that a flat, unstructured bag-like organisation would be more appropriate for a "blackboard" central store where many data objects might not have a "logical" place in a hierarchical directory structure. For example, one does not have to worry about clients needing to re-synchronise to a new location when a producer changes the directory in which an output is stored. Whereas

Figure 6-3: Example of class model for event data objects.

the persistent store, which is more stable, would more naturally be hierarchically structured. The final decision is left to the Architecture Team.

The logical description will need to include some sort of **version identification** and a **selection** mechanism. For example, one may want to run with two different modules using different pattern recognition algorithms where each module produces its own set of tracks. Downstream modules will need some simple and clear means to select which track set they want to look at. This implies some sort of version identification. Note that in part, this issue is related to identifying the modules themselves and with which set of parameters they run to produce a particular output. The D0 EDM incorporates this concept by introducing a run control parameter object that provides parameters to modules and is used to identify, or tag, their output objects. An example of this is shown in the code given below. Finally, this selection of data objects can also be extended to a general mechanism which allows, for example, selection on any characteristic of a data object, i.e. any value returned by one of its methods.

#### Type-safe Access to Data Objects

In C++, this means that there is templated access to data objects via a pointer-like object which assures that an object of the correct type is returned1.

Footnote 1: i.e. no type casting is required by the user.

There may also be a variety of ways data objects may be returned:

* in iterator mode, where one gets directly an iterator over tracks,
* as single objects via a (smart) pointer,
* as a collection of pointers to data objects, or
* as a collection of pointers to copied objects, which can then be modified by a module2.

Footnote 2: This allows a module not to have to copy a whole collection itself in order to modify the contained objects. In this case, the module would become “owner” of the received data objects.

Note that in C++, one may also "enforce" the non-modifiability requirement of Section 6.3.3 by the use of the const mechanism. In this case, one would return a pointer to a const object and the compiler would not allow clients to call methods of this object which would modify the object.

We end this section with a (pseudo) C++ example from CDF which accesses a jet collection:

 // Create a collection of jets.  // Provide a pointer to the event, and a selector object  // (rcpKey will select jets in a pseudo-rapidity cone of 0.4).

 EdmCollection=Jets> myJetCollection(anEvent,rcpKey("Cone0.4"));

 // The collection has now been filled from the event.  // Iterate over the jets in an STL manner.  // One can also use STL algorithms, e.g. to select or sort  // the collection.

 EdmConstIterator<Jets> itJet;

 for(itJet=myJetCollection.begin();  itJet!=myJetCollection.end(); ++itJet) {  cout << " Jet number " << itJet->id() << " Has pt "  << itJet->pt() << endl; }

1. i.e. no type casting is required by the user.
2. This allows a module not to have to copy a whole collection itself in order to modify the contained objects. In this case, the module would become "owner" of the received data objects.

### Detector Description

As described in Section 5.3, the Detector Description contains the set of classes describing the geometry, materials and read-out information for the detector.

#### Design Choices

In the following, we list a set of design choices which have been taken. These will be discussed in more detail in the rest of Section 6.4.

1. _There must be a single source of detector description related information for all applications._ This means that applications such as the simulation, reconstruction, raw data decoding and event display all derive their detector description related information using the same service.
2. _Correlation between event data and detector description data is established via unique identifiers._ No decision was taken yet as to whether these identifiers should have a hierarchical structure, for example resembling directories of a file system.
3. _Applications access transient detector description information only through a logical interface. The logical interface is independent of the physical representation of the transient and persistent detector description store._

Some of these choices come from an earlier set of requirements (see Detector Description requirements contained in [20]) and as well were some of the underlying concepts of the ATLAS Muon Database (AMDB) [21]. While the ATF has not discussed all of these requirements, the discussion in this section reflects some of the relevant issues.

#### Issues and Requirements

Figure 6-4 gives a view of the Detector Description classes. As is true for the other data stores, clients interact with the Detector Description Service to obtain access to the detector descriptions in the Transient Data Store. The persistent detector description is separated from the Transient Data Store. The Detector Description Service interacts with the Detector Description Persistency Service to convert descriptions to/from the transient and persistent forms. Note that in Figure 6-4 we have omitted the some of the persistency details which are described in Figure 6-7.

Within the Transient Data Store, there is a central generic model from which are derived the specific models or views needed by the different applications. This is how design choice 1 is satisfied. The generic model is a succinct description where, for example, the positions of several detector elements are specified in a parametrised form. It is the generic model which is made persistent.

##### 6.4.2.1 Basic Detector Description Capabilities

Some of the key issues for the Detector Description are to provide the capability to:

1. Extract multiple detector description views with different levels of detail.
2. Access different tagged versions of a detector description.

To understand these capabilities, we refer to the Detector Description Data Store shown in Figure 6-4, which contains:

* Objects comprising the generic model.
* Converters which extract and perhaps transform objects from the generic model for specific views.
* Specific views available for the different applications.

Figure 6-4: Overview of Detector Description classes.

The first capability is realised by the converters that are used inside the Data Store to construct the different views. For example, one must be able to extract detailed descriptions for the Geant4 simulations as well as simpler descriptions for the reconstruction where a coarse granularity with average radiation or interaction lengths may be sufficient. Thus the definition of the generic model must be sufficiently general and complete to satisfy the various clients.

The second capability derives from the basic configuration management needed to be able to identify fully the detector description which is associated to each event being processed. For example for simulated events, one must be able to specify with a configuration tag the particular detector description used in the generation. This tag then should be sufficient to be able to retrieve fully the detector description for the reconstruction.

It is important to note that the view converters contain part of the "knowledge" needed to identify fully a particular detector description which is used by an application. One can discuss whether this architecture is cleanly decomposed where _both_ data objects and converters are part of the data store. However, the point is that at least today we consider that both the generic model and the converters need to be made "persistent" and be part of the configuration management tagging. That is _both_ should be part of the Detector Description and managed coherently. As an example, one can foresee that in the near term, the generic detector description will be stored as ASCII files, e.g. in XML format [22], and in the longer term, in a database. The view converters typically will just be stored in the standard code repository.

##### 6.4.2 Detector Description Tagging

One should note that the "tagging" of detector descriptions may be of two sorts:

* Time interval dependent tagging, as for the Conditions Data.
* For any given time interval, there may be different description versions.

During data-taking the detector description will change with time and thus most likely use the time interval infrastructure of the Conditions Data. But until beam turn-on, one will mostly need the latter form where a tag is used to identify versions within the evolution of the description itself.

##### 6.4.2.3 Links between Event and Detector Description Data

For the reconstruction of raw data, the Detector Description contains the objects which "know how" to decode the raw data, for example convert channel numbering to position in space. As each event is read in to be reconstructed, a connection must be made between the objects holding the raw data and the corresponding detector description objects. One way of doing this is to store identifiers with both the raw data and in the detector description. Then as each event is read in and objects brought into the Transient Event Data Store, one uses the identifiers to established direct pointers between the raw data and detector description objects. An example is to use hierarchical identifiers which specify active detector elements and read-out channels in a logical manner. For a more detailed discussion on the use of logical identifiers in ATLAS, see [23]. Finally, not only do raw data need to be decoded, but also they must be corrected for alignment and calibration. Thus there is another connection to the Conditions Data Store:

* Alignment and calibration corrections are _accessed_ via the detector description.

This means that the detector description objects mentioned above provide the event raw data objects with an interface both to decode their information and as well to access the needed corrections. Note that the Conditions Data Service itself provides a separate interface to the calibration and alignment corrections for input, examination, manipulation, etc.. It is only the event which accesses this information via the detector description.

### Conditions Data

Conditions data are characterised by the fact that they vary as a function of time (e.g. detector calibrations and alignments). Thus the value for any one item will have a time interval for which it is valid and access to the information will be based upon a key, typically being a time-stamp which is used to determine which time interval is valid.

Examples of conditions data such as detector alignment are further characterised by being derived from the event data themselves. Thus a reprocessing of event data might result in a more precise determination of the detector alignments, with different validity ranges than the earlier determinations. Therefore, the conditions data must not only be accessible by key, but also they must be capable of supporting multiple versions; and furthermore, any particular version corresponding to a given time interval must be accessible.

#### 6.5.1 Design Choices

1. _Conditions Data objects are derived from a set of unambiguous, non-redundant information independent of the client application._ All applications, no matter how detailed information they request, use the same underlying Conditions Data Store. The database does not contain information which can easily be derived from other information therein. For development purposes, it must be possible to work temporarily on a modified copy of the Conditions Data Store.
2. _Correlation between Event Data and Conditions Data is established via unique identifiers._ No decision was taken yet as to whether these identifiers should have a hierarchical structure, for example resembling directories of a file system.
3. _Applications access transient conditions information only through a logical interface. The logical interface is independent of the physical representation of the Transient and Persistent Conditions Data Store._
4. _Run dependent information is considered a type of time-dependent information and is stored in the Conditions Data Store._

### Transient and Persistent Issues

Each of the Data Components described in Section 5.3 have a similar architecture where Module Interface objects ("modules") can access data objects from different transient data stores. These data stores are in turn connected to persistent data stores so that data objects can be read in and written out. This section deals with issues that are pertinent to the connection of the transient and persistent stores. In the following, we use the term **database interface** to refer to the interface between these two stores.

#### Design Choices

In the following, we list a set of design choices which have been taken. These will be discussed in more detail in the rest of Section 6.6.

1. _There should be a database interface making ATLAS independent of database supplier._ This choice is the same as in the Mandate of the ATF (see Section 1.2), which resulted from the ATLAS Computing Review [4].
2. _The baseline choice for the persistency manager is an Object-Oriented DataBase Management System (ODBMS)._ This has been the baseline choice since the Computing Technical Proposal (see [24]). However, the final choice of which technology to use has not yet been taken.
3. _Persistent objects, when needed by the application, have a transient counterpart, and applications access only the transient representation._
4. _The database interface allows on-demand access to data objects coming from the persistent store._

#### Issues and Requirements

It is clear that when designing the overall architecture of an I/O system, one must consider many issues that affect performance and the fact there is a wide-area distribution of both the users and the data. However, what we are concerned with here is not so much the architecture of the I/O system itself but rather: "What is the desired coupling of the software system to the persistency mechanism?"

As in many situations this is a question of trade-offs where one must balance:

* access to the _advantages_ of a desired persistency technology, e.g. a particular ODBMS, and
* the ability to migrate to new technologies.

To understand better the question of coupling, let us start by taking the point of view of the application. Application clients, e.g. the modules, want to see data objects as if they were transient. In other words:

* They do not want to "know" about persistency details.
* They just want to be able to use standard C++ de-referencing semantics, e.g. given a pointer to an object, one can directly call its methods: data_object_pointer->data_object_method()
* They want to be able to traverse relationships, e.g. for the classes in Figure 6-5 one would like to go from a track set to each track, to each track's set of hits, and to each individual hit.
* They want on-demand access to data objects. This means that one should not be forced to read in the whole event just to be able to access a particular set of tracks and their hits. Rather objects are brought in from disk as needed, i.e. by requesting a specific object or by navigating relationships. One should note that this is one of the key advantages of an ODBMS.

This transparent access to persistent objects is certainly a highly desirable capability. However, one must keep in mind that somewhere in the software system there must be knowledge aboutwhich objects reside in memory, or are on disk or tape or across a WAN (wide area network). As well, the system must somehow keep track of and manage the I/O implications when relationships are followed or pointers de-referenced1.

Footnote 1: Of course, transparent access is both a blessing and a curse: without any system resource control, any “innocent” user may unwittingly abuse the system by simple navigation to objects of his choice.

As stated in design choice 1 in Section 6.6.1, we would like to design an architecture with a database interface which makes us independent of database supplier.

We interpret this to mean that application objects, e.g. modules, should be insulated from the knowledge of the specific database technology. To understand this a bit better, let us examine the two example architectures for persistency given in Figure 6-6.

Here both architectures maintain knowledge of the ODBMS to the left of the vertical dashed line and thus hidden from the application modules. Architecture 1) performs a systematic conversion from a persistent object to a transient object when the persistent object is brought into memory. In addition, there may be a subsequent conversion for a specific optimisation. The application modules point only to transient objects. In architecture 2), there is no systematic conversion and application modules may point directly to the persistent objects. Knowledge of the ODBMS remains "hidden" from the application module through the use of (smart) pointers2.

Footnote 2: For example, in Objectivity one can pin objects in the database cache and provide regular C++ pointers to application modules. Or smart pointers (objects which behave as pointers) can be used to hide the ODBMS details. For different technologies, one would only have to change the implementation of the smart pointers and not the application modules.

Architecture 1) provides a simple model for the separation of the transient and persistent worlds and the converters provide hooks, for example, to optimise storage and manage schema evolution. However, it requires one to implement on-demand access on the transient side and one must manage extra classes for each data object (converter plus persistent and transient versions).

Architecture 2) becomes architecture 1) if one uses conversions everywhere, e.g. for schema evolution. Architecture 2) does allow one to directly use ODBMS features, e.g. on-demand access, and reduces the number of classes. However, it may be more difficult to cleanly provide navigation from one persistent object to another independently of the specific technology.

Evaluating the two possible architectures, we have decided to propose architecture 1) for ATLAS.

Figure 6-5: Example of relationship traversal.

The basic architecture expressed using the components described in Section 5.3 is shown in Figure 6-7 where we see that there are three service components:

* manages the **Transient Data Store**. When clients (not shown) request objects not in the transient store, the **Data Service** communicates with the **Persistency Service**.
* manages the **Data Converters** which transform persistent objects to transient objects and vice versa. To do so, it communicates with the specific persistency manager, in this case **ODBMS**, which of course does the bulk of the I/O work.
* manages the transfer of persistent objects from disk to its own memory cache.

The elements of our architecture which have "knowledge" of the specific I/O system are well localised in the Persistency Service and the Data Converters. In this way we provide a clean and simple architecture which satisfies our first design choice of Section 6.6.1.

It should be noted that this architecture matches well the Event Filter requirements, allowing the mapping of the data in the on-line memory buffers into the objects of the Transient Event Data Store. Application objects, i.e. modules, are unaware of whether the objects are coming from the on-line memory buffer or some other source1.

Footnote 1: [https://www.david.org/](https://www.david.org/)

Defining the Data Converters which transform objects from one representation to another can be done in different ways. One way is to describe user data types to the system (this is usually

Figure 6-6: Example architectures for persistency: 1) systematic conversion of persistent objects to transient objects, application modules “see” only transient objects, 2) conversion of persistent objects only for specific purposes, application modules may “see” persistent objects via (smart) pointers. Architecture 1) is the proposed choice for ATLAS.

called _meta-data1_). In this way, one can have utilities that use the meta-data and automatically generate the converters or perform the conversions themselves. This is an elegant and relatively easy approach for simple data types. However, in more complicated situations this automatic conversion is not feasible, for example when aggregating a number of separate transient objects into a single persistent object, when dealing with complex object relationships, or when treating schema evolution. For this more complicated situation, explicit hand-written converter objects are needed. We do not explicitly state which of these mechanism should be used. This is left to the Architecture Team and the Database Group to define. As examples, we can say that today BaBar, CDF and Gaudi all have some sort of hand-written converters, and that D0 is using D0OM [25] as a means of automatic conversion.

Footnote 1: Of course the final implementation for the Event Filter must assure that the efficiency requirements are also satisfied.

The actual implementation of the transient/persistent "decoupling" can allow the switching between different persistency mechanisms, e.g. zebra and Objectivity, at compile-time, link-time, or run-time. It was requested by the ATF that

* We should aim for at least link-time, if not run-time switching between persistency mechanisms.

As a final point on data converters, it may be useful to note that in terms of user interaction with the system, it will be users that define the various transient types that are used as data objects, and if hand-written converters are needed, this will also entail user participation.

Figure 6: Separation of the transient and persistent representations (similar to illustration in the Gaudi Design Document [5]).

Finally, as discussed above and stated in design choice 4 of Section 6.6.1, we would like to see on-demand access to data objects coming from the persistent store. To understand better this point, look at the example in the Appendix where there is a concrete solution for on-demand access that uses proxy objects and smart pointers.

### Visualisation

#### Design Choices

The following design choices have come from those working in the Visualisation Domain:

1. _All data objects are independent of whether or not they can be visualised._ This means, for example, that visualisation attributes (e.g. colour) are defined within the Visualisation Domain, and are not attributes of the data objects itself.
2. _Applications communicate with the visualisation through an abstract interface._ Adding another visualisation toolkit does not require any changes on the application side.

We note that these choices ensure that the Visualisation is largely decoupled from the aspects of the Architecture considered here, and therefore we are confident that the Visualisation will fit smoothly into the Architecture. Hence, Visualisation is not discussed further in this report.

## 7 Physical Design

Large software systems such as that needed for ATLAS will necessarily be decomposed into a number of smaller, more manageable components. Due to coupling of the various components, a particular decomposition of the system can have important implementation-related consequences such as compile-time coupling, link-time dependencies, size of executables, etc.. For example, compile times can increase due to unwanted header files being included, or it may be impossible to work independently on a small part of the system because there are link dependencies to too much of the software system. The importance of these consequences means that the physical design should be integrated into the development before one can reasonably implement the logical design.

Using the terminology of Lakos [26], physical design focuses on:

* grouping the units of the software, e.g. classes, into **physical components**, typically a header and an implementation file, and then further grouping these physical components into larger units called **packages1** and Footnote 1: This is synonymous with the term class category which comes from Booch [27].
* both minimising and managing the dependencies between physical components and between packages.

A physical component contains a single class or a few closely coupled classes. Physical components depend upon each other: for example, class E may inherit from class A, or class E may use class A in either its interface or its implementation, and thus one can say component E depends upon component A. For large systems, one generally needs a higher level of physical organisation where related physical components are grouped into packages. Inter-package dependencies derive from the dependencies of their physical components. This can be seen in Figure 7-1.

Figure 7-1: Physical components, packages and their dependencies. Here package Y depends upon package X via the physical component dependencies: E on A, and G on B, C.

Dependency means some sort of compile-time coupling: if class E depends upon class A, then either the header file E.h or the implementation file E.cxx, must include the class definition in the file A.h. This can cause a cascade of inclusions: if E.h includes A.h then any other file that must include E.h also includes A.h, etc., and this can eventually increase the compilation time. Similarly, there are link-time implications: supposing there is a single library for package X and one for Y of Figure 7-1. Then any other package which uses classes E or G, which are only in library Y, will have to link with the libraries for the two packages. This increases the link time and the size of statically linked executables. One might simply say, "why don't we eliminate all coupling?" Taking the example of a track class which wants to use a three-vector class from CLHEP, the only decoupling solution would be to copy the three-vector class into every package that uses it. But then one would be left with a maintenance nightmare when one wants to make a change to the three-vector class which is located in many different places.

There are a number of practices which can limit the amount of coupling. One example is the use of abstract base classes. For example, the Application Manager described on page 26 will be calling a sequence of modules to execute during an event loop. In order to decouple the Application Manager from the packages containing the concrete types of modules, the Application Manager should only "know" about (i.e. have a pointer to) an abstract base class for the Module Interface class and not the module classes which inherit from it. Further useful techniques to manage the physical design are given in Lakos [26] and Martin [28].

Finally, it is important to stress that both packages and the physical components they contain should be organised into an acyclic hierarchy. This means that if class E uses class A, one should **not** allow class A to use class E1. Acyclic physical designs are relatively easy to understand, test and re-use incrementally, whereas cyclic dependencies tend to couple tightly components and packages and make the overall system difficult to decompose. Cycles between packages can mean that one may need to link twice with the library from the same package. In Figure 7-1 a cyclic dependency would arise if A, B, or C depended on any of the components D to H2.

Footnote 1: Often this is introduced inadvertently, e.g. class A uses class B and class B uses class C and then class C uses class A to make a cyclic dependency.

In summary, physical design is concerned with the grouping of classes, or physical components, into packages. Both minimising and managing package dependencies have important consequences for the implementation, i.e. compiling, linking and running. The most important point is that these issues need to be understood and managed; then one decomposes the logical design into various parts and begins the implementation. This is true for both the overall architecture as well as for the various pieces of the system.

## 8 Recommended Continuation of Work

### Architecture Team

In this Report, the ATF have set out the basic ideas for the ATLAS Architecture, in particular that associated with the Infrastructure. There are several tasks which lie ahead:

* More detailed **Design** of the key components of the Infrastructure, in particular, the responsibilities and interactions of the components, leading to the definition of interfaces.
* Realisation of the Architecture into a **Framework**.
* Support of the **Subsystems** to help them develop their own designs and coordination of areas of overlap.

As was emphasised at the start of Chapter 3, we believe that the development of the ATLAS Architecture is sure to benefit from the two-pronged approach of considering the problem anew and learning from experience. The continuation of this work, leading to an implementation of the ATLAS Architecture, should now be carried forward by a small dedicated group of people: **the Architecture Team**.

Ideally, the members of this Team

* Will have experience in OO Design and C++.
* Will understand the needs of the Detector and Physics Communities, with whom they will need to interact.
* Will be tightly coupled so that they can work efficiently.
* Will be able to dedicate a large fraction of their time to this work.

This Team should be led by a **Chief Architect** and should report to the Computing Steering Group (CSG). The Team should be selected by the Computing Coordinator, in consultation with the various interested parties.

The details of how the Team sets about its work are best left to the members themselves. However, we would identify the following tasks as very important:

* Develop the USDP design
* Develop the design for:
* Control flow and Data flow
* Event Data Model
* Detector description and other time-dependent information
* Merge the two approaches into a final design for ATLAS.

It is obvious, but we emphasise it nevertheless, that the Architecture Team must not work in isolation, but must engage in close and frequent interaction with the Detector and Physics communities and with the people working on the Database, Visualisation, etc..

It is important that this Team starts working as the ATF comes to the end of its mandate.

### Development Process

The process which has been used by the ATF in commencing the ATLAS software design work was introduced in Section 4.1. We propose that it should be used for the on-going development of the ATLAS Framework. Furthermore, it should be applicable to the development within the Subsystems. The methodology will have the most direct effect on the work of the Architecture Team and the various Coordinators.

### Work Packages and Time-table

The identification of components (see Chapter 5) allows some division of the work associated with the infrastructure into individual **work packages**. We would encourage the Computing Coordinator and Steering Group (CSG) in conjunction with the Architecture Team to undertake this task. These work packages should be accompanied by time-tables.

In accordance with the cyclic procedure to code development foreseen by the USDP, we would encourage the CSG to propose a suitable cycle time.

It would be useful if the ATLAS designs could be kept as close to those from other experiments as is compatible with ATLAS's own requirements. This would allow for the possibility of greater **collaboration**, either at the level of ideas, or design or even code re-use.

We would encourage the **public release of the first version of the ATLAS Framework** around **1st May 2000**. To ensure that this has a chance of success, it would be valuable to prototype various parts of the framework with a smaller group of developers and users. Coupled to this release, it would be valuable to have an **external review** of the design and code. The date of this should be determined by the Computer Coordinator and ATLAS Management.

## 9 Conclusions

### General Remarks

* We have attempted to listen to the community and by design the ATF membership is fairly representative. Therefore, we hope that our proposals can be broadly accepted.
* The ATF has not been able to look at everything in the detail they would have liked.
* We believe that this report provides an initial direction and focus for ATLAS in developing the ATLAS Common Framework.
* Of course, when it comes to **detailed** design and implementation, some of our proposals may need to be revisited.
* It will be valuable to follow the developments in other experiments (for example, BaBar, CDF, D0, LHCb and Alice) in the next years.

Concerning the Architecture itself:

* We have shown that the methodology which we have followed (based on the USDP) is capable of producing a design of the Architecture. However, to do this, it needs to be pursued with vigour.
* The results from the USDP approach must be reconciled with the components identified in Chapter 5 and with the on-going work in various domains. This comparison cannot be made at present; but assuming the USDP approach is followed, then the comparison will have to be made, and decisions should be taken to define a unique path for ATLAS.
* It is important to proceed to a first implementation of the ATLAS Common Framework as soon as possible. This is the only real way to verify the design and choices and to take corrective action in an iterative manner.
* Prototyping of the Event and the Detector Description can proceed immediately and indeed has already started.

Concerning the work in the Detector Subsystems:

* The subsystems should pursue their own design work in conjunction with the Architecture Team.
* To enable the subsystems to make progress in the near-term, they should aim for a high level of modularity in their code so that it is easier to incorporate when the ATLAS Common Framework comes into existence.
* The Subsystems should anticipate the use of the Event for communication and continue to participate in the development of the Detector Description.
* Until the first release of the ATLAS Common Framework, they are encouraged to use the Geant4 and Pago programs for simulation and reconstruction testing, respectively.

### Open Issues

Clearly, with a great deal of the design work still needing to be tackled, there are many open issues related to implementation. Some of these are highlighted with examples in Chapter 6.

Open issues which have been touched on by the ATF include:

* The separation or merger of Detector Description and Conditions Data (Section 5.3).
* The granularity of modules (page 33).
* The modifiability of event data objects. We propose that once put into the Transient Event Data Store, data objects should not be modified (page 38). This is one approach to managing the modification of data objects. However, this may result in unacceptable constraints, and therefore a different approach may need to be considered (Section 6.3.3).
* The structure of the Event Data Store: flat or hierarchical (page 40).

### Major Decisions

The major decisions contained in this report are:

* Data and algorithms should be implemented as different objects (page 11).
* Access to information from an input event should be "on-demand" (also called "deferred" access). This means that only the data which is actually required is read into memory (page 34).
* All access to event data information by modules should be via the Transient Event Data Store (page 34).
* There should be no direct communication between modules (page 34).
* A traditional control flow scheme should be adopted for the ATLAS Framework (Section 6.2.3).
* There should be a single source of detector description related information for all applications (page 42).
* There should be a database interface making ATLAS independent of database supplier (page 46).

## Acknowledgements

We would like to thank those people who have contributed to the work of the ATF, in particular:

* John Harvey (LHCb), Pere Mato (LHCb), Marc Paterno (D0), Federico Carminati (Alice), Julius Hrivnac, Alan Poppleton and Lassi Tuura who gave us presentations,
* Craig Tull and colleagues from LBL who investigated control and data flow
* and all of those in ATLAS who contributed ideas, and especially use-cases.

## References

* [1] ATF Auxiliary Report: "Analysis and Design of the ATLAS Common Framework".
* [2] ATF Auxiliary Report: "Use-case Model and Supplementary Requirements of the ATLAS Common Framework".
* [3] Management Computing Action Plan [http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/action_plan](http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/action_plan)
* [4] Report of the ATLAS Computing Review Committee.
* [5] Gaudi Web page [http://lhcb.cern.ch/computing/Components/html/GaudiMain.html](http://lhcb.cern.ch/computing/Components/html/GaudiMain.html) LHCb, Architecture Design Document [http://lhcb.cern.ch/computing/offline/postscript/add.ps](http://lhcb.cern.ch/computing/offline/postscript/add.ps)
* [6] ATF Web page [http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/](http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/)
* [7] E.D. Frank et al., "Architecture of the BaBar Reconstruction System", Proceedings of the Computing in High Energy Physics Conference, Berlin, April 1997.
* [8] M. Paterno, talk to that ATF on 13/08/99, [http://cdspecialproj.fnal.gov/talks/D0_EDM_for_ATLAS/D0_EDM_for_A](http://cdspecialproj.fnal.gov/talks/D0_EDM_for_ATLAS/D0_EDM_for_A) TLAS.ps
* [9] ATLAS Detector and Physics Performance TDR, Vol I & II, CERN/LHCC 99-14 & 99-15.
* [10] Geant4 Web page [http://wwwinfo.cern.ch/asd/geant4/geant4.html](http://wwwinfo.cern.ch/asd/geant4/geant4.html)
* [11] R. Candlin, D. Candlin, Pasco Documentation [http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/applications/Pa](http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/applications/Pa) so/
* [12] ATF Web page on Short-term Prototyping [http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/pr](http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/pr) prototyping_2.html
* [13] DIG Working Group, Architecture Design Document Set [http://home.cern.ch/~lat/notes/architecture.html](http://home.cern.ch/~lat/notes/architecture.html)
* [14] I. Jacobson, G. Booch, J. Rumbaugh, "The Unified Software Development Process", Addison-Wesley (1999).
* [15] K. Amako, talk on Methodology at ATF Meeting #5 [http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/methododology.ps](http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/methododology.ps)
* [16] G. Booch, J. Rumbaugh, I. Jacobson, "The Unified Modelling Language User Guide", Addison-Wesley (1999)
* [17] For ATLASIM, see ATLAS Simulation Web page [http://www.cern.ch/Atlas/GROUPS/SOFTWARE/DOCUMENTS/simulation.htm](http://www.cern.ch/Atlas/GROUPS/SOFTWARE/DOCUMENTS/simulation.htm) l#atlsim
* [18] L. Tuura, Master's Thesis, "Component Based Reconstruction Prototype for ATLAS Electromagnetic Calorimeters" (1996) [http://home.cern.ch/1/lat/www/exports/thesis/](http://home.cern.ch/1/lat/www/exports/thesis/)* [19] C. Tull et al., Control Framework Issues [http://arc.nersc.gov/control/onvsg/framework_issues.html](http://arc.nersc.gov/control/onvsg/framework_issues.html)
* [20] Requirements for ATLAS Software [http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/deliverables/AT](http://atlasinfo.cern.ch/Atlas/GROUPS/SOFTWARE/OO/deliverables/AT) L-SW-Atlas-Requirements-V2/
* [21] Saclay Muon Software Group, talk at the ATLAS Database Group meeting on 11/03/97 [http://atlasinfo.cern.ch/Atlas/GROUPS/MUON/AMDB_SIMREC/vm/doc/dtb.ps.gz](http://atlasinfo.cern.ch/Atlas/GROUPS/MUON/AMDB_SIMREC/vm/doc/dtb.ps.gz)
* [22] ATLAS Detector Description Web page [http://www.cern.ch/Atlas/GROUPS/DATABASE/detector_description/](http://www.cern.ch/Atlas/GROUPS/DATABASE/detector_description/)
* [23] RD. Schaffer, talk at CHEP98, [http://home.cern.ch/s/schaffer/www/slides/chep98/overview-db-activities.pdf](http://home.cern.ch/s/schaffer/www/slides/chep98/overview-db-activities.pdf)
* [24] ATLAS Computing Technical Proposal, CERN/LHCC/96-43.
* [25] D0 Collaboration, [http://www-d0.fnal.gov/newd0/d0atwork/computing/infrastructure/d0om/d0om.html](http://www-d0.fnal.gov/newd0/d0atwork/computing/infrastructure/d0om/d0om.html)
* [26] J. Lakos, "Large Scale C++ Software Design", Addison-Wesley (1998).
* [27] G. Booch, "Object-Oriented Analysis and Design with Applications", Benjamin-Cummings (1994).
* [28] R. Martin, "Designing O-O C++ Applications using the Booch Method", Prentice Hall (1995).
* [29] E. Gamma et al., "Design Patterns", Addison-Wesley (1998).