# ATLAS Internal Note

DAQ-No-63

28 January 1997

Evaluation of a Congestion Avoidance Scheme

and Implementation on ATM Network based Event Builders

D. Calvet, P. Le Du, I. Mandjavidze

CEA Saclay, 91191 Gif-sur-Yvette CEDEX, France

M. Costa, J.-P. Dufey, M. Letheren, C. Paillard

CERN, 1211 Geneva 23, Switzerland

###### Abstract

This paper addresses the design of event builders based on switching networks for data acquisition systems in the field of High Energy Physics. We show that the use of a simple congestion avoidance scheme based on a connection permutation algorithm can eliminate the potential contention in the switching network and make most popular network topologies adequate to carry event building type of traffic. The performance of this scheme is presented and some techniques to improve its efficiency are outlined. We describe implementation issues and show how to adapt our scheme to build a system based on standard ATM components. We present simulation results that show the expected performance of this implementation. We detail the additional benefits of using ATM technology for event builders and outline some possible applications to future physics experiments.

## I Introduction

The next generation of High Energy Physics experiments, ATLAS [1] and CMS [2], proposed at the CERN Large Hadron Collider (LHC), will place heavy demands on the data acquisition and on-line filtering systems. Sophisticated multi-level selection systems will reduce the raw data flow from a few tens of TBytes/s down to the several tens of Mbyte/s that will then be recorded on permanent storage for subsequent off-line analysis. The use of large switching networks is envisaged to handle the several tens of Gbit/s of data bandwidth that remains after a first level of selection. The RD-31 project [3] aims at evaluating an approach to data acquisition based on the use of standard Asynchronous Transfer Mode (ATM) packet switching technology [4]. The main activities of the collaboration are to propose event builder architectures, evaluate their performance by means of simulation and implement small scale demonstrators to validate the concepts that were developed.

This paper deals with event builder architecture and performance evaluation. It is organized as follows. The problem of event building and some requirements for the design of an event builder are presented in Section II. The problem of congestion in event builders is briefly described in Section III. Some network topologies suited to event builders as well as a congestion avoidance scheme are described in Section IV. The performance of this scheme is expounded in Section V. Some possible improvements are presented in Section VI. General implementation issues are discussed in Section VII and a solution based on ATM is presented in Section VIII. The additional benefits of using ATM for event builders along with possible applications to future physics experiments are outlined in Section IX. Section X summarizes and concludes the paper.

## II The Problem Of Event Building

### _Basics_

For LHC experiments, it is commonly admitted that the last level of on-line selection of events will be based on sophisticated algorithms that will process a large part of the available event data. It is envisaged that the processing of a given event will be done by a single processing node. For each event, the data that has to be transferred to the selected processing node is spread across a variable number of sources (\(\sim\)1000 at most). Furthermore, a single processor will certainly not have enough computing power to cope with the rate of incoming events. Therefore, a farm of processors will be needed. It is foreseen that as many as \(\sim\)1000 processing nodes might be needed for each of the ATLAS and CMS trigger systems to process in real-time the incoming events at the targeted rate of \(\sim\)1-10 KHz. A large switching network will be required to link all data sources to the farm of processors. The system that collects data from many sources and delivers complete events to a given processor is called an event builder [3]. A very simplified view of an event builder is shown in Figure 1.

An event builder includes a switching network with its appropriate control, a number of data sources, a number of des

Figure 1: Event Buildertination modules that collect event fragments and re-assemble the corresponding events. The software that controls each part of the system is beyond the scope of this paper.

We shall now present some of the most desirable properties of the switching network.

### _Requirements for the network of an event builder_

For LHC experiments, one of the most important requirements of the network of the event builder is that it has to offer an aggregate bandwidth of the order of \(\sim\)10-100 Gbit/s to interconnect several hundreds of nodes. The network has to work in a reliable way under the event building type of traffic (i.e. concentration of event data from many sources towards one destination, correlated sources...). The traffic is almost unidirectional with most of the flow going from the data sources to the destination processor farm. The packet routing latency should be low enough to allow for the routing of several thousands of packets at a few kilo-Hertz rate.

In order to reduce the cost of development and ease maintenance, the use of industrial standard equipment is strongly recommended. Technical support during the lifetime of the experiment is essential (\(\sim\)10-15 years). To facilitate installation and test, it is also desirable that the network can be partitioned. The system should be scalable to accommodate future upgrades. Efficient tools for network management, monitoring, fault diagnosis and isolation are needed. Obviously the cost of the system must fit within the budget of the physics experiments.

## III Congestion In Event Builders

As previously mentioned, the event building type of traffic is very specific. A potentially large number of sources can simultaneously send data to the same destination. If no care is taken, this continuous concentration of traffic can exceed the available bandwidth of the output link. This can induce long transmission latencies and even data losses due to buffer overflow in some elements of the network. Several techniques have been proposed by the RD-31 collaboration to reduce the contention in event builders based on ATM switches. These techniques fall into two main categories. Rate division techniques are used to prevent the sources from exceeding the available bandwidth of an output link [5]. Traffic shaping techniques are used to artificially break the time correlation between different sources in order to avoid contention within the switching fabric [3]. In this paper, we will study in more details a particular traffic shaping technique, and more specifically how a simple congestion avoidance scheme can eliminate contention within the switching network of an event builder.

## IV Non Blocking Event Builders

### _Network topologies for event building_

At first, we shall recall some of the requirements for the network of an event builder with regard to the connectivity. It is required that the network can provide a path from any data source to any destination processor. It may or may not be required that every destination has a path to every source. A separate path can be envisaged to carry the relatively small amount of traffic flowing from the destination processors to the data sources. It may not be required that a path can be established between sources. A path between destination processors might not be needed either. It is acceptable that a given destination has its connection to each source established one at a time in a sequential manner. Reciprocally, it is acceptable that each source is connected successively to all destinations. Ideally, all connections between _any_ pair of source/destination can be established at a given instant in a non-blocking fashion. Nonetheless, it is sufficient that all connections between _selected_ pairs of source/destination can be established at a given instant in a non-blocking way. We shall now describe how popular network topologies can fulfill those requirements.

Since the dawn of the telephony era, there has been an enormous amount of research to design non blocking switches with a number of crosspoints less than that of a single-stage crossbar matrix. Many different multistage interconnection networks have been proposed: Banyan, Delta, Clos [6]. The performance of those networks under various types of traffic has been studied extensively [7, 8]. It is well known that a Banyan switch is blocking: even when every input is assigned to a different output, as many as \(\sqrt[]{\mathrm{N}}\) connections may be contending for the use of the same internal link. Several techniques have been proposed to render Banyan networks non-blocking. One possibility is to add a sorting network in front of the Banyan network (Batcher-Banyan network) [9]. Another possibility is to run internal links faster than input and output links. For example, the 8-port AT&T Phoenix switch is fully non-blocking because it has an internal bandwidth expansion of 2 [10]. For the problem of event building, the key point is that the use of a non-blocking network is not mandatory. A non-blocking network provides a path between any pair of source/destination, but, as previously stated, it is sufficient that a path between selected pairs of source/destination can be established in a non-blocking way. When every input is assigned to a different output correctly selected, Banyan switches, Delta switches and others are non-blocking. This is illustrated below on an Omega network.

By successive shuffling and exchange operations, the Omega network can perform not all but some important permutations of its inputs in log\({}_{2}\)N steps [11]. In particular, it can perform cyclic shifts and permutations that connect input i to output (t+i) Mod N. We show in Figure 2 the configuration where t=1, that is input i is connected to output (i+1) Mod 8.

Figure 2: Omega Network

[MISSING_PAGE_FAIL:3]

\[\mathrm{T_{s}}=\frac{\mathrm{Max_{i\,e\,[0,N\,-1]}(S_{i})}}{\mathrm{L}} \tag{5.3}\]

shared between all destinations. \(\mathrm{T_{B}}\) is proportional to N. The distribution of the largest event fragment among N is:

\[\mathrm{f_{Max(S_{j})}(x)}=N\cdot\mathrm{F(x)}^{N\,-1}\cdot\mathrm{f(x)} \tag{5.4}\]

* case b. C\(>>\)Max(S\({}_{i}\))/L

This case will be referred to as the "event based barrel shifter" [3]. We have to consider now the amount of time required to send the last (and not the largest) event fragment. That is:

\[\mathrm{T_{s}}=\frac{S_{\mathrm{last}}}{\mathrm{L}} \tag{5.5}\]

Where S\({}_{\mathrm{last}}\) is the size of the last fragment required to complete the event. In fact, S\({}_{\mathrm{last}}\) can be any fragment, hence the distribution of S\({}_{\mathrm{last}}\) is f(x).

We will now discuss the second term of the event building latency, T\({}_{\mathrm{Q}}\). If the load on each link is low, the amount of time that a non-zero size event fragment is queued in a source is a fraction of (N-1)C time units. It is the amount of time required for the system to change from its current connection configuration pattern to the configuration that allows the transmission of a particular fragment. The distribution of T\({}_{\mathrm{Q}}\) is uniform in [0,(N-1)C].

At least one complete cycle of connection permutations is needed for the event building if all event fragments are of non null-size. Otherwise, a fraction of the complete cycle might be sufficient for case a), and is always sufficient in case b). For the latter case, if we consider that the k-last event fragments are of null-size, T\({}_{\mathrm{Q}}\) is in the interval [(N-1-k)C,(N-1)C]. The probability that the k-last fragment are of null-size is f(0)\({}^{\mathrm{k}}\).

The distribution of the event building latency T\({}_{\mathrm{B}}\) can be obtained by linear convolution of the distribution of T\({}_{\mathrm{Q}}\) and T\({}_{\mathrm{S}}\). It can also be obtained by means of simulation. We present in Figure 4.a the results obtained by simulation with N=64, L=1, C=1, S uniformly distributed in [0,31] and R arbitrarily large.

This corresponds to an operation in mode a). It can be seen that T\({}_{\mathrm{Q}}\) is negligible compared to T\({}_{\mathrm{S}}\). Hence the distribution of T\({}_{\mathrm{B}}\) follows a law in x\({}^{\mathrm{N-1}}\) as stated in (5.4). We also present the results obtained by simulation for operation in mode b) with the same parameters but C=32 and S in [0,1]. It can be seen that T\({}_{\mathrm{S}}\) is negligible compared to T\({}_{\mathrm{Q}}\). The highest peak in the distribution correspond to the case where the last fragment is not of null size. Other peaks correspond to an increasing number of null size event fragments. It can be seen that the height of the peaks follows a law in 2\({}^{\mathrm{k}}\) since the probability that an event fragment is of null size is 1/2.

When the load on source links is increased, the probability that several event fragments to be routed to the same destination are present in a particular source at a given instant becomes non null. Hence an additional queuing delay is introduced. This delay is null if the time required to send an event fragment to a destination is always shorter than the amount of time between two successive events assigned to this destination. With the approximation made for case a), this happens when:

\[\mathrm{R>\frac{Max_{\mathrm{x}}(S_{i})}{L}} \tag{5.6}\]

This correspond to \(\sim\)50% load if f(x) is uniform and if the time interval between events is constant. This condition is almost satisfied if R is Poisson distributed for large N. For case b), it can be estimated that no additional queuing is introduced if R\(\sim\)C. When queuing occurs, the performance is rapidly degraded while the load is increased. The limitation does not come from the saturation of the links of the switching network but from that of the sources. We present in Figure 5.a the average event building latency versus load for case a) and b) with R constant, N=64, L=1 and S in [0,31] for various values of C.

The normalized event building latency, T\({}_{\mathrm{Bnorm}}\) equals T\({}_{\mathrm{B}}\) divided by NS/L which represents the time needed to transfer an event of average size. It can be seen that the minimum normalized event building latency equals 2. This comes from the fact that S follows a uniform distribution, hence S\({}_{\mathrm{max}}\) is twice S\({}_{\mathrm{av}}\). The system does not saturate for a load up to \(\sim\)75%. However, as soon as several event fragments for the same destination are queued in a source (condition stated in 5.6), the corresponding destination can have more than one event in reassembly phase at a given moment. The average number of events in re-assembly phase per destination is plotted in Figure 5.b.

## VI Increasing Efficiency

As was previously explained, the switching network with an appropriate synchronization can be viewed as a non-blocking

Figure 4: Event Building latency distribution

Figure 5: Performance versus load

network providing a bandwidth L/N from each source to each destination. It is therefore important that all sources have more or less the same needs in terms of bandwidth. Furthermore, there should not be any privileged destination in the system. A correct load balancing is important. If the amount of data generated by a source has a large dispersion, long queuing delays will be introduced. It is therefore desirable to create groups of sources connected to a single network node in a coherent way.

## VII Implementation Issues

The implementation of the proposed scheme can be done in several ways. It can be envisaged to develop all components with specific hardware. A connection switching network is adequate. Each switching element can be very simple because it does not need to include buffers. All sources and the switch have to be synchronized and specific hardware has to be implemented for this purpose. The obvious advantage of this method is that the network can be tailored to the exact needs of the users. It is rather simple and cost effective. The main disadvantage of this approach is that all components are specific. The entire hardware and software has to be designed, documented and maintained. For LHC experiments, it is strongly recommended that industrial technologies are used whenever possible. Hence packet switching technologies can be considered. The implementation of this scheme on a circuit switched or packet switched network requires that:

* all sources can be synchronized,
* each source has the capability to transmit to a given destination within a given time slot.

If none of those requirements are met, the operation of the event builder is possible only at very low load. This is obviously the most inefficient solution.

Both requirements must be met if the network has little or no buffering capabilities. In most cases, it should be possible to relax one of those constraints. This is particularly true for the first requirement which might be difficult to meet if the time slot C is of the order of a few \(\upmu\)s. A dead-time between time slots cannot be avoided. The results we presented remain valid if we consider that the fraction of the link bandwidth that corresponds to this dead-time is wasted. If an operation with a larger period C is acceptable, software implementation of source synchronization can be envisaged. This is certainly the case in ALICE [14] since the event fragments are of relatively large size. The second requirement is difficult to meet with most networking technologies (e.g. Ethernet) that only allows to send packets on a first in first out basis, at link speed (i.e. they implement the source model described in Figure 3.a). We will show in the next paragraph that this limitation does not exist in the case of ATM. If it is acceptable that the system operates in the event based barrel shifter mode and at low load, the second requirement need not to be satisfied if the first one is met.

## VIII Implementation With ATM

The proposed scheme needs to be adapted in order to facilitate its implementation using standard ATM components. There is no simple way to achieve a perfect synchronization of all sources if the time slot is of the order of a few \(\upmu\)s. Hence requirement r1) can be difficult to meet especially if a software synchronization mechanism is used. On the other hand, requirement r2) is very easy to meet using ATM. One of the main features of ATM is that it allows to share the bandwidth of a link between users. When several Virtual Connections (VC) share the same physical link, the packets transmitted on different VCs are interleaved at the level of ATM cells. A possibility for the implementation of requirement r2) is to open a permanent VC with a constant bit rate of L/N between each source and each of the N destinations. In this way, each source will send to a given destination during a time slot proportional to 1/N. Another possibility is to open variable bit rate channels and specify that the average rate of each VC is L/N, and that the maximum size of a burst is 1 cell (that is the peak bit rate is L/N). Although the ATM standards do not specify the exact method for partitioning bandwidth among applications, designers of Segmentation And Re-assembly (SAR) chips use variations of the same mechanisms in their implementations. For example, the transmit scheduler of the IDT NicStar [14] uses a transmit schedule table that specifies explicitly how to interleave the segmentation of packets to be transmitted on different VCs. More sophisticated algorithms are used in Siemens' SARE [16] or in PMC-Sierra's Lazar [17]. Future chips will implement the Available Bit Rate service (ABR).

We have shown in section IV that the network of an event builder does not need any buffering when all sources are correctly synchronized. Reciprocally, the buffering capability of the network can be used to absorb the contention induced by the phase difference of de-synchronized sources. The network should include enough buffering so that contention does not create data losses due to internal buffer overflow. We have simulated the operation of an ATM based event builder where sources implement a NicStar-like interface with no specific synchronization. This corresponds to r2) satisfied but not r1). The model for the ATM switching fabric is a Banyan network connecting 256 sources to 256 destinations. Each cross-point uses a common buffer for its 16 output queues. All sources are identical and generate event fragments of 4 Kbytes (Erlang distribution). The size of the system and the amount of event data correspond roughly to what is expected for the ATLAS experiment.

We present in Figure 6.a the average normalized event

Figure 6: ATM based event builder operation

building latency \(\text{T}_{\text{Bnorm}}\). At low load, \(\text{T}_{\text{Bnorm}}\) is determined by the ratio \(\text{S}_{\text{max}}/\text{S}_{\text{av}}\). It can be seen that the operation of the event builder is satisfactory for loads up to ~70%. We plot in Figure 6.b the probability that the number of cells buffered in a given switching element exceeds a certain value for different loads. It is sufficient that each switching element can buffer ~600 cells as shown in Figure 6.b. This is a reasonable value since existing ATM switches like Fore Runner ASX-100 include 4096 cells of buffering per output, that is 64K cells, for a 16 port configuration [18]We can therefore conclude that the operation of the ATM event builder is possible without source synchronization, but with the partitioning of bandwidth supported by ATM technology.

## IX Additional Benefits of using ATM

There are several advantages of using ATM components in event builders. A first set benefit come from the fact that ATM seems to gain acceptance in the telecommunication market and for LAN backbone switches. Prices become affordable and more and more vendors offer a large choice of ATM components, switches, interfaces, testers, etc... The large investments and support from the industry offer a good guarantee for the availability of ATM hardware and software in the long term.

Other advantages of using ATM to implement event builders for data acquisition applications are the capability of sharing the bandwidth (rate division) so as to avoid congestion in the network as previously explained. We have obtained encouraging results with an 8 x 8 event builder demonstrator as reported in [19]. We showed in [5] the benefits of the rate division technique and explained how the same physical ATM link can accommodate various types of traffic. The proposed architecture "C", for the ATLAS experiment, implements phased event building where only a subset of sources participate in the event building process [20]. The processor analyzing the event decides on the fly which data is needed and sends requests to the corresponding sources. A single ATM network to carry both data and protocol traffic has been proposed.

This paper focused on the transfer of data from sources to destination processors. However, in a real system, some information (run control, performance monitoring, error recovery, etc...) is exchanged between all nodes. All these messages could use the same ATM network that is used to transfer data. If a separate network is considered, all network interfaces, links and switches have to be duplicated. This increases system complexity and cost but may not improve its robustness and reliability.

We believe that ATM technology is a sensible choice for the design of event builders and that it can solve a number of issues in an elegant way.

## X Summary

This paper addressed the design of event builders based on switching networks for data acquisition systems in the field of High Energy Physics. We first recalled the basic principles of event building. The requirements for the network of an event builder with regard to the connectivity were analyzed. We showed that the use of a simple congestion avoidance scheme based on a connection permutation algorithm can eliminate the potential contention in the switching network and make most popular network topologies adequate to carry event building type of traffic. We presented both analytical and simulation results that show the performance of this scheme. Some techniques to improve its efficiency were outlined. We described implementation issues and presented a design based on standard ATM components. Simulation results showed the expected performance of this implementation. We detailed the additional benefits of using ATM technology in the design of event builders. Some possible applications to future physics experiments were outlined.

## XI References

* [1] ATLAS collaboration, "Technical Proposal for a General-Purpose pp Experiment at the Large Hadron Collider at CERN", CERN/LHCC/94-43, December 1994.
* [2] CMS collaboration, "The Compact Muon Solenoid Technical Proposal", CERN/LHCC/94-39, December 1994.
* A high performance data-driven event building architecture based on an asynchronous self-routing packet-switching network", CERN / LHCC/95-14.
* the Broadband Telecommunications Solution, IEE Telecommunications Series 29, ISBN 0 85296 815 9, London, 1993.
* [5] D. Calvet et al., "A Study of Performance Issues of the ATLAS Event Selection System Based on an ATM Switching Network", IEEE Transactions on Nuclear Science, Vol. 43. No 1. February 1996, pp. 90-98.
* [6] H. Ahmadi et al., "A Survey of Modern High-Performance Switching Techniques", IEEE Journal on Selected Areas in Communication, Vol. 7. No 7. September 1994, pp. 1091-1103.
* [7] Y-C. Jenq, "Performance Analysis of a Packet Switch Based on Single-Buffered Banyan Network", IEEE Journal on Selected Areas in Communication, Vol. Sac-1. No 6. December 1983, pp. 1014-1021.
* [8] D.M. Dias et al., "Analysis and Simulation of Buffered Delta Networks", IEEE Transactions on Computers, Vol. C-30, No 4, April 1981, pp. 273-282.
* [9] K. E. Batcher, "Sorting Networks and Their Applications", in Proc. Spring Joint Computer Conference, AFIPS, 1968, pp. 307-314.
* [10] V. P. Kumar et al., "PHOENIX: A Building Block for Fault Tolerant Broadband Packet Switches", IEEE Global Telecommunications conference, December 1991, pp. 228-233.
* [11] T. Lang, "Interconnections Between Processors and Memory Modules Using the Shuffle-Exchange Network",IEEE Transactions on Computers Vol. C-25, No 5, May 1976, pp. 496-503.
* [12] M. J. Karol et al., "Input Versus Output Queueing on a Space-Division Packet Switch", IEEE Transactions on Communications, Vol. COM-35, No 12, December 1987, pp. 1347-1356.
* [13] M. Nomachi, "Event Builder Queue Occupancy", SDC Note 93-566, August 1993.
* [14] ALICE Collaboration, "Technical Proposal for A Large Ion Collider Experiment at the CERN LHC", CERN/LHCC/95-71, December 1995.
* [15] "IDT 77201 NicStar", User Manual, IDT Inc., Santa Clara, November 1995.
* [16] "SARE PXB 4110", IC for Communications Data Book, Siemens, August 1995.
* [17] "PM 7375 Lazar 155", Programmer's guide, PMC-Sierra Inc., March 1996.
* [18] S. Bush et al., "Switching to ATM", Network World Magazine, February 1994, pp. 37-39.
* [19] M. Costa et al., "Lessons from ATM-based event builder demonstrators and challenges for LHC-scale systems", Proceedings of the 2nd Workshop on Electronics for LHC Experiments, Balatonfuered, Hungary, September 23-27 1996.
* [20] J. Bistricky et al., "Single Processing Farm Option for the ATLAS Event Selection", Proceedings of the 2nd Workshop on Electronics for LHC Experiments, Balatonfuered, Hungary, September 23-27 1996.