**Full Supersymmetry Simulation for ATLAS in DC1**

**Michela Biglietti\({}^{a}\), Frederic Brochu\({}^{b}\), Davide Costanzo\({}^{c}\), Kaushik De\({}^{d}\), Ehud Duchovni\({}^{e}\), Ambreesh Gupta\({}^{f}\), Ian Hinchliffe\({}^{c}\), Chris Lester\({}^{b}\), Anna Lipniacka\({}^{g}\), Peter Loch\({}^{h}\), Else Lytken\({}^{i}\), Hong Ma\({}^{i}\), Jakob L. Nielsen\({}^{i}\), Frank Paige\({}^{j}\), Giacomo Polesello\({}^{k}\), Srini Rajagopalan\({}^{j}\), Dan Schrager\({}^{e}\), Georgios Stavropoulos\({}^{c}\), Dan Tovey\({}^{l}\), Monika Wielers\({}^{m}\)**

\({}^{a}\)Universit di Napoli \({}^{*}\) Federico II\({}^{*}\), Napoli, Italy

\({}^{b}\)Cambridge University, Cambridge, UK

\({}^{c}\)Lawrence Berkeley National Laboratory, Berkeley, CA USA

\({}^{d}\)U. of Texas at Arlington, Arlington, TX USA

\({}^{e}\)Weizmann Institute, Rehovot, Israel

\({}^{f}\) University of Chicago, Chicago, IL USA

\({}^{g}\) University of Bergen, Norway

\({}^{h}\) University of Arizona, Tuscon, AZ USA

\({}^{i}\) Niels Bohr Institute, Copenhagen, Denmark

\({}^{j}\)Brookhaven National Laboratory, Upton, NY USA

\({}^{k}\)INFN, Sezione di Pavia, Pavia, Italy

\({}^{l}\) University of Sheffield, Sheffield, UK

\({}^{m}\)LPSC Grenoble, France

This note reports results from a simulation of 100k events for one example of a minimal SUGRA supersymmetry case at the LHC using full simulation of the ATLAS detector. It was carried out as part ATLAS Data Challenge 1.

###### Contents

* 1 Introduction
* 2 Event Generation and Simulation
* 3 Event Reconstruction
* 4 Reconstruction Performance
	* 4.1 Monte Carlo Truth and Comparison Methodology
	* 4.2 Muons
		* 4.2.1 Moore/MuID
	* 4.3 Electrons
	* 4.4 Jets
		* 4.4.1 Effect of Calorimeter Noise on Jets
	* 4.5 Missing Energy
	* 4.6 Hadronic Tau Decays
	* 4.7 B tagging
* 5 Physics Signals
	* 5.1 Global
	* 5.2 Dilepton Signals
	* 5.3 Jet Signals
	* 5.4 Tau Signals
	* 5.5 Combined Signals
* 6 Conclusions and Future plans

Introduction

This paper reports on the experience using fully simulated LHC events containing supersymmetric (SUSY) particles produced and reconstructed in the framework of Phase 1 of the ATLAS Data Challenges (DC1) [1]. The primary motivation for this work is to better understand SUSY events as they would actually appear when reconstructed by the ATLAS detector. Previous SUSY studies have used parametrized detector performance, which provides an estimate of the performance of ATLAS sufficient for the development of analysis strategies. Full simulation better reflects what would be required to study such events. Furthermore, since typical SUSY events contain the complete set of physics objects that can be reconstructed in the detector (with the possible exception of isolated photons), they can be used as a powerful tool to test and evaluate the reconstruction software. This was the main use of this event sample, which was reconstructed with different releases of the reconstruction software for comparison purposes.

The events were generated according to a model phenomenologically similar to SUGRA Point-5 discussed in the ATLAS Detector and Physics Performance TDR [2]. The SUGRA model was used with this parameter set:

\[\mathrm{m}_{0}=100\,\mathrm{GeV},\quad\mathrm{m}_{1/2}=300\,\mathrm{GeV}, \quad\mathrm{A}_{0}=-300\,\mathrm{GeV},\quad\mathrm{\tan\beta}=6,\quad\mathrm{ sgn}(\mathrm{\mu})=+\,,\]

and a top quark mass of 175 GeV. The Higgs mass is larger than in Point 5 as the latter's value is now excluded. The superpartners' mass spectrum is shown in Table 1, and some selected branching ratios are shown in Table 2. This SUSY sample is dominated by the production of squarks and gluinos, the latter decaying to the former via quark emission. The left-handed squarks decay dominantly via \(\widetilde{\mathrm{q}_{L}}\to\mathrm{q}\widetilde{\mathrm{\chi}}_{2}^{0}\) and \(\widetilde{\mathrm{q}_{L}}\to\mathrm{q}\widetilde{\mathrm{\chi}}_{1}^{\pm}\). Both decays can lead to final states containing isolated leptons, in particular the former with the subsequent decay \(\tilde{\mathrm{\chi}}_{2}^{0}\to\tilde{\ell}\ell\to\ell\ell\widetilde{\mathrm{ \chi}}_{1}^{0}\). The dominant decay of right-handed squarks is \(\widetilde{\mathrm{q}_{R}}\to\mathrm{q}\widetilde{\mathrm{\chi}}_{1}^{0}\) which does not provide additional isolated leptons to the final state. The \(\tilde{\mathrm{\chi}}_{2}^{0}\) leptonic decays are particularly useful, producing a pair of same flavor and opposite sign leptons whose invariant mass is constrained kinematically. Leptons of all flavors can be produced, including taus. The observation and reconstruction of tau hadronic decay modes is particularly challenging and will be addressed later in this note. Final states containing b-jets are also abundant, as the masses of bottom and top squarks are lower than those of the squarks of the first two generations, so a good understanding of b-tagging is vital for the study of this SUSY case. Finally, when \(\widetilde{\mathrm{q}_{R}}\) are produced in pairs, we obtain the simpler final state of two energetic jets and missing transverse momentum which will be illustrated below.

Events were generated using Herwig (6.4) [3] with the Isawig interface so that the mass spectra and decay rates were given by Isasusy (from Isajet 7.64 [4]). The 100k event sample which was produced corresponds to an integrated luminosity of \(5.13\,\mathrm{fb}^{-1}\).

No backgrounds are included in this note, as their complete simulation is very demanding and is beyond the scope of this study. We rather use the cuts derived from previous fast simulation studies which demonstrated that the backgrounds are small in the channels discussed here.

In Sections 2 and 3 of this note we will briefly describe the event processing chain, starting with the generation and full simulation of the detector and finishing with the details of the reconstruction phases. In Section 4 we will go through the reconstruction performance study, object by object, starting with isolated leptons (electrons, muons), and them moving to QCD jets, missing E\({}_{T}\), tau hadronic jets and b-tagging. In Section 5 we will illustrate several physics signals taking into account selection cuts based on fast simulation. We will close this note by comparing the analysis performance obtained based on this sample with that described in the TDR [2]. The final section summarizes the results and open questions.

## 2 Event Generation and Simulation

Minimal SUGRA events with the parameters stated in the Introduction were generated in batches of 10k each using Herwig 6.4 in Atlas Software release 4.5.0 in October 2002. The SUSY spectrum and decays were taken from Isajet 7.64 via the Isawig interface. The complete jobOptions file for the generation can be found at [http://phyweb.lbl.gov/susy/gen.txt](http://phyweb.lbl.gov/susy/gen.txt). The resulting events were stored using AthenaRoot for subsequent use by fast and full simulation. The events were put in Castor and in the Magda database. They are named dcl.002315.gen.0xxxx.susy.sugra_100_300_6.root where xxxx lies in the range 0101 and 0200.

Atlsim release 3.2.1, the release used for event simulation in DC1, was used to perform the Geant-3 simulation of the full ATLAS detector. The atlasim parameters for a typical job are given at

[http://phyweb.lbl.gov/susy/sim.txt](http://phyweb.lbl.gov/susy/sim.txt). Note that the full rapidity range of the detector was active as we

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multicolumn{1}{c}{\(\overline{\mathrm{S}}\)} & \multicolumn{1}{c}{717} & \\ \(\widetilde{\chi}^{\pm}_{1}\) & 218 & \(\widetilde{\chi}^{\pm}_{2}\) & 480 \\ \(\widetilde{\chi}^{0}_{1}\) & 118 & \(\widetilde{\chi}^{0}_{2}\) & 219 \\ \(\widetilde{\chi}^{0}_{3}\) & 464 & \(\widetilde{\chi}^{0}_{4}\) & 481 \\ \(\widetilde{\mathrm{u}}_{L}\) & 631 & \(\widetilde{\mathrm{u}}_{R}\) & 612 \\ \(\widetilde{\mathrm{d}}_{L}\) & 636 & \(\widetilde{\mathrm{d}}_{R}\) & 611 \\ \(\widetilde{\mathrm{t}}_{1}\) & 424 & \(\widetilde{\mathrm{t}}_{2}\) & 651 \\ \(\widetilde{\mathrm{b}}_{1}\) & 575 & \(\widetilde{\mathrm{b}}_{2}\) & 611 \\ \(\widetilde{\mathrm{e}}_{L}\) & 230 & \(\widetilde{\mathrm{e}}_{R}\) & 155 \\ \(\widetilde{\mathrm{v}}_{e}\) & 217 & \(\widetilde{\mathrm{\tau}}_{2}\) & 232 \\ \(\widetilde{\mathrm{\tau}}_{1}\) & 150 & \(\widetilde{\mathrm{v}}_{\tau}\) & 216 \\ \(\mathrm{h}^{0}\) & 115 & \(\mathrm{H}^{0}\) & 513 \\ \(\mathrm{A}^{0}\) & 512 & \(\mathrm{H}^{\pm}\) & 518 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Masses of the superpartners, in GeV, for the case being studied. Note that the first and second generation squarks and sleptons are degenerate and are not listed separately.

\begin{table}
\begin{tabular}{c c} \hline \hline Mode & BR \\ \hline \(\overline{\mathrm{d}}_{L}\rightarrow\widetilde{\chi}^{0}_{2}\)u & 32.5\% \\ \(\overline{\mathrm{d}}_{L}\rightarrow\widetilde{\chi}^{+}_{1}\)d & 65.7\% \\ \(\overline{\mathrm{d}}_{R}\rightarrow\widetilde{\chi}^{0}_{1}\)u & 99.3\% \\ \(\widetilde{\chi}^{0}_{2}\rightarrow\widetilde{\ell}^{\pm}_{R}\)\(\ell^{\mp}\) & 8.8\% \\ \(\widetilde{\chi}^{0}_{2}\rightarrow\widetilde{\tau}^{\pm}_{1}\)\(\tau^{\mp}\) & 75.4\% \\ \(\widetilde{\chi}^{0}_{2}\rightarrow\widetilde{\chi}^{0}_{1}\)Z & 2.6\% \\ \(\widetilde{\chi}^{\pm}_{1}\rightarrow\widetilde{\chi}^{0}_{1}\)W\({}^{\pm}\) & 28.9\% \\ \(\widetilde{\chi}^{\pm}_{1}\rightarrow\widetilde{\tau}^{\pm}_{1}\)V\({}_{\tau}\) & 68.1\% \\ \hline \end{tabular}
\end{table}
Table 2: Selected branching ratios for the SUSY case being studied in this note. Here \(\ell\) is either e or \(\mu\). The branching ratios of other light squark are similar.

are interested in jets over the entire range and in a realistic missing \(\E_{T}\) reconstruction. These files are named \(\dcl.002315.simul.0xxxx.susy.sugra_100_300_6.zebra\) and are again stored in Castor and Magda.

The simulation produced Zebra output files each containing 100 events. This size was dictated by the requirement that the files be less than 1 GB. These events are complex and have a lot of hadronic energy, and the full calorimeter was activated. As a result, the simulation took approximately 15 minutes per event on a 1 GHz i686 processor, and the resulting file was 380MB. Samples of 10k events were produced at each of the following sites: Cambridge University, LBNL, Niels Bohr Institute, Sheffield University, and Weizmann Institute. A further 50k events were simulated on the US-Grid Testbed using facilities at LBNL, U. of Texas (Arlington), and U. of Chicago. All the simulated events were retained on disk at LBNL to facilitate rapid reconstruction turn around.

Subsequent to simulation, the events had to be redigitized to correct a problem in the muon MDT's. Most of this was done at LBNL; it was very I/O intensive, but the CPU used was small. These files are named \(\dcl.002315.redigi.0xxxx.susy.sugra_100_300_6.zebra\) and are stored in Castor.

The simulated events were reconstructed with Athena. The output from reconstruction was a Paw Ntuple [5], the "Combined Ntuple" or CBNT, containing all the relevant variables associated with each reconstructed object in each event.

Reconstruction was run at a few sites (primarily at LBNL) resulting in a total of 7GB of output. The Ntuples were then distributed in the form of gzipped tar files to all authors via a Web page. Following a discussion among the authors it was decided to use Paw Ntuples, since these could be converted to Root with h2root. In retrospect this was a mistake: Paw contains a number of size limits that hindered the analysis. But, because the Athena Root Ntuple is a flat structure, using Root does not otherwise provide a significant advantage. The analysis of the Ntuples was done with Fortran code, compiled and linked with the Hbook and Paw libraries. Most of the final plots were made with Paw.

## 3 Event Reconstruction

Most of the results in this note are based on reconstruction using Athena 6.0.3. Recently a new production release (7.0.2) has become available. Some plots (notably in Sections 4.2.1 and 4.4.1) are included in the cases where there are differences or where new reconstruction code became available. The standard RecExCommon_jobOptions.txt was used with a few modifications, providing reconstruction of the inner detector with both iPatRec and xxLman, the calorimeters, the muon system, jets, electrons, muons, taus, and missing \(\E_{T}\). Both the \(\K_{T}\) and the \(\R=0.4\) seedless cone jet algorithms were run simultaneously for calorimeter and Monte Carlo jets. This required setting (nonstandard) VarTag values to distinguish the variables. Essentially all cuts were removed for hadronic \(\tau\)'s; cuts using a new \(\tau\) likelihood function were made at the analysis stage. Finally, CBNT_SpclMC was used to provide Monte Carlo truth information about leptons, \(\b\) and \(\c\) hadrons, missing \(\E_{T}\) and SUSY particles. The complete jobOptions file used for reconstruction is given at [http://phyweb.lbl.gov/susy/recon.txt](http://phyweb.lbl.gov/susy/recon.txt).

Calorimeter noise can be added during reconstruction. Some of the results shown below include the effects of noise. Currently pileup is not available in the FCAL and therefore its effect on missing \(\E_{T}\) and forward jets cannot be studied. In the Athena reconstruction, the FCAL digitization was re-done correctly on the fly using the Geant3 hits. However, it is technically not viable to store all hits for events with pileup added to them, because this would imply the hits from around 700 minimum bias events would need to be stored for the high luminosity scenario. Therefore for signal events with pileup added to them only the Geant digits are stored and thus, the digitization cannot be redone at reconstruction time.

Reconstruction consumed approximately 1 minute per event (on 1 GHz processor). Approximately 12 reconstruction passes were made as the reconstruction software evolved towards release 6.0.3 and 7.0.2. The total CPU time on reconstruction and simulation were therefore comparable. Initially the reconstruction was performed without electronic calorimeter noise or pileup. The effects of noise were found to be large for jets and for hadronic \(\tau\)'s. This is not unexpected, and the effects of noise are discussed extensively below.

Both ATLAS tracking algorithms, iPatRec and xKalman, were run simultaneously, and xKalman (iPatRec) tracks with \(\mathrm{p}_{T}>0.5(1.0)\,\mathrm{GeV}\) were saved in the Ntuple. Due the fixed size of the Ntuple, in approximately 1% of the events a few of the tracks reconstructed by iPatRec can be lost. In our 7.0.2 reconstruction, we changed the defaults to prevent this loss. Since there is not yet a b-tagging algorithm in Athena, all analysis of \(\mathrm{b-jets}\) was done at the analysis stage using these reconstructed tracks. Tracks were also used to a lesser extent for \(\tau\) identification.

## 4 Reconstruction Performance

This section describes the performance of reconstruction with Athena for muons, electrons, jets, missing energy, hadronic \(\tau\)'s, and \(\mathrm{b}\)-tagging. In each case, the results of reconstruction are compared with Monte Carlo truth from the event generator. The entire 100k SUSY event sample is used without any cuts to suppress Standard Model backgrounds. Physics measurements including cuts are described in Section 5.

### Monte Carlo Truth and Comparison Methodology

To make a comparison of reconstructed objects with Monte Carlo truth, the latter must be defined precisely. For muons and electrons this is straightforward; the generated particles can be used directly. Some care is needed to select only final particles from the generator and to eliminate secondary particles produced by Geant-3 and stored in GENZ. The CBNT_SpclMC algorithm was used to provide truth information for muons and electrons. In the latter case, a weak isolation requirement was imposed to eliminate, for example, additional electrons from internal brehmstahlung.

In the case of jets, the same jet algorithm used for reconstruction was applied to the stable interacting particles produced by the event generator. No attempt was made to compare to the (unphysical) energy of the parton that initiated the jet. Such a reconstructed Monte-Carlo jet is defined to be a b-jet if there is a weakly-decaying \(\mathrm{B}\) hadron inside the jet cone. For the cone algorithm this is unambiguous. For the \(\mathrm{K}_{T}\) algorithm the jet size is determined dynamically. While it would not be unreasonable to use a cone of \(\mathrm{R}\sim 0.7\) for \(\mathrm{b}\) tagging with \(\mathrm{K}_{T}\) jets, it was decided to present results only for cone jets. Note that others have used the parton that initiated the jet to define "truth" for \(\mathrm{b-tagging}\). But \(\mathrm{b-quarks}\) can be produced by a gluon shower, and it is impossible uniquely to associate a collection of hadrons with a colored quark or gluon. Since the vertex detector relies on the decay products of long-lived \(\mathrm{B}\)'s, using them seems more appropriate.

Truth for hadronic \(\tau\) decays was defined using the \(\tau\)'s and \(\nu_{\tau}\)'s from CBNT_SpclMC to calculate the visible \(\tau\) energy and direction. Leptonic decays were removed by vetoing nearby \(\nu_{e}\) and \(\nu_{\mu}\).

### Muons

Athena contains two muon reconstruction packages, MuonBox [6] and Moore/Muid [7]. Muid was not available in Athena 6.0.3, so muons reconstructed with Moore could not be matched to the inner detector. Moore results are shown using release 7.0.2.

Figure 1 (left plot) shows the "efficiency" with which MuonBox finds muons as a function of \(|\eta|\) for three \(\mathrm{p}_{T}\) ranges with \(\mathrm{p}_{T}>10\,\mathrm{GeV}\). This plot is obtained by dividing the histogram of reconstructed muons as a function of \(\eta\) by that of the corresponding generated "truth" muons from the event generator. This includes muons from \(\mathrm{B}\) and \(\mathrm{D}\) decays but not from \(\pi\) and \(\mathrm{K}\) decays. As these latter muons can be found and reconstructed, their effect can be to produce an "efficiency" that is greater than one. No isolation cut is made;

one would help to eliminate those muons from \(\pi\) and \(\mathsf{K}\) decays which, at high \(\mathsf{p}_{T}\), are usually embedded in jets. Three \(\mathsf{p}_{T}\) ranges are shown; by comparing these it is evident that the efficiency is insensitive to \(\mathsf{p}_{T}\). The dip at \(|\eta|=0\) is due to gaps in the muon system for services. Apart from this small region, the efficiency is very good, falling slightly in the forward region. The drop in efficiency for \(|\eta|>2\) is not understood.

Figure 1: Left: Muon reconstruction efficiency as a function of \(|\eta|\) from MuonBox. Right: Integral distribution of distance \(\mathsf{R}_{\mu,\text{MC}}\) between muon reconstructed with MuonBox and nearest Monte Carlo one for \(\mathsf{p}_{T}>10\,\text{GeV}\) (solid) and \(\mathsf{p}_{T}>25\,\text{GeV}\) (dashed).

Figure 2: \(\mathsf{E}_{T}\) and \(|\eta|\) distributions for reconstructed muons that do not have a good match to a Monte Carlo truth muon (solid) compared to muons (dash) with a good match.

The purity of the reconstructed muons was measured by searching for the closest Monte Carlo muon as measured by:

\[R=\sqrt{(\Delta\eta)^{2}+(\Delta\phi)^{2}}.\]

The integrated distribution,

\[\int_{R}^{\infty}dR^{\prime}\,\frac{dN}{dR^{\prime}},\]

is shown in Figure 1 (right plot) for two ranges of muon \(\rho_{T}\); reconstructed muons in events with no Monte Carlo muon are included at the upper limit of the integrand. From this figure the background level, defined as the ratio of the intercept of a linear fit over the range \((0.1,1.0)\) to the value at zero, is 4.2% for \(\rho_{T}>10\,\mathrm{GeV}\) and 1.3% for \(\rho_{T}>25\,\mathrm{GeV}\). Since there are about 0.30 muons and 6.9 jets per event, 4.2% represents an effective \(\mu/\mathrm{jet}\) fake rate of about \(2\times 10^{-3}\).

The \(\rho_{T}\) and \(|\eta|\) distributions for bad muons, those with \(R_{\mu,\mathrm{MC}}>0.1\) in Figure 1 (right plot), are shown in Figure 2. The fraction of bad muons falls with \(\rho_{T}\) and seems fairly constant throughout the barrel region, although there are hints of enhanced bad muons in specific \(\eta\) regions. The distribution of the distance \(R_{\mu,\mathrm{jet}}\) to the nearest jet for bad muons is shown in Figure 3 (left plot). All these distributions are qualitatively consistent with the conclusion that the dominant source of bad muons is \(\pi\) and \(K\) decay in jets and possibly punch-through. The \(K/\pi\) source would decrease with \(\rho_{T}\) because of the soft fragmentation of jets into \(K\) and \(\pi\). It would be roughly constant with \(\eta\) in the barrel, whereas punch-through should decrease at larger \(\eta\) because of the greater calorimeter thickness. Finally, most of the muons from this source should be fairly close to jets. A much more detailed study would be required to establish the source of the fake muons with certainty.

The resolution \(\rho_{T,\mu}/\rho_{T,\mathrm{MC}}\) is shown in Figure 3 (right plot). This ratio is obtained by comparing the reconstructed \(\rho_{T}\) to that of the closest Monte Carlo muon in \(\Delta R\). Only muons that have a good match, i.e. \(R_{\mu,\mathrm{MC}}<0.1\), are included. The mean value is 99.5%, and the overall resolution is 3.5%. The resolution tails

Figure 3: Left: Distance \(R\) from nearest jet for reconstructed muons that do not have a good match to a Monte Carlo truth muon. Right: Muon resolution for all muons with \(\rho_{T}>10\,\mathrm{GeV}\) and a good Monte Carlo match from MuonBox.

have not yet been studied but are fairly small.

Overall, MuonBox works well, providing good efficiency and about the expected resolution. There is a significant background of fake muons at relatively low \(\mathrm{p}_{T}\), but it seems to be consistent with \(\pi\) and \(\mathrm{K}\) decays.

Figure 4: Muon reconstruction efficiency as a function of \(|\eta|\) for \(\mathrm{p}_{T}>10\mathrm{GeV}\). Solid: Moore. Dash: MuidStandalone. Dash-dot: MuidComb.

Figure 5: Left: Integral \(\mathrm{R}_{\mu,\mathrm{MC}}\) distribution for Moore muons with \(\mathrm{p}_{T}>10\mathrm{GeV}\), Solid: MuidStandalone. Dash: MuidComb. Right: same for \(\mathrm{p}_{T}>25\mathrm{GeV}\).

#### 4.2.1 Moore/MuID

Moore/MuID reconstruction is organized into three independent Athena algorithms.

* Moore: reconstruction of the muon track-segment only in the muon spectrometer, with the momentum determined at the first measured point.
* MuidStandAlone: the track-segment reconstructed in the previous algorithm is tracked back to the beam-spot, taking into account the mean energy loss and multiple scattering effects in the calorimeter. (This is similar to MuonBox.)
* MuidComb: Inner Detector (from iPatRec) and MuidStandAlone tracks are matched by forming a \(\chi^{2}\) with five degrees of freedom using the track parameter differences and combined covariance. A combined fit is performed for all combinations with a \(\chi^{2}\) probability above a certain cut. If no combination satisfies the \(\chi^{2}\) cut or if the combined fit fails, then the combined fit is redone using all inner detector tracks within a road around the muon track. This can result in more than one MuidComb candidate in the Ntuple for the same MuidStandAlone track.

Only the first of these algorithms was available in Athena 6.0.3, making it difficult to compare Moore with Monte Carlo truth. All Moore/MuID results given here are therefore based on Athena 7.0.2.

The good muons from the combined fit were selected by requiring \(\tt{fitcodecb}=0\) and \(0<\tt{chi2cb}<10\). In the events where MuidComb has more than one InDet track matching the same MuidStandAlone (indexed by \(\tt{nstaindexcb}\)) track, the combined muon-track with the minimum \(\tt{chi2cb}\) was selected.

The reconstruction efficiency for Moore/MuID was determined by comparison with Monte Carlo muons in the same way as for MuonBox. The efficiency as a function of \(|\eta|\) for \(\pT>10\) GeV is shown in Figure 4; it is generally similar to that from MuonBox and even somewhat better for large \(|\eta|\). The purity of the reconstructed muons is indicated by the integral R-matching distributions shown in Figure 5. It can be seen the fake rate is lower when the inner detector is used. This again suggests that the fake muons mainly

Figure 6: Muon resolution for all muons with \(\pT>10\) GeV and a good Monte Carlo match from Moore. Dot-Dash: Moore. Dash: MuidStandAlone. Solid: MuidComb.

come from \(\pi\) and \(K\) decays. The peaks at \(R_{\mu,\mathrm{MC}}=0\) are of course much narrower for MuidComb since the reconstructed muon direction is more precisely determined by the inner detector.

The resolution for muons with a good match, \(R_{\mu,\mathrm{MC}}<0.1\), to the Monte Carlo truth is shown in Figure 6. The improvements from Moore to MuIdStandAlone and MuIdComb from including the mean energy loss and the inner detector are evident. The fitted Gaussian resolution for MuidStandAlone is 4.6%, somewhat worse than the 3.5% for MuonBox in Figure 3 (right plot). The low-side tails for MuidStandAlone also seem somewhat bigger. The Gaussian resolution for MuidComb is 2.6%, significantly better than either MuonBox or Moore without the inner detector.

### Electrons

The electron reconstruction in ATLAS starts with the identification of an electromagnetic cluster in the calorimeters. Cuts on the shower shapes in the 1st and 2nd sampling of the EM calorimeter, and the leakage into the first sampling of the hadronic calorimeters, are used to separate clusters produced by electrons or photons from ones produced by jets. Some predefined cuts on the shapes which retain a high electron efficiency (\(\sim 95\%\)), while giving a good jet rejection, are applied at reconstruction time. The result is stored in the variable \(\mathrm{eg}\_\mathrm{IsEM}\), which is zero if the cluster survives the cuts. The cuts were tuned for electrons with \(\mathrm{p}_{T}=25\) GeV and will currently reject too many good electrons at lower \(\mathrm{p}_{T}\). There is currently no additional isolation cut. A cluster passing all cuts has \(\mathrm{eg}\_\mathrm{IsEM}=0\) in the Ntuple. The track matching cuts were left loose in Athena. At the analysis stage, the same \(E\,/\mathrm{p}\) cuts as used in the Event Filter were applied,

\[0.8<E\,/\,\mathrm{p}<\,1.3, \left|\eta\right|<1.37\] \[0.7<E\,/\,\mathrm{p}<\,2.5, \left|\eta\right|>1.37\]

Since there are two tracking algorithms available in the ATLAS software (xKalman and iPatRec) the track matching was done with both and the performances compared.

The reconstruction efficiency was obtained in a similar way as for the muons. Since the shower shape cuts impose an effective isolation cut, an isolation cut \(E_{T}<5\,\mathrm{GeV}\) in a cone \(R=0.2\) was made on Monte Carlo electrons using information from CBNT_SpclMC. The resulting efficiency is plotted as a function of \(|\eta|\) and of \(\mathrm{p}_{T}\) for xKalman and iPatRec in Figure 7. The performances of the two tracking algorithms are similar, but iPatRec is slightly better. For both algorithms, a deterioration is visible in the region around \(|\eta|=1.5\), as is expected due to the transition between the central and the endcap region of the LAr calorimeter.

The resolution for all the electrons was obtained by fitting the distribution of the ratio between the \(E_{T}\) of the reconstructed electron and that of the closest truth electron with \(R<0.1\). The resolution is shown for all the electrons in the sample and for those with \(E_{T}>25\,\mathrm{GeV}\) in Figure 8. The \(R\) cut on the matching has a significant effect on the non-Gaussian tail. The mean reconstructed \(E_{T}\) is 2% lower than the truth \(E_{T}\) because the EM calorimeter calibration in Athena has been performed for photons rather than electrons. There is a systematic variation of the response with \(\eta\), as can be seen in Figure 9. The average miscalibration but not the \(\eta\) dependence will be taken into account later when electrons are used to evaluate the mass of sparticles.

The fake electron rate can be evaluated by looking at the distance in \(R\) between a reconstructed electron and the closest truth electron. The integral of this distance distribution is plotted in Figure 10 for both tracking algorithms. The integral distribution has a peak at zero, as most of the reconstructed electrons match one found in the MC truth. The width of the peak gives also an idea of the angular resolution, being better than 0.05 in the (\(\eta\), \(\phi\)) plane. The tail for \(R_{e,MC}>0.1\) is due to fake electrons which do not match any electron in the MC truth. From Figure 10 it appears that about 6% of the reconstructed electrons in the sample are fakes. If we take into account that there are about 0.17 electrons* and 6.9 jets per event, the overall \(\mathrm{e}/\mathrm{jet}\) separation is about 0.15%. We have not applied any requirement on the measured transition radiation. This fake rate is somewhat worse than the \(\mathrm{e}/\mathrm{jet}\) separation quoted in the TDR, which used different criteria. This level of fake electrons does not represent a problem in the reconstruction of SUSY events.

These fake electrons have several origins. A significant fraction of them come from hadronic \(\tau\) decays, as can be seen from the integral distance plot to the nearest Monte Carlo \(\tau\), shown in Figure 11 (left plot).

Figure 8: Electron resolutions for \(\mathrm{E}_{T}>10\,\mathrm{GeV}\) (left) and \(\mathrm{E}_{T}>25\,\mathrm{GeV}\) (right) for iPatRec (solid) and xKalman (dashed). The Gaussian fits are for iPatRec.

Figure 7: Electron efficiency as function of \(|\eta|\) (left) and \(\mathrm{p}_{T}\) (right) for xKalman (dashed) and iPatRec (solid).

Normally this source would be small, but in these SUSY events the number of \(\tau\)'s is much larger than the number of electrons. A large fraction of fakes also arises from the TileCal gap region as can be seen from Figure 11 (right plot). Due to the limited coverage of the hadron calorimeter in this region, the cut on the fraction of hadronic energy is less efficient in rejecting fake electrons from jets or from hadronic \(\tau\)'s. More study of the electron background near \(|\eta|=1.1,1.5\) is needed. Finally, the fakes have a poorer \(\E/p\) match, and the \(\E_{T}\) is less efficient in rejecting fake electrons from jets or from hadronic \(\tau\)'s.

Figure 10: Integral distributions for matching between reconstructed and Monte Carlo electrons for \(\E_{T}>10\,\GeV\) (left) and \(\E_{T}>25\,\GeV\) (right) using iPatRec (solid) and xKalman (dashed).

Figure 9: Profile histogram showing mean and RMS values of \(\E_{T}/\E_{T,MC}\) vs. \(\eta\) for iPatRec electrons with \(\E_{T}>10\,\GeV\) (left) and \(25\,\GeV\) (right). The xKalman distributions are very similar.

as can be seen from Figure 12.

Figure 11: Left: Integral distance plot of a fake electron to the nearest Monte Carlo \(\tau\).Right: Electron fakes as function of \(\left|\eta\right|\). xKalman (dashed) and iPatRec (solid) are shown.

Figure 12: \(\mathrm{E}\left/\mathrm{p}\right.\) ratio for reconstructed good (left) and bad (right) electrons using xKalman (dashed) and iPatRec (solid).

### Jets

Hadronic jets were reconstructed using the JetRec seeded cone algorithm (with \(\mathsf{R}=0.4\)) and \(\mathsf{K}_{T}\) clustering algorithm (with \(\mathsf{D}=1\) and the default Athena preclustering). Both algorithms were run in the same reconstruction pass through a suitable specification of jobOptions parameters, thereby simplifying comparison of results. In both cases full correction was performed using CombinedJetRec for both the non-linear response of the calorimeter to hadronic showers and for low \(\mathsf{p}_{T}\) tracks swept out of the jet by the solenoidal B-field in the Inner Detector. This corrected jet energy is significantly better than the raw EM scale but is far from perfect; more work is needed.

The "efficiency" for jet reconstruction was defined as the ratio of the number of reconstructed detector jets in a given \(\mathsf{p}_{T}\) or \(|\eta|\) bin to the number of jets reconstructed from truth particles in the same bin. Thus it includes not only a failure to find the jet but also the effect of mismeasuring its energy convoluted with the true jet energy distribution. In Figure 13 this efficiency is plotted as a function of \(|\eta|\) for \(\mathsf{K}_{T}\) and cone jets in three \(\mathsf{p}_{T}\) ranges in the absence of pileup or electronic noise. Significant loss of efficiency is observed for both algorithms in the \(|\eta|\) ranges 1.2-2.0, 2.5-3.4, and \(>\)4, corresponding to the barrel-endcap region, the endcap-FCAL region and the high \(|\eta|\) FCAL region respectively. Figure 14 shows the actual rapidity distributions for reconstructed and Monte Carlo jets. The poor reconstruction of jets around \(|\eta|=3.2\) is particulary apparant in this plot. The loss of efficiency at high \(\eta\) is due to shower leakage. Jets that are centered close to \(|\eta|=5\) loose some of their energy out the end of the calorimeter. The efficiency is lower at higher \(\mathsf{E}_{T}\), reflecting both greater energy loss from punch-through and the more steeply falling distribution at high \(\mathsf{E}_{T}\). Correcting for the average longitudinal and transverse leakage should improve the efficiency at high \(|\eta|\), but this has not been done. In general, in the absence of noise the \(\mathsf{K}_{T}\) algorithm gives the more uniform response as a function of \(|\eta|\), with smaller inefficiencies in the transition regions.

The jet reconstruction efficiency can also be examined as a function of \(\mathsf{p}_{T}\) for all \(\eta\). This quantity is

Figure 13: Left: Efficiency for jet reconstruction using the \(\mathsf{K}_{T}\) algorithm as a function of \(|\eta|\) for jets with 50 GeV \(>\mathsf{p}_{T}>\) 25 GeV (solid), 100 GeV \(>\mathsf{p}_{T}>\) 50 GeV (dash) and \(\mathsf{p}_{T}>\) 100 GeV (dot-dash). The efficiency exceeds 100% in regions where the finite jet energy resolution has smeared excess detector jets into a higher \(\mathsf{p}_{T}\) bin. No noise or pileup was added to the events. Right: Same for \(\mathsf{R}=0.4\) cone jets.

plotted in Figure 15 for jets reconstructed with the \(\K_{T}\) and fixed cone algorithms. No jets with \(\p_{T}>100\GeV\) are reconstructed with \(|\eta|>4\). As is to be expected, there is a severe loss of efficiency near threshold at low \(\p_{T}\). At higher values of \(\p_{T}\) the efficiency \(\gtrsim 90\) % for both algorithms for \(|\eta|<2.5\) as shown in Figure 13.

Figure 14: Left: Rates for jets reconstructed using the \(\K_{T}\) algorithm as a function of \(|\eta|\) for jets (from lowest to highest) with \(50\GeV>\p_{T}>25\GeV\) (solid), \(100\GeV>\p_{T}>50\GeV\) (dash) and \(\p_{T}>100\GeV\) (dot-dash). The dashed lines are reconstructed jets and the solid are Monte Carlo. Right: Same for \(R=0.4\) cone jets.

Figure 15: Left: Efficiency for jet reconstruction using the \(\K_{T}\) algorithm as a function of \(\p_{T}\). No noise or pileup was added to the events. Right: Same for \(R=0.4\) cone jets.

The uniformity of the efficiency as a function of \(\mathrm{p}_{T}\) is greater for the cone algorithm. At very high \(\mathrm{p}_{T}\) both algorithms give efficiencies greater than 100%. This is presumably due to a miscalibration of the hadronic energy scale at high \(\mathrm{E}_{T}\); the H1 weights were tuned over a limited range.

Another useful quantity for comparison of performance of the different jet-finding algorithms is the jet multiplicity. This is plotted for three different \(\mathrm{p}_{T}\) ranges (no noise and no pileup) for the \(\mathrm{K}_{T}\) and the cone algorithms (Figure 16). In both cases the average jet multiplicity for high \(\mathrm{p}_{T}\) events (\(\mathrm{p}_{T}>25\,\mathrm{GeV}\)) is \(\sim 6\), however the distribution obtained with the \(\mathrm{K}_{T}\) algorithm has a somewhat smaller tail at large values of the multiplicity.

#### 4.4.1 Effect of Calorimeter Noise on Jets

The results on jets presented so far did not take into account calorimeter noise or pileup. Pileup will not be discussed here. The figures in this section are made using release 7.0.2. Noise was added for each calorimeter cell during reconstruction with Athena using a suitable Gaussian distribution.

The jet algorithms in Athena work through a ProtoJet interface based on four-vectors. This forces a cut \(\mathrm{E}_{T}>0\) on ProtoJets, since a ProtoJet with \(\mathrm{E}_{T}<0\) has direction \((-\eta,\phi+\pi)\) rather than its true direction \((\eta,\phi)\). As consequence of this cut, positive and negative noise does not cancel except in cells or towers that have a sufficiently positive signal. Since the \(\mathrm{K}_{T}\) algorithm can include cells with \(\mathrm{R}\lesssim 1\) or about \(300\,0.1\times 0.1\) CaloTowers, the effect on \(\mathrm{K}_{T}\) jets, Figure 17, is large. The \(\mathrm{E}_{T}>0\) cut produces a huge effect even for \(50<\mathrm{E}_{T}<100\,\mathrm{GeV}\) jets: the multiplicity distribution becomes a Gaussian centered at about 17 jets. Making a \(2\sigma\) noise cut is considerably better, but the peak of distribution for \(50<\mathrm{E}_{T}<100\,\mathrm{GeV}\) still shifts from about 3 in Figure 16 to about 11 here. The effect on \(\mathrm{R}=0.4\) cone jets, Figure 18, is smaller; these jets contain fewer cells and a larger fraction of those cells contain large positive energy.

The obvious solution is to cancel positive and negative energies before forming ProtoJets, thereby giving no shift in the mean \(\mathrm{E}_{T}\) and a contribution to the resolution proportional to the square root of the

Figure 16: Left: Jet multiplicity for events with \(50\,\,\mathrm{GeV}>\,\mathrm{p}_{T}>25\,\,\mathrm{GeV}\) (solid), \(100\,\,\mathrm{GeV}>\,\mathrm{p}_{T}>50\,\,\mathrm{GeV}\) (dash) and \(\mathrm{p}_{T}>100\,\,\mathrm{GeV}\) (dot-dash), reconstructed with the \(\mathrm{K}_{T}\) algorithm. No noise or pileup was added to the events. Right: Same for \(\mathrm{R}=0.4\) cone jets.

number of cells. The Athena algorithms described here work with CaloTowers. Presumably it would be better to use CaloCells, but this requires geometrical information that is not yet easily available. Each CaloTower has an \(\eta_{1},\phi\) that describes its true direction independent of the sign of its energy. The first algorithm, CaloTowerPJArrayBuilder, uses these to create a vector of exactly \(N_{n}N_{\phi}\) CaloTowerProtoJets indexed by \(N_{n}i_{\phi}+i_{\eta}\). This makes it trivial to find the neighbors for any tower. The second algorithm,

Figure 17: Multiplicity of \(K_{T}\) jets including noise with an \(E>0\) cut (left) and an \(E>2\sigma_{E}\) cut (right) for three \(E_{T}\) ranges. Compare with Figure 16 without noise.

Figure 18: Multiplicity of \(R=0.4\) cone jets including noise with an \(E>0\) cut (left) and an \(E>2\sigma_{E}\) cut (right) for three \(E_{T}\) ranges. Compare with Figure 16 without noise.

JetsFromTowerPJ, cancels the negative energies by adding to them nearby positive energies, in so doing trying to move as little \(\E_{T}\) as short a \((\eta-\phi)\) distance as possible. The specific algorithm, which is somewhat arbitrary, sorts the positive energy towers in a \(7\times 7\) region first by distance and then by \(\E_{T}\) at a fixed distance.

The H1-style weights currently included in Athena depend only on the cell energies. This means they can be applied by CaloTowerPJArrayBuilder, so that the jet clustering algorithms effectively work at the hadronic energy scale, not at the electromagnetic scale. This pre-weighting has no effect for jets with high \(\E_{T}\), but it improves the agreement between reconstructed and Monte Carlo jets with low \(\E_{T}\).

Figure 19 shows that the combination of noise cancellation with this simple algorithm and pre-weighting produces good agreement between reconstructed and Monte Carlo jets down to very low \(\E_{T}\). This is probably fortuitous, and pileup will certainly produce additional jets in this \(\E_{T}\) range. Nevertheless, it might be interesting to explore whether, e.g., the threshold for the central jet veto used in selecting Higgs production via \(\WW\) fusion [8] could be lowered. Noise cancellation without pre-weighting provides good performance above about \(20\GeV\). The simple approach described here thus seems to provide an adequate interim solution for calorimeter noise.

### Missing Energy

Missing \(\E_{T}\) was reconstructed using the Athena package MissingET with H1-style cell weighting to improve the calibration and linearity for hadronic showers. The distributions of \(\E_{T}^{\rm\ miss}\) for MC truth particles and for calibrated calorimeter cells are shown in Figure 20 without electronic noise and in Figure 21 with noise, both with and without the symmetric \(2\sigma\) noise cut. In all three cases there is reasonably good agreement between the shapes of the truth and detector distributions, however small shifts of the latter towards lower values of \(\E_{T}^{\rm\ miss}\) are observed. The origin of these shifts is still unknown.

The observed shifts in the reconstructed event \(\E_{T}^{\rm\ miss}\) distribution become more apparent when differential (\(\Delta\E_{T}^{\rm\ miss}=\E_{T}^{\rm\ miss}({\rm detector})-\E_{T}^{\rm\ miss}({ \rm MC})\)) distributions are constructed. Figures 22 (left plot) shows this

Figure 19: Multiplicity distribution (left) and \(\E_{T}\) distribution (right) for \(K_{T}\) jets. Solid: reconstructed with noise cancellation and pre-weighting. Dashdot: reconstructed with noise cancellation only. Dash: Monte Carlo truth.

distribution in the case with no noise; a shift of about \(-15\) can be clearly seen. When electronic noise has been added this distributions shows a shift of \(-18\,\mathrm{GeV}\). Interestingly, in the case where electronic noise has been added and the \(2\sigma\) noise cut has been applied the shift is still larger, \(-28\,\mathrm{GeV}\). The widths of the distributions demonstrate that performance is degraded when noise is added; the width increasing from 21 to 26 GeV. However when the noise cut is applied the width of the distribution increases to 28 GeV showing that the performance is degraded still further.

Figure 21: Same as Figure 20 but with electronic noise, without (left) and with (right) a \(2\sigma\) noise cut.

Figure 20: Distributions of \(\mathrm{E}_{T}^{\,\mathrm{miss}}\) calculated from Monte Carlo truth (dash) and reconstructed events in the absence of noise or pileup (solid).

For a calorimeter with energy resolution dominated by the stochastic (\(\sqrt{\mathrm{E}}\)) term the widths of distributions of \(\Delta\mathrm{E}_{T}^{\,\mathrm{miss}}\) for different event \(\mathrm{E}_{T}^{\,\mathrm{sum}}\) (scalar sum of \(\mathrm{E}_{T}\)) should be proportional to \(\sqrt{\mathrm{E}_{T}^{\,\mathrm{sum}}}\). This

Figure 23: Same as Figure 22 (right plot) but with electronic noise included, without (left) and with (right) a \(2\sigma\) noise cut.

Figure 22: Left: Distribution of the difference in \(\mathrm{E}_{T}^{\,\mathrm{miss}}\) between reconstructed detector events and the corresponding Monte Carlo truth, \(\Delta\mathrm{E}_{T}^{\,\mathrm{miss}}=\mathrm{E}_{T}^{\,\mathrm{miss}}( \mathrm{detector})-\mathrm{E}_{T}^{\,\mathrm{miss}}(\mathrm{MC})\) without electronic noise or pileup. Also shown is a Gaussian fit to the peak of the distribution. Right: Dependence of the widths \(\sigma\) of distributions of \(\Delta\mathrm{E}_{T}^{\,\mathrm{miss}}\) on \(\sqrt{\mathrm{E}_{T}^{\,\mathrm{sum}}}\) of events, in the case where no electronic noise is added. Also shown is a linear fit to the data points with intercept P1 and gradient P2.

\(\mathbb{E}_{T}^{sum}\) dependence was studied by binning events in \(\mathbb{E}_{T}^{\text{sum}}\) and fitting the resulting \(\Delta\mathbb{E}_{T}^{\text{miss}}\) distributions with Gaussian functions. The widths \(\sigma\) extracted from these distributions are plotted as functions of \(\sqrt{\mathbb{E}_{T}^{sum}}\) in Figures 22 (right plot) and 23 for the cases where no electronic noise was added, where noise was added and where noise was added and the \(2\sigma\) noise cut was applied, respectively. The functional dependence on \(\sqrt{\mathbb{E}_{T}^{sum}}\) is seen to be approximately linear in all three cases except at large \(\sqrt{\mathbb{E}_{T}^{sum}}\). Here the constant term becomes more important, but the nonlinearity might also be due to miscalibration or other effects. Again the widths of the distributions are seen to be greatest for the same \(\sqrt{\mathbb{E}_{T}^{sum}}\) when both electronic noise is added and the \(2\sigma\) noise cut is applied.

The effect of a symmetric noise cut on the \(\mathbb{E}_{T}^{\text{miss}}\) resolution is not understood quantitatively. It is clear, however, that such a cut has two effects. It reduces the total \(\mathbb{E}_{T}\) considered and hence the stochastic fluctuations proportional to \(\sqrt{\mathbb{E}_{T}}\); this should improve the resolution. But it also introduces a shift because positive and negative noise no longer cancel in the presence of positive signal energy. More study is clearly needed.

### Hadronic Tau Decays

Since leptonic \(\tau\) decays cannot be effectively separated from prompt leptons, it is necessary to rely on hadronic decays to detect taus. To suppress the large jet background a new set of selection criteria was developed to replace the default cuts in tauRec. A \(\tau\) was required to have a calorimeter \(\mathbb{E}_{T}>35\,\text{GeV}\), hadronic energy greater than \(0.1\mathbb{E}_{T}\), exactly one xKalman track within \(R<0.3\) with \(\rho_{T}>2\,\text{GeV}\), and no additional tracks with \(\rho_{T}>1\,\text{GeV}\).

The \(\tau\) candidate was also required to have an appropriate shower shape as measured by four variables that should not strongly depend on the \(\tau\)\(\mathbb{E}_{T}\).

* \(R_{\text{EM}}\), the \(\mathbb{E}_{T}\)-weighted radius of the cluster in the EM calorimeter.
* \(F_{\text{iso}}\), the fraction of the EM energy contained in an annulus \(0.1<R<0.2\).
* \(\Delta\eta^{2}\), the width of the cluster in the \(\eta\) strips.
* \(\mathbb{E}_{T,\text{had}}/\rho_{T,1}\), the ratio of the hadronic \(\mathbb{E}_{T}\) to the \(\rho_{T}\) of the leading track.

The distributions of these quantities for \(\tau\)'s and for jets are shown as histograms in Figure 24 for SUSY events reconstructed without any calorimeter noise or pileup. Analytic parameterizations of these distributions, shown as smooth curves in Figure 24, were used to construct log-likelihood functions. The resulting \(L_{\tau}\) distributions for \(\tau\)'s and for jets are shown in Figure 25 both for all candidates and for those with only one track. (About 10% of the jets have one of the variables outside of the fitted range; these are assigned \(L_{\tau}<-10\) and are not shown in the plot.) A cut \(L_{\tau}>0.5\) was chosen as a good compromise between purity and efficiency.

The \(\tau\) selection described above was applied to the entire SUSY event. The \(\mathbb{E}_{T}\) and \(|\eta|\) distributions of \(\tau\)'s reconstructed with no calorimeter noise or pileup are shown in Figure 26. Also shown in the figure are the Monte Carlo distributions for visible energy from hadronic \(\tau\) decays. These were obtained from CBNT_SpclMC by subtracting the momentum of the nearest \(v_{\tau}\) from that of the \(\tau\) and vetoing nearby \(\mathrm{e}\)'s and \(\mathrm{\SIUnitSymbolMicro}\)'s. The efficiency is fairly constant as a function of \(\mathbb{E}_{T}\) above its minimum value, as would be expected from the nature of the cuts. The efficiency as a function of \(\eta\) is shown in Figure 27 (left plot). Since the \(\tau\) hadronic branching ratio is 64.8%, of which 15.3% decay into three or more charged particles, the ideal efficiency for 1-prong decays is 76.4%.

The purity of the reconstructed \(\tau\)'s was assessed by finding the distance \(R\) to the nearest Monte Carlo \(\tau\) decaying hadronically. Figure 28 shows the integral distribution, that is, the number of events having \(R>R_{\tau,\text{MC}}\), for various \(\mathbb{E}_{T}\) cuts. Events with no match, i.e., with \(R=\infty\), are included in the integral.

Evidently a reconstructed \(\tau\) with a good match has \(\mathsf{R}_{\tau,\mathrm{MC}}\lesssim 0.1\), so the background under each curve in Figure 28 was taken to be the intercept of a linear fit to the range \((0.1,1.0)\). This gives \(\mathsf{S}/\mathsf{B}=8.2,6.2,4.9,2.9\) for \(\mathsf{E}_{T}>50,75,100,150\,\mathrm{GeV}\). The decrease of the purity with increasing \(\mathsf{E}_{T}\) at first seems surprising, since the multiplicity cut should be more effective at larger \(\mathsf{E}_{T}\). But \(\tau\)'s in SUSY events result from cascade decays and so are softer than jets. Hence the \(\tau/\mathrm{jet}\) ratio, also shown in Figure 28, decreases with \(\mathsf{E}_{T}\).

Since the ATLAS calorimeter is not compensating, tauRec uses H1-style weights to correct the \(\tau\) energy. The resulting resolution for reconstructed \(\tau\)'s with a good match (\(\mathsf{R}<0.1\)) to a Monte Carlo \(\tau\) is shown in Figure 29 with a double Gaussian fit. While the energy scale is better than what would be obtained from the uncalibrated EM scale, it is too high by several percent.

All of the preceding plots have been made without including either calorimeter noise or pileup. When

Figure 24: Distributions of variables used for \(\tau\) identification for \(\tau\)’s (dashed) and jets (solid). The histogram shows the SUSY events.

noise was included, without otherwise changing the analysis or adjusting any cuts, significant changes were observed. The \(L_{\tau}\) distributions, Figure 29 (left plot), show a somewhat degraded separation between \(\tau\)'s and jets. As a result, the \(\tau\) reconstruction efficiency, Figure 27 (right plot), is significantly reduced, particularly at small \(\eta\). The purity as measured by the integral distribution of \(R_{\tau,\mathrm{MC}}\), also shown in Figure 27 (right plot), is not significantly worse. The mean value of \(E_{T}/E_{T,\mathrm{MC}}\), Figure 30, is lower, but the resolution is not significantly affected. More study of \(\tau\) reconstruction and energy calibration in the presence of calorimeter

Figure 26: \(E_{T}\) and \(\eta\) distributions for reconstructed \(\tau\)’s without pileup or calorimeter noise (solid), and corresponding Monte Carlo distributions (dashed).

Figure 25: Likelihood distributions for \(\tau\)’s and jets based on the quantities shown in Figure 24.

noise and pileup is needed.

Figure 28: Left: Integral distribution of distance \(\mathsf{R_{\tau,MC}}\) between reconstructed and nearest Monte Carlo hadronic \(\tau\). Right: Ratio of Monte Carlo hadronic \(\tau\)’s to jets in SUSY events. This ratio explains the decrease in purity with increasing \(\mathsf{E_{\mathit{T}}}\) seen in the left plot.

Figure 27: Left: Reconstruction efficiency for \(\tau\)’s vs. \(|\eta|\) without (solid) and with (dashed) noise. Right: Integral distribution of distance \(\mathsf{R_{\tau,MC}}\) between reconstructed and nearest Monte Carlo hadronic \(\tau\) without noise (solid) and with noise (dashed).

### \(\mathsf{B}\) tagging

Heavy flavor jets are produced from the decay of gluinos for the point chosen, so tagging those jets allows the measurement of the masses of some of the third generation squarks. The tagging of heavy flavor jets in a complex environment as in the SUSY events gives also some useful information on the ATLAS flavor tagging potential.

Figure 30: Left: \(\mathsf{L_{\tau}}\) distributions for \(\tau\)’s with calorimeter noise; compare with Figure 25. Right: Resolution \(\mathsf{E_{\mathit{T}}}/\mathsf{E_{\mathit{T,\mathrm{MC}}}}\) for hadronic \(\tau\)’s with calorimeter noise; compare with Figure 29.

Figure 29: Left: \(\mathsf{E_{\mathit{T}}}\) resolution for reconstructed hadronic \(\tau\)’s with a double Gaussian fit.

There is no publicly available code accompanying the release to perform b-tagging, so our tagging is performed on the combined Ntuples. Jets are tagged using the signed impact parameter as described in the Physics TDR [2]. Tagging of jets reconstructed using the \(\mathrm{k}_{T}\)-algorithm has not been performed, as \(\mathrm{k}_{T}\) jets do not have a well defined shape creating some extra difficulties.

For comparison purposes and to evaluate the efficiency of the algorithm we define a jet as a \(\mathrm{b}\mathrm{-}\mathrm{jet}\) (\(\mathrm{c}\mathrm{-}\mathrm{jet}\)) if in the MC truth record there is a \(\mathrm{B}\) hadron (\(\mathrm{C}\) hadron) within a radius of 0.4 from the jet axis. This definition is different from that used for \(\mathrm{b}\mathrm{-}\mathrm{tagging}\) by the Higgs group [2], as they are defining a \(\mathrm{b}\) jet as a jet coming from the fragmentation of an initial \(\mathrm{b}\) quark. We chose this definition as it can be applied to any jet provided that the MC event is available and has no ambiguity due to the association of a jet to a parton. This association is dependent on the QCD showering approach used as well as on the type of events generated.

All the reconstructed tracks inside the jet cone are examined. Tracks are required to have 9 precision hits including 2 in the pixel system one of which must be in the \(\mathrm{b}\mathrm{-}\mathrm{layer}\). At least 20 TRT hits are required. The impact parameter of the track is then computed and a weight assigned using both the transverse and longitudinal impact parameters, performing a three dimensional \(\mathrm{b}\) tagging. The procedure is repeated for all tracks in the jet and a jet weight computed. The weight distribution for jets is centered on zero for jets containing particles for which all tracks originate at the primary vertex. For jets that are tagged as \(\mathrm{b}\) jets by our truth algorithm, the weight distribution is skewed to positive values, as these jets are expected to contain some long-lived hadrons. By placing a cut at some value of the weight, a rejection factor against light quark jets and an efficiency for tagging \(\mathrm{b}\) jets is obtained. This is shown for the SUSY sample in Figure 31. Note that the light quark sample is obtained from the same SUSY data; this is essential as we are interested in \(\mathrm{b}\mathrm{-}\mathrm{jet}\) fakes from the same event, as the main background for SUSY events is combinatorial from the SUSY events themselves.

In order to check and validate the algorithm, the DC1 datasets of \(\mathrm{W}\mathrm{H}\) events with the decay \(\mathrm{H}\to\mathrm{b}\overline{\mathrm{b}}\) and \(\mathrm{H}\to\mathrm{u}\overline{\mathrm{u}}\) were used. Previous studies [2] have used these processes as a benchmark. Our definition of the

Figure 31: Left: \(\mathrm{b}\) weights for SUSY events using xKalman for non-heavy flavor jets (solid), \(\mathrm{b}\) jets (dash), and \(\mathrm{c}\) jets (dash-dot). Right: Resulting \(\mathrm{b}\) tagging efficiency vs rejection for SUSY events (squares) and \(\mathrm{W}\mathrm{H}\) events (triangles).

truth for a b\(-\)jet (i.e. that the jet cone contains a B hadron) is different from the one used in these studies, so the results are expected to be slightly different. We use these two samples and, in each, select a dijet pair with an invariant mass within 20 GeV of the reconstructed Higgs mass peak. The peak is approximately 10 GeV below the nominal value; we have not recalibrated the jet energy scale to take account of, in particular, the losses of particles outside of the jet cone. Our truth algorithm is then applied to these jets to obtain b\(-\)jet and light quark jet samples. The rejection against jets containing neither b\(-\) or c\(-\) hadrons as a function of the efficiency for tagging b\(-\)jets is shown on the right side of Figure 31 as diamonds (xKalman is used). The resulting rejection is worse than that shown in Figure 10-21 of Ref. [2]. This is at least partially explained by the greater complexity of the SUSY events.

The rejection against non-heavy flavor jets as a function of the efficiency for tagging b\(-\)jets in the SUSY sample is shown on the right side of Figure 31 as squares. The results obtained with xKalman and iPatRec are very similar, and only xKalman is shown. The performance is somewhat worse than that from the Higgs control sample. For a 50% b-tagging efficiency a rejection of about 30 can be obtained. This is worse than what is normally quoted as the ATLAS performance, presumably because of the more complex event topology. More jets and more particles are present in SUSY events, thus making the identification of heavy flavor jets more difficult. It should be noted, however, that in SUSY studies the flavor tagging performances do not need to be stretched since the backgrounds are not large in comparison with, e.g., the \(\PH\to\bbbar\) process. As we are interested in rejecting fake b-jets from the same events the rejection needs to be a few times bigger than the average jet multiplicity.

## 5 Physics Signals

Section 4 focused mainly on the performance of Athena 6.0.3/7.0.2 for reconstructing physics objects (jets, electrons, muons, etc.) in the full sample of SUSY events. This section describes some physics measurements that could be made with such a sample after cuts needed to reject Standard Model backgrounds [2].

### Global

Following any discovery of SUSY, the next task will be to measure parameters describing the SUSY model chosen by Nature. The first such quantity to be measured would likely be the 'SUSY mass scale' \(\M_{\text{SUSY}}\)[9], defined as the production cross-section weighted mean of the masses of the initial SUSY particles (mostly squarks and gluinos) [10]. This quantity is strongly correlated with the 'effective mass' \(\M_{\text{eff}}\) of SUSY events, defined as the scalar sum of the jets in the event and the event \(\E_{T}^{\text{\it miss}}\)[9, 10]. The peak of the distribution of \(\M_{\text{eff}}\) values for SUSY events should lie at around twice \(\M_{\text{SUSY}}\) due to the kinematics of the SUSY particle decay processes.

The distributions of \(\M_{\text{eff}}\) values for fully simulated SUSY signal events (no noise or pileup) and for Standard Model \(\ttbar\), \(\W\j\), \(\Z\j\) and light quark QCD background events using data from earlier fast simulation [10] are shown in Figure 32. Events appearing in these distributions were required to satisfy the following criteria in order to minimize Standard Model backgrounds:

* At least 4 jets with \(\pT>50\) GeV.
* At least 2 jets with \(\pT>100\) GeV.
* \(\E_{T}^{\text{\it miss}}>\max(100\) GeV, 0.25 \(\E_{T}^{\text{\it sum}})\).
* Transverse sphericity (circularity) \(\S_{T}>0.2\).
* No reconstructed muons or isolated electrons in \(|\eta|<2.5\).

The peak of the signal event distribution (circles) lies at around 1120 GeV, which is consistent with the expectation from the value of \(\mathsf{M}_{\rm SUSY}\) of 590 GeV. It is worth noting that by counting the number of signal and background events with \(\mathsf{M}_{\rm eff}>1000\) GeV one obtains a large signal significance \(\mathsf{S}/\sqrt{\rm B}\sim 286\) even for 5.1 fb\({}^{-1}\) of data. Discovery of SUSY in this particular benchmark scenario would not therefore be difficult!

### Dilepton Signals

After the discovery of SUSY and the measurement of the effective mass, a precise measurement of the parameters of the model is possible by measuring the masses of the particles produced in SUSY events. Due to R-parity conservation, all SUSY events contain two neutralinos \(\widetilde{\chi}^{0}_{1}\) which escape the detector, producing the characteristic missing energy signature. Since not all of the decay products can be detected, we will measure kinematic end-points in the invariant masses distributions rather than mass peaks.

The simplest decay chain to be observed is

\[\widetilde{\chi}^{0}_{2}\to\bar{\ell}^{\mp}\ell^{\pm}\to\widetilde{\chi}^{0}_ {1}\ell^{\pm}\ell^{\mp}.\]

The \(\chi^{0}_{2}\) undergoes two sequential two-body decays, and one of its final decay products is the invisible massive \(\chi^{0}_{1}\). If \(\ell\) is an electron or a muon, the final state charged leptons can be directly detected; the ATLAS performances for electrons and muons discussed in Section 4.3 and in Section 4.2 will play a crucial role for the study of this channel. In particular, the mass difference between the two neutralinos is such that the two charged leptons will be produced with \(\mathrm{p}_{T}\geq 20\) GeV and thus they can be detected with good efficiency in the pseudorapidity region covered by the ATLAS tracking detectors.

The invariant mass distribution of the two charged leptons is expected to have a triangular shape with an

Figure 32: Distribution of effective mass values for fully simulated SUSY signal events (circles), Standard Model \(\mathrm{t}\overline{\mathrm{f}}\), \(\mathrm{W}\,\mathrm{j}\), \(\mathrm{Z}\,\mathrm{j}\) and light quark QCD background events (hatched histogram) and the sum of signal and background distributions (histogram), normalized to \(5\,\mathrm{fb}^{-1}\). Standard model backgrounds were simulated using PYTHIA 6.136 + ATLFAST.

edge at

\[\mathsf{M}^{\rm max}_{\ell\ell}=\mathsf{M}_{\widetilde{\chi}^{0}_{2}}\sqrt{1- \frac{\mathsf{M}^{2}_{\widetilde{\ell}}}{\mathsf{M}^{2}_{\widetilde{\chi}^{0}_ {2}}}}\sqrt{1-\frac{\mathsf{M}^{2}_{\widetilde{\chi}^{0}_{1}}}{\mathsf{M}^{2}_ {\widetilde{\ell}}}}=100.31\,\mathrm{GeV}\,.\]

The invariant mass distribution for dielectron and dimuon events is plotted in Figure 33. Leptons in these plots are required to be separated from jets by \(\Delta\mathsf{R}>0.4\) so as to eliminate leptons from charm and bottom decay. The expected triangular distribution has to be convoluted with a Gaussian to account for the finite resolution in the measurement of the momentum of electrons and muons. Hence this resolution will influence the sharpness of the edge. The position of the edge itself will be influenced by the calibration of the detector, which can be performed to a high level of accuracy by using \(\mathsf{Z}\to\ell\ell\) events. The lepton calibration is beyond the scope of this note, so here we assume that the original momentum of the leptons can be reconstructed and we rescale the electron momentum by a factor of 1.02 to account for a small reconstruction effect as evidenced by Figure 8. We have not taken into account the \(\eta\) dependence as evidenced by Figure 9

To combine the signal from the electron and muon distributions together, we also have to account for the difference in reconstruction efficiency, so the electrons invariant mass distribution needs to be rescaled by a factor of \(\beta^{2}=(\epsilon_{\mu}/\epsilon_{e})^{2}=1.22\), before being combined together. Here the efficiency was defined as in the previous sections, as a result of a matching to the Monte Carlo truth. In the real experiment, when the MC truth will not be available, the ratio between the two efficiencies will have to be measured again using samples of \(\mathsf{Z}\to\ell\ell\) and/or \(\mathsf{W}\to\ell\nu\) events.

Finally, the combinatorial background, from dileptons coming from uncorrelated SUSY decays can be estimated by looking at other-sign other-flavor lepton pairs. So the final distribution to estimate the mass edge is given by:

\[\beta^{2}\mathsf{M}(\mathrm{e}^{+}\mathrm{e}^{-})+\mathsf{M}(\mu^{+}\mu^{-})- \mathsf{\beta}\mathsf{M}(\mathrm{e}^{\pm}\mu^{\mp}),\]

which is plotted in Figure 33. This final distribution is fitted with a triangular distribution convoluted with a Gaussian to estimate the edge position at \(\mathsf{M}_{\rm edge}=100.25\pm 1.14\,\mathrm{GeV}\) with a \(\chi^{2}/\mathrm{n.d.f}\)=91/84, which is,

Figure 33: Left: \(\mathrm{e}^{+}\mathrm{e}^{-}\) (dashed), \(\mu^{+}\mu^{-}\)(solid), and \(\mathrm{e}^{\pm}\mu^{\mp}\) (dash-dot) mass plots. Right: Flavor subtracted dilepton mass plot after scaling for electron efficiency and energy scale for \(5\,\mathrm{fb}^{-1}\).

within the error, consistent with the expected value.

### Jet Signals

Since the \(\widetilde{\chi}_{1}^{0}\) in mSUGRA is mostly bino, light right-handed squarks decay via \(\mathrm{q}_{R}\to\widetilde{\chi}_{1}^{0}\mathrm{q}\) with branching ratios close to 100%. Thus the only signal for \(\mathrm{q}_{R}\) is two hard jets plus large \(\mathbb{E}_{T}\). In the Physics TDR the \(\mathrm{p}_{T}\) distribution of the jets in such events was used to estimate \(\mathrm{M}(\mathrm{q}_{R})\), but this was not very precise.

A much better determination of \(\mathrm{M}\!\cdot_{\mathrm{q}}\) can be made by assuming that \(\mathbb{E}_{T}\) comes from just the two \(\widetilde{\chi}_{1}^{0}\)'s and

Figure 34: Plots showing \(\mathrm{q}_{R}\) mass determination using the \(\mathrm{M}_{T2}\) variable for the nominal \(\mathrm{M}_{\widetilde{\chi}_{1}^{0}}=100\,\mathrm{GeV}\) without (top left) and with (top right) noise. The lower two plots are without noise for \(\widetilde{\mathrm{M}}_{\widetilde{\chi}_{1}^{0}}=90\,\mathrm{GeV}\) (bottom left) and \(110\,\mathrm{GeV}\) (bottom right). All plots are for \(5\,\mathrm{fb}^{-1}\).

using the "stransverse mass" [11]

\[M_{T2}^{2}\equiv\min_{\vec{p}_{T,1}+\vec{p}_{T,2}=\vec{p}_{T}}\left\{\max\left[M_ {T}(p_{T,j_{1}},\,\hat{p}_{T,1};M_{\vec{\chi}_{1}^{0}}),M_{T}(p_{T,j_{2}},\, \hat{p}_{T,2};M_{\vec{\chi}_{1}^{0}})\right]\right\}\]

constructed by partitioning \(\vec{p}_{T}=\vec{p}_{T,1}+\vec{p}_{T,2}\) in all possible ways, computing the transverse masses of the two jets \(j_{i}\) with the \(\vec{p}_{T,i}\), and taking the minimum of the larger value. If \(M_{\vec{\chi}_{1}^{0}}\) is known, then the endpoint of the resulting distribution evidently should be just \(M_{\cdotq}\).

Events were selected to have \(\E_{T}>200\GeV\), two jets with \(\E_{T}>150\GeV\), and no reconstructed electrons or muons. The resulting \(M_{T2}\) distributions are shown without calorimeter noise and with noise and a \(2\sigma\) cut in Figure 34 for the nominal \(M_{\vec{\chi}_{1}^{0}}=100\GeV\) and for masses of \(90,110\GeV\). (An error of about \(\pm 10\%\) on \(M_{\vec{\chi}_{1}^{0}}\) has been previously obtained from analyses of \(\ell^{+}\ell^{-}+\jets\) signatures at similar points based on fast simulation [2, 12].) The fitted endpoints without noise, \(619.6\pm 8.2\GeV\) for the nominal \(M_{\vec{\chi}_{1}^{0}}\) and \(614.1\pm 6.7\GeV\) and \(601.8\pm 8.8\GeV\) for a \(\pm 10\%\) variation of it, are in quite good agreement with the actual values, \(M_{\vec{\mu}_{0}}=611.8\GeV\) and \(M_{\vec{\mu}_{0}}=610.7\GeV\). The fitted values with calorimeter noise are somewhat lower; \(572.6\pm 26.0\GeV\) for the nominal \(M_{\vec{\chi}_{1}^{0}}\). This shift has not been understood.

The sensitivity of the fitted \(M_{T2}\) endpoint to the assumed \(M_{\vec{\chi}_{1}^{0}}\) is in all cases smaller than the statistical error for this data sample, equivalent to about \(5\fb^{-1}\). The resulting error on \(M_{\cdotq}\) should be much better than that from the method used in the Physics TDR, even though this analysis is based on full simulation and reconstruction. In particular, the Physics TDR analysis is much more sensitive to the production kinematics than this one.

### Tau Signals

While \(\tau\)'s are much more difficult to detect and to measure than \(\e\)'s and \(\mu\)'s, they provide important additional information about the SUSY model. Even in minimal SUGRA, for which all leptons are assumed to be identical at the unification scale, Yukawa terms in the renormalization group equations, \(\bar{\tau}_{L}\)-\(\bar{\tau}_{R}\) mixing proportional to \(m_{\tau}\), and gaugino-higgsino mixing in the charginos and neutralinos combine to give different behavior for \(\tau\)'s at the weak scale. For example, the SUGRA point considered here has \(B(\vec{\chi}_{2}^{0}\to\bar{\ell}_{R}^{\pm}\ell^{\mp})=8.8\%\) but \(B(\vec{\chi}_{2}^{0}\to\bar{\tau}_{1}^{\pm}\tau^{\mp})=75.4\%\).

If the momenta of the produced \(\tau\)'s could be measured directly, the \(\tau^{+}\tau^{-}\) mass distribution from \(\vec{\chi}_{2}^{0}\to\bar{\tau}_{1}^{\pm}\tau^{\mp}\) would have a shape similar to Figure 33 with a slightly different endpoint. Of course each \(\tau\) decays to visible hadrons plus a \(\nu_{\tau}\), and only the visible \(\tau\tau\) mass can be measured. Background \(\tau\)'s from misidentified jets should have random signs. To the extent that \(\cancel{q}\) and \(\cancel{q}\) production dominates, \(\tau\) pairs from two independent decays should also have random signs. Figure 35 shows the reconstructed visible \(\tau^{+}\tau^{-}\) and \(\tau^{\pm}\tau^{\pm}\) mass distributions after the cuts described above (Section 4.6) to reject Standard Model backgrounds. Calorimeter noise and pileup are not included. There is a hint of an endpoint at approximately the right value,

\[M_{\tau\tau}^{\max}=\sqrt{\frac{(M_{\vec{\chi}_{2}^{0}}^{2}-M_{\vec{\chi}_{1}^ {1}}^{2})(M_{\vec{\chi}_{1}^{1}}^{2}-M_{\vec{\chi}_{0}^{0}}^{2})}{M_{\vec{\chi} _{1}^{1}}^{2}}}{=98.3\GeV}}\]

but of course the visible mass distribution is smeared by the missing \(\nu_{\tau}\)'s.

Hadronic \(\tau\) decays are well understood, so the expected visible \(\tau\tau\) mass distribution from \(\vec{\chi}_{2}^{0}\to\bar{\tau}_{1}^{\pm}\tau^{\mp}\) can be calculated given knowledge of the \(\tau\) polarizations. Information on these polarizations can in principle be extracted by comparing various \(\tau\) decay modes: \(\tau\to\pi\nu\) is the most sensitive to the polarization, while high-mass modes are not very sensitive. Extracting such polarization information is difficult [13], so as a first approximation it can be ignored. The visible \(\tau\tau\) mass distribution calculated from CBNT_SpclMc information -- and so using the correct polarization -- from all \(\tilde{\chi}^{0}_{2}\to\tau^{\pm}_{1}\tau^{\mp}\) decays is shown as the histogram on the left side of Figure 36. The smooth curve in the figure is an analytic parameterization.

The difference of the reconstructed \(\tau^{+}\tau^{-}\) and \(\tau^{\pm}\tau^{\pm}\) distributions from Figure 35 is shown in the right side of Figure 36 as points with statistical error bars. The shape of the parameterization derived from the

Figure 35: Reconstructed \(\tau^{+}\tau^{-}\) (solid) and \(\tau^{\pm}\tau^{\pm}\) (dashed) mass distributions for \(5\,\mathrm{fb}^{-1}\) after cuts to reject Standard Model backgrounds.

Monte Carlo was fit to these points varying the endpoint and normalization. The resulting fit, shown as the smooth curve on the figure, gives a fitted endpoint of \(103.5\pm 4.9\,\mathrm{GeV}\) compared to the expected \(98.3\,\mathrm{GeV}\). The shape of the distribution at low masses is not consistent with the parameterization because of acceptance effects, mainly the minimum \(\mathrm{p}_{T}\) required for \(\tau\) reconstruction. Since acceptance corrections have not been applied, the fitted endpoint does depend on the fit region. Nevertheless, this analysis of full simulation data represents a significant improvement over results in the Physics TDR.

### Combined Signals

The dileptons reconstructed in Section 5.2 can be combined with jets to reconstruct the decay chain

\[\mathrm{q}_{L}\to\widetilde{\chi}^{0}_{2}\mathrm{q}\to\tilde{\ell}^{\pm}\ell^ {\mp}\to\widetilde{\chi}^{0}_{1}\ell^{+}\ell^{-}\mathrm{q}\]

If one can correctly identify the jet arising from the squark decay, then the invariant mass of the \(\ell^{+}\ell^{-}+\mathrm{jet}\) system is bounded by \(\mathrm{M}^{\mathrm{max}}_{\ell\ell q}\) and \(\mathrm{M}^{\mathrm{min}}_{\ell\ell q}\), where

\[\mathrm{M}^{\mathrm{max}}_{\ell tq}=\left[\frac{\left(\mathrm{M}^{2}_{ \widetilde{\nu}_{q}}-\mathrm{M}^{2}_{\widetilde{\chi}^{0}_{2}}\right)\left( \mathrm{M}^{2}_{\widetilde{\chi}^{0}_{2}}-\mathrm{M}^{2}_{\widetilde{\chi}^{0 }_{1}}\right)}{\mathrm{M}^{2}_{\widetilde{\chi}^{0}_{2}}}\right]^{1/2}=501 \mathrm{GeV}\,,\]

and

\[(\mathrm{M}^{\mathrm{min}}_{\ell\ell q})^{2} = \frac{1}{4M^{2}_{\widetilde{\nu}_{q}}M^{2}_{e}}\Biggl{[}-\mathrm{ M}^{2}_{1}\mathrm{M}^{4}_{2}+3\mathrm{M}^{2}_{1}\mathrm{M}^{2}_{2}\mathrm{M}^{2}_{e}- \mathrm{M}^{4}_{2}\mathrm{M}^{2}_{e}-\mathrm{M}^{2}_{2}\mathrm{M}^{4}_{e}- \mathrm{M}^{2}_{1}\mathrm{M}^{2}_{2}\mathrm{M}^{2}_{q}-\] \[\mathrm{M}^{2}_{1}\mathrm{M}^{2}_{e}M^{2}_{q}+3\mathrm{M}^{2}_{2 }\mathrm{M}^{2}_{e}M^{2}_{q}-\mathrm{M}^{4}_{e}\mathrm{M}^{2}_{q}+(\mathrm{M}^ {2}_{2}-\mathrm{M}^{2}_{q})\times\] \[\sqrt{(\mathrm{M}^{4}_{1}+\mathrm{M}^{4}_{e})(\mathrm{M}^{2}_{2} +\mathrm{M}^{2}_{e})^{2}+2\mathrm{M}^{2}_{1}\mathrm{M}^{2}_{e}(\mathrm{M}^{4} _{2}-6\mathrm{M}^{2}_{2}\mathrm{M}^{2}_{e}+\mathrm{M}^{4}_{e})}\Biggr{]}\] \[\mathrm{M}^{\mathrm{min}}_{\ell\ell q} = 271.8\,\mathrm{GeV}\]

Here

\[\mathrm{M}_{q}=\mathrm{M}_{\widetilde{\nu}_{q}},\;\mathrm{M}_{2}=\mathrm{M}_{ \widetilde{\chi}^{0}_{2}},\;\mathrm{M}_{e}=\mathrm{M}_{\widetilde{\nu}_{R}}, \;\mathrm{M}_{1}=\mathrm{M}_{\widetilde{\nu}_{1}}\,.\]

The hardest jets in SUSY events are expected to come from squark decays. For each event, the two jets with the highest \(\mathrm{p}_{T}\) are selected, and the invariant mass of each of these with the dilepton system is formed. The distribution of the smallest and largest of these masses is shown in Figures 37. In order to reduce the effect of contributions where the leptons arise from \(\chi^{\pm}\) decay again we have performed a flavor subtraction as described in Section 5.2. The dashed histograms arise from events where the jets are tagged as \(\mathrm{b}-\mathrm{jets}\).

These plots show similar structure to those for Point 5 (see Figures 20-20 and 20-28 in Ref. [2]) obtained with fast simulation. The upper edge is clearly visible in Figure 37 (left): the true end point is at 501 GeV. A fit to the end point gives \(422\pm 9\,\mathrm{GeV}\); this is similar to the result from fast simulation (see Figure 20-20 of [2]) which was 10% below the nominal value. A careful jet energy calibration taking into account out-of-cone losses needs to be made. The end point for events with \(\mathrm{b}\)-jets is somewhat lower due to the smaller mass of \(\widetilde{\mathrm{b}}_{1}\). The statistical sample is too small to make a precise measurement, but the end point is still visible. The lower edge shown in Figure 37 (right) is less clear: the true threshold is at 249 GeV. In particular there seem to be more events below the true threshold than were seen in the fast simulation studies. The reason for this is not understood.

## 6 Conclusions and Future plans

In this note we have shown that the current ATLAS reconstruction software can be used to extract physics signals from very complex events. The results presented here have taken more than a year to produce, but valuable lessons have been learned. These SUSY events contain most of the important signatures for ATLAS physics -- muons, electrons, jets, \(\mathrm{E}_{T}\), \(\tau\)'s, and b-jets -- everything except isolated photons. Thus, these events provide an excellent testing ground for Athena reconstruction. We did not attempt to simulate any background. By using the results of previous fast simulation studies, we are confident that little Standard Model background will appear on the plots shown in Section 5.

The authors of this note have been pleased by the overall performance of the reconstruction software at this stage in its development. Nevertheless, the reconstruction performance found in this study is less than optimal. While some of the problems may be due to changes in the detector design resulting from engineering constraints, it is premature to draw conclusions about the ultimate performance of ATLAS at this time. Among the issues that should be addressed are:

* While the muon efficiency is very good, the fake rate is not negligible, although it is adequate for the present case. A tighter matching between the inner detector track and the muon track should help. Most of the fakes seem to come from \(\pi\) and \(K\) decay in jets, so an isolation cut should also help.
* To obtain adequate electron efficiency it was necessary to make quite loose cuts on the track-shower matching, especially in the endcap regions, leading to backgrounds that are not negligible. Since this is true for both xKalman and iPatRec, it may reflect a real effect of the material in the inner detector; more study is needed. The background from hadronic \(\tau\) decays in this sample is significant due to the high \(\tau\) multiplicity, and cuts against it could probably be optimized. TR information was not used and should reduce the background. An explicit isolation cut in addition to the implicit one in eg_IsEM could also reduce the background.

Figure 37: Left: Distribution of the larger of the two \(\ell^{+}\ell^{-}\mathrm{q}\) masses for \(5\,\mathrm{fb}^{-1}\). Right: Distribution of the smaller of the two \(\ell^{+}\ell^{-}\mathrm{q}\) masses with a cut on the dilepton mass. The solid curves are for all jet flavors, and the dashed ones are for those jets tagged as \(\mathrm{b}-\mathrm{jets}\).

* Jet reconstruction is significantly affected by the hadronic energy scale and by electronic noise. Simple H1-style weighting and tower-based clustering schemes have been developed to mitigate these problems, but work on more sophisticated approaches is needed.
* \(\mathbb{E}_{T}\) reconstruction shows deviations from Monte Carlo truth. Perhaps these problems result from the hadronic energy calibration, but they are not understood and should be studied.
* Hadronic \(\tau\) reconstruction is badly degraded by electronic noise, especially at small \(|\eta|\). Nothing has been done on this in the SUSY group, but progress has been reported recently [14].
* There is currently no b-tagging algorithm in Athena. While it is possible to do b tagging at the analysis level, a reconstruction algorithm would be much more convenient.
* We were not able to do any complete studies of the effects of pileup as we needed to be able to simulate pile up over the full rapidity coverage, and this was not available.

Athena 7.0.2 contains a number of important technical improvements, but in most cases the reconstruction performance is quite similar to that of Athena 6.0.3.

The collaboration that worked on this study was distributed over ten time zones (West coast USA to Israel). A few meetings were held at CERN with many people present, but most of the meetings were via phone with only one or two people at each location. By starting these meetings at 5:00 pm CERN time, all the collaborators could be accommodated, although potential collaborators in Japan and similar time zones were effectively excluded. Material was made available prior to the meetings. We did not use video conferencing; experience has taught us that there is too much time wasted on technical problems.

The event simulation was done using facilities from eight sites. Magda proved to be a very useful tool for migrating the data between sites, but problems with GRID authentication were a barrier to individuals at a single site. Reconstruction was done almost exclusively at LBL for several reasons:

* Enough disk space was available at LBL so that all the simulated ZEBRA files could be retained on disk.
* Sufficient CPU resources were available at LBL that the full data sample could be reprocessed overnight.
* Rapid turn around is essential to provide feedback to the reconstruction community. We wrote a custom script to make a binary only installation which could be used to move a release from CERN (including a nightly release if needed).

To our knowledge, equivalent resources were not available elsewhere. We note that the ATLAS code distribution system is still such that new releases cannot be installed promptly at remote sites, although the installation of a binary only release has been improved significantly since the introduction of the InstallArea.

There is currently no ATLAS analysis framework. We wrote a simple minded one in Fortran as some collaborators were more comfortable with Fortran than C++. Therefore we made Paw rather than Root Ntuples from Athena. This framework was then used to enable the (1000) Ntuples to be processed in less than 30 minutes in batch mode. We only used iterative PAW for making the final plots. However, the complexity of the Ntuple resulted in about 5000 lines of Fortran analysis code, much of it not SUSY specific. Common Routines to select "good" muons, electrons, taus, etc., from the Ntuple would have been helpful. A key goal of DC2 should be to provide an ATLAS-wide C++-based analysis framework together with tools to identify reconstructed physics objects (muons, electrons, etc.) and to associate these with Monte Carlo truth. If this is not done, then we will rewrite our framework in C++ for future work starting with Root Ntuples and again processing them in batch mode.

The physics case that was simulated was deliberately chosen to be close to cases that had been studied with fast simulation so that we could focus primarily on software issues. The ATLAS SUSY group is still considering its goals for DC2. Since the present sample is such a good testbed of the whole range of physics signatures, it may be worthwhile to use it or a similar point to test Geant-4 and AOD-based reconstruction.

In view of the results from WMAP on the dark matter, the SUGRA parameter space is severely constrained, although these constraints can be avoided by leaving minimal SUGRA. Of particular interest for minimal SUGRA is the "coannihilation region", in which there is a near degeneracy between \(\widetilde{\chi}^{0}_{1}\) and the sleptons. In this region one of the leptons in the cascade decay \(\tilde{\chi}^{0}_{2}\to\tilde{\ell}^{+}\ell^{-}\to\ell^{+}\ell^{-}\widetilde{ \chi}^{0}_{1}\) will be soft. The fast simulation has not been validated for such slow leptons, so a full simulation and reconstruction must be used to assess this case. Soft taus are also expected from \(\widetilde{\chi}^{0}_{2}\to\bar{\tau}^{+}_{1}\tau^{\mp}\). The results can then be used to improve the fast simulation.

This work has been performed within the ATLAS Collaboration, and we thank collaboration members for helpful discussions. We have made use of the physics analysis framework and tools which are the result of collaboration-wide efforts. In particular, we are grateful to Alan Poppleton and Donatella Cavalli for clarifications of tracking and missing \(\mathrm{E}_{T}\) issues.

## References

* [1] ATLAS Data Challenge 1, ATL-SOFT-2003-012.
* [2] ATLAS Collaboration, ATLAS Detector and Physics Performance Technical Design Report
* [3] G. Corcella et al., arXiv:hep-ph/0201201. G. Corcella et al., JHEP **0101** (2001) 010.
* [4] H. Baer, F. E. Paige, S. D. Protopescu and X. Tata, arXiv:hep-ph/0312045.
* [5][http://documents.cern.ch/cgi-bin/setlink?base=atlnot&categ=Note&id=daq-2003-002](http://documents.cern.ch/cgi-bin/setlink?base=atlnot&categ=Note&id=daq-2003-002)
* [6][http://atlas.web.cern.ch/Atlas/GROUPS/MUON/Reconstruction/Muonbox.html](http://atlas.web.cern.ch/Atlas/GROUPS/MUON/Reconstruction/Muonbox.html)
* [7] D. Adams et al., ATL-SOFT-2003-007; 28 May 2003
* [8] S. Asai, et al., SN-ATLAS-2003-024, Eur Jour. Phys (to appear).
* [9] I. Hinchliffe, F. E. Paige, M. D. Shapiro, J. Soderqvist and W. Yao, Phys. Rev. D **55** (1997) 5520 [arXiv:hep-ph/9610544].
* [10] D. R. Tovey, Phys. Lett. B **498** (2001) 1 [arXiv:hep-ph/0006276].
* [11] A. Barr, C. Lester and P. Stephens, J. Phys. G **29**, 2343 (2003).
* [12] B. C. Allanach et al., with baryon-number R-parity violating couplings," JHEP **0103**, 048 (2001).
* [13] L. Vacavant, [http://agenda.cern.ch/fullAgenda.php?ida=a0159](http://agenda.cern.ch/fullAgenda.php?ida=a0159).
* [14] M. Heldmann, [http://agenda.cern.ch/askArchive.php?base=agenda&categ=a03246&id=a03246s13t5/transparencies](http://agenda.cern.ch/askArchive.php?base=agenda&categ=a03246&id=a03246s13t5/transparencies).