**Validation of Monte Carlo event generators in**

**the ATLAS Collaboration for LHC Run 2**

ATLAS Collaboration

This note reviews the main steps followed by the ATLAS Collaboration to validate the properties of particle-level simulated events from Monte Carlo event generators in order to ensure the correctness of all event generator configurations and production samples used in physics analyses. A central validation procedure is adopted which permits the continual validation of the functionality and the performance of the ATLAS event simulation infrastructure. Revisions and updates of the Monte Carlo event generators are also monitored. The methodology behind the validation and tools developed for that purpose, as well as various usage cases, are presented. The strategy has proven to play an essential role in identifying possible problems or unwanted features within a restricted timescale, verifying their origin and pointing to possible bug fixes before full-scale processing is initiated.

## 1 Introduction

One of the crucial components of the ATLAS experiment's [1] software framework, Athena[2], is the Monte Carlo (MC) simulation of collision events (referred to as MC events or MC event samples). Smooth operation of this simulation is crucial to the ATLAS physics programme in the ongoing Run 2 of the Large Hadron Collider (LHC). A plethora of MC event generators (e.g. Alpgen[3], EvtGen[4], Herwig[5] and Herwig++[6], Madgraph[7], Madgraph5_aMC@NLO[8], Photos++[9], Powheg-Box[10], Pythia 6[11] and Pythia 8[12], Sherpa[13], Tauola[14] and many others) are available in the ATLAS experiment to provide the user with a complete set of tools for testing several Standard Model (SM) and beyond Standard Model (BSM) physics models [15]. Event generation is executed within the Athena framework using a set of well-defined and highly configurable interface packages, which are designed to be flexible, and wherever possible to be factorised from the external packages. Most of the MC generators are written and maintained by authors external to ATLAS and are continually being updated. Moreover, the ATLAS simulation infrastructure is regularly upgraded, and a reliable validation of official ATLAS MC generator configurations and production samples for physics analyses is critically important.

To accomplish this task, an automated and central MC event generator validation procedure is adopted in ATLAS. This includes careful monitoring of the functionality and performance of the event simulation infrastructure, as well as constant validation of the revisions and updates of the MC generators and/or the ATLAS interface packages before a large-scale MC production. The validation team defines common rules and centralised validation steps, collects information from different contact people, tests the MC generators and validates their physics content in the framework of the ATLAS software. The validation is designed to be as uniform as possible to reduce the risk of using buggy samples for physics analyses. By comparing with sufficient statistical accuracy and precision the results of different MC generators, it is possible to identify a variety of problems, ranging from simple bugs or misspellings to subtle-but-intentional changes in physics modelling, as well as test distinct model-dependent sub-components of the MC event generation. Upon incorporation of new MC generator versions into Athena, the validation team performs a standard validation procedure within a restricted timescale; this allows for essential rapid feedback and bug reporting to the authors of the external packages. On several occasions, quick bug fix releases of external MC programs were created as a result of ATLAS MC validation findings.

In the following section, the methodology and structure of the ATLAS MC validation procedure is presented. The last section is devoted to various illustrative results of specific usage cases, beginning from a technical validation of the simulation infrastructure to more nuanced cases during major or minor updates by the MC generator authors.

## 2 Methodology

The main task of the validation team is to spot possible problems or unwanted features in the validation samples, propose more complex checks to understand the origin of the issue and point to possible bug fixes before the next production cycle. In practice, this means comparing two MC samples, in the following referred to as "monitored" and "reference", for consistency between shapes of numerous observables. To ensure that the validated quantities are relevant to physical measurements, generator-independent observables defined in a theoretically safe and unambiguous way are monitored. In some cases, technical MC generator quantities are verified, which helps to identify MC generator internal changes. The validation strategy depends strongly on the usage case, for example major new release of an existing MC generator,minor MC generator revision, changes to modelling of a specific sub-component in MC setup or upgrades in the ATLAS simulation infrastructure. Hence, several MC validation procedures have been adapted:

* _Technical validation_ is performed by default and after a particular ATLAS simulation infrastructure upgrade. It includes basic sanity checks on the job option and output file-naming conventions, monitoring of the job completion, submission errors and log-files from the World-wide LHC Computing Grid [16], comparison of the process cross-section values reported by ATLAS Metadata Interface [17] for the monitored and reference sample, and other technical aspects;
* _Matrix element validation_ is used to initially validate new MC generators. For this, an unmodified code from the MC generator authors is used to produce hard process/partonic four-vectors in the the "Les Houches" format [18], which are used as input to the parton shower algorithms and represent the first stage of MC generation. Basic properties of the event (energy scale, event weights and numbering scheme identifier [19]) and kinematic distributions of initial, intermediate and final state particles are monitored;
* _Particle-level validation_ is based solely on physical observables whose definitions are maximally independent of any technical details of the MC generators, i.e. those built from stable particles (HepMC [20] status code = 1) and physical decayed particles (HepMC status code = 2). During this validation the particle content and their properties, final state objects kinematics and parton distribution function (PDF) information of the event are compared. Some internal MC event generator quantities are also looked at (e.g. quark masses) using truth event record;
* _Validation of a particular process_ is performed using ATLAS official Rivet[21] analysis routines which use observables defined in real data measurements and provide more complex signatures. In particular, processes involving top-quarks, vector bosons and photons can be further validated.

### Production of MC validation samples

For validation purposes, the production of a set of multiple physics samples is launched. Typically, a standard monitored and reference sample includes one million events for each physics process to be validated. The monitoring is performed over several relevant SM processes (inclusive inelastic scattering, and dijet, vector boson, diboson and top-quark pair production, as well as signatures involving photons, \(b\)-quarks and Higgs boson) and few exotic processes (e.g. supersymmetric models). The broad spectrum of signatures and event composition of the validation samples has been chosen to test all aspects of the ATLAS physics programme, as well as the different possible combinations of matrix element, shower and afterburner generators. It has been found that relatively large validation samples may expose minor problems that could not be found with lower statistics, for example a minor change in the composition of the produced final hadrons due to an update in the underlying event or PDF tune. MC generator validation campaigns completed so far include the validation of EvtGen 1.2, Herwig++ 2.7, Madgraph 5.2.3.2, Photos++ 3.56, Powheg 2.0, Pythia (versions 6.428, 8.183, 8.185, 8.186, 8.205 and 8.210) and Sherpa (versions 1.4.5, 2.0, 2.1, 2.1.1 and 2.2).

### Certification of a validated MC sample

If no outstanding issues are found, the monitored sample is marked as "validated" and further distributed for large-scale MC production and use in physics analyses. In the case of an (unexpected) observation during the validation of a particular monitored MC sample, the issue is raised by the validation team and sent to a dedicated group of experts, including several contacts for the specific MC generator, MC production, MC software and MC tuning teams, as well as experienced members from detector performance (e.g. tracking, \(b\)-tagging, jet reconstruction groups) and physics groups (e.g. Standard Model, Higgs and Exotics search groups). Caution is taken when interpreting the validation results, which are discussed in a case-by-case scenario. In the case of discrepancies with respect to the reference sample, feedback from MC authors is crucial to establish whether the observed feature is a genuine problem or expected. If the problem is solved, a new ATLAS offline software release is made available which solves the specific issue and supersedes the previous one. This strategy has proven to be very effective in detecting possible problems or unwanted features in the MC samples within a restricted timescale. Any new MC production request in ATLAS needs to use a generator/process version which has been certified as validated.

### Automated MC validation framework

The HepMCAnalysis framework [22, 23] interfaced to Athena contains a collection of generator-independent validation tools based on the HepMC event record, and is the core tool used by the MC validation team for particle-level validations. The framework gives easy access to generator-level quantities and provides broad information about final state objects kinematics (e.g. jet and lepton masses and kinematics, global event and object properties), particle content and properties (e.g. masses, decay multiplicity and flight lengths of light and heavy-flavour hadrons), as well as PDF information about the event.

The Job Execution Monitor (JEM) [24] is used to provide additional MC validation functionality. A Web interface is used to configure and display the predefined sets of monitored and reference samples: it submits the HepMCAnalysis validation tools and provides a histogram-based output. The results of the validation tests are quickly available and presented in a flexible website, thus highly automatising and simplifying the validation task. The agreement between the resulting pairs of histograms is quantified with several statistical tests, which include Kolmogorov-Smirnov, Pearson's \(\chi^{2}\) and a bin-by-bin method [25]. For simplicity, the information about the outcome of all the statistical tests is displayed in a colour-coded summary table, as illustrated in Figure 1.

## 3 MC validation use cases

In the following, specific ATLAS use cases of the MC validation applying the procedures described in Section 2 are presented. Among them, various illustrative results with diverse (unexpected) features found during validation of a particular MC sample are discussed.

### Case study 1: ATLAS simulation infrastructure upgrade

In anticipation of the first 13 TeV collision data, the ATLAS simulation infrastructure underwent a very rapid development and several major Athena framework releases were built. A collection of Athena algorithms allows for the application of event filters to select a subset from the full event generator output. During the LHC shutdown, this and other functionalities connected to the MC event generation were updated, and a technical validation between two different releases was required. The validation of one particular filtered sample showed clear differences in the \(\Delta R\) distributions1 between the jets produced with the old and new software release, such as the one shown on the left plot of Fig. 2. It was found that the observed feature was caused by an undocumented change in the jet filter reconstruction algorithm used in the new software release, and allowed a timely solution to the problem.

Footnote 1: \(\Delta R=\sqrt{\left(\Delta\eta\right)^{2}+\left(\Delta\phi\right)^{2}}\) defines the distance between objects in (\(\eta\)-\(\phi\)) space.

Another technical issue was found while exploiting the ATLAS High Performance Computing (HPC) facilities for use by ATLAS. The HPC system uses a different CPU architecture, operating system and job submission framework, so it was very important to validate that system to ensure it produced results that are consistent with mainstream grid jobs run on x86 systems. Differences were found while monitoring the strange hadron flight length, presented on the right plot of Fig. 2, caused by the generator lifetime settings getting overwritten by default values during submission. Both of these issues were found because of the joint technical and particle-level MC validation procedures.

### Case study 2: changes to modelling of a specific sub-component in MC setup

As mentioned in Section 2, the validation procedures depend on the use-case, and for the particular case of a change in a specific MC sample sub-component, the level of agreement between monitored and reference shapes might differ depending on the process.

In the case of underlying event tune changes between the monitored and reference sample, the MC validation team focuses on the specific processes and observables (e.g. charged particle multiplicities in minimum bias interactions). After a change in the modelling of the parton distribution function, the differences are clearly visible in the particle-level validation with the HepMCAnalysis framework. Similar criteria apply to photon radiation or \(\tau\)-decays changes in the MC configurations.

Figure 1: A snapshot of the JEM graphical summary table with the colour-coded statistical tests output. In this example, JEM has highlighted in red the particle content (PartCont) validation as having a large number of histograms failing the regression tests. Roughly 200 histograms produced by the HepMCAnalysis tool are validated per sample.

In particular, in Run 2 vast majority of ATLAS simulated samples use EvtGen particle decay simulation package to model the decays of heavy-flavour hadrons. The effect of using EvtGen is visible in the left plot of Fig. 3 which shows the bottom hadrons' decay multiplicity (only for the first decay chain). Part of the MC setup validation is to ensure that other sub-components are not affected. For this purpose, during the EvtGen 1.2 validation campaign, it was checked that the \(\tau\)-lepton polarisation is preserved, as shown in the right plot of Fig. 3.

### Case study 3: minor MC generator revision

Most of the MC generators are constantly revised by the authors and minor updates relative to the baseline generator version are regularly provided. The update history of each MC generator clearly states the bug fixes and modified routines. Within the ATLAS MC validation effort, these differences are being identified, tested and confirmed.

In particular, the development of the extensively used general-purpose event generator Pythia 6 has stopped (if any, new revisions are motivated by bug fixes), while Pythia 8 is being further improved and extended in several directions. Therefore, the ATLAS MC validation team has recently concentrated on the testing of Pythia 8 releases, and checks of numerous versions have been recently accomplished (i.e. versions 8.183, 8.185, 8.186, 8.205 and 8.210). A bug affecting the handling of initial-state radiation in diffractive systems that was introduced in version 8.183 (notably giving too many high transverse momenta particles within the particle-level validation) was identified; significant extensions to the charmonium and bottomonium machineries in the version 8.185 (raising big differences connected to strange particle

Figure 2: Example of obtained results while performing a validation after an upgrade of the ATLAS simulation infrastructure: \(\Delta R\) between the two leading transverse momenta jets (left) and strange hadron flight length (right). The distributions are shown for the dijet and \(W\)+jets production, respectively. The reference samples are plotted in blue, while the monitored samples are depicted in red.

production within the particle-level validation) were tested. This and other Pythia 8 release history issues were successfully confirmed in our validations.

In the case of Herwig++, a flexible generator with a large number of built in processes, during the validation of the version 2.7.1 also including a new underlying event tune, UE-EE5 (now being used in ATLAS), with respect to the previous version (2.6.3 with UE-EE4 tune), large discrepancies in the distributions of strange hadrons were noted. In particular, differences in the number of strange mesons and the pseudorapidity distribution of strange hadrons were observed, as shown in Fig. 4. This issue was reported back to the Herwig++ authors, and it was finally tracked down that the differences seen were mainly due to the new UE-EE5 tune.

### Case study 4: major new release of an existing MC generator

In the case of a major updates of an existing event generator, the validation strategy includes the use of all the validation procedures available:

* technical validation, particularly the comparison of the production cross-sections;
* matrix element validation of the basic event properties;
* usual regression test of physical observables at particle-level;
* usage of ATLAS official Rivet analysis routines for the validation of specific processes.

Figure 3: Example of obtained results while performing a validation after a change to a specific sub-component in MC setup: bottom hadrons decay multiplicity (left) and cosine of the polar angle \(\theta\) between the \(\pi^{\pm}\) and the \(\tau\)-lepton (right). The distributions are shown for the top-quark pair production as an example. EvtGen was used in the monitored sample, shown in red, while it was not included in the reference sample, plotted in blue.

As a case example, several versions (1.4.5, 2.0, 2.1, 2.1.1 and 2.2) of the general-purpose event generator Sherpa have been validated within the ATLAS MC validation effort. During the validation of Sherpa 2.1, it was noticed that the pseudorapidity distribution of bottom hadrons was asymmetric compared to previous version 2.0 (see left plot of Fig. 5). Further investigation led to the fact that the crossing of the initial state was not correctly accounted for in the multiple parton interaction matrix elements, and a fix was implemented by the authors in a subsequent 2.1.1 patch release. Later, while performing the validation of Sherpa 2.2 taking as reference the version 2.1.1, a common characteristic in all analysed processes was identified related to the heavy-flavour hadron decay multiplicities and flight lengths, as shown in the right plot of Fig. 5. After fruitful discussions with the Sherpa authors, the heavy-flavour feature seen in the validation was confirmed to be coming from a change in the shower/hadronisation in the new release connected with improvements in the \(b\)-quark fragmentation function.

For Run 2, the ATLAS experiment moved towards using the latest release of the Powheg-Box 2.0 event generator, and monitored samples have been compared to reference Powheg-Box 1.0 samples at \(\sqrt{s}=8\) TeV used in Run 1 analyses. During the matrix element validation, the lack of gluon-initiated events in diboson production samples in the newest release was found, as can be seen in the left plot of Fig. 6. In addition, harder jet transverse momenta spectra (right plot of Fig. 6) due to a change in the veto scale for parton shower emissions between the two Powheg versions were observed. Powheg-Box authors were contacted to understand the issue, which brought changes in the ATLAS default settings and interface. One of them is that the Powheg-Box option to discard events with negative weights is switched off in Run 2 samples.

Figure 4: Example of obtained results while performing a validation after a minor MC generator revision: number of strange mesons (left) and the pseudorapidity of strange hadrons (right). The distributions are shown for the \(Z\to e^{+}e^{-}\) process as an example. The reference samples are plotted in blue, while the monitored samples are depicted in red.

Figure 5: Example of obtained results while performing a validation after major new releases of Sherpa: pseudorapidity distribution of bottom hadrons (left) and charm hadron flight length (right). The distributions are shown for the \(W\)+jets production as an example. The reference samples are plotted in blue, while the monitored samples are depicted in red.

Figure 6: Example of obtained results while performing a validation after a major new release of Powheg-Box 2.0: incoming gluon Bjorken \(x\) (left) and leading jet \(p_{\mathrm{T}}\) (right). The distributions are shown for the \(WW\)+jets production as an example. The reference samples are plotted in blue, while the monitored samples are depicted in red.

## 4 Conclusions

This note provides a review of the robust, flexible and highly functional MC validation setup developed in ATLAS. Various illustrative results of specific usage cases, beginning from a technical validation of the simulation infrastructure to more nuanced cases during major and minor updates of the MC event generators, have proven that the chosen strategy is very effective at detecting problems or unwanted features in the simulated samples within a restricted timescale. Several unexpected features in generator behaviour have been identified in this way, and reported back to the MC generator authors; this process was found to be very helpful to both parties, and has resulted in several bug fix releases of the external MC tools. Overall, the MC validation has performed well during the preparation of Run 2 simulated samples, yet several developments and new functionalities will be made available to keep apace with the most recent event generator developments.

## References

* [1] ATLAS Collaboration, _The ATLAS Experiment at the CERN Large Hadron Collider_, JINST **3** (2008) S08003.
* [2] ATLAS Collaboration, _The ATLAS Simulation Infrastructure_, Eur. Phys. J. C **70** (3 2010) 823, arXiv: 1005.4568 [hep-ph].
* [3] M. L. Mangano et al., _ALPGEN, a generator for hard multiparton processes in hadronic collisions_, JHEP **07** (2003) 001, arXiv: 0206293 [hep-ph].
* [4] D. J. Lange, _The EvtGen particle decay simulation package_, Nucl. Instrum. Meth. **A462** (2001) 152.
* [5] G. Corcella et al., _HERWIG 6: An Event generator for hadron emission reactions with interfering gluons_, JHEP **01** (2001) 010, arXiv: 0011363 [hep-ph].
* [6] M. Bahr et al., _Herwig++ Physics and Manual_, Eur. Phys. J. **C58** (2008) 639, arXiv: 0803.0883 [hep-ph].
* [7] T. Stelzer and W. F. Long, _Automatic generation of tree level helicity amplitudes_, Comput. Phys. Commun. **81** (1994) 357, arXiv: 9401258 [hep-ph].
* [8] J. Alwall et al., _The automated computation of tree-level and next-to-leading order differential cross sections, and their matching to parton shower simulations_, JHEP **07** (2014) 079, arXiv: 1405.0301 [hep-ph].
* [9] E. Barberio, B. van Eijk and Z. Was, _PHOTOS: A Universal Monte Carlo for QED radiative corrections in decays_, Comput. Phys. Commun. **66** (1991) 115.
* [10] S. Alioli et al., _A general framework for implementing NLO calculations in shower Monte Carlo programs: the POWHEG BOX_, JHEP **06** (2010) 043, arXiv: 1002.2581 [hep-ph].
* [11] T. Sjostrand, S. Mrenna and P. Skands, _PYTHIA 6.4 physics and manual_, JHEP **05** (2006) 026, arXiv: 0603175 [hep-ph].
* [12] T. Sjostrand, S. Mrenna and P. Skands, _A brief introduction to PYTHIA 8.1_, Comput. Phys. Commun. **178** (2008) 852, arXiv: 0710.3820 [hep-ph].