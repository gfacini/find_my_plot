**DAQ-NOTE 118**

22 July 1998

**Level-2 \(\lnot\) Pilot Project \(\times\)**

Atlas Trigger-DAQ

Comments to ledu@hep.saclay.cea.fr

speranza.falciano@roma1.infn.it

fjw@v2.rl.ac.uk

_Abstract_

This document presents the status of the next step of integration work for the Level-2 Trigger.

[MISSING_PAGE_FAIL:2]

8.2.3. Cost Overview

8.2.4. Integration with other systems

9. ORGANISATION

9.1. TECHICAL ORGANISATION

9.2. ORGANISATION OF MEETINGS

9.3. INTEGRATED WORKPLAN UP TO THE TECHICAL PROPOSAL

9.3.1. Phase 1 from March 98 to June 98

9.3.2. Phase 2 from June 98 to October 98

9.3.3. Phase 3 from October 98 to February 99

9.3.4. Phase 4 from March 99 to June 99

9.3.5. Phase 5 from June 99 to October 99

9.4 DETAILED WORKPLANS

9.4.1 Functional component worldplan

9.4.2 Testbed worldplan

9.4.3. System design workplan

30

## 1 Definition of the Pilot Project

This chapter presents the motivation for this programme that should bring the Level-2 community from several parallel R&D projects to an integrated common work focussed to understand the fundamental task of the Level-2 trigger: i.e. " how to reduce the Level-1 rate by a factor of \(\geq\) 100? ".

### 1.1 Basic guidelines

The purpose of this document is to define and present the Level-2 activity programme for the next 2 years. This programme should satisfy the following requirements:

* A duration of 18 months starting from March 1998 ( T/DAQ Review) and completed by the end of 1999 (Technical Proposal).
* The details should be described by a clear and complete activity list, with realistic goals, workplans and specified deliverables and milestones.
* It should be formed as a set of coherent and complementary activities, that map the resources and commitments of all of the Level-2 groups.

It should allow a steady evolution from the present to the future work of the Level-2 community.

### 1.2 Inputs - results from the last 3 years work

Since the ATLAS Technical Proposal, much work has been done which will help to define the new programme :

* A User Requirements Document has been written
* Results and conclusions have been obtained from the "Demonstrator Programme" including modelling and emulation.
* There has been a pragmatic analysis of " where are we?" in the Trigger DAQ work Discussions within the Level-2 community Discussions with DAQ/EF in Marseille Workshop (October 97)

### 1.3 Goals

The goals of the Pilot Project are to produce material for the Technical Proposal:

* Evaluate the best selection strategy to perform a reduction factor of \(\geq\)100.
* Contribute to the definition of the full High Level trigger (HLT) architecture including integration with EF/DAQ.
* Select a few technology candidates.

## 2 Level-2 Timescale and Milestones

The following timetable (see figure 1) shows the three different steps and milestones.

* used either in parallel or singly; various network technologies (DS link, SCI, ATM). These components could be combined to form various architectures, but may be understood most simply in terms of the following simple cases:
* using a few data driven FPGA processors (ENABLE\(++\)) to extract features in each detector using simple algorithms and minimum latency
- followed by a second step which combines the features for each event in one of a farm of general purpose processors.
* (Local-Global Farms) uses a number of parallel farms to extract the features in each detector and as in Architecture A there is again a second step where features are combined in a global farm.
* (Single Farm) performs all of the algorithm processing for an event sequentially in a single processor in a processor farm, receiving the data via a single network. In addition to these architecture/technology studies some " related " activities (modelling, emulation, communication benchmarks) completed this R&D programme. The results of this programme have been used to select the main options to be retained for the next step. The second phase, the "Pilot Project", as described in this document will study the main critical issues for Level-2. The aims are: to refine architectural ideas and trigger selection strategies; to optimise critical components; and to study system integration issues. In this way to determine the information required for the Technical Proposal due in December 99. The third and final phase before the construction, "Prototype & Design" (2000-2001), will be when the specification and technical choices will be made prior to submitting the Technical Design Report (TDR)
Figure 1: Timetable and milestones

## 3 Conclusions of the "Demonstrator Programme"

### Network Technologies

Perhaps the most important conclusions from the demonstrator programme are in the area of network technologies. A better general understanding of switching technologies was obtained, especially from the measurements in the large scale MACRAME fabric. Measurements here have demonstrated that Clos networks are good at handling random traffic and that this applies also to the traffic patterns expected for the Level-2 system. However even in this configuration the links in a large network typically achieved only ~60% of their nominal performance with these traffic patterns, but will the final figure be better or worse than this? In addition if there are particularly active ports in the network head-of-line blocking can cause the system to become unstable even though most links are well below this loading. The paper models indicated that the total network capacity required will be a few Gbyte/s, but that if the processors to be used have a speed of ~500 MIPS individual links only need to handle 10-20 Mbyte/s. With current trends in network technologies there is now increased confidence that commercial networks will offer these levels of performance in the timescale. Within the programme important measurements were also obtained with two candidate technologies. Firstly it was demonstrated that ATM networking was able to support the performance required in a small system if hard sequential selection was used, however, this required the use of custom software drivers. Secondly many encouraging results were obtained with SCI, which exhibits very high performance, but although this technology is gaining an increased acceptance it is still not fully mature and it is not clear that all the components required to build a large system will be generally available.

### Processor Technologies

#### RISC/CISC Processors

General purpose RISC/CISC processors are the easiest to use and the most flexible for algorithms. Dual processor boards which are becoming increasingly common offer interesting possibilities, particularly to use one as an I/O processor. At the start of the demonstrator programme, there was a tendency to assume that many of the processors would use a VME form factor, however it soon became clear that for most of the processors a PC form factor was generally preferable (better price and performance) and the extra facilities offered by VME modules were not required. This conclusion remains true for the next phase and will need to be reviewed again for the final system. The continuing increase in processor speeds would now suggest that by 2003 processors with 2 GIPS performance will be appropriate - however this would also imply increasing network link bandwidths to ~50 Mbyte/s. Within the computing industry there are various developments of HPCN (High Performance Computing and Networking) systems, which provide distributed computing across large systems of processors. Initial measurements on these system seem to be encouraging and if current industry trends continue, HPCN systems could offer a very attractive solution, but they would have major influence on event building and event filter, and our concept of a ROB.

#### 3.2.2 DSPs and I/O Co-processors

The paper models had generally assumed an I/O processing overhead of 50 \(\upmu\)s, measurements in the demonstrators indicated that with current processors the overhead was much lower, perhaps as little as 10 \(\upmu\)s. Previously there had been interest in using Digital Signal Processors (DSPs) in various roles including for feature extraction. However, it was concluded that with the relative developments in DSPs and other processors, the niche for DSPs in our application is increasingly limited and the main interest remaining was for I/O processing.

#### 3.2.3 FPGA's

FPGAs have demonstrated a clear advantages for pre-processing and for algorithms like the TRT full scan. In addition the FPGA based ENABLE system demonstrated that it can support parallel feature extraction (for simple algorithms) at high rate. However, this requires RoI data collection which was partially demonstrated, but if it were to be used it would have a major effect on the ROB design. Given the substantial cost of this form of feature extraction (i.e. low-level custom programming, embedding the RoI Collector into the ROBs) it was agreed that this should be limited to as few (sub-)detectors as possible, and only used if other technologies are unable to solve the problem or lead into comparable complications.

### 3.3 Architectural Issues

It is evident that sequential processing steps are required (at the very least for B-physics) so that the architecture must be able to handle this. It is also evident that parallel processing within an event is needed (e.g. Pre-processing, data-collection, TRT full-scan), although if sequential selection is used the case for parallel feature extraction is less clear. It was also noted that whilst the advantages of sequential selection have been well stated, there needs to be a Trigger Performance study as to whether the cuts assumed introduce any bias.

The RoI-Distributor demonstrated within Demo-B illustrated a scalable mechanism for the distribution of RoI information from the supervisor to the ROBs and the allocation of feature extraction processors within the local-global farm. The implementation, however, was limited by its use of VME to communicate to the ROB emulators, but even if an alternative path were used the performance would still be limited by the single PCI in the ROB emulators. It is clear that for the ROB emulators there is no fundamental difference whether the data requests are received via the general data network interface or via a separate path. It was concluded that there was no real advantage gained and significant extra complications of having a separate path for data and control messages and it was therefore agreed that in future we should use the general data network to pass control messages to the ROBs. (This might not apply for requests for data to be supply to ENABLE, although even here it would be acceptable if the network supports a broadcast facility).

Similarly for the ROB there is no distinction between the push and pull architecture, indeed even for a group of ROBs there is effectively no difference (in one case the request comes from the supervisor and in the other it comes from a processor). The fundamental difference between these modes is in the supervisor and the processors and on balance these were simpler to handle in the pull mode, it was therefore agreed to drop investigation of the general push architecture.

Concerns had previously been expressed about the extra load that an operating systemmight have on the processors within the LVL2 system. However, with the light-weight systems used the benefits far out-weighed the very slight loss in performance. It was therefore concluded that we should in general use an operating system - one possible exception being in the ROB. However, it was important that the next phase should include comparative tests of the performance of various micro-kernels and operating systems.

Error rates in the technologies used were low and in the small systems used very simple error handling was adequate. However, the full size systems will need better error handling.

Data Collection from ROBs was identified as a critical problem - especially for the calorimeter. Further study is required for all aspects of the ROB Complex.

Studies of using a sub-farm for the Level-2 tasks indicated that the CPU vs I/O balance did not match those of a probably subfarm. Essentially more CPU intensive tasks would be required to obtain a good match.

A supervisor with a design which allowed for multiple supervisor processors has been demonstrated. Very promising performance had been obtained with a single supervisor processor, but scaling had yet to be demonstrated. The supervisor had been used with both a push and pull architecture and in each case had been used to allocate a single processor from a farm (single farm or global) for each event. The allocation scheme used in both case was a round robin with free-list/tariff scheme to allow multiple events to be outstanding with a given processor. For the next phase it was agreed that the supervisor should pass full control for each event to a single processor, thus providing flexible distributed supervision within the event.

### Final conclusions

The demonstrators had shown that it was not possible in this case to factorise the architecture and the technology, the two being tightly coupled. It was suggested that for the next phase a few technologies should be chosen and then the architecture optimised for those technologies. If it was found that the technology does not match the problem then it may be possible to change the specification of the problem.

All of the demonstrator systems were smaller than originally foreseen (~1/4), however, an impressive set of results have been produced with many groups participating. Most of these results have now been written up as ATLAS Internal Notes.

## 4 Level-2 Primary Critical Issues

This chapter presents the main issues that should be studied in the short term since they will influence the design of the front end electronics, the Read-Out Driver (ROD), and the overall integration of the Trigger/DAQ system.

### Data Collection from ROBs

The Read Out Buffer (ROB) is the functional component common to both Level-2 and the DAQ/EF dataflow. The requirements for the extraction of the RoI data for Level-2 is a critical constraint on the ROB's and may have some important consequences on their design parameters. Details of the mapping of each subdetector into the ROB's and the event data format could also have a major impact on the Level-2 system. It is important that these are chosen to minimise the overheads of extracting, transferring and any necessary reformatting of the restricted data required by Level-2.

#### Fe-ROD-ROB Mapping

The volume of data to be extracted from the ROB's into Level-2 is reduced by the concept of Region of Interests (RoIs). The frequency at which a given ROB has to supply data and the volume of data it has to supply are determined by: the number of ROBs used for each detector; the mapping of the detector into those ROBs; the size of an RoI and the rate of RoI's. In addition the possibility of having partial data access within ROBs may influence the ROD raw data formatting. Matching the standard data input bandwidth (1Gbit/s) into a ROB, with acceptable frequencies and bandwidths out to Level-2 has led to the concept of the \(\approx\) multiROB \(\approx\) described in the section 6.1.1. Table 1 shows how the ROD-ROB mapping has evolved during the last 2 years. However, it should be noted that these numbers are subject to further optimisation (particularly in the LArg and Tile Calo) that may have some influence on the RODs (since there is a one to one mapping between RODs and ROBs).

#### Raw Data Format and Preprocessing.

Another important parameter is the format of the raw data coming from the front end detector electronics. Optimisation of this format at the early stage of the chain can save a lot of bandwidth and minimise the data manipulation in the ROBs or Level-2 system. Some problems could be reduced by preprocessing at the ROD level, but clearly this

\begin{table}
\begin{tabular}{c|c c c} \hline \hline  & _DAQ note 70 (96)_ & _Detector TDR_ & _FE-ROD mapping_ \\ \hline SCT -Pix & 256 & 337 & Layer \\ TRT & 512 & 256 & RoI sector \\ EM calor & 432 & 714 & Tower \\ HAD calor & 48 & 120 & Tower \\ Muon Trigger & 19 & 32 & RoI tower \\ Muon Ch. & 144 & 256 & RoI tower \\ Trigger & - & 16 & \\ Total & 1311 & 1721 & \\ \hline \hline \end{tabular}
\end{table}
Table 1: ROD- ROB count and type of mappingwould influence the design of the RODs. The balance between extra complexity of the RODs and benefits for the ROB data extraction and formatting is only partially understood at this stage and requires further study.

### 4.2 Data Flow optimisation

The dataflow optimisation is a key element of the architectural design. This aspect depends on both trigger process strategies and optimised data/control paths

#### Trigger process strategies

The organisation of the data flow should be adapted to the various strategies from low luminosity, where B-Physics is an important element, to high luminosity where the event pile up increases the complexity and decreases the efficiency of the on-line rejection. The way the various algorithms are organised as well as the strategy to accept or reject events is strongly connected to the physics requirements. Whilst many details of the trigger process can be justified using physics arguments and realistic rate estimates, it is important not to ignore other effects such as unforeseen or underestimated backgrounds, inefficiencies or failures in detector or trigger elements. Flexible organisation of the trigger process, plus redundancy in the system will be important aspects of the solution. Evaluation of the performance of various steering sequences and their consequence for the architecture will help to understand and solve such fundamental issues.

#### Optimal data/control paths

The trigger strategy and the hardware architecture will determine the message passing rate and the resulting bandwidth, which in turn will influence the design of the dataflow. With the many components involved in each trigger decision it will be necessary to optimize fully the interfaces and various software overheads.

### 4.3 Large system aspects

The final system will connect some 1700 ROBs to a few hundred or more processors. The size of this system as well as the evolution of commercial technologies make it impractical to realise a " full scale " prototype. Thus the tests are to be performed on relatively small slices of a complete system and we are obliged to rely heavily on models to scale the behaviour to a full size system. In addition, these initial studies are being made separately from the other parts of the Trigger/DAQ system and the latter stage of the next phase should start to address this integration.

#### Division between Level-2 and EF

The present logical division between Level-2 and EF is the following:

* The Level-2 task is to reduce the rate by a factor \(\sim\)100 using RoI data fragments collected from the ROBs. The global Level-2 processor issues a Level-2 accept or reject to the Supervisor. Because of the rate requirements of the Level-2 system it uses optimised and simple 'on-line' code for the algorithms and possibly reduced calibration and alignment parameters.
* The DAQ/EF collects for each Level-2 accepted event the ROB raw data fragments and assembles the entire event prior to processing a filter algorithm based on off-line code.

This division may change according to the Level-2 and Event Filter final strategies, and depending on progress in the technology available and our understanding it may be possible to integrate them into a single High Level Trigger (HLT) sequence.

#### Can Level-2 and event building for EF use the same network technology?

Level-2 needs a fast network capable of transferring small distributed packets of \(\sim\)1Kbyte at high rate (10 Khz) and with low latency. The full event building for the EF needs a network capable of transferring uniformly from every ROB an \(\sim\)1 Kbyte packet at 1KHz rate with modest latency requirements. Thus the technical requirements for the data collection networks are different, but taking into account the evolution of commercial networking technologies, it is possible that they may be able to use the same technology. Such a decision would have to be taken closer to the construction phase when a full analysis of the advantages and disadvantages, including total system complexity and cost can be properly assessed for the candidate technologies.

### Other issues

As a conclusion of the Demonstrator Programme, baselines were chosen for data and control protocols. The Pilot Project should validate and refine these choices.

#### Event data transfer controlled via Supervisor or processor

The RoI Builder receives information from the Level-1 processors that allow it to build the record of RoI pointers to be used in the Level-2 processing. This record is then passed to the Supervisor which assigns the processor(s) for the event and then distributes the record to the Level-2 system. Two options have been considered for this distribution:

* The Supervisor allocates a free processor and sends the appropriate RoI pointer to each ROB containing RoI data. The selected ROBs extract their data fragments for that event and PUSH them to the selected processor through the collection network.
* The Supervisor allocates a free processor and sends the full RoI record to that processor. The processor then controls the data flow requesting the data fragments directly from the ROBs (often referred to as a PULL, but more strictly a REQUESTED PUSH).

The second option can be considered as distributing part of the Supervisor function to a farm of processors. As such it allows a simpler scaling to more complex supervision within an event, and particularly for the case where the processing of events passes through sequential steps - which is foreseen at least for b-physics studies for all architectures.

#### Separate or integrated control network?

Two ways have been considered to send the RoI record from the Supervisor to either the ROBs or the processors:

* A specific external link that connects every ROB or Processor to the Supervisor System. This scheme separates clearly the control path from the data collection path.
* Integrating the control traffic and data collection traffic into a single network.

At first sight the former scheme appears advantageous, it separates the control and data traffic with their different properties and allows them to be separately optimised. This is particularly important if the control traffic passes straight from the Supervisor when the frequency of the control messages in some parts of the network can be very high, leading to problems in many general networks. However, experience in the Demonstrator Programme showed that this view could no longer be sustained. Firstly more control traffic could be supported in the commercial networks than previously assumed - it is also generally flowing in the opposite direction to the data flow. Secondly even where a separate route was used to access the emulated ROBs for control in and data out there was still contention internally in the ROBs on the internal PCI bus. Thus there appeared to be little benefit of a separate path and there was considerable extra complications.

#### Processor allocation strategies

There are a number of ways to allocate processors within the system. For the selection of a processor in the'single farm' or a 'global farm' the simplest strategies are to allocate by:

* a << round robin >> mode
* a random mode
* a free list

For reasons of efficiency it is preferable to allow several events to be handled concurrently in a given processor. To allow for this one can add the idea of a counter for each processor so that an event is only assigned to a processor provided that a maximum number of events is not already out-standing in that processor and the overheads are not becoming too prohibitive. This also has the advantage that it also handles over-loaded or failed processors. (This was the basis used to assign the Global processors in Demo B and the Single Farm processors in Demo C.) Another idea is to split the processors into sub-farms which manage themselves internally and the Supervisor merely assigns an event to a sub-farm.

When the processing in an event is distributed amongst several processors, (whether it be for parallel processing for feature extraction, data fragment collection, preprocessing etc.) the allocation of the extra processors can be done in similar ways or they can be assigned geographically - e.g. according to their proximity to the ROBs (or main processor) or according to the RoI co-ordinates. Assigning the processors geographically is simple to implement, and indeed was the scheme used for additional processors in the Demonstrator Programme for both Demo B (FeX's) and Demo C (RSI's) - however in its simplest form it does raise the questions of robustness and possible load sharing.

### Conclusions

Taking into account the conclusions of the Demonstrator Programme and the analysis of the issues to be addressed, the following list of topics should be addressed in the next phase of the work:

* Quantify the advantages of different Level-2 trigger strategies and architectural implementations.
* Evaluate alternative approaches to the ROB complex.
* Demonstrate an RoI builder and the scalability of the supervisor.
* Draw conclusions about where FPGA processing is relevant
* Investigate and quantify the problems of building very large networks and of interfacing efficiently to them
* Design, test and optimise the Level-2 trigger "evaluation" software.
* Model full (alternative) architectures, including trigger strategies and network technology choices.

* Make revised cost estimates for the Level-2 system.
* Investigate integration issues with other parts of ATLAS, in particular the Level-1 trigger and the DAQ/event filter (including networks and other components, and the division of event selection between Level-2 and the event filter).
* Carry out a revised cost estimate based on a revisited model.

## 5 Activity matrix - General overview

The main objective is to develop the themes progressed in the Demonstrator Programme into a more fully integrated scheme mapping the various activities of the Level-2 community and the technical issues involved. Figure 2 shows the organogram of the different activities.

It is divided logically into 3 main complementary areas:

* The << Functional Components >> study and optimise in a'stand-alone' mode each functional element of the system : the ROB complex; the Supervisor; processors, co-processors and networks plus watches general technology developments.
* The << Software Testbeds >> develop and evaluate all the software aspects using small functional 'full-slice' modular test systems. The << reference >> software is for the development of the software and uses a small system based entirely on commodity hardware. The << Application Testbeds >> are used to evaluate various selection strategies and operational control modes as well as measuring the performance requirements of the full system. These application tests are run on somewhat larger systems also based largely on commodity hardware. It is also foreseen that some tests will be run integrating into the testbeds items from the functional components.
* The << System Integration >> uses a top-down approach to the system and combines the information from the other two areas to form a picture of the complete system. Modelling tools and techniques are used to simulate and scale the results from the small testbeds to full systems. Overall system designs are checked against the URD and cost estimates made.

Figure 2: Activity matrix

## 6 Functional components

This chapter describes the activities studying the hardware components and their optimisation.

### ROB Complex

This activity studies the ROB complex from a LVL2 perspective. The work divides into four areas, studies of the MultiROB concept, the ROBin, preprocessing and the ROB URD.

#### 6.1.1 MultiROB concept

The multiROB aims to match the input data rate from a standard Read-Out-Driver link to an optimum data request frequency and data size for the Level-2 system. It contains the following functional components:

* The input Buffer (called a ROBin) which receives data from a single Read-Out-Driver (ROD). The local buffer manager task of the ROBin is performed by a microcontroller which is either on the ROBin or shared between several ROBin's
* A bus or local network, used to collect RoI data for Level-2 from several ROBin's
* A Control Processor
* An output interface to the Level-2 (local or global) network
* An optional Co-Processor performing local processing or preprocessing

#### 6.1.2 ROBin developments

Because of the importance of the ROBins a number of implementation options are being studied within this co-ordinated activity. These option build on developments which have been proceeding for a considerable time in several institutes.

* The ROBin being developed originally for DAQ/EF-1 has an S-link data input feeding memories controlled by an i960 microcontroller. The data is read out via a PCI bus. The ROBin may exist in PCI or PMC form factors. This development is being done as part of the DAQ/EF-1 project and will be used by that project for studies of the readout crate.
* The other ROBin being developed also uses an i960 for the buffer management and a PCI bus for the readout, but here the use of memory based on SDRAM is being investigated. In this case the initial aim is to study the internal memory and the buffer management and there is no input link (at least in the first phase). The first implementation uses a PMC format, in later phases it is planned to use compact PCI, with a compact PCI crate to collect data from a number of such ROBins.
* The third development is a SHARC-based card. This design study aims to investigate: moving as many functions as possible from hardware to software; a low-power alternative technology; using a ring buffer for the main memory, rather than page-managed memory; using SHARC links to provide a MultiROB capability. The readout of the card would go via SHARC links to a SHARC which outputs the data via PCI In addition solutions using SCI technology components in combination with direct interfacing to SCI and the exclusive use of FPGAs in combination with commercial general-purpose systems are being investigated.

#### Preprocessing

An integral part of the ROB Complex is the processor used to prepare data from one or more ROBin's prior to the data being sent out on the general network. At the present time the main candidate for the preprocessing processor is the micro-ENABLE. In the case of a Multi-ROB it is vital that this be studied as an integral part of the ROB complex since several ROBin's could supply data to a single preprocessor. However even in the case of a preprocessor only handling data from a single ROBin there is still a tight coupling within the ROB complex because of the shared use of the ROB PCI bus.

#### Input to the ROB URD

An initial User Requirements Document has been written for the ROB's and this group has the responsibility to ensure that the requirements of the Level-2 system are included in this Document.

### Supervisor

The Supervisor complex receives data from Level-1 with information about the RoIs found, reformats this data into a form usable by Level-2, assigns a main processor for each event and passes control for the event to that processor. After the Level-2 system has finished processing each event it receives the decision for the event and after taking into account issues such as pre-scaling and the current mode of operation (e.g. Level-2 selection or tagging) it makes the final Level-2 decision. These decisions are then grouped prior to broadcasting to all of the ROBs. The grouping of decisions reduces the frequency with which the ROBs have to handle decision records, since there is a significant I/O overhead associated with handling each record, independent of the record length. This mechanism reduces significantly the load on the ROBs.

The Supervisor complex consists of two main parts:

* which receives the data from Level-1 and reformats it for Level-2
* which contains a farm of processors to handle the allocation of events to the Level-2 processors and the processing of the decision records back from the Level-2 system.

The basic principles of the Supervisor were demonstrated as part of the Demonstrator Programme where a Supervisor with a single processor to handle the event allocation was run at event frequencies of up to 27 kHz. However although scalability was built into the basic design it has still to be fully demonstrated.

Detailed work on the RoI Builder has started only recently. With the submission of the Level-1 technical design report the interface between Level-1 and Level-2 is now better defined and it is timely to start working on the detailed design of a prototype of this component and to demonstrate a solution compatible with the extreme demands of both the Level-1 system and the existing Supervisor design.

### Processors, Interfaces and Networks

This activity covers a wide area of technologies, but the inter-relationships make any split artificial. The main headings can be summarised as:

* here we consider especially the FPGA work of ENABLE++ foreseen for such tasks as the TRT full-scan. More general FPGA solutionscould be included in this study.
- High Performance Computer Networks are currently receiving much emphasis within the computing industry. If as some people predict these machines become more affordable they offer some exciting possibilities for the Level-2 problem. In particular they promise an appropriate mix of the CPU power and the inter-processor communication. In the first instance the activity would be limited to small scale studies, in conjunction with the manufacturers of such systems, to see if the systems are as good as the promise and if so how the raw data can be fed into the memories of the system.
* Networks. Networking technologies are currently developing very fast. A number of technologies, listed below, are being investigated for the Level-2 networks. Others such as Myrinet and Raceway will not be actively pursued, but will be included in the technology watch. In addition some HS-link components already exist in CERN and these might be used internally as part of a switch for other technologies. Fast Ethernet is an established and very widely available commodity technology. Over the last year the status of the Gigabit Ethernet standard has evolved rapidly, with ever increasing numbers of products becoming available. The Ethernet set of standards is now well placed to offer the building blocks necessary to construct networks for the ATLAS second level trigger. Given the very large number of sites which are likely to upgrade campus networks largely based on Ethernet over the next few years, this technology promises to give very good price/performance ratios. A key area to be evaluated, however, is the performance of large switching fabrics. The performance of switches under heavy loads is already seen to be very dependent on internal details which vary greatly from manufacturer to manufacturer. It will be necessary to understand the critical factors and to evaluate whether the solutions chosen by manufacturers yield the performance in our particular problem. SCI has over the last year or so become more established, especially inside high-end server machines. However the availability of switches is currently very limited, with only very small switches available, as yet commercially unproven. However this technology gave the best communication results within the demonstrator programme and so this technology, and particularly switches, justifies further study. ATM is an established technology for wide area networks (WAN) and more generally in the telecommunications industry. However its promised take-up for LANs has been much slower to develop. Over the last year a new generation of modular ATM switches (with typically 16-64 ports) has led to an expansion in this area and an increasing number of sites are using ATM for backbones. However, with the rapidly changing availability of Giga-Ethernet equipment it is difficult to predict how ATM in the LAN market will continue to evolve
* we have listed above many technology developments which could impact the ATLAS Level-2 problem. There are clearly far too many developments to study them all, however, it would seem prudent to have as a small activity a group keeping an eye on new developments across the technologies listed (and others) so that if something especially relevant emerges we are aware of it as early as possible.

## 7 Software Testbeds

The main objective of this integrated activity is to complete and complement items from the Demonstrator Programme, e.g. the full Level-2 trigger software chain from extracting the RoI data to the Level-2 Accept/Reject signal. This software includes four main topics:

* Production of realistic raw data samples from physics simulations
* Optimisation of appropriate algorithms in on-line form
* Selection strategies and steering sequences
* A common software framework including the data collection and request/response protocol

The common framework, together with the steering sequences and online algorithms combine together to form the'reference software'.

To achieve the goal of this activity, it is foreseen to assemble a set of full-slice testbeds to develop and run the reference software. The testbeds will take the form of a reference hardware configuration and larger application testbeds.

### Reference Testbed

The reference testbed hardware configuration (see figure 3) will allow the development of the reference software. This includes the message passing protocols and the infrastructure to run the Level-2 applications. In addition it should provide an environment to optimize the code of the algorithms and for their benchmarking. Features of the testbed are that:

* All <<data sources>> connect to all processors
* A single physical network is used for data collection and control
* Processor elements can be used for local or global processing

#### 7.1.1 Training samples

Data samples are needed for algorithm benchmarking, data preprocessing and to run the full trigger strategies on the application testbeds. The production of data samples will be accomplished off-line using a special version of the trigger simulation program, ATRIG. The input is simulated detector data from DICE which are written onto data files in a pseudo-raw data format, together with Level-1 and Level-2 trigger results. Starting from these files, which contain selected physics channels or background data, new files will be

Figure 3: Reference testbed configurationproduced with a realistic raw data format as soon as this becomes available for each sub-detector. These files will be particularly useful in the evaluation of data preprocessing.

The "training samples" will consist of a set of data files containing a collection of different event types (signal and noise), which will be used to study trigger strategies and processing sequences.

#### 7.1.2 Algorithm optimization and benchmarking

The algorithm optimization and benchmarking will need the selection of at least one platform to be used as reference system. In this way a library of standalone versions of the trigger code running on the same type of processors (e.g. the testbed farms) will be available and also provide a coherent set of processing times to be used for modelling.

The online versions of the algorithms should be coded in C or C++. The library should contain all the feature extraction code for muons, electron-gammas, hadrons, jets and missing ET, including combined reconstruction of trigger objects (e.g. muon isolation, matching of calorimeter and inner detector, etc.) and specialized algorithms (e.g. B-vertex tag). The steering sequence of the Level-2 trigger (menu driven) should call this library to identify trigger objects and take the global decision on the event.

#### 7.1.3 On-line Framework

The Reference Software will be constructed in an on-line framework which provides the following services:

* Run control, monitoring and error handling
* Configuration (from text file to database?)
* Request/Response protocol (Event flow control and message passing)
* Technology and OS specific libraries

#### 7.1.4 Software overview

The aim of the "Reference Software" group is the development of the full Level-2 Trigger Process which will run on the application testbeds.

It is proposed to prepare a flexible, well engineered technology independent software whose development will be based on the following assumptions:

* it will re-use and adapt previous work where appropriate;
* it will have a modular structure to map easily across nodes and networks;
* it will not be the final software.

The design of this software (see figure 4) should be as flexible as possible, supporting "generic" and "specific" implementations, and except for the lowest layers be independent of the Operating System and hardware platform. In particular the code should be portable between WNT, LynxOS, Linux and applicable to ATM and Fast-Ethernet technologies in a transparent way. The use of suitably layered software with appropriate API's should match this goal.

The effort in the development of software modules (such as the Run Control) which will be provided in future by the DAQ group will be kept to a minimum. These modules will only have the minimal functionality required to run the entire Level-2 software process. In contrast the development of "core software" will concentrate on specific Level-2 issues and optimization of message passing protocols which match the Level-2 requirements.

### Application testbeds

The full slice application testbeds will embed the Level-2 Trigger Process as developed in the reference software on relatively larger and better optimized systems (see figure 5) to address the following issues:

* the study of full algorithms, steering sequences and global menus (menus based on trigger RoIs only, full menus with secondary RoIs, B-physics menus);
* the evaluation of protocols and measurements of the hardware performance required for a complete system (latency, link occupancy and software overheads);
* the integrated tests of optimized components (e.g. ROB prototypes, data preprocessing units, etc.) and better commercial switching networks once they become available For the first two issues the hardware platforms which have been selected use: ATM and Fast-Ethernet for the network technology; Pentium processors for the processing farms and for ROB and Supervisor emulation; optionally PowerPC based cards for ROB emulation and Supervisor prototypes. The software should allow the use of WNT and Linux in PCs and LynxOS in the VME cards.

Figure 4: Software overview

#### 7.2.1 atm

The proof of principle to use Asynchronous Transfer Mode (ATM) technology for the central network was demonstrated with a limited number of 155 Mbit/sec nodes (10) and a simple message exchange scheme. The ATM testbed will be used to study the options for the trigger process as detailed above, but it will also allow further evaluation of the technical details and performance of this technology with a larger number of nodes. The idea is to assemble several (3 to 4) standard commercial 16 nodes sub-systems in order then to reconfigure the components into a larger one (48 to 64 max). This will allow us to gain experience with multiswitch and multipath network by building in a first step a two stage 16 port interconnect and then a Clos multipath network. The final 48-64 port full slice system would also be used to evaluate the performance of the trigger process itself on a moderately large system. An important element of the programme will be measurements of the various parameters (overheads, latencies and occupancies) throughout the system as input to the modelling - see below.

#### 7.2.2 Fast-Ethernet

Ethernet is a popular and widely used networking technology. The emergence of faster 100 Mbit/s - 1 Gbit/s devices (interfaces and switches) now makes this technology very attractive and very cost-effective. However, a demonstration is needed in a large full-slice system that the performance and technical features are adequate to solve our specific problems, in particular the efficient transmission of small messages. Issues of scalability and performance for larger switching fabrics also need to be addressed. In addition to studying the basic technology (interface cards and custom drivers) within the 'technology activity', it is planned to use a large system of 32 to 64 nodes to study the options for the trigger process and to evaluate and measure the various critical parameters using the same criteria and trigger models as in the ATM testbed.

Figure 5: Application testbed configuration

## 8 System Design

This area groups together the information from the component studies, the software testbeds, and the requirements and constraints from other parts of ATLAS to obtain a view of the complete Level-2 trigger system. In particular it has to understand the consequences of a given option or implementation including integration aspects with the rest of the detector and T/DAQ.

### Modelling

Whilst the application testbeds allow the study of moderately sized systems with large traffic loads, they do not give the complete picture of how the final full size system will work under the full load. To obtain information on how the performance would continue to scale to the final system it is essential to use modelling techniques. The main aspects of this activity are: to optimise various ROB and network options; to study congestion and contention within the system.

#### Paper model

The paper model is a very simple calculation used for initial optimization and to estimate the network size and the computing needs of a given option for the trigger process. It uses a simplified model of the process under study and a complete set of average parameters: trigger rate, data volumes, transfer rates, execution times and software overheads. No account is taken of congestion or queuing or variations in processing times or data volumes, and thus the results give a lower bound to the size and performance of the system required. (Note in reality the Paper Models use large spreadsheet calculations, but in essence they are simple calculations which could be done by hand.) However, as was shown during the Demonstrator Programme this technique allows a relatively rapid study of many options and to obtain the initial optimisation worthy of further study by more detailed models. In particular it is now necessary to update this work with revised parameters.

#### Computer model

This is a complementary activity to the paper models using more refined and complex 'Discrete Event Simulation' programs (SIMDAQ written in C++, Ptolemy code). The average parameters are replaced by distributions or dynamic values and full account is taken of queuing and congestion. Detailed models of each individual component of the Level-2 system (ROB complex, Supervisor, Networks, Interfaces...) should help to optimize and understand the critical features of these devices. A generic model of the various application testbeds will allow the model to be normalised and to check the validity of the model. The generic model would then be used to scale the performance to a full-system. Finally adding technology dependent elements into the model will allow a complete cross-check with the testbeds and will give more confidence for the scaling to a full-system with those technologies.

The current work plan is to complete additions to SIMDAQ in order that it can simulate all architectures currently being considered, and also include sequential trigger strategies. Then comparative simulations of these architectures under different conditions will be made to provide information for decisions to be taken later on. A next step is generic simulation (with relatively simple models) of laboratory set-ups and identification of main factors determining behaviour. This is to be followed by detailed simulation of technologies calibrated using results of laboratory measurements andreusing code/models developed earlier for ATM, SCI and DS-link technologies.

### Integration

This activity uses the information from all the other Level-2 activities, plus information from other systems to produce a top-down view of the complete Level-2 trigger system, including models of the cost.

#### User Requirements Document

The User Requirements Document (URD) cannot yet be considered to be a frozen document, parameters of other parts of ATLAS continue to change and the understanding of the Physics needs continues to evolve. This task is to determine, with the appropriate parties within ATLAS, the changes required in the URD and to incorporate these changes in new versions.

#### System Requirements Document

Over the duration of the Pilot Project the URD should become sufficiently stable and our understanding of the possible implementation of the Level-2 trigger system, including the interfaces to other systems, sufficiently advanced that we can proceed to the next phase of the design process with the production of a System Requirements Document (SRD).

#### Cost Overview

Among the parameters to be taken into account in choosing the final Level-2 system will be the relative costs of different options. We aim for the Technical Proposal to have information on the likely cost of each component, and to combine these costs and the best estimates of the size of system required to obtain a revised costing model for the complete system.

#### Integration with other systems

The proper design, and later integration, of the Level-2 system requires a detailed knowledge of the requirements and constraints from other parts of ATLAS. In addition the Level-2 system will impose some requirements and constraints on other parts of ATLAS. This task is to collect and document these requirements and constraints, and where necessary to ensure that changes are made in an optimal way. It is assumed that much of this work will take place as part of the activity of the Detector Interface Group. Particularly important examples are: the mapping of detector elements into ROBs; the format of the event data in ROBs.

The Pilot Project will also include work on the interfaces between Level-2 and the other parts of T/DAQ. In the case of Level-1, the Level-1 trigger sends RoI information to Level-2 via the RoI-builder in the Level-2 supervisor. In addition the Level-2 supervisor must be able to throttle the Level-1 trigger, in case of congestion in the supervisor or other parts of Level-2. Both of these interfaces will be studied. For DAQ/EF the main interfaces are with the data-flow and with the back-end DAQ. The former has a very close coupling around the ROBs, the later provides configuration services for Level-2, but also acts as the master of the Level-2 system during data taking and calibration.

## 9 Organisation

The main objectives of the Pilot project are to integrate all of the activities connected to the Level-2 trigger into a single programme and to produce results required for the Technical Proposal.

### Technical organisation

The Level-2 community is widely distributed around the world and it is sometimes difficult, especially for software tasks, to get a coherent strong team. In order to solve this issue, a core of seven structured specific activities has been selected, mapping the overall system. In order to avoid too heavy a bureaucracy and keep some flexibility at this stage of the project, the local management of each activity is delegated to one or two persons (convenors) active in this field.

The main tasks of each convenor are to:

* Call "working" meetings to organise the work
* Organise Web pages and collect/make available documentation and information.
* Report to the community and the Level-2 Coordinators.

The role of the coordinators being to check that the overall programme is coherent and that each activity achieves the milestones associated with their individual workplan.

Figure 6 gives an overview of the activities with the convenors and co-ordinators responsible for each area of the work.

Figure 6: Co-ordinators, Convenors responsible for each activity and Institutes involved

### 9.2 Organisation of meetings

Given the wide-spread community, it is proposed to be very flexible in the organisation of meetings. Four kinds of meeting are foreseen:

* "Local working" meetings between individuals, groups or labs.
* Specialised meetings called by the convenor(s) of one or more activity. (e.g. NIKHEF-January 98 for ROB and Modelling, Paris-April 98 for Reference Software)
* ATLAS-Level-2 plenary meeting during each ATLAS week with the agenda prepared by the coordinators
* Workshops or special meetings for the overall T/DAQ community.

The use of telephone and/or video conference facilities is encouraged, especially for the more frequent smaller meetings.

### 9.3 Integrated workplan up to the technical proposal

Workplans for each of the activities are described in a separate Appendix (contact FJW, SF or PLD for details). Figure 7 summarises the major phases and milestones of the project. There are five main phases, each one finishing with a series of milestones to be reviewed in a specific meeting or workshop.

#### Phase 1 from March 98 to June 98

This corresponds to the definition of the workplans for each activity. This phase ends by the June 98 ATLAS Level-2 plenary meeting, where the workplans and milestones were definitively approved. A review of the standard parameters values, required for the modelling activity, commenced in this period.

#### Phase 2 from June 98 to October 98

Figure 7: Integrated workplan and milestonesThis corresponds to the development phase of each component (Reference Software, RoI-builder) for the Level-2 testbeds. A review of the work will be done during the October T/DAQ workshop in Chamonix. In addition at this workshop we should confirm the decisions taken at the end of the Demonstrator Programme and consider if they are still valid and review ROB Complex studies.

#### Phase 3 from October 98 to February 99

Start the local integration of the testbeds. At the end of this period, corresponding to the'mid-term' of the project, the detailed plans for measurement and validation of the ATM and Ethernet testbeds will be finalised. In addition, it is planned to hold an internal review of the various networking technology developments.

#### Phase 4 from March 99 to June 99

This corresponds to the global integration of all elements in a large application testbed at CERN. It is during this phase that the various performances of the full slice system can be measured. A first set of results should be reviewed during the June ATLAS week.

#### Phase 5 from June 99 to October 99

This is the continuation of the measurements on the full scale application testbed(s). This is the continuation of the measurements on the full-scale application testbed(s). It is also during this time that we can discuss the next phase of integration towards the ATLAS high-level trigger, DAQ and DCS architecture. Towards the end of this phase it is foreseen to hold a trigger/ DAQ workshop, to review all the material necessary (from Level-2, the URD, SRD, SDD and ROB URD - see above) for the technical proposal.

## 9.4 Detailed workplans

### Functional component workplan

The functional component activity (Figure 8) is structured into three main areas, the ROB Complex, Supervisor and Technology studies (Processors, interfaces and networks).

#### 9.4.1.1 ROB complex

The tasks and milestones in this area are:

\(\bullet\) Complete development of functional prototypes of buffers (ROBin) to be used as elements of the ROB complex and integrated into the application testbeds (Q4/98). (For these studies one of the options is the same intelligent PMC to be used in the studies of the ROB input channel in for the DAQ-1 prototype).

\(\bullet\) Establish the Level-2 hardware and software requirements for the ROB complex. (Q3/99)

\(\bullet\) Study different design options and implementation scenarios with respect to possible architectures. (Q3/99).

#### 9.4.1.2 RoI builder and multisupervisor

The milestone established in this area is:

\(\bullet\) Demonstrate CPU scaling of supervisor. (Q4/98)Figure 8: Functional components workplan and milestones

#### 9.4.1.3 Technologies for processors, interfaces and networks

The programme in this area is to evaluate a small number of promising technologies, track industry trends, and develop systems which match ATLAS requirements. The tasks and milestones are:

\(\bullet\) Model HPCN traffic patterns. (Q4/98)

\(\bullet\) Implementation and test of HPCN system. (Q4/98)

\(\bullet\) Establish baseline performance measurement of commodity, off-the-shelf, Ethernet hardware and software, and identify shortcomings with respect to Level-2 needs. (Q4/98)

\(\bullet\) Measure and understand I/O performance of SCI switches. (Q4/98)

\(\bullet\) Evaluate 622 Mbps ATM interface. (Q1/99)

\(\bullet\) Gain experience with multiswitch and multipath ATM networks. (Q1/99)

\(\bullet\) Design and implement dedicated optimised drivers under Linux. (Q1/99)

\(\bullet\) Gain early experience with scalable Gigabit Ethernet switch. (Q1/99)

\(\bullet\) Build a prototype of Ethernet switch tester. (Q2/99)

\(\bullet\) Evaluate performance and limit of FPGA used as co-processor (TRT Full scan). (Q2/99)

\(\bullet\) Study general application of FPGA in Level-2. (Q3/99)

\(\bullet\) Produce a design study for 100 Mbps and Gbps Ethernet Network Intelligent Interface. (Q3/99)

\(\bullet\) Report on industry trends and developments in processor and networking technologies. (Q3/99)

**9.4.2 Testbed workplan**

The testbed activity (Figure 9) is structured into three main areas: the two parts of the reference software and the application testbeds.

**9.4.2.1 Reference software algorithms**

The reference software algorithms work plan is divided into the following tasks:

\(\bullet\) Definition of representative menus and training samples: select a set of physics reactions (signal and background) which will be sufficient, over a time span of the pilot project, to optimise trigger strategies and algorithms. (Q3/98)

\(\bullet\) Establishment of training samples: provide a choice of input data for studies of trigger strategies and for testbed algorithms.(Q4/98)

\(\bullet\) Preparation of tools for analysing training sample, in C-language: provide algorithms, later to become part of the testbed reference software. These algorithms take training samples as input, and serve as tools for trigger studies and algorithm optimisation. (Q1/99)

\(\bullet\) Provision of general algorithm steering in object-oriented environment: develop a more general steering program for decision making, using available feature extraction algorithms; calls to the feature extraction algorithms are according to rules written down in tables (menus, algorithms, trigger objects). Document the available routines for other users. (Q2/99).

\begin{tabular}{|l|l|l|l|l|} \hline
**Activity** & & & \multicolumn{2}{c|}{**Milestones**} \\ \hline
**Description of the task** & **Type** & **Deliverable** & **First** & **Last** \\ \hline
**Algorithms** & & & & \\ \hline Define representative menus & doc & note & Jun 98 & Oct 98 \\ \hline Establish training samples & s/w & data files & & Nov 98 \\ \hline FEX development and benchmarks & s/w & files & Jul 98 & Feb 98 \\ \hline Steering sequences & & files & Dec98 & May 99 \\ \hline
**Reference software framework** & & & & \\ \hline Design & doc & note & Jun 98 \\ \hline Implementation & s/w & & & Dec 98 \\ \hline Operation & s/w & report & Jan 99 & Jul 99 \\ \hline
**ATM test bed** & & & & \\ \hline Algorithms and reference software & s/w integration & Jun 98 & Feb 99 \\ \hline RoIB-multi-processor supervisor & h/w, s/w & & Dec 98 \\ integration & & & \\ \hline Enable\(++\) full scan TRT integration & h/w, s/w & & Nov 98 \\ \hline ROBin and ROB complex integration & h/w, s/w & & Oct 98 & Dec 98 \\ \hline Performance 32/64 port network & measurement & report & Jan 99 & Jun 99 \\ \hline \end{tabular}

#### 9.4.2.2 Reference software framework

The reference software framework work plan covers the following areas:

* Framework software design. The framework allows implementation as a distributed system and defines a protocol and messages for control and exchange of data between the system components. It includes an interface to algorithms (global, feature extraction, preprocessing) which may be linked in and executed on-line. An API for message passing will be provided but only tested with TCP/IP; optimisations for particular technologies are outside the scope. Likewise, ROBs and supervisor will be emulated but integration of custom hardware is outside the scope. The result will be a design based on analysis of the requirements and pre-implementation studies. OO design following good software engineering standards has been adopted. (Q2/98)
* Framework software implementation. Implementation of the software as described under design for PCs running Windows NT or Linux and TCP/IP for communication. Supervisor and ROBs will be emulated on PCs. Algorithms will be integrated and run on simulated data that may be preloaded in emulated ROBs. (Q4/98)
* Framework software operation. Continuous monitoring of the quality and performance of the common framework software; new releases when appropriate. (Q3/99)

#### 9.4.2.3 Application testbeds

The testbeds will be used to integrate and test the reference software and, also, to integrate and test special components. The tasks and milestones are given below.

* Integrate ROBin modules in ATM testbed. Study data transfer, partial readout, and preprocessing. Demonstrate feasibility of grouping several data sources on a single network port. (Q4/98)
* Integrate RoI builder and multi-supervisor. (Q4/98)
* Evaluate performance, limits, savings and cost of hybrid full-scan TRT. Integrate and optimise track-finding on Enable\(++\) with global farm. Compare to reference algorithm. (Q4/98).
* Implement trigger algorithms. Read raw data from training samples into testbed ROBs. Perform sequential Level-2 algorithms with multiple data requests. Install full trigger menus and global selection algorithms. (Q1/99)
* Set up a system of PCs running Linux (8 initially) connected together by a Fast Ethernet switch. Port the Reference Software to the system. Implement a range of process models on the commodity testbed. (Q1/99)

Figure 9: Testbeds workplan and milestones* Establish a 32 to 64-port testbed for system performance studies with commercial components. Measure full system performance on a large testbed, including preparation and transfer of raw data and execution of trigger algorithms based on flexible trigger menus. (Q3/99)
* Measure performance parameters of testbed(s) running agreed menus and strategies. (Q3/99)
* Provide a platform for the integration and test of optimised components. (Q3/99)

#### System design workplan

The system design activity (Figure 10) is structured into two main areas: modelling and global integration.

#### 9.4.3.1 Modelling

The work programmes in computer modelling and system emulation are to:

* Measure performance for simulated ATLAS Level-2 traffic on a large system using state-of-the-art multiprocessor servers and workstations interconnected by a commercial ATM backbone network. (Q3/98)
* Review parameters and update the paper model. (Q3/98)
* Measure the response of the Macrame and ARCHES switching testbeds to ATLAS trigger traffic. (Q1/99)

Figure 10: System Design workplan and milestones

* Implement and study the behaviour of the full but generic system model. Study different options with respect to possible designs of the Level-2 system. (Q3/99)
* Implement and study the behaviour of generic models (i.e. without detailed models of different technologies used) of testbeds systems. (Q3/99)
* Implement and study the behaviour of full models (i.e. with detailed models of different technologies used) of testbeds and options for complete systems. (Q3/99)

#### Integration

The following documents are needed as input for the technical proposal. The milestone for their completion has been set as Q3/99.

* The user requirements document (URD) is our principal benchmark for the specification of the function of the Level-2 trigger. The present version will need to be updated if this valuable tool is to remain of any use for the next phase.

\(\bullet\) A system requirements document (SRD).

\(\bullet\) A system design document (SDD) with an outline Level-2 design and costing.

During the Pilot Project work will be done on the interfaces between Level-2 and the other parts of T/DAQ with the following timescales:

\(\bullet\) Demonstrate event routing of LVL1 information to the LVL2 RoI-builder. (Q4/98)

\(\bullet\) Propose a scheme for informing LVL1 of a potential overload in LVL2. (Q4/99)

\(\bullet\) LVL2 - dataflow integration (Q4/99)

\(\bullet\) Definition of all functions which the LVL2 and back-end DAQ sub-systems will request of each other (Q3/99)

\(\bullet\) Definition and use by Level-2 of back-end DAQ configuration databases (Q4/99)