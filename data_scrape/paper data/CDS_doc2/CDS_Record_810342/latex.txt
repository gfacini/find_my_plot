# Report of the Online - Tier-0 Taskforce

14 December 2004

T. Akesson

 D. Asbury

 D.G. Charlton

 R.J. Hawkings

 R.W.L. Jones

 G. Mornacchi

 M. Nessi

 D. Quarrie

 V. Vercesi

 F.J. Wickens

1

Footnote 1: Taskforce chair: contact Dave.Charlton@cern.ch

###### Abstract

The taskforce was charged with the development of a strategy for the interface between the surface facilities at the ATLAS experiment (point 1) and the computer centre. This is interpreted to mean both the location of key hardware resources, in cases where there is still uncertainty, and the software issues of connecting the data collected at the pit with the offline world of the Tier-0 facility sited in the computer centre.

The main issues considered by the taskforce were:

* The network links between point 1 and the computer centre
* The siting of the event filter (EF), in its initial and full baseline configuration
* The flow of data, both events and conditions data, from the relevant sources to Tier-0

These are discussed in more detail in the following sections with recommendations where appropriate. In addition, we have considered the possibility of the sharing of resources between EF and Tier-0, and we make further recommendations. For the most part, we have addressed the requirements for stable running of ATLAS: data taking in the early weeks of LHC operation will, in some cases, have rather different requirements. We have not explored those in detail in this taskforce.

Most of these issues have been, or are being, addressed separately elsewhere in ATLAS: for example, the EF is, of course, the responsibility of the TDAQ project; the conceptual model of data flow between the EF and the computer centre is a significant part of the work of the computing model group (CMG); and the operation of the Tier-0 facility is the responsibility of IT division. Expertise in all of these areas is available in members of the taskforce - a large part of our work has been to put this expertise together.

A recurrent theme has been that the information available at present is not adequate to formulate a full policy on the above issues. This arises from a variety of external factors that we cannot control, and will require further work in this area over the remaining years until data-taking begins.

## 1 Introduction

The taskforce was charged with the development of a strategy for the interface between the surface facilities at the ATLAS experiment (point 1) and the computer centre. This is interpreted to mean both the location of key hardware resources, in cases where there is still uncertainty, and the software issues of connecting the data collected at the pit with the offline world of the Tier-0 facility sited in the computer centre.

The main issues considered by the taskforce were:

* The network links between point 1 and the computer centre
* The siting of the event filter (EF), in its initial and full baseline configuration
* The flow of data, both events and conditions data, from the relevant sources to Tier-0

These are discussed in more detail in the following sections with recommendations where appropriate. In addition, we have considered the possibility of the sharing of resources between EF and Tier-0, and we make further recommendations. For the most part, we have addressed the requirements for stable running of ATLAS: data taking in the early weeks of LHC operation will, in some cases, have rather different requirements. We have not explored those in detail in this taskforce.

Most of these issues have been, or are being, addressed separately elsewhere in ATLAS: for example, the EF is, of course, the responsibility of the TDAQ project; the conceptual model of data flow between the EF and the computer centre is a significant part of the work of the computing model group (CMG); and the operation of the Tier-0 facility is the responsibility of IT division. Expertise in all of these areas is available in members of the taskforce - a large part of our work has been to put this expertise together.

A recurrent theme has been that the information available at present is not adequate to formulate a full policy on the above issues. This arises from a variety of external factors that we cannot control, and will require further work in this area over the remaining years until data-taking begins.

## 2 Networking at point 1 and between point 1 and the computer centre

### Network framework at point 1

Networking at point 1 consists of four functionally separate networks: the data flow network, the CERN public network, the experiment control network, and the high performance data links to Tier-0, sited in the CERN computer centre. The data flow and CERN public networks fulfil special functions, the transport of event data, and connectivity for external computers, respectively; their topology and implementation lie outside the scope of this report. The high performance lines to Tier-0 are planned as a set of 10Gb Ethernet links used to transport the data produced by the TDAQ system to the Tier-0 centre. The general ATLAS control network connects all the computing equipment participating in the ATLAS experiment, e.g. ROD crate controllers, TDAQ machines, control room computers etc. It is organized in terms of satellite networks, connecting equipment that participates in fulfilling a particular function (e.g. the central DCS), interconnected by a high performance, redundant core.

The redundant core consists functionally (the detailed implementation will be decided by the technology available at the moment of purchasing the equipment) of two routers connected by 10Gb Ethernet links. The first router will be located underground, in USA15, and will provide the connectivity for the underground satellite networks (e.g. networks associated to sub-detectors) to the core. The second router will be located in a surface building, probably SDX1, and will provide:

* the connectivity of the surface satellite networks (e.g. the event filter) to the core
* the connectivity between the ATLAS control network and the CERN public and technical networks, via the gateways protecting the access to the ATLAS control network.

It can be envisaged that the same surface router can be sized in such a way as also to support the high-speed data lines to Tier-0, which are the main subject for this report. This assumption, that is to say the use of the surface backbone router as the starting point for the high performance lines, is assumed in the following text. While not being essential, this assumption fixes a framework where the networking issues can be discussed.

### Data links requirements

The TDAQ architecture is described in reference [1]. For the purpose of this report and with respect to the transport of event data we consider the following stages:

* Event building: merging all the detector data fragments belonging to the same bunch crossing
* Event filter: reducing the rate and data volume
* Tier-0 where events are stored and analysed.

Two levels of buffering decouple the stages above:

* The sub-farm input (SFI), decoupling the event builder from the event filter. In the final system there will be O(100) SFI buffers, each handling O(10\({}^{2}\)) of the event builder rate, on average.
* The sub-farm output (SFO), decoupling the event filter from Tier-0 (and providing mass storage for e.g. 24 hours of data taking without the links to Tier-0). In the final system there will be O(25) SFOs.

In the following, "data link" is used to indicate a 10Gb Ethernet link between two locations (typically ATLAS point 1 and the CERN computer centre, bldg. 513). Note that to implement a full duplex data link 2 fibres are necessary.

If the full event filter is located at point 1, a pair of data links is enough to fulfil the data transfer requirements of the experiment taking into account a factor of two for contingency. As the event filter, or part of it, may be used at some times as additional Tier-0 processing power (discussed further in section 4.6), a further factor of two is used to calculate the need in data links for point-1 to Tier-0 data transfers. This results in a requirement of 4 live data links.

The reliability of the link between point 1 and bldg. 513 is obtained by means of redundancy in the data links. Reliability with respect to the failure of a fibre is achieved by having spare fibres that may be activated (connected) if and when a live fibre fails; this level of redundancy is achieved by requiring a further factor of 2 in the number of fibres installed (but not connected). Live fault tolerance would be achieved by the duplication of data links (i.e. the contingency factor) - a transient performance degradation would be experienced during the time needed to disconnect a faulty fibre and connect a dormant one instead. A second reliability concern is related to the physical safety of the cable galleries linking point 1 to bldg. 513: for example, a disruption of a gallery due to civil engineering works. In this case reliability may be obtained by routing fibres through different galleries between point 1 and bldg. 513: for example a minimal number of fibres run through an indirect gallery.

In the case that part, or the entire, event filter is located away from point 1, the number of data links needed to fulfil the data transport requirements should be recomputed as a function of the displaced fraction of the event filter. Given that the SFIs are expected to remain at point 1 (there is no reason to the contrary), the data links will need to transport a fraction of the event builder output. A simple model of splitting the event filter is used 

[MISSING_PAGE_FAIL:3]

(40%) EF is located at point 1 and then it is expanded (60%) into a location remote to P1 but managed by IT; and 3) the full EF is located in a location remote from P1 (and managed by IT). In addition, for the two latter cases, the requirements in fibres to link the remote EF location to Tier-0 are also indicated (taking into account contingency and the reverse factor).

### Status of the communication infrastructure between point 1 and building 513

The location of the Tier-0 centre is expected to be the CERN computer centre, building 513. The same building is also the location where the links to the outside of CERN start. The infrastructure for fibres running between point 1 and bldg 513 consists of:

1. [label=0)]
2. Cable galleries: there are a few of them in particular one direct and a second one via the former MCR (Meyrin control room) on the premises of the CERN PS.
3. Ducts running in the cable galleries. Each duct contains a number of fibre tubes into each of which fibres (in units of 12 or 24 fibres depending on the tube type) may be blown. Currently there are 48 single mode fibres already running between SR1 (a building at point 1) and bldg. 513 (note that most of them are already in use).
4. The status of the fibres is monitored centrally.

While some of the existing fibres are currently not used and could be allocated to ATLAS, there is the possibility of having a duct specially laid for ATLAS between SDX1 and bldg. 513. Such a duct could carry 10 tubes, each of which could in turn carry 24 single mode fibres for a potential total of 240 fibres that should certainly cover the current and future needs of ATLAS. It is noted that the tubes do not need to be populated with fibres from the start; once the infrastructure (i.e. the duct and the tubes) is in place a single tube could be populated at the beginning (24 single mode fibres) to cover the initial requirements including spares. If we also require redundancy with respect to the cable galleries, one could consider having in addition to the "ATLAS" duct also 4 fibres (2 data links: one for event data and one for control and monitoring) already existing and routed to bldg. 513, via e.g. the MCR path, that would be live from the start. Service disruption of the main cable gallery (i.e. the one through which the ATLAS duct would be routed) is expected to happen, if at all, for a short time.

### Security

While the control network will be protected by gateways regulating the access from the CERN public network, the situation is different for the high-speed lines. Performance requirements do not make the use of a firewall feasible. A CERN wide committee, CNIC, has been set up to address the security requirements of the experiment networks (including the data links) and to make recommendations on the subject.

### Summary and recommendations

In summary, a minimum of 8 fibres should be foreseen for event data transport from the event filter to Tier-0 (and reverse if needed). Further data links would be required if part of the event filter is sited away from point 1, as accounted above. An additional 2 data links will be needed for control and monitoring; and further data links may be required to cope with calibration requirements.

**Recommendation 2.5.1**: Network bandwidth needs away from the pit for calibration requirements need to be understood properly.

**Recommendation 2.5.2**: Since the cost of laying fibre-carrying infrastructure is significant compared to the cost of the fibre, we recommend to choose the option of an "ATLAS duct" that would allow a sufficiently large number of fibres to be laid when required, also to allow for future possible upgrades. Costs are saved by: 1) not blowing fibre bundles until they are needed; and 2) not connecting laid fibres until they are needed.

**Recommendation 2.5.3**: Live redundant network links should be available: this could be implemented by utilising available data links at less than half capacity. At least one data link for the event data and one data link for control and monitoring should be routed through a separate path. We suggest to ask for the use of 4 (already) existing fibres routed via the MCR path.

**Recommendation 2.5.4**: Network security is a vital concern: ATLAS should follow CNIC recommendations when available.

## 3 The Tier-0 resources available to ATLAS

The CERN Tier-0 centre will provide several critically important facilities to the LHC experiments, including:

* Central data recording of the raw data to mass storage media
* Large staging disk buffers for incoming raw data, which should be sufficient to hold all data waiting to be copied to Tier-1 sites or to be processed for calibration, alignment and first-pass reconstruction
* First-pass reconstruction of the raw data
* Disk staging space for ESD and AOD data, and recording of those also to mass storage
* Handling, some processing, and long-term storage of calibration data on disk

Clearly the proper functioning of all these areas is very important to the successful operation of the offline system from raw data to physics analysis. It is being discussed widely both in the ATLAS Computing Model Group and the LCG [2].

We note that for ATLAS data-taking, the most time-critical functionality is the copying of raw data from the SFO output buffer disks and the recording of that data to permanent mass storage. This is the first point where ATLAS data is written to a permanent store, so is effectively part of the ATLAS online data flow. It is thus essential that this part of the Tier-0 system operates with high reliability and fast response times: buffer disks at the SFO are only envisaged to allow a buffer of around a day, and this should only be used in case of emergency. Even if there are problems downstream - in the reconstruction process or in copying data to Tier-1 sites, for example - it remains essential that the permanent raw data storage subsystem of the Tier-0 continues to function fully.

The overall CPU power of the Tier-0 centre planned to be available to ATLAS is based on the assumption of a 200 Hz output event rate, a reconstruction requirement of 15 kSI2k seconds per event, and reasonable assumptions about processor efficiency and running time each year [3]. This leads to a total CPU requirement of 3.5 MSI2k for reconstruction, with a 5% increase for possible calibration requirements. A disk buffer of 127 TB is currently foreseen for ATLAS central data recording. In addition, 560 TB is foreseen for longer-term storage of raw and calibration data [3], dominated by an expected need to keep O(10%) of the raw data on disk for calibration. This is an area where progress in ATLAS planning will assist the Tier-0 sizing and costing discussions.

Note that these numbers refer to a "steady running" scenario - canonically from 2008. It is assumed that roughly 30% of the CPU will be purchased in 2006, 30% in 2007 and 40% in 2008. Thus the CPU power at start-up in 2007 can be expected to be around half of the 3.5 MSI2k stated above.

With reasonable costing assumptions [4], the total equipment cost of the ATLAS share of the CERN Tier-0 is currently estimated to be 12.3 MCHF [3] excluding common infrastructure costs.

## 4 The Event Filter and its location

The event filter configuration was discussed in the _High-Level Trigger_, _Data Acquisition and Controls Technical Design Report_[1] (referred to in the following, for brevity, as the "HLT TDR"). The configuration of the event filter is staged as part of the deferral funding plan:

* The initial configuration for start-up in 2007 is limited by funding to approximately half of the baseline configuration. The baseline configuration is designed to handle a 75 kHz level1 trigger rate, although the more important rate parameter for the EF is the event-building rate. The initial configuration can be expected to handle a level-1 rate of around 35 kHz, if a linear model of the event-building rate as a function of level-1 rate is applied.
* The baseline EF configuration is referred to as the "2008" set-up in the HLT TDR. In this document we refer to it as the "ATLAS baseline".

* An extension beyond the baseline configuration was also considered in the TDR, referred to there as the "2009" set-up. This EF configuration would be needed to handle a 100kHz level-1 rate. We stress that this is not part of the approved ATLAS detector at this time, but rather would be a possible upgrade.

It is flagged up in the HLT TDR (Table 5-5) that the baseline system may require some of the EF racks to be located away from the SDX15 barrack, for example "50% of EF racks possibly located in the CERN computer centre".

For the HLT TDR, assumptions had to be made about CPU requirements and the possible packing density of CPUs. The assumption about packing density of CPUs was that 23 CPU boxes would be sited in each EF rack. In addition, it was assumed that space would be reserved for the "2009" system, i.e. reserving racks for the "2009"-size level-2 and EF farms.

In the HLT TDR, the CPU usage needed per event is quoted as "~1 sec" ([1] page 8), and the planning figures use this round figure of 1 second per event. The event filter CPU boxes are referred to in the TDR as "dual 8 GHz CPUs". We have taken this to mean that they are approximately four times the CPU power of a modern dual 2 GHz box. In practice this may not be implemented as a dual 8 GHz CPUs, as discussed in more detail below.

**Recommendation 4.0.1**: EF CPU requirements should be stated consistently with the offline requirements, _e.g_. in SI2k units.

We note here a further point which will be important in optimising the trigger system when data arrive, if not before: in the HLT TDR a linear model was assumed between the EF CPU requirement and the first-level trigger rate. In practice, there is more flexibility than this: what matters more for the EF is the output rate of the second-level trigger: this is the event-building (EvB) rate. The EvB rate can be tuned to some degree separately from the level-1 rate by adjusting level-2 thresholds. The linear model used in the HLT TDR was therefore a simplifying, but not a necessary, assumption.

The possible expansion scenarios we have considered of the initial EF system up to the ATLAS baseline size are as follows:

* Install the entire system on the surface at point-1: this may require the construction of a new surface barrack
* Install the initial EF in the SDX surface barrack currently being built, but then expand into space managed by IT
* Install the whole EF in IT-managed space

In this section of the report we first summarise the best current estimates of the EF size needed, followed by a summary of the discussions with IT regarding a possible siting of part or all of EF system in IT-managed space. Finally we make recommendations on the best way to proceed.

### Best current estimates of the EF size, and uncertainties

The estimate in the TDR of the size of the computing farm required for the EF was based on the following key assumptions: the CPUs purchased would on average have an increase in their computing power of approximately a factor of 4 compared to the typical 2 GHz PCs used for algorithm timing tests for the TDR; on such a CPU the average processing time per event would be ~1 sec. In addition the estimates were based on very limited measurements with very preliminary software, requiring large assumptions about the time extra functionality would add, and that software improvements would save. Given this, a safety factor of 30% was included in planning the size/cost of the final farms. With the safety factor, the estimates translates into a processing power of ~3.5 kSI2k-seconds per event, assumed to be delivered on dual processor machines, with each machine delivering ~5.5 kSI2k.

The baseline system (for 75 kHz LVL1 rate) presented in the TDR assumed an Event Building rate of 2.4 kHz, with the possibility to upgrade to 3.2 kHz. With the processing power assumptions above this leads to a farm size of \(\sim\)1600 dual processor PCs to handle the 2.4 kHz Event Building rate. Furthermore, for the initial running the EF would only have half this capacity.

PCs will evolve throughout the period, but to simplify the discussion we consider a processor of the characteristics likely for early 2008. By that date the computing power assumed above is consistent with a Moore's law extrapolation from today's PCs (i.e. cost effective PCs of 5.5 kSI2k, compared to today's 3.06 GHz dual Xeon with \(\sim\)2.1 kSI2k or leading-edge 3.6 GHz dual Xeon with \(\sim\)3.0 kSI2k). Note that this performance increase is now predicted to be achievable with architecture changes and particularly using multi-core technology, rather than a simple increase in clock speed.

For the TDAQ counting room in SDX it is planned to use 1U rack-mounted server PCs. This format of PC is now very mature, is widely used in the IT industry for processing farms of high-performance servers and provides efficient use of space.

During recent years the power requirement of CPUs (and hence of PCs) has been steadily increasing. By 2008 we expect a dual CPU PC to use 250-300W (cf. current 2.4 GHZ dual servers use \(\sim\)200W). Note that if recent trends continued it would be even higher than this, however, there has been a growing realisation that the problems of cooling PCs are becoming unmanageable. Consequently the CPU manufacturers have said that they will limit the increases by moving to architecture improvements as noted above. The weight of such a 1U PC is estimated to be \(\sim\)15 kg.

The racks planned for SDX use horizontal air flow (as required by 1U PCs) and have water cooled heat exchangers rated at \(\sim\)9.5 kW which should remove 90-95% of the heat generated in the rack. (The room air-conditioning can handle at most 10% of the total power supplied to SDX.) The electrical power distribution system allows up to 11 kVA per rack, which matches the cooling power given a typical PC power factor of 0.95. The racks provide space for 47U of equipment with a weight limit of \(\sim\)500 kg (this weight limit derives from the limit allowed by SDX structure after correcting for the weight of an empty rack with power distribution and cooling system).

Combining the limits of the rack with the characteristics of the PCs it can be seen that some 30 PCs can be mounted in each rack. Thus the initial EF farm of 800 PCs requires 27 racks and the baseline system fits within 54 racks. SDX allows for a maximum of 99 racks (although this would leave no working space at all), the estimate for the other TDAQ equipment is 40 racks. Thus the baseline EF system will fit into SDX with a small amount of work space remaining.

Clearly should the actual PCs exceed the power or weight estimates above, or fall short of the computing power estimate, the space required would increase. Equally, there are large uncertainties in the computing power required per event. The above space estimates include the safety factor of 30%, so hopefully more equipment should not be needed, although it cannot be excluded. If, however, the estimates turn-out to be pessimistic then the same EF farm would cope with a higher event building rate.

(The use of blade servers has also been considered, but they are less mature and more expensive than 1U servers and until recently they have generally used lower performance CPUs. They may offer a higher packing density than 1U servers, but it is unclear that we could handle the higher power density this implies.)

The EF processors for the initial system will be purchased by several funding agencies, with a substantial fraction funded by CERN non-member states - this includes an in-kind 1.5 MCHF common project contribution from the USA. The additional processors required for the baseline system (plus the later upgrade for the 3.2 kHz event building rate) will use the TDAQ funds deferred as part of the ATLAS Completion Plan, when they are returned to TDAQ as funding to cover the ATLAS short-falls is obtained. This returned TDAQ Deferrals is subject to the special CERN rules applied to the LHC Collaborations, which means that the purchases are not restricted to CERN member states, but may be bought from companies in states of the collaboration. In contrast the PCs purchased by IT (including the Tier-0 machines) are subject to the strict CERN purchasing rules and should be sourced from a CERN member state. In practice it is not clear how much of a problem this will pose, although the strict CERN rules do exclude some companies used for similar farms at a substantial number of North American and European laboratories and universities.

Using the figures noted above the total EF processing power to support an event building rate of 2.4 kHz is ~8.8 MSI2k. The ATLAS TDAQ financial plan allows a total of ~6.6 MCHF for the purchase of the PCs. This can be compared to estimates made for the Tier-0 machines [4]. In 2003 the price per delivered SI2k was predicted to fall according to the figures in the table:

\begin{tabular}{|l|l|l|l|l|l|l|} \hline
**Year** & 2004 & 2005 & 2006 & 2007 & 2008 & 2009 & 2010 \\ \hline
**CHF/SI2000** & 1.89 & 1.25 & 0.84 & 0.55 & 0.37 & 0.24 & 0.18 \\ \hline \end{tabular}

However, since then it has been noted that there has been some slow-down in the rate of reduction and the move to multi-core CPUs will require substantially more memory, such that even with falling memory prices the box price will increase significantly in later years. In addition the above numbers have to be reduced by ~10% for the infrastructure costs (the TDAQ price is for the PCs only) and a premium added for 1U PCs, this is currently estimated to be ~30%, but has been reducing. Taking all of these factors into account would give a revised table for the purchase years with 2006 to 2008 as:

\begin{tabular}{|l|l|l|l|} \hline
**Year** & 2006 & 2007 & 2008 \\ \hline
**CHF/SI2000** & 1.28 & 0.97 & 0.74 \\ \hline \end{tabular}

The TDAQ plan falls short of the figures in the final table, as to be expected given that the substantial change is the extra memory cost, which has emerged since the TDR. However, the difference (~15%) is well within the uncertainties in the estimates - for example changes in the cost of the extra memory or delaying the purchases by 6 months.

### Discussions with IT regarding EF siting

Initial discussions have taken place with responsibles in IT division: as part of these Tony Cass and Bernd Panzer-Steindel have taken part in taskforce meetings, in addition to the IT ATLAS linkman, David Asbury. The focus of discussions has been in the context of the HLT TDR planning, namely that for the ATLAS baseline system part of the EF system will need to be sited somewhere other than in the current SDX barrack. IT have been studying possible options, and the most viable appears to be to commission a new machine room in an existing building on the CERN site: this would need substantial refurbishment and installation, as it would not be the current computer centre building 513, but elsewhere on the CERN site. This new machine room is regarded as potentially important for IT, to allow for possible future changes in tape technology on the timescale of 2008. Other options, for example possibly renting commercial machine room space in Meyrin, are also being considered. Siting significant parts of the EF in the existing building 513 machine rooms is excluded by a lack of sufficient space, cooling and power.

At the present time, no formal decision is needed yet on whether to go ahead with such additional machine room space. However, for a refurbished machine room to be ready for 2008, a decision will be needed within the next year or so. It is not clear on what timescale a decision would be needed for a machine room in rented space. At present, no commitments have been made, or needed, from either IT or ATLAS.

These discussions with IT have also been useful in other respects: for example the IT experience with the "quattor/lemon" system configuration management tools is very positive and they recommend them to us. It is an interesting option for configuration management of the EF boxes (and possibly other systems), as it allows for example for rapid updates of  sets of systems, with automated logging etc. It can also provide for speedy reconfigurations of CPU boxes into alternative states, for example to switch between "online" and "offline" configurations during shutdowns.

At the current time only indicative costings are available for the different machine room sites managed by IT. The total cost of refurbishing an existing room on the CERN site into a machine room is estimated to be of the order of 5MCHF. The annual cost of rental of machine room space in Meyrin is unclear, and likely to be subject to negotiation, but an indicative cost is 1-2 MCHF per year. In both cases the new machine room would be shared between IT and ATLAS, so ATLAS could expect to bear a fraction of the cost. Details would be subject to negotiation in the event ATLAS decided to pursue this option.

**Recommendation 4.2.1**: The positive contacts with IT initiated in the work of the taskforce should be continued.

**Recommendation 4.2.2**: We should keep the option of a partial listing of the EF in a building provided by IT open, at least until either money would need be committed, or it is clear we will not need it. However, there would be concerns about network and other reliability issues if rented space in Meyrin were to be the only option.

**Recommendation 4.2.3**: Quattor/lemon should be followed up by the TDAQ team. This may be particularly useful if some sharing of tasks between EF and Tier-0 is to be possible, as discussed in section 4.6 below.

### The initial EF installation

During commissioning and initial data-taking in 2006/2007, there will be significant operational advantages to having the entire data acquisition system sited at Point 1, including the EF up to the large SFO buffer disks. For example this will give better tolerance in networking faults, in commissioning of firewalls, and so on. Since there is no problem in siting the initial EF in the SDX barrack, we consider this clearly preferred for simplicity of commissioning of the complicated system of the ATLAS DAQ.

**Recommendation 4.3.1**: The initial EF system should be installed in the SDX barrack, as planned.

### The baseline EF installation

In all scenarios the SFIs, responsible for building  the events and storing them until they are requested by the EF, are sited in SDX. The network connections between the SFIs, the EF sub-farms and the SFOs are provided by the TDAQ Back-end network.

In the first scenario with all of the EF sited at point-1, the back-end network and all of the nodes connected to it would be at point 1. Each SFO has a separate network connection to pass the data to Tier-0, these would then be concentrated at SDX and use dedicated 10 Gb links to transfer the data to Tier-0 in building 513. The data link and bandwidth requirements for this, and the other, scenarios are discussed in Section 2.2, above. In the second scenario, with the initial EF at point-1 and the remainder at some IT managed space, the back-end network would be split between the two sites. In this scenario there would be separate links connecting each site (point 1 and the IT managed space) to Tier-0 in building 513. Each such link would carry the accepted events from the SFOs at one site to Tier-0. In the third scenario with all of the EF at some IT managed space, the back-end network would again be split between the two sites.

From the network perspective option 1 is the simplest and cheapest. The extra cost of the network links in the other scenarios are significant, but are believed to be small enough that they are not a major factor in where to site the EF.

Another possible issue is the size of disk buffers in the SFOs and SFIs should links go down. Clearly in these scenarios, assuming that the IT managed space is remote from building 513, the buffer size required in the SFOs is the same in all scenarios \(-\) indeed if one also takes into account the possibility of Tier-0 being down then this would be the same even if sited in building 513. The argument for the SFIs is clearly different, however. To provide buffering for 1 hour of down time of the link would require ~200 GBytes of disk buffer on each SFI (100 SFIs are planned for the final system). However, at the full event building rate of an SFI (~30 Hz) the data rate is around 50 MB/s: buffering such a rate via disk has not been studied and almost certainly goes beyond current technology. This can be contrasted with the alternative, when events are only buffered in memory, but for which only very short network downtimes (tens of seconds) could be tolerated. If buffering of events to disk in the SFI were required, development work would be needed to establish if it is a viable option and the rate an SFI could sustain. However, current limits of disk I/O would suggest it is unlikely to be better than half that currently planned, which would require the number of SFIs to be doubled to attain a given total event building rate.

In a worst case, a new barrack could be required at point 1 of a size large enough to accommodate up to 50 racks. There is sufficient space in the surface buildings to accommodate such a barrack near to the current SDX barrack, inside an existing hall. A simple scaling of the cost of the SDX barrack indicates a likely cost of 1MCHF for such an additional barrack, should it be needed. Costs are substantially lower than for newbuild/refurbishment of IT managed space because of the availability of chilled water at point 1, reducing the need for air conditioning very substantially: but at the cost of needing to use rack-mounted PCs. The overall cost balance currently favours the rack mounted solution. The use of rack-mounted PCs for all EF nodes also has the advantage that they can be interchangeable.

**Recommendation 4.4.1**: We should try, if possible, to install the full ATLAS baseline HLT system in the surface SDX barrack. If this would be possible, it would offer substantial operational advantages (local access by shift crews, less DAQ reliance on networks off-point-1, single location for EF system). If the system needs more space we should endeavour to make additional space available, e.g. by displacing other racks from SDX if only a few extra racks are needed, or by creating a new nearby barrack.

**Recommendation 4.4.2:** Since the uncertainties are too large to be confident that the baseline system will fit in the foreseen space at point 1, we should keep the option open to site part of the EF system in IT-managed space elsewhere. However, the cost evidence available so far suggests a point-1 solution is likely to have significantly lower overall cost.

### Further EF expansion

In due course, after commissioning and operation of the ATLAS baseline HLT system, it may be desirable to consider an upgrade of the system located on the CERN site to cope with higher data rates - this could be either via higher level-1 rates, via higher event building rates, or both. The siting of such an extended system is beyond the scope of this report, but we note that the evolution of CPU-boxes may mean that it could be fitted within the existing SDX barrack by replacing some or all of the original HLT processing boxes. The exact feasibility would have to be addressed nearer to the time of any such upgrade.

We further note that it has been suggested that off-site computer farms may be a way to augment the event filter capacity. However, such use would require highly reliable large area networking, potentially also with strong requirements for real-time operational reliability.

**Recommendation 4.5.1**: We consider that the use of off-site EF farms (away from CERN) are a potentially interesting way to involve a wider community in the excitement and immediacy of data-taking. Against this must be balanced the paramount requirement that such off-site facilities must not be able to stop ATLAS data-taking. We therefore recommend that the use of such sites should be restricted to well-defined and agreed purposes, for example specific data monitoring and calibration purposes.

### Sharing resources between EF and Tier-0

The similar - and very large - overall CPU capacities of the EF and of the ATLAS share of the Tier-0 suggest that one should explore the technical feasibility of a sharing between the two. In steady-state running, if both systems have been designed correctly (i.e. if there are no unpleasant surprises!), this will not be relevant, as both systems will be running at close to full capacity. There will be times, however, when the EF farm will be under-used, for example in LHC shutdowns. There may also be times - if, for example, background conditions are very bad for a few weeks or months - when the EF capacity is insufficient to handle the event-building rate. These two cases were considered separately.

It is envisaged, after the initial start-up, that data will typically be re-processed a couple of months, and again after a year, after the initial reconstruction. This work will be done primarily at Tier-1 sites. If these reprocessing periods - which will last many months - coincide with long LHC shutdowns, the Tier-0 site could also be able to assist with reprocessing, and the EF nodes could provide a valuable additional resource. However, various caveats have to be made:

* Use of part of the EF for reprocessing must not prevent development work on the EF system
* Use of the EF for re-processing should stop long enough before physics running resumes to be sure that the nodes have been properly reconfigured
* Significant reconfiguration of nodes is likely to be needed, for example with different filesystems, software configuration, etc. Given the large number of EF nodes, this requires automated system management tools.

Network reconfiguration may be needed, for example of firewalls, to allow event data to flow at high speed back to point-1 as well as away from it
* Physical network bandwidth could be a problem if a minimal network bandwidth is in place between point-1 and the computer centre. If the duplicated live links recommended above are used, however, this should not be such an issue (although one would lose some of the full bandwidth redundancy for this non-time-critical processing).

The likely specification of CPU and memory requirements for EF and Tier-0 nodes are not, however, expected to differ significantly - the main issue for the EF being the rack-mount form-factor.

**Recommendation 4.6.1**: We recommend that the option should be kept open to use the EF for data reprocessing: this potentially has implications for network and EF CPU/memory configurations, but we expect the impact to be manageable.

Given that the EF would only be available for part of the time; that it is unclear how often reprocessing will be scheduled in shutdowns; it is not known how much, and how often, EF nodes will be needed for development work; and a clean switch-over with automated system management tools has not been demonstrated for this application, we make an additional recommendation.

**Recommendation 4.6.2**: It should not be assumed for computing model calculations that the EF will be available for reprocessing. It is important to plan for a full capacity for reprocessing even in the event that the EF will not be available.

An alternative possible requirement for sharing of resources between Tier-0 and EF could conceivably arise in the event of discovering that the EF was significantly under-powered. This should be considered an unlikely scenario, but it is not unthinkable: it could arise if, for example, the LHC beam conditions were very bad. While an EF upgrade might be merited, it is conceivable that for a period of weeks or even months there might be a serious lack of CPU power for event filtering. There are at least two ways this problem might be attacked: firstly one could increase the data volume written from the EF - this would clearly have implications for downstream processing, but a possible solution would be to install a "pre-processing selection" stage before reconstruction, and perhaps also before mass central data recording. An alternative might be to second some of the Tier-0 nodes to act as part of the EF: this would require high bandwidth links between point-1 and the computer centre, and would possibly require some SFOs to be relocated (or added) to the Tier-0 setup. Network reconfiguration would also be required to repartition Tier-0 nodes into the point-1 EF network.

It is evident that if either approach were needed, it is likely there would be insufficient CPU resources remaining to provide prompt full reconstruction of all the ATLAS physics data at CERN. There would need to be a decision at that time if first-pass reconstruction could be moved out to Tier-1 sites, or simply delayed. In any case this has a substantial impact, but if the alternative were being unable to take data, or to severely cut into physics by raising trigger thresholds too far, it might be considered acceptable.

Fortunately the Tier-0 and EF nodes are expected to have similar specifications, and so allowing for this possibility may not not need to incur expenses or significant advance work.

**Recommendation 4.6.3**: Efforts should be made to ensure that EF processors and Tier-0 nodes may technically be used to fulfil each other's functions. This cannot be used as a justification for reducing the capacity of either system, as both will be fully occupied in data-taking. Similarly the network infrastructure should allow both the use of the EF for data reprocessing, and the assignment of Tier-0 nodes to the EF if needed.

## 5 The flow of data between point 1 and Tier-0

During ATLAS data-taking, both event and non-event data flow continuously from point 1 to the Tier-0 farm, where the aim is to perform a first pass reconstruction as quickly as possible. Ideally, the reconstruction latency should be determined only by the time taken to gather all non-event (conditions) data needed to reconstruct a given file of events, and not by available Tier-0 computing resources, which should be sufficient to keep up with the steady-state data-taking rate. Initial discussions have shown that a goal of beginning the reconstruction of the physics data 24 hours after the end of the corresponding LHC fill and completing it within a further 24 hours seems reasonable, though the details will depend somewhat on the role of the different event streams discussed below.

Many of the considerations in this section touch upon aspects of the work of the computing model group, and the relevant experts have been fully involved in the taskforce discussions. This includes representatives from the subdetector calibration, event filter, database and offline reconstruction communities, as well as the CERN-IT representatives responsible for the Tier-0 farm.

### Subsystem requirements for conditions data

The detector subsystems (including the trigger and event filter) produce various types of conditions data online, all of which will be stored in the online instance of the conditions database hosted at point 1. This data includes:

* Subdetector configuration data, for setting up and configuring the detector hardware and data-taking software. Some of this data will be static, but some will change according to detector and data-taking conditions.
* Subdetector conditions data, including detector calibration and alignment constants and other parameters required for reconstruction.
* Online monitoring information (mainly in the form of histograms).
* DCS (slow controls data) produced by the ATLAS DCS system.

This data is produced by various sources in the online system, including calibration and monitoring tasks running in the subdetector RODs, the event filter system, and on dedicated processors. The data has various destinations beyond the conditions database, including subdetector monitoring workstations, level2 trigger and event filter algorithms, and first-pass reconstruction in the Tier-0 farm. The storage technology will vary with the data type, with some being stored in relational databases and some as streamed sequential file -based data, but all will be managed by the conditions database infrastructure and are considered part of the conditions data.

An attempt has been made to catalogue all such items of conditions data together with their sources and destinations. At the present time, this is incomplete, but rough estimates and extrapolations can be given. These indicate that subdetector configuration and relational database conditions information is likely to amount to O(100 GB)/year, file-based calibration information to O(10 TB)/year and monitoring information O(100 TB)/year. All this data will have to be transferred offline to the Tier-0 centre and beyond, but this is a small fraction (few percent) of the event data volume, so will not add significantly to the network bandwidth requirements.

The processing power required to produce this data varies widely between subdetectors, from calibration tasks that are foreseen as part of ROD processing or small 'post-processing' stages spying on data in the event filter, to expensive calibration and monitoring tasks with dedicated triggers and event selection streams, before the event filter and even before the level2 trigger. For many subdetectors, these requirements have not been worked out in much detail, beyond vague assumptions about the availability of an online monitoring/calibration farm at point 1. Although such a farm is foreseen in principle, no estimates have been made of the size, space and power/cooling requirements, and a location has yet to be found ([1], page 87). Some subdetectors have also expressed needs for calibration streams which cannot obviously be satisfied in normal data-taking conditions. For example, the muon system wants to process all level1 muon RoIs, to extract muon tower calibration parameters. This autocalibration procedure requires RoIs to be processed at the output rate of level1 - O(10 kHz), for a sustained period of several hours. Other subdetectors (e.g. the TRT) have expressed interest in streaming data before or after the event filter to remote institutions for monitoring purposes.

It is clear that much more work is needed in this area, to determine the detailed strategy for online detector calibration and the required computing resources. The event filter architecture is very flexible and offers many possibilities, but the pros and cons of each need to be understood in more detail, in conjunction with the subdetector requirements. This also applies to subdetector calibration beyond the event filter, using reconstructed data - there is presently little understanding of what calibration can be done with ESD or AOD, and what needs access to the raw data. This may well have serious impact on the ATLAS computing model.

These issues need to be addressed with higher priority both by the subdetectors and TDAQ, presumably in the context of the existing calibration and alignment working group.

**Recommendation 5.1.1:** Subdetectors must produce detailed quantitative plans for calibration and monitoring as soon as possible, including online processing requirements, and clearly-documented requests for any calibration streams or remote processing. Dialogue between the subdetectors and TDAQ is required to ensure the calibration needs are reastiAOD/TAG definition. This will allow express-stream analyses to be migrated to the full event store architecture as soon as possible, and eliminate the need to reprocess the express physics stream separately in subsequent reconstruction passes with improved algorithms and calibration constants.

Not all the events will flow through the Event Filter harmlessly: some will crash the Processing Tasks (PTs) in the sub-farms due to bad data content, calibration problems, selection/reconstruction bugs or even unforeseen event topologies. Those events will not be fed back into another PT, because they would presumably fail again, but will be forcibly accepted. They should be included in the main physics stream, and may be diverted in addition to a further, low rate, debugging stream. Of course the rate of such events must be small, or it would indicate a more serious pathology. A debugging stream should allow as rapid as possible identification of problems related to the EF selection procedure or to the detector or to general data acquisition misbehaviour, as a copy of the stream could be immediately available at Point 1, without having to wait for any Tier-0-related delays such as having to strip out damaging events from the main stream data files.

The issue of streaming data for calibration (and monitoring) purposes is somewhat different and is strictly linked to the latency acceptable for obtaining the first-pass calibration constants needed to reconstruct the physics data samples (see section 4.3). Streaming calibration data is also related to the usage of remote farms, addressed in section 3.

Taking all the above considerations into account, the following proposal is made as a starting point for defining the event filter streams. There should be four classes of streams, with the first two consisting of single streams, and the last two consisting of separate sub-streams with the events placed in several different physical files according to type:

1. The main physics stream, containing all the physics events of interest. With current trigger menus, the output rate is estimated at 180 Hz, with 1.6 MB/event.
2. The express physics stream, limited to 10 Hz, and containing for example W\(\rightarrow\)e,\(\upmu\) and di-lepton candidates, together with events with high \(\mathrm{E_{F}}\) jets or high \(\mathrm{E_{F}}\) miss. This stream will also contain top events useful for jet calibration. Note that a systematic evaluation of the selection thresholds required to fit into 10 Hz has not yet been made. These events also go into stream 1, but are duplicated here to allow them to be processed separately at Tier 0.
3. Calibration streams. Several have been identified, including dedicated calibration streams for the ID, electromagnetic calorimeter and muon system, and inclusive electron/muon streams as discussed above. These streams have partial detector readout (by subsystem or RoI), and will be used only for calibration, monitoring and debugging tasks, not physics analysis. The EF processing model allows some of these tasks to be carried out in the EF processors, if CPU allows. An initial estimate gives a total bandwidth for these streams of 45 MB/second, or 13% of the total. We note that readout of single subdetectors is currently foreseen in TDAQ planning, but readout of restricted regions, such as RoIs, is not.
4. A diagnostic stream, consisting of 'problem' events and perhaps including also forced accept events for debugging the trigger and event filter, though these might also be put in the calibration or physics streams. The overall rate for this stream should be kept very low, as the aim is to provide events for rapid and detailed individual debugging.

The Event Filter dataflow architecture has always foreseen the processing of a mixture of physics and calibration/monitoring data in its sub-farms. As discussed in Section 4.1, the processing requirements for calibration/monitoring are as yet rather ill-defined. If the hardware at point 1 (including both event filter and online monitoring farms) is not sufficient to satisfy both these requirements and the primary task of online event selection, there are two possibilities:

* If the remote farm architecture is considered viable in terms of DAQ operation (run control, information system, etc.) and found to be dependable in terms of network infrastructure, calibration events may be sent to remote farms, using standard SFI and received back, together with the calibration results, in a SFO element. This way of operating the whole EF does not differ from the standard one, with the exception of the existence of remote farms.
* If this is not possible, or if the needed computing power at remote sites is not sufficient, one may consider passing more of the calibration events (stream type 3 above) through the EF untouched passing them on to Tier-0 for separate processing. In this case one creates a'short circuit' between SFI and SFO and the local EF performs no processing of these events, hence moving more of the CPU burden to the Tier-0

Of course, any combination of the mentioned solutions can be considered, depending on the global ATLAS needs. For high-level physics or detector monitoring a similar approach holds or it is also possible to use the debug stream, as illustrated above.

**Recommendation 5.2.1:** ATLAS should adopt a model where the event filter writes multiple output streams, with the four-stream definition given above (physics, express, calibration and diagnostics) as a starting point. The implications for the event filter and Tier-0 of handling multiple streams, and using remote farms for the processing of some calibration streams, should be examined.

**Recommendation 5.2.2:** The use of multiple physics streams from the event filter should be restricted to the main and express streams only. The main stream should not be split up into sub-streams by event type.

**Recommendation 5.2.3:** Calibration streams would benefit greatly from being able to write partial events, both by subdetector and by region of interest (and hence varying the event contents as a function of event type). The implications of this for the TDAQ system should be followed up.

### The latency of reconstruction

Reconstruction latency (how soon after data-taking the data are reconstructed) is limited by several factors - the latency and speed of transferring files from the event filter SFOs to the Tier-0 farm, the time taken to prepare the necessary conditions data to reconstruct the events, and the CPU time taken for reconstruction itself. When discussing latency targets, it is also important to distinguish initial data-taking (when the primary objective is to understand the detector and look for problems, as well as to provide fast access to initial high-interest physics datasets) from normal high-rate steady-state operation (when the objective is to maximise the integrated luminosity of problem-free recorded data).

In normal operation, each of the 30-50 event filter SFOs will fill and close a 2 GB file every five minutes or so. To keep up with the rate, the network bandwidth must be such that this can be transferred to Tier-0 within a similar time, thus data should be at the Tier-0 centre ready for reconstruction within about ten minutes of being taken. The Tier-0 processing time per file will be around five hours for a 2 GB file, contributing significantly to the reconstruction latency. For express calibration and physics streams, it may therefore be worth writing smaller files to split the load over more processors and reduce the latency of this step.

The latency involved in the preparation of conditions data is much more difficult to assess. Several subdetectors have calibration constants that are expected to change significantly over, and will be re-determined, every LHC fill or every day, using dedicated calibration tasks independent of the main event stream processing. It seems unlikely that the calibration processing to arrive at the first-pass calibration constants could be completed in much less than 24 hours after the end of the fill (this may involve first processing the dedicated calibration streams, then performing some verification steps on independent data samples). This issue needs to be explored more with the subdetectors to see what sort of 'prompt' calibration processing is feasible, and what would be gained by e.g. waiting 48 hours instead of 24. However, increasing the latency before pass 1 reconstruction would also have implications for the amount of disk buffering necessary at the Tier-0 centre to avoid staging all the data out to tape and then in again for reconstruction. The latency appropriate for the express physics stream also needs to be considered - it may well be necessary to use calibration constants derived from previous fills, if the express stream latency is to be kept below 24 hours. Such a fast turnaround would also be very useful for general data quality monitoring tasks requiring access to physics data from all detectors combined.

As with other subdetector calibration issues, these calibration latency questions need to be discussed more widely and in more detail with subdetectors, to understand what can really be done on what timescale, and the likely calibration quality vs. calibration dataset size/time. These issues are likely to drive the targets adopted for first-pass reconstruction latency. It is clearly in the interest of ATLAS to minimise this latency, but there is a trade-off with calibration quality and the usefulness of this data for physics analysis.

**Recommendation 5.3.1:** Once the discussions with subdetectors on their calibration needs have concluded, ATLAS should adopt clear targets for the latency of express and normal stream physics processing, meaning the total latency from the event trigger time to the point where the event is available for analysis. With present information, realistic targets appear to be a few hours for the express stream and 24-40 hours for normal physics data processing. The latter figures assume reconstruction of the normal physics data collected during a fill can be started within 24h of the end of that fill.

### Data transfer mechanism

Each event accepted by the HLT will be passed to one of some 30 or so SFO's. At the SFO the events will be written to a disk file, in byte-stream format. Each file will be closed when its size approaches 2 GB. Properties of the files are as follows: Files will always be closed before they reach 2 GB; no events will span 2 files; at the end of a run all open files will be closed (i.e. each file contains events from only a single run) - this will result in a few small files, but the majority of files will be close to 2 GB; under normal conditions it will take a few minutes to write a file. A service will run on each SFO to read back the closed files and transmit them (probably using RFCP) to Tier-0. Each SFO will have \(\sim\)1 TB disk space, to provide a buffer of order of 1 day in the case the link to Tier-0 goes down. To maximise the safety reserve the service should normally keep the occupancy of the disk at a low level. When a file has been completely transferred to Tier-0 the original file on the SFO will be deleted and a message sent to an off-line database giving the details of the file transferred - to help with off-line book-keeping.

**Recommendation 5.4.1:** Tier-0 should contain a large disk buffer (\(\sim\)100 TB) to hold the transferred data before its processing in Tier-0, obviating the need to store these data in the interim in Castor.

## 6 Overall Summary and Conclusions

The taskforce has addressed several issues in the context of the interface between the experiment at point 1 and and the CERN computer centre. In some cases real progress has been made on the issues involved, in others we have mainly aimed to stimulate discussions in other fora.

In the area of networking, progress has been made in understanding the factors determining the requirements, and in quantifying the bandwidth needed for event data transfer. Regarding the issue of event filter sitting, it now seems reasonably plausible that the full baseline EF will fit into the SDX barrack. If not, the overflow is expected to be relatively small. In the case that significant additional space is needed, the option to site the overflow at point 1 appears to be favoured over the use of IT-managed space, as the latter is not available in the current IT planning, and so can be expected to have significant cost implications. However, a decision on the use of such space is not needed at this time. This is advantageous, as the uncertainties on the event filter resources requirements will decrease with time.

In the area of event data-flow from experiment to Tier-0, the most urgent issue is to sort out calibration requirements and corresponding data-flow: this work has been stimulated by the taskforce, and is making solid progress. The expected reconstruction latency in steady state running should be dominated by calibration considerations. It is clear already that the timescale will most probably be from one to several days rather than a few hours. There has also been progress reaching a model for streaming of event data, in collaboration with the computing model group, for whom it is a key design issue.

In summary, we note that fruitful exchanges of information between:

* The TDAQ community
* Commissioning responsibles
* The offline community
* IT infrastructure and fabric responsibles
* Sub-detector systems

have taken place in the context of our work, and need to be nurtured. As a final recommendation, we consider that many of the issues dealt with by this taskforce would fit naturally with an extended computing model group, following the completion of the CMG report to the LHCC at the end of 2004. Such a "data-flow and computing model" forum would need to take care to ensure involvement of all interested parties, as we approach data-taking. The networking and EF siting discussions are possible exceptions, perhaps best handled by the networking responsibles, and the HLT community, respectively.

**Recommendation 6.1**: After the computing model group report to the LHCC is completed, establish a continuing body which takes on both the CMG work and many of the additional issues dealt with in this taskforce.

**Bibliography**

[1]_High-Level Trigger, Data Acquisition and Controls Technical Design Report,_ ATLAS Collaboration, CERN/LHCC/2003-022, June 2003.

[2]_The ATLAS Computing Model_, D. Adams _et al_, ATL-SOFT-2004-007, December 2004.

[3]_Sizing and Costing of the CERN T0 center_ (draft 1.2), Bernd Panzer-Steindel, September 2004.

[4]_Price extrapolation parameters for the CERN LCG Phase II Computing Farm_ (draft 1.0), Bernd Panzer-Steindel, July 2004.

**Appendix: Recap of recommendations**

**Recommendation 2.5.1**: Network bandwidth needs away from the pit for calibration requirements need to be understood properly.

**Recommendation 2.5.2**: Since the cost of laying fibre-carrying infrastructure is significant compared to the cost of the fibre, we recommend to choose the option of an "ATLAS duct" that would allow a sufficiently large number of fibres to be laid when required, also to allow for future possible upgrades. Costs are saved by: 1) not blowing fibre bundles until they are needed; and 2) not connecting laid fibres until they are needed.

**Recommendation 2.5.3**: Live redundant network links should be available: this could be implemented by utilising available data links at less than half capacity. At least one data link for the event data and one data link for control and monitoring should be routed through a separate path. We suggest to ask for the use of 4 (already) existing fibres routed via the MCR path.

**Recommendation 2.5.4**: Network security is a vital concern: ATLAS should follow CNIC recommendations when available.

**Recommendation 4.0.1**: EF CPU requirements should be stated consistently with the offline requirements, _e.g_. in SI2k units.

**Recommendation 4.2.1**: The positive contacts with IT initiated in the work of the taskforce should be continued.

**Recommendation 4.2.2**: We should keep the option of a partial siting of the EF in a building provided by IT open, at least until either money would need be committed, or it is clear we will not need it. However, there would be concerns about network and other reliability issues if rented space in Meyrin were to be the only option.

**Recommendation 4.2.3**: Quator/lemon should be followed up by theTDAQ team. This may be particularly useful if some sharing of tasks between EF and Tier-0 is to be possible, as discussed in section 4.6 below.

**Recommendation 4.3.1**: The initial EF system should be installed in the SDX barrack, as planned.

**Recommendation 4.4.1**: We should try, if possible, to install the full ATLAS baseline HLT system in the surface SDX barrack. If this would be possible, it would offer substantial operational advantages (local access by shift crews, less DAQ reliance on networks off-point-1, single location for EF system). If the system needs more space we should endeavour to make additional space available, e.g. by displacing other racks from SDX if only a few extra racks are needed, or by creating a new nearby barrack.

**Recommendation 4.4.2:** Since the uncertainties are too large to be confident that the baseline system will fit in the foreseen space at point 1, we should keep the option open to site part of the EF system in IT-managed space elsewhere. However, the cost evidence available so far suggests a point-1 solution is likely to have significantly lower overall cost.

**Recommendation 4.5.1**: We consider that the use of off-site EF farms (away from CERN) are a potentially interesting way **b** involve a wider community in the excitement and immediacy of data-taking. Against this must be balanced the paramount requirement that such off-site facilities must not be able to stop ATLAS data-taking. We therefore recommend that the use of such sites should be restricted to well-defined and agreed purposes, for example specific data monitoring and calibration purposes.

**Recommendation 4.6.1**: We recommend that the option should be kept open to use the EF for data reprocessing: this potentially has implications for network and EF CPU/memory configurations, but we expect the impact to be manageable.

**Recommendation 4.6.2**: It should not be assumed for computing model calculations that the EF will be available for reprocessing. It is important to plan for a full capacity for reprocessing even in the event that the EF will not be available.

**Recommendation 4.6.3**: Efforts should be made to ensure that EF processors and Tier-0 nodes may technically be used to fulfil each other's functions. This cannot be used as a justification for reducing the capacity of either system, as both will be fully occupied in data-taking. Similarly the network infrastructure should allow both the use of the EF for data reprocessing, and the assignment of Tier-0 nodes to the EF if needed.

**Recommendation 5.1.1:** Subdetectors must produce detailed quantitative plans for calibration and monitoring as soon as possible, including online processing requirements, and clearly-documented requests for any calibration streams or remote processing. Dialogue between the subdetectors and TDAQ is required to ensure the calibration needs are realistic and can be met without significant impact on normal physics data-taking.

**Recommendation 5.1.2:** The event filter architecture is very flexible, and offers many possibilities for calibration and monitoring to be performed during or after the event selection procedure. Based on the results of the subdetector plans, an overall strategy and subdetector guidelines should be defined, and limits placed (e.g. 10%) on the fraction of event filter resources to be devoted to calibration and monitoring tasks.

**Recommendation 5.1.3:** The possible need for, and size of, an online processing farm providing additional resources for calibration and monitoring at point 1 should be clarified. If such a farm is necessary, space at point 1 and resources need to be identified.

**Recommendation 5.2.1:** ATLAS should adopt a model where the event filter writes multiple output streams, with the four-stream definition given above (physics, express, calibration and diagnostics) as a starting point. The implications for the event filter and Tier-0 of handling multiple streams, and using remote farms for the processing of some calibration streams, should be examined.

**Recommendation 5.2.2:** The use of multiple physics streams from the event filter should be restricted to the main and express streams only. The main stream should not be split up into sub-streams by event type.

**Recommendation 5.2.3:** Calibration streams would benefit greatly from being able to write partial events, both by subdetector and by region of interest (and hence varying the event contents as a function of event type). The implications of this for the TDAQ system should be followed up.

**Recommendation 5.3.1:** Once the discussions with subdetectors on their calibration needs have concluded, ATLAS should adopt clear targets for the latency of express and normal stream physics processing, meaning the total latency from the event trigger time to the point where the event is available for analysis. With present information, realistic targets appear to be a few hours for the express stream and 24-40 hours for normal physics data processing. The latter figures assume reconstruction of the normal physics data collected during a fill can be started within 24h of the end of that fill.

**Recommendation 5.4.1:** Tier-0 should contain a large disk buffer (\(\sim\)100 TB) to hold the transferred data before its processing in Tier-0, obviating the need to store these data in the interim in Castor.

**Recommendation 6.1**: After the computing model group report to the LHCC is completed, establish a continuing body which takes on both the CMG work and many of the additional issues dealt with in this taskforce.