###### Abstract

In the present view, the ATLAS High Level Trigger will base its event selection software on the offline reconstruction framework, Athena. It is therefore imperative that the offline software - and its relevant components - are able to handle the large CPU and bandwidth loads required in a real-time environment. This note presents a first set of measurements aimed at validating Athena as the ATLAS online event selection framework. Although Athena is at an early development stage, detailed profiling can already yield clues as to which components can be optimized. In this note such areas are identified and a proposal is made on a road map to full performance.

January 31, 2002

**HLT Validation of Athena**

**M. Bosman, K. Karr**

IFAE, Barcelona

**C. Bee**

CPPM, Marseille

**S. Gonzalez1, W. Wiedenmann**

Footnote 1: Contact: Saul Gonzalez®-cern.ch

University of Wisconsin

## 1 Introduction

The ATLAS High Level Trigger [1] will use reconstruction algorithms developed by the offline community in its Event Filter selection stage [6]. This requirement minimizes duplication of work and ensures consistency between the offline and the online event selections.

The re-use of offline algorithms in the online environment implies that the online framework, where the algorithms reside, must provide the same interfaces and services as the offline framework. Furthermore, the demanding online environment at the LHC places severe constraints on the performance of the event selection software.

This note presents a first study of the suitability of Athena [3] as the event selection framework for the ATLAS High Level Trigger (HLT), addressing the use of Athena at the Event Filter (EF) only. Re-using Athena code in the Level-2 trigger is a possibility that will be studied once Athena is validated as an EF framework.

In addition to the validation activities, an important aim of this work is to establish reference points and metrics that can be used to monitor the performance of Athena as it evolves. This is especially important given that Athena is in full development.

It is important to remember that the framework choice influences the programming model (i.e., architecture) used for the HLT software. For example, the separation of data and algorithms in Athena requires an "intelligent" Event Data Model (EDM), which may have performance consequences because algorithms are forced to communicate via a TransientEvent Store (TES). The HLT software Design Document [4] already foresees a TES-based framework.

The evaluation of alternatives to Athena is beyond the scope of this note.

## 2 Athena as an Online Event Selection Framework

### Scope of Validation

Both functional and performance aspects need to be considered when evaluating Athena as an HLT candidate framework. Since the ATLAS offline software is in an early development stage, it is impossible to conclude today whether Athena is a reasonable basis for the HLT software or not. However, it is essential to evaluate Athena now with an HLT point-of-view, so that weaknesses and problems can be identified early.

The functional aspects of Athena must be considered in the context of the HLT software requirements [6, 7]. Although it is too early for such a functional evaluation, the validation of Athena must include an appraisal of its capabilities as a trigger framework.

In addition to fulfilling the functional requirements \(\mathbf{-}\) and since the online software is a mission-critical application \(\mathbf{-}\) the Athena framework must be designed and use core software components that are fast, flexible, and reliable.

### Caveats on Athena Validation

The HLT requirements will eventually translate into a detailed architectural design of the trigger software. Until such a design is available, only preliminary evaluations of Athena can be made. This note is based on a very simple "computing model" of the Event Filter. This model, shown in Figure 1, incorporates the basic elements of event selection at the Event Filter:

* Reconstruction algorithms: imported from the offline and including, e.g., steering, calorimetry, tracking.
* Dataflow and data model: a critical component since it controls how much and in what way data is organized and accessed.
* Raw data conversion: conversion of event data from raw Event Builder (EB) format to a format that algorithms understand (using the EDM).
* Meta-data handling: includes "low-frequency" components like configuration, calibration, monitoring, and run control.

However, presently all of these components are either under development or in a prototype stage. In this note we concentrate on the more critical "high-load" components, including algorithms, data flow, and raw data conversion.

## 3 Performance Measurements

As mentioned in Section 1, one of the aims of measuring the performance of the software is to provide a "first data-point" in order to gauge whether development of Athena components that are relevant for the HLT is headed in the right direction. Many measurements of ATRECON-based HLT software exist (see [1] and references therein). However, performance measurements of Athena and Athena-related components must be brought - at least - to the same level as presented in the HLT/DAQ/DCS Technical Proposal [1].

Performance measurements of the HLT framework should include, for example, measurements of:

* Execution time
* Memory utilization
* I/O requirements

Ideally, the above should be measured using a representative data sample on a full or a prototype HLT system. The measurements presented in this note are mainly on execution time, although memory utilization issues will be addressed.

A decision on the suitability of Athena as an HLT framework will follow only after the HLT software is mature and its performance is well understood. The consequences of not adopting the offline framework and associated reconstruction algorithms for the Event

Figure 1: A validation perspective of Event Filter components. The domain of the EF considered here falls within the dotted lines.

Filter need to be carefully examined. Adopting a different framework at the HLT would demand much more manpower than presently available, it would increase the complexity of the ATLAS software, and it would slow the transfer of new physics selections to the online system.

### Benchmarking tools

In order to measure execution time, the HLT prototype code has been instrumented with the TAU profiling tool [12]. More details on TAU and technical aspects of the instrumentation can be found in Appendix A.

All available components of a prototype EF system (shown in Figure 1) have been instrumented, including:

* The Event Filter byte stream converter [5] (only the Silicon and Pixel detectors were available at the time of this study);
* The event data model ("RD Event");
* All Gaudi base libraries and selected Athena components;
* One full set of algorithms, including XKalman++ and the calorimeter reconstruction.

Since the ATLAS software is developing at a fast rate and since the instrumentation work is very time-consuming, the instrumentation has only been carried out on officially tagged and released ATLAS code. This approach should help ensure reproducibility in benchmarking. Table 1 shows the ATLAS software versions that were instrumented for this study.

### Platforms and data sets

While development work was carried out in two different machines, the results shown here were obtained in a 733 MHz Pentium III machine with 512 MB of RAM, running Linux 6.1 (Standard CERN installation). The machine is a standard desktop residing at CERN in building 32. The equivalent SpecInt95's for this machine is approximately 30.

The data sets used for benchmarking Athena were \(Z\to\mathrm{b}\overline{\mathrm{b}}\) events and jets (electron stream). The latter sample was used in profiling the algorithms. Samples with and without pileup were used.

\begin{table}
\begin{tabular}{|l|c|c|} \hline Package & Athena version & Gaudi version \\ \hline \hline EFTDRCnv & 2.0.2 & 0.7.4 \\ \hline RD Event & 2.0.2 & 0.7.4 \\ \hline XKalman+Calo. & 1.3.5 & 0.7.2 \\ \hline \end{tabular}
\end{table}
Table 1: ATLAS release versions used in TAU instrumentation.

### Profiling Results

In the following, the measurements performed on Athena, and related components, are summarized. Since we are presently most concerned with Athena's steady-state performance in the High Level Trigger framework, all methods that are solely part of the initialization or finalization phase of program execution were excluded from the analyses described in this section.

The absolute execution times presented in this section, even today, do not reflect the best possible performance of the software. The source code has not been optimized for performance and the executables have not been built with optimal compilation flags. However, the scaling behavior and the measurements of the relative performance of the components are not affected by this lack of optimization.

#### 3.3.1 Algorithms (electron-ID)

The electron ID test example of the LAR group [11] was used as an example for a typical algorithm application. Based on Atlas release 1.3.5/1.3.6, it was integrated with StoreGate2 and included the complete reconstruction chain from the raw data handling to the final identification of electrons. As mentioned above, the code was not run in an optimized form, In addition, due to a memory leak not more than 30 pileup events could be processed. This memory leak should also affect the execution times, so that any total times should be taken with caution.

Footnote 2: This is the version referred in Section 3.3.4 as “old StoreGate”.

The algorithm was executed in the following configurations:

* On single electron data;
* On jet data;
* On jet data with pileup;
* On jet data with pileup and a \(p_{\perp}=15\) GeV cut for the calorimeter and a \(p_{\perp}=5\) GeV cut for XKalman++;
* On jet data with pileup with a seeded reconstruction from the calorimeter3. Footnote 3: At the time of the test the code to do this was not yet officially released. A private version was used \(|10|\).

In the first four cases the reconstruction was attempted for the complete detector; in the last case only in a certain region defined by the seed.

Detailed profiles for each case can be found in Appendix B. It can be observed that typically the execution times are dominated by methods for building the calorimeter information. The contribution to the total execution time from reconstruction algorithms like Xkalman++ is very small in the case of high \(p_{\perp}\) electrons but it dominates for pileup events, where reconstruction in the complete detector (B physics case) was attempted. With increased \(p_{\perp}\) cuts and with seeded reconstruction for events with pileup, the execution times are again dominated by methods for building the calorimeter information.

The total execution times for the algorithm, run in the aforementioned configurations, were: 2.35 sec./event (electrons), 7.35 sec./event (jets), 80 sec./event (jets with pileup), 31 sec./event (jets with pileup, \(p_{\perp}\) cuts), and 24 sec./event (jets with pileup, seeded reconstruction).

#### 3.3.2 EFTDREventCnv

The _EFTDREventCnv_ package [5] is an Athena service designed to convert packed raw event persistent data, in the format expected from the Event Builder (EB), into a form suitable for access from the TES of Athena. To simulate the input from the EB several samples of packed raw events were produced for both high and low luminosity using another package called _TE2REConverter_ (See Figure 1.)

The _EFTDREventCnv_ package was instrumented with TAU along with a copy of the Gaudi software. TAU profiling output was produced by running the instrumented package via a simple test algorithm over two different packed raw event data samples: one with 100 low luminosity \(Z\to\mathrm{b}\overline{\mathrm{b}}\) events and one with 20 high luminosity jet events. For each run the resulting profile output file was used to extract the following information for all steady-state methods of the _EFTDREventCnv_ package called during the given run:

* Average total exclusive time consumed per event (including multiple calls to the method).
* Average exclusive time consumed per call to the method.
* Average total inclusive time consumed per event (includes total exclusive times of all subroutines called by the method).
* Average number of calls to the method.
* Average number of subroutine calls made by the method.

Similar information was obtained for all steady-state methods of the Gaudi framework that were called during the run. Figure 2 shows a sample profiling output from TAU.

Analysis of the profile data is discussed in the following sections.

3.3.2.1 Low Luminosity ResultsThe average total real time consumed per event was measured to be approximately 470 ms. The largest contribution to the total exclusive time (231 ms) came from some encapsulated methods of the _EFTDREventCnv_ package called _GetID_ which are used to extract the offline identifier of a Digit from its online representation. Although the average exclusive times per call for these methods were quite small (\(\simeq 23\mu\)s) they had to be called for every Digit. The profile output shows that the average number of Digits for the Pixel and SCT detectors in the low luminosity data sample are approximately 2,800 and 7,000, respectively.

The second largest total exclusive time (101 ms) was consumed by methods of the _EFTDREventCnv_ package called _UnpackRo b_, which performs the following tasks: Loop over each of the event's RODs; For each ROD, loop over all of its Digits; Unpack each Digitand 2 data words and get its offline identifier (via a call to _GetID_); For each offline identifier corresponding to a new wafer, a SiDetector object is created; For each Digit, a new SiDigit object is created and added to the SiDetector object. Then a pointer to the latter is inserted into a map using the offline identifier as its key. Similarly to the _GetID_ methods, the average exclusive time per call for the _UnpackRob_ methods was relatively small (0.38 ms for Pixel and 1.00 ms for SCT) but they had to be called for an average of 68 RODs for the Pixel detector and 64 RODs for the SCT detector. In addition, for each ROD, the above mentioned operations had to be performed.

The third largest contribution to the total exclusive time (46 ms) came from a single call to the method _EFTDREventSource::next_event_. This method is dominated by uninstrumented ATLAS software. Further analysis using time stamps showed that most of the time is consumed by the Event::accept method, which fills the detector hierarchy structure with the information of a given event. This software is part of the old event data model ("RD Event"), which will eventually be replaced.

The time taken to read in the packed raw event from the input stream was measured to be 0.76 ms. The total average time consumed per event by all of the Gaudi methods called was approximately 12 ms, which is less than 3% of the total average event time.

3.3.2.2 High Luminosity ResultsThe average total real time consumed per event was measured to be approximately 4900 ms. This is an order of magnitude greater than the average total time consumed per event at low luminosity.

The largest contribution to the total exclusive time (2200 ms) again resulted from the _GetID_ methods. This is about 10 times greater than the time consumed in the case of low luminosity, which is consistent with the fact that there are about 10 times more Digits for the pileup date sample: \(\simeq 26,000\) for Pixel and \(\simeq 64,000\) for SCT. The average exclusive time per call remained the same at 23\(\mu\)s.

This time the second largest contribution to the total exclusive time (1600 ms) came from the single call to the method _next_event_. This is about 35 times greater than the time consumed for the low luminosity case and therefore it does not scale with the increased

Figure 2: Profiling results for _EFTDREventEnv_ for low luminosity \(Z\to\) b\(\bar{\mathrm{b}}\) events.

number of Digits. Again, this method is dominated by software from the "RD Event" event data model, which is discussed in Section 3.3.3.

The third largest total exclusive time (830 ms) was consumed by the _UnpackRob_ methods. The average number of RODs accessed by these methods did not increase very much from the low luminosity case: 80 RODs for Pixel and 70 RODs for SCT. However, as mentioned above, the number of Digits increased by an order of magnitude for the high luminosity case which accounts for the factor of 10 increase in the total average exclusive time. The average exclusive times per call of the _UnpackRob_ methods for pileup (3 ms for Pixel and 9 ms for SCT) are also consistent with the increase in the number of Digits.

The time taken to read in the packed raw event from the input stream was measured to be about 12 ms which scales well with the increased number of Digits. The average time consumed per event by the Gaudi framework was approximately 116 ms, which is consistent with the increase in Digits and is again less than 3% of the total average event time.

The results from the analysis of the _EFTDREventCnv_ benchmarks can be summarized as follows:

* The average number of Digits in the high luminosity sample is about 10 times greater than that in the low luminosity sample.
* The average time per event is dominated by Digit processing and scales linearly with increasing number of Digits.
* The old Raw Event Data Model makes a significant contribution to the average event time and increases non-linearly with increased pileup.
* The average time taken to read an event from the byte stream is relatively small and scales linearly with increasing number of Digits.
* The time consumed by the Gaudi framework scales linearly with pileup and makes a negligible contribution to the total event time.

Table 2 contains the results discussed in the last two sections.

3.3.2.3 Results with Time StampsIn order to obtain an independent measurement of the real time consumed per event and test the consistency of the TAU results, time stamps were placed in the test algorithm of the _EFTDREventCnv_ package.

The algorithm was executed over both the 100 low luminosity \(Z\to\mathrm{b}\overline{\mathrm{b}}\) events and the 20 high luminosity jet events. The results were used to calculate the average time consumed per event for both samples.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|} \hline Step & Input data & Read byte-stream & Unpack ROBs & Fill maps & RD Event \\ \hline \hline no pileup & 9.8K digts & 0.76 ms & 231 ms & 101 ms & 46 ms \\ \hline pileup & 90K digts & 12 ms & 2200 ms & 830 ms & 1600 ms \\ \hline \end{tabular}
\end{table}
Table 2: Execution times for the main EFTDREConverter tasks. For each task, the times are given without pileup and with design luminosity pileup. “RD Event” denotes the time it takes to fill event objects using the EDM.

In addition, the TAU profiling tool was turned off and the above procedure was repeated to get an estimate of the overhead due to TAU itself.

Relatively large variations in event-to-event execution times were observed. In addition to the variations due to event occupancy, an additional execution time variation was observed by running the code on the same event multiple times. Figure 3 shows such a spread when executing the converter on a few low luminosity events. For example, for a single event, 5% of the time the measurement will yield a latency 50% higher than "normal". This same event latency tail can be attributed to system- and OS-related contributions, e.g., context switching. One way to minimize these contributions is to repeat the measurement a few times and quote the minimum observed latency as the final result.

The following conclusions were drawn from the time-stamped code:

* The average time per event for low luminosity is 430 ms, which is consistent with the TAU profiling result of 470 ms (see Section 3.3.2.1);
* The average time per event for high luminosity is 4800 ms, which is consistent with the TAU profiling result of 4900 ms (see Section 3.3.2.2);
* The overhead due to TAU was estimated at 15% for both the high and low luminosity data samples. This is consistent with estimates obtained from studying the profile output mentioned in section 3.3.2 with the aid of the TAU overhead specifications mentioned in Appendix A;
* OS-related overheads can contribute up to 50% of additional latency per event, skewing the performance measurements.

Figure 3: Fraction of events as a function of measured time using time stamps for low luminosity \(Z\to\mathrm{b}\overline{\mathrm{b}}\) events. To generate this plot, 8 events were processed with _EFT-DREventCnv_ 10 times each. For each event, the 10 execution times were then normalized to the minimum execution time. The figure shown is the integral of the latency distribution.

#### 3.3.3 RD Event

The electron ID application was used to estimate the overhead due to the present Atlas Event Data Model. In addition to the standard libraries, also the code for the present EDM was instrumented with TAU. However, due to the design of the EDM software, certain sections of the code with fast execution times were called very frequently during a typical run. They, in turn, caused considerable distortions of the time profiles in the case of a complete instrumentation of the EDM due to the additional TAU overhead (see Appendix A). TAU was therefore only used to identify the main entry and exit points to the EDM, which were then instrumented to get an estimate of the integral time spent in the EDM during a run.

In the case of pile-up events, about 30% of the total execution time was spent in the EDM code. However, it should be mentioned that the present EDM also handles certain geometry information, so that the access time for this is included in the above estimate.

#### 3.3.4 StoreGate

Two versions of the Atlas Transient Event Store (TES) management software StoreGate (StG) have been evaluated and compared to the standard Gaudi TES implementation. A first implementation of StoreGate on top of the existing Gaudi TES implementation (subsequently called "old StoreGate"), as it was available in Athena release 1.3.2, was first evaluated. An improved version (subsequently called "new StoreGate"), which was available as a private implementation4 at the time of this test, was also evaluated. The software was not compiled in an optimized form; however, all tests were done with the same settings on the same machine so that relative comparisons are still valid. The software was instrumented with TAU to get an idea about the distribution of execution times among the different modules. When possible, timing measurements were done with the uninstrumented software and simple timestamps in dedicated places to avoid distortions of the execution times due to TAU overheads.

Footnote 4: The functionality of this version is now available in the official Atlas software since the recent Atlas release 2.4.1

The test algorithm first stores \(N\) objects in the plain Gaudi TES and reads them back afterwards. The time for each step is measured with the Athena ChronoSvc. The same procedure is then repeated in the same run with StoreGate as the TES management software. Running the two tests in the same job exposes the last run test to all objects stored by the first test in the TES, i.e if the pure TES test is run first and the StoreGate test afterwards, the StoreGate test will be exposed not only to the objects created by itself but also to the objects created by the TES test before.

In the case of the "old StoreGate" implementation the following observations can be made:

* The execution time did not scale linearly with the number of handled objects (see Figure 4).
* The measured times for StoreGate were completely different if the TES test was run first or the StoreGate test was run first (Column 3 and 4 in Figure 4 show different heights).

* If TES and StoreGate tests were run individually, so that no interaction could take place between them, the time to store and retrieve objects with StoreGate was about five times the time needed by the original Gaudi TES implementation. This was due to 5 times more calls to the "Gaudi RegistryEntry" module in the "old StG" implementation when searching through objects for retrieving.

The same tests were repeated with the "new StoreGate" implementation in the same environment. The main observations were now:

* TES and StoreGate showed the same total execution times (see Figure 5).
* There was no interaction anymore between the StoreGate and the TES tests if they were run one after the other in the same job (Column 3 and 4 in Figure 5 have almost the same height).
* Registering objects was very fast for the "new StoreGate" : 1 ms for StG compared to 16 ms for the plain TES with 1000 objects of the same type (see Figure 6).
* Reading back objects with a key was about two times slower with StG than with the plain TES (see Figure 6).

Figure 4: Total access times for registering and retrieving \(N\) objects of the same type with TES or the “old StoreGate” implementation. The 4 columns show the access times for running the TES test only (TES only, column 1), the StG test only (StG only, column 2), first the StG test and then the TES test (StG+TES, column 3) in the same job and finally the reversed combination (TES+StG, column 4). Since in this implementation of StG also the objects which have been registered by the TES test are exposed to StG the columns 3 and 4 don’t show the same access times.

* The total time for registering and reading back objects was the same for TES and StG. With the TES it took about the same time to register and retrieve an object, whereas in the case of StG registering an object was much faster than reading back an object (see Figure 6).

The above tests were all done with \(N\) objects of the same time. For the more realistic case of \(N\) objects of different types the "new StG" implementation showed a performance advantage over the plain TES implementation.

* For reading back e.g. 1000 objects of 5 different types (= 5000 objects in total) a performance advantage of the "new StG" respective to the TES was observed (see Figure 7): \[\mbox{newSTG read}(N\mbox{ objects of }M\mbox{ types})\simeq 0.3\cdot\mbox{TES read}(N\mbox{ objects of }M\mbox{ types})\]
* The approximate scaling of access times for reading back \(N\) objects of \(M\) different types (= \(N\cdot M\) objects in total) were (see also Figure 7) in the case of the TES TES read(\(N\cdot M\) objects of 1 type) \(\simeq\) TES read(\(N\) objects of \(M\) types) and \[\mbox{newStG read}(N\cdot M\mbox{ objects of 1 type})\simeq M\cdot\mbox{newStG read}(N\mbox{ objects of }M\mbox{ types})\] for the "new StG".

Figure 5: Total access times for registering and retrieving \(N\) objects of the same type with TES or the ‘new StoreGate” implementation. The 4 columns show the access times for running the TES test only (TES only, column 1), the StG test only (StG only, column 2), first the StG test and then the TES test (StG+TES, column 3) in the same job and finally the reversed combination (TES+StG, column 4).

## 4 Implications for the HLT

### Conclusions of validation exercise

The bottom line when evaluating Athena for online use is whether it adds undue resource requirements to the trigger system. A first measurement of these overheads, in the context of Figure 1, was presented in Section 3.

It is worth noting that the notion of what is a "reasonable overhead" can be difficult to define. Depending on how the various system components factorize, it can be very difficult to cleanly separate algorithmic work from data movement work. This is especially important in the case of the Data Model, where the data organization and access methods have intelligent - and thus algorithmic - components. For example, if a data model is very "smart", it can make event selection work more efficient while making the data model appear slow. This performance trade-off needs to be optimized during the ATLAS EDM and the HLT selection software design process.

One way to place bounds on what a reasonable overhead should be is to compare with other frameworks or experiments. For the electron identification studies presented in the HLT Technical Proposal [1], the data access and framework overheads of ATRECON amounted to 40% to 50% of the algorithm execution time. Similar overheads in BaBar amount to 10% to 15% [8].

Figure 6: Access times for registering and retrieving \(N\) objects of the same type with the TES or the “new StoreGate” implementation. For the access to the TES almost equal times for registering and reading objects are observed, whereas in the case of the “new StG” the time is dominated by reading back the objects. The time to register objects with the “new StG” is almost invisible in the plot.

Section 3.3.3 shows that the EDM benchmarked in this study can take up to 30% of the total execution time for an event. This is a very large fraction of the CPU budget for selecting events and is an area that should be monitored closely.

A question that must be answered, however, is how smart does the EDM need to be for the HLT (this is being addressed by the PESA Requirements Document [6]). The danger is that the "offline EDM" may incorporate functionality that - while essential for the offline task - is not necessary for the HLT. This excess functionality may, in turn, degrade the performance of the HLT software. This is especially crucial for the HLT because the HLT software suffers the inefficiencies of the EDM in two fronts: in the byte-stream conversion to EDM format; and in algorithm access to the data.

Another area that must be monitored is data unpacking. As shown in Section 3.3.2, converting raw digits to an offline format can take considerable time; typically unpacking times are more than algorithm execution times. Pure framework overheads, that is, overheads due to Gaudi/Athena methods, consume just a few percent of the overall budget, as shown in Section 3.3.2.

Clearly, as of today, the ATLAS offline software is not good enough to be used as an HLT event selection framework. However, this is not unexpected since ATLAS software is in an early development phase. The HLT validation work must ensure that Athena development follows a course compatible with the HLT performance requirements.

Ultimately, the decision on whether an overhead is acceptable will rely more on costing constraints than on code efficiency arguments. However, the same "performance vs. cost" issues that the HLT is facing today will be faced eventually by the offline. In this sense, the HLT software can be a "testbed" for the offline community.

Figure 7: Comparison of access times for registering and retrieving n objects in total of the same type and of 5 different types with the TES or the “new StoreGate” implementation.

### Recommendations

More work needs to be done before making an informed decision on the suitability of the offline software for the HLT. Although we are years away from full deployment, a few steps taken today can ease tremendously the task of using the offline software (or a subset thereof) in the High Level Trigger.

First, the functional evaluation of the offline software should be an integral part of the validation exercise. Compliance with requirement documents (e.g., [6], [7]) should also be monitored. There are a few requirements of PESA SW that are particularly important for the validation work:

* **modularity:** We should start to test framework modularity now. For example, one should be able to build and run Athena without certain services. (Experience says things do not become modular, they start modular.)
* **independence:** We must be able to build Athena (and its components) completely stand-alone, with all libraries, auxiliary files, and services on a local processor. The possibility to build a stand-alone version should be a standard option in the regular builds. Otherwise, we need a special EF repository.
* **Athena light:** We should have a minimal version of Athena (related to modularity above). We should be able to build the Athena "kernel" and nothing more (just an event loop for example), in order to be able to replace services at will. Once the core functionality (minimal set of ATLAS components) is factorized, performance evaluations will be much easier.

The strategy of evaluating the system performance of development code needs to be accepted as a valid approach in validation. The HLT cannot afford to wait until a fully-optimized and finalized version of the offline software is available. This strategy has the distinct advantage of ensuring that the offline software performance improves by providing concrete feedback to the offline community. A recent instance of this feedback is documented in Section 3.3.4.

Once the ART migration is complete in ATLAS, the offline code should be instrumented with a profiling tool. All components relevant to the HLT must be fully instrumented (e.g., reconstruction, StoreGate, converters). It is not necessary to use TAU -certain compilers have sufficient profiling options. This instrumentation should be part of a standard release, and a user ought to be able to turn it on or off at will. System metrics, both CPU and memory utilization, should be integral parts of the offline code. It should also be possible to correlate these metrics to events or to sections of an event.

Ideally, the profiling work documented in this note should be carried out in the ATLAS offline community. It should also fall in the domain of code quality control. Code developers should also have access to the profiling results - they are the people best suited for optimizing their code. Creating a standard "test suite" that is executed after each release may help automate this procedure. Achieving quality software (in terms of system performance) will be ensured by integrating this activity in the offline software early enough. This, in turn, will guarantee an efficient spending of the CPU budget, both for the HLT and for the offline.

The High Level Trigger will probably only need a subset of the full offline functionality in the online environment. It should be possible to have a light version of Athena, where unnecessary functionality can be stripped away or disabled with no performance penalties. For example, neither graphic displays nor MC truth bookkeeping will be needed in the Event Filter farms. This requires a high level of modularity in the framework.

On the HLT side, we need to create a "Trigger Coding Standards" document or guidelines that go beyond proper naming conventions and documentation requirements. These guidelines would address known programming practices that - although possibly elegant - may make trigger code slow and inefficient5.

Footnote 5: This is of course a very difficult task since it depends on programming language, OS, platform, etc.

The final evaluation of Athena as an online framework will have to rely on a detailed computing model - and an implementation - of the HLT selection processing. The HLT community must provide such a model, including:

* Handling, frequency, and direction of meta-data flow
* Realistic environment description (e.g., what is disk-resident, memory-resident, and network-resident)
* Ultimately, performance constraints
* List of Athena/Gaudi components relevant for HLT ("Athena Light").

A few specific recommendations can be gleaned from this report:

* The EDM performance is critical to the success of the HLT. It must therefore be closely monitored. The final EDM design should be developed with the HLT functional and performance requirements in mind, as the HLT is the weakest "performance link".
* Adding the e-ID execution times of Section 3.3.1 to the raw data conversion times of Section 3.3.2 yields a first rough estimate of EF execution time for electrons: 5 s (30 s) at low (high) luminosity. Correction factors of 1.8 (3) for low (high) luminosity were applied to the algorithm execution times to account for the expected gains from a seeded reconstruction [9]. At low luminosity, this is equivalent to 0.8 s on a 180 SI95 machine. This extrapolation to 2006 performance is a factor of five [14] higher than the budgeted CPU cycles for this trigger. A roadmap to reach this performance should be provided by the offline community6. Footnote 6: This is of course a very difficult task since it depends on programming language, OS, platform, etc.
* Performance goals should be set for the ATLAS offline software. These goals should be monitored at each major release.

## 5 Summary and Outlook

No hard conclusions can presently be drawn - given the state of the software - on Athena's suitability as a High Level Trigger event selection framework. However, by starting a program of measurements now, the HLT community can provide feedback to the offline community on possible areas for optimization.

The short term aim of this work is to establish software metrics to be monitored. The longer term aim is to ensure that these metrics help point the way to efficient software that satisfies the stringent performance requirements of the HLT.

The HLT validation studies so far have helped in optimizing the performance of the TES access interface (StoreGate). They have also shown that the Event Data Model is a performance-critical component of the event selection and that it must be monitored carefully. The raw data converter, unique to the Event Filter, is another component whose performance will be critical to the success of the Event Filter.

This note contains a snapshot of the performance of the offline code at an early stage of development. This one "data point" will be used as the first reference point from which to track performance improvements over time. The next "data points" will be: the new StoreGate, the full set of converters, and the new EDM.

Once a suitability decision is reached on Athena, two important points must be considered. If Athena is validated as an HLT framework, then an understanding must be reached with the offline community on the need and implications of maintaining Athena's suitability. If instead Athena is found to be unsuitable for the HLT task, then an alternative solution must be developed. Furthermore, in such a case, the implications on manpower and future maintenance must be considered.

## Appendix A Profiling with TAU

TAU [12] is a free profiling tool, which can collect information in a distributed computing system, it can deal with multi-threaded programs and it can handle programs using shared libraries. TAU was also used for performance evaluations for the Atlas second level trigger reference software [13].

Code instrumentation can be done in two ways

* Source code instrumentation : Each source code file of the program parts which should be instrumented has to be analysed by a sequence of special preprocessing steps. They automatically modify the original source by inserting the necessary calls to the TAU profiling API [15]. The newly generated instrumented source files can then be compiled and linked in the same way as the original program.
* Dynamic instrumentation : Here the executable object code of the program is modified by TAU. The main advantage is that code can be profiled without compilation and access to the program sources is not required. This instrumentation version, however, was at the time of writing still marked as "experimental" by the TAU developers and was not used for the studies presented in this note.

A basic installation of TAU for source code instrumentation requires besides the distribution kit for TAU itself also the distribution kit for the "Program Database Toolkit" (PDT) [16]. It contains the necessary preprocessors for analysing the source code and producing the "program databases", which are then used for automatic source instrumentation in TAU. For the studies presented in this paper version 2.9.13 of TAU and version 1.3 of PDT were used.

### Source Code Instrumentation for Gaudi/Athena

The calls to the TAU profiling API can either be inserted by hand in the source code or automatically with a sequence of preprocessors available from the TAU and PDT tool kits. For the presented studies the second method was used.

The automatic instrumentation procedure inserts macro statements with the calls to the TAU profiling API in the source code. These macro statements expand to zero when no profiling options are used in the compile step. This makes it easy to produce instrumented and not instrumented versions of a program from the same source code.

For automatic instrumentation the normal compile and link step has to be augmented by the following sequence of preprocessing steps for a typical source file (_fn.cxx_):

* cxxparse fn.cxx <options> _output files: fn.il, fn.pdb_ In cxxparse the original source file _fn.cxx_ is first parsed with the EDG compiler front end edgcpfe[17] and the results are stored in an "intermediate-language tree" file _fn.il_. This file is then analysed by taucpdisp and informations about the program structure are written to a "program database" [16] file _fn.pdb_ for further use with tau_instrumentor.
* tau_instrumentor fn.pdb fn.cxx _output file: fn.inst.cxx_ tau_instrumentor creates from _fn.pdb_ and the original source code file _fn.cxx_ an instrumented source code file _fn.inst.cxx_, which contains macros with calls to the TAU profiling API.
* g++ <options> <TAU_options> fn.inst.cxx _output file: fn.inst.o_ link step with fn.inst.o (+ TAU libraries) _output: executables, libraries_ Builds with the instrumented source code files the instrumented versions of the object files, the executable programs and the (shared) libraries. If the TAU_options are omitted, not instrumented program versions are produced.

For the Atlas software this sequence can be automatically initiated by overwriting the normal compile and link process in the software release tools used by Atlas. At the time of writing these are, CMT for the management of the Gaudi base libraries and SRT for all the other Atlas code7.

Footnote 7: At the time of writing an evaluation is ongoing with the aim to make CMT as source code manager available for the complete Atlas codebase.

* For CMT a package "InstrumentTAU" was created, which automatically instruments a source package when included in the respective requirements file of the package. Typically this is done with a CMT use directive use InstrumentTAU v* The package "InstrumentTAU" and more instructions together with an instrumented example of the Gaudi base libraries can be found at [18].
* For SRT _Makefile_ fragments have been created. They have to be manually inserted in the _Makefiles_ of the packages which should be instrumented. Instructions and examples can be found at [18].

Since the EDG compiler front end is used to parse the original C++ source code to produce the "intermediate-language tree" files and the "program database" files, the source code must support this front end as an additional compiler platform. This may produce parsing errors on the different platforms if e.g. not the same language set is implemented on both platforms. Most of the parsing problems with _edgepfe_ with respect to _g++_ arise therefore from different resolution strategies for templates, from different implementations of the STL library and from language enhancements only supported by _g++_ and not by _edgepfe_.

### TAU overheads

The calls to the profiling API which are inserted in the source code produce an additional time overhead which may significantly distort timing profiles for frequently called program parts. As shown in Figure 8, the time overhead introduced by a typical call to the profiling API was measured on an Intel Pentium III (733 MHz clock frequency) as \(\simeq 1\,2\,[\mu s]\) for function registration and first call of the API within a function and as \(\simeq 1-2\,[\mu\,s]\) for all subsequent calls. The time for registration depends on the length of the function name and on the function type. The numbers are in good agreement with similar numbers [19] quoted by the TAU developer team for measurements on SGI machines, namely \(8-40\,[\mu\,s]\) for function registration and \(\simeq 0.8-1.6\,[\mu\,s]\) for all subsequent calls.

## Appendix B Profiles for Electron ID

In the following profiles always the top 20 time consumers are shown for the respective ordering of the profiles.

Figure 8: TAU execution time as a function of number of calls..

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_EMPTY:24]

## References

* [1] The HLT/DAQ/DCS Technical Proposal, CERN/LHCC/2000-17
* [2][http://proj-gaudi.web.cern.ch/proj-gaudi](http://proj-gaudi.web.cern.ch/proj-gaudi)
* [3][http://atlas.web.cern.ch/GROUPS/SOFTWARE/OO/architecture](http://atlas.web.cern.ch/GROUPS/SOFTWARE/OO/architecture)
* [4] HLT Software High-Level Design Document (in preparation)
* [5][http://atlas.web.cern.ch/Atlas/GROUPS/DAQTRIG/EF/documents/EFTDREventCnvSvc.pdf](http://atlas.web.cern.ch/Atlas/GROUPS/DAQTRIG/EF/documents/EFTDREventCnvSvc.pdf)
* [6] PESA SW group, PESA High Level Trigger selection software requirements, ATL-DAQ-2001-005
* [7] EF requirements Document (in preparation)
* [8] E.Frank, Private Communication.
* [9] J. Baines _et al._ First Study of the LVL 2-EF boundary in the high-\(p_{\perp}\) electron/photon High Level Trigger, ATL-DAQ-2000-045
* [10] M.Wielers, Private Communication.
* [11][http://atlas.web.cern.ch/Atlas/GROUPS/LIQARGON/software/Reconstruction](http://atlas.web.cern.ch/Atlas/GROUPS/LIQARGON/software/Reconstruction)
* [12][http://www.acl.lanl.gov/software](http://www.acl.lanl.gov/software) ; Instrumentation and Measurement Strategies for Flexible and Portable Empirical Performance Evaluation, S.Shende, A.D.Malony, R.Ansell-Bell, Department of Computer and Information Science, University of Oregon; [http://www.cs.uoregon.edu/research/paracomp/tau](http://www.cs.uoregon.edu/research/paracomp/tau)
* [13] Performance Analysis of the ATLAS Second Level Trigger Software, J.A.C Bogaerts et al., paper submitted to the Real Time Conference 2001
* [14] Summary of Standard Values
* [15] TAU User's Guide, version 2.9, Department of Computer and Information Science, University of Oregon, [http://www.cs.uoregon.edu/research/paracomp/tau](http://www.cs.uoregon.edu/research/paracomp/tau)
* [16][http://www.acl.lanl.gov/software](http://www.acl.lanl.gov/software), Program Database Toolkit, PDT
* [17] C++ Front End, Internal Documentation, Edison Design Group, Inc., Dec 26, 2000, Version 2.45
* [18][http://www-wisconsin.cern.ch/~wiedenma/Profiling](http://www-wisconsin.cern.ch/~wiedenma/Profiling)
* [19][http://www.acl.lanl.gov/tau](http://www.acl.lanl.gov/tau) \(\rightarrow\) Docs \(\rightarrow\) FAQ