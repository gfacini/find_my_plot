## 1 Roadmap towards future combinations and Effective Field Theory interpretations of top+X processes

The ATLAS Collaboration

This document describes the challenges of combining top+X measurements to produce coherent probes of the Standard Model predictions and Effective Field Theory (EFT) interpretations in the ATLAS experiment. Different approaches for combinations and EFT parameter extractions are outlined, and prerequisites on the harmonisation of physics objects and phase-space regions are described. A plan for the top quark sector is prepared with steps of increasing complexity and potential, for the interpretation of future measurements.

## 1 Introduction

The Large Hadron Collider (LHC) has a wide program of direct searches for physics beyond the Standard Model (BSM), making use of its unique reach in energy. However, these searches have not shown any strong evidence for BSM physics and new particles might only exist outside the mass range directly accessible at the LHC. In order to increase the discovery reach of the LHC, it is therefore essential to pursue in parallel indirect BSM searches. If new particles are too heavy to be directly produced at the LHC, they could leave imprints in the kinematic distributions of the Standard Model (SM) particles via virtual effects or interference. Precise measurements of total cross-sections and differential distributions are therefore essential and should be compared to SM predictions with the hope to reveal glimpses of BSM effects in the interactions between SM particles.

One powerful framework to parameterise, identify, and constrain potential deviations from the SM predictions is the Standard Model Effective Field Theory (SMEFT) [1; 2; 3]. This framework describes a low-energy approximation of a more fundamental theory involving interactions at a higher mass scale \(\Lambda\). Assuming that the SM is a low-energy approximation to a higher-energy theory, additional higher-order terms can be added to the SM Lagrangian to approximate minor deviations from SM originating from the fundamental theory at lower-energies.

This note focuses on EFT interpretations of measurements of processes where one or more top quarks are produced in association with other particles, labelled X (top+X processes), such as \(t\bar{t}Z\), \(t\bar{t}W\), \(t\bar{t}\gamma\), \(t\bar{t}H\), \(tZq\), \(t\gamma q\), \(tHq\), \(t\bar{t}t\bar{t}\) and \(t\bar{t}\)+heavy-flavour (HF)/light-flavour (LF) jets. Note that "q" refers to light quarks. Experimental measurements directly testing the coupling of the top quark to other SM particles became possible at the LHC. The current and future ATLAS data sets will provide a great opportunity to study top+X processes in more depth. This note discusses possible combinations of multiple top+X measurements in a consistent way for an EFT interpretation where the effects of the EFT operators in all considered top+X measurements are included. These combinations can be challenging and careful harmonisation of physics objects as well as phase-space regions are essential.

The motivation for this document and the harmonisation of the top+X processes for future ATLAS analyses is provided in Section 2. An overview of the different experimental approaches to an EFT fit are detailed in Section 3. Recommendations on the usage of Monte Carlo simulations with EFT models are described in Section 4. Section 5 outlines the motivation and approaches to harmonisation of the physics objects and phase space regions. Proposed steps towards a coherent top+X EFT interpretation are outlined in Section 6. Finally conclusions are given in Section 7.

## 2 Motivation

ATLAS and CMS published a few precision differential cross-section measurements that use EFT methods to search for new physics associated with top-quark production [4; 5; 6; 7]. This document focuses on challenges particular to top+X processes [8; 9; 10; 6; 11]. Some of the approaches, as described in Section 3, are also relevant for SM interpretations and are not unique to EFT interpretations. One of the challenges in producing the best top+X measurements for SM interpretations and also for EFT interpretations from process-separated measurements is the difficulty in isolating high-purity samples of each process. For example, both electroweak \(tZq\) and \(t\bar{t}Z\) production have similar final states and contribute to the three-lepton final state where one pair of leptons is a same-flavour, opposite-sign pair with an invariant mass within the \(Z\) peak.

Similarly, same-sign dilepton and trilepton final states outside the \(Z\) peak originate with comparable probability in the SM from \(t\bar{t}H\) and \(t\bar{t}W\) production. It is difficult to reliably disentangle the multiple top+X processes, so one has to carefully harmonise the definition of the physics objects and phase-space regions, as well as plan for a consistent treatment of correlations of the systematic and statistical uncertainties, allowing for a combined study of multiple processes. Moreover, when considering BSM searches there are a number of EFT operators that affect one or more of the physics processes contributing to a certain phase space. Some of the top+X processes provide complementary information about the effects of the different EFT operators which allows for better separation between otherwise degenerate operator variations. It is therefore essential to analyse the impact of these operators simultaneously across all components. This note discusses these challenges in detail along with proposed solutions towards future top+X combinations for a coherent EFT interpretation. It should be noted that measurements optimised to observe a given process might not be optimised for sensitivity to EFT operators and alternative approaches to EFT interpretations are available and are presented in the note.

## 3 Experimental techniques for EFT interpretations of top+X measurements

This section describes the two most frequently used approaches for EFT interpretations. The first approach relies on parameterising the predictions of the EFT contributions directly in the measured distributions at the detector level, see e.g. Ref. [12, 13]. The second approach estimates the impact of the EFT contributions on differential cross-section measurements obtained from unfolded data, i.e. where the detector acceptance and resolution effects are removed from the measurements, see e.g. Ref. [14]. The following sections describe the individual methods, their advantages and disadvantages.

### Detector-level fit

Each bin of a detector-level differential distribution can be parameterised using an EFT prediction obtained from the MC simulation. The parameterisation can be provided for every process contributing to the differential distribution, this can either be a SM process with an EFT extension or a process that is negligible in the SM but has a non-negligible EFT contribution. The parameterisation changes the parameter of interest from the cross-section into an extraction of the central values and uncertainties of the EFT parameters. Fitting the combined prediction of the processes to the observed data leads to the extraction of the EFT parameters or to setting limits on them.

The main advantage of this approach is its simplicity. As long as the bins of the differential distribution do not share the same events, i.e. are statistically uncorrelated, this simple approach coherently propagates the impact of EFT operators which affect multiple processes. Furthermore, since the extraction of the EFT parameters happens on the detector level, all acceptance and selection-efficiency effects are properly accounted for via the MC simulation of the individual processes. Additionally, the detector-level fits enable the optimisation of the observable used in the fit to have maximum sensitivity to a given EFT coefficient e.g. using machine-learning techniques. A similar machine-learning approach is, generally speaking, not suitable for an unfolded measurement due to the difficulty of applying the observable definition at the truth-level.

The main disadvantage of the approach is its inflexibility in the reinterpretation of the results. The method relies heavily on the MC simulation, thus any update in the theory prediction either from the SM prediction or from the EFT prediction requires rerunning the full chain of the simulation, including the detector-level simulation and reconstruction. Hence, to update the obtained result with an improved prediction, a significant effort is needed to propagate the improvements to the final result. One possible mitigation of the inflexibility could be to provide the results publicly in such a way that the whole analysis can be automatised (see e.g. Ref. [15]), to a large extent allowing the full analysis chain to be rerun by a third party.

### Unfolding

Several statistical techniques are available to unfold a differential cross-section measurement, i.e. undo the detector effects, to a stable particle or parton1 level [16, 17, 18]. Both parton and particle level unfolded results are commonly presented in top-quark physics analyses [19, 20, 21, 14]. Particle-level unfolding does not suffer from ill-defined truth-level objects as is the case for the parton-level unfolding, thus it is the preferred option for legacy measurements. The advantages of the unfolding approach are applicable to all types of unfolded results. Unfolded differential cross-section measurements can be used to estimate the EFT contributions by comparing the EFT prediction to the unfolded data. Several tools are available for the extraction of the EFT contribution from an unfolded distribution, e.g. Ref. [22]. The unfolded measurements are designed and validated to be robust against significant variations in the target distribution, up to some reasonable range of the variations. For example, the measurements should be tested to ensure that they can recover distributions even under the assumption of non-zero EFT coefficients within currently available limits and that they are not biased towards a given model used in the unfolding corrections. This makes the results of the unfolded distributions reliable even under updated SM or EFT predictions.

Footnote 1: The parton level, a description of physics interactions without any hadronisation or detector effects, is defined using particles directly from the matrix element, while the stable particle level is defined using particles after hadronisation that have a mean lifetime, \(\tau\), fulfilling \(c\tau>10\) mm.

The main appeal of the unfolding approach is the simplicity of reinterpretations. Since the unfolded distributions do not contain any detector effects, updated theory predictions only require the re-application of parton or particle-level selections, followed by a re-extraction of the EFT parameters. The full detector simulation and reconstruction is not required, unlike in the detector-level fits. This approach produces valuable SM results which remain impactful even if no new physics is found in the top-quark sector at the LHC.

In the standard unfolding approaches, the background contribution is subtracted from data and then unfolding corrections due to selection efficiency, event migrations and acceptance effects are applied. The main disadvantage of the unfolding approach is the traditional separation between signal and background processes that are treated differently. However, some of the same EFT coefficients could impact both signal and background processes, obscuring the differences between the categories. In particular, the acceptance effects from possible EFT contributions on the background processes are difficult to parameterise efficiently in the unfolding algorithms. Nevertheless, unfolding techniques that do not distinguish between the signal and background contributions can be used to overcome the difficulties. In the approach where no distinction is made between signal and background processes, referred to as a "multi-signal" unfolding, contributions from each truth-level bin for each process are treated as unconstrained, removing any potential biases towards the SM prediction from the background processes. Thus, when multiple processes provide significant contributions, profile-likelihood unfolding leads to less assumptions for the EFT interpretation compared to the unfolding approaches where backgrounds are assumed to follow the SM prediction and are subtracted. The result of the multi-signal unfolding procedure is a consistent set of unfolded differential cross-section measurements with a consistent set of uncertainties and correlations that can then be interpreted in terms of EFT contributions. The choice of the bin edges for the truth-level bins for each process can have a large importance on keeping the correct dependence on EFT operators and it should be part of the optimisation of the measurements. In profile-likelihood unfolding, regularisation may be used to reduce statistical fluctuations from the inversion of the response matrix. Unregularised unfolding, however, may remain viable for measurements with small migrations between the truth and detector-level bins. Migrations can be suppressed by using machine-learning (ML) techniques to identify regions which are pure in contributions from predefined truth bins, this is the approach commonly used in the simplified-template cross-section (STXS) measurements in the Higgs boson sector, see e.g. [23]. An illustration for when the multi-signal unfolding approach is beneficial is presented in Figure 1, where contributions of two intertwined processes are shown for a single phase-space region. The results of the unfolded measurements should be stored in the HEPData database [24] and with links to Rivet routines [25].

## 4 Development of Monte Carlo simulations for EFT

This chapter briefly reviews some of the currently available EFT models that are best suited to the physics programme described in this document. Possible limitations coming from the absence of higher-order corrections are also discussed.

Figure 1: Histogram illustrating the case where the multi-signal unfolding approach would be beneficial. The distribution of a detector-level object \(p_{\mathrm{T}}\) is shown, assuming that only two processes A and B contribute. These processes could be \(t\bar{t}Z\) and \(tZq\) and the observable could be the \(p_{\mathrm{T}}\) of the dilepton system. The bin number in the legend describes the bin in which the parton/particle-level dilepton \(p_{\mathrm{T}}\) value is located, i.e. bin 1 in the legend represents events falling into truth-level dilepton \(p_{\mathrm{T}}\) between 0 and 50 GeV, bin 2 between 50 and 100 GeV, bin 3 between 100 and 150 GeV and bin 4 between 150 and 200 GeV. Due to migration between the truth and the detector-level bins, each detector-level bin can have contributions from several truth bins for each physics process. Using several regions, the individual contributions from each process and each truth-level bin can be extracted from data. This approach can be extended to all contributing processes effectively treating each contribution as a “signal”.

### UFO models in the top quark sector

Two UFO2 models are of particular interest in the generic top quark sector:

Footnote 2: “Universal FeynRules Output” [26], the input model that dictates the physics (particles, couplings, Lagrangians, etc.) to be used by the Monte Carlo generator.

* SMEFTsim3.0[27, 28, 29]: a complete implementation of the lepton- and baryon-number conserving dimension-6 Lagrangian. Choices between several flavour-restrictive scenarios and two input electroweak parameter schemes are possible. It is recommended to use the top flavour assumption, which introduces \(U(2)^{3}\) symmetry in the quark sector and \(U(1)^{3}\) in the lepton sector (instead of topU31 with \(U(3)^{2}\) lepton flavour symmetry3), and the \(\{\mathrm{m}_{\mathrm{W}},\mathrm{m}_{\mathrm{Z}},\mathrm{G}_{\mathrm{F}}\}\) input electroweak parameter scheme. This model should be considered where combinations with Higgs or Standard Model analyses are planned. It is accurate only to LO in QCD, but allows for loop-induced Higgs production.
* SMEFTatNLO[30, 31]: this model allows for both LO and NLO QCD precision, and implements \(\mathrm{U(2)_{q}\times U(3)_{d}\times U(2)_{u}}\) symmetry in the quark sector and \([\mathrm{U(1)_{l}\times U(1)_{e}}]^{3}\) in the lepton sector, also with the \(\{\mathrm{m}_{\mathrm{W}},\mathrm{m}_{\mathrm{Z}},\mathrm{G}_{\mathrm{F}}\}\) input electroweak parameter scheme.

SMEFTsim3.0 and SMEFTatNLO include largely the same set of operators, but one needs to be aware of differences in naming and sign conventions when performing a comparison. However, as opposed to SMEFTsim3.0, SMEFTatNLO does not implement CP violating operators nor operators involving \(b\)-quarks in the initial state. Furthermore, SMEFTsim3.0 distinguishes between EFT insertions in vertices, and EFT corrections to the widths of propagators of massive unstable particles. These linearised propagator corrections can be evaluated separately for simplicity, and added to the vertex-based parameterisation. In SMEFTatNLO, this distinction is not made, and the widths of unstable massive particles should be re-computed when decaying a final state with EFT contributions. Finally, note that top quark FCNC interactions are not compatible with the SMEFTatNLO flavour symmetry and are therefore unavailable.

For combinations with the ATLAS Higgs and Standard Model analyses, SMEFTsim3.0 is the preferred choice of model.

### Higher-order corrections

Monte Carlo simulations of the signal processes are typically generated at the best available precision, while EFT UFO models allow for LO QCD generation or NLO QCD at most. This results in SM signal samples including corrections at higher orders than the EFT samples. It is therefore important to ensure that if Wilson coefficients are set to zero in the samples used for the EFT interpretation they give a result compatible with the best available SM prediction, within uncertainties. If the difference between the nominal SM sample and the SM part in the EFT sample is small compared to the non-closure uncertainty of the analysis (e.g. due to unfolding), it can be ignored. If the difference is sizeable two possibilities can be considered, once the parameterisation of a given observable has been obtained:

1. Swap the pure SM part in the EFT parameterisation with the best SM prediction. The EFT part (SM-EFT interference and pure EFT) is not changed and it is kept at lower precision, typically LO QCD.

2. Scale the entire EFT parameterisation so that the pure SM part matches the best known SM prediction. An inclusive SM K-factor, a ratio of a formally higher accuracy prediction to a lower accuracy prediction, is often known, but one that is differential in the observable at hand is of course preferable.

While the second option is often adopted, there is no guarantee that the different operators will contribute in a way such that higher orders scale in the same way as the SM. For example, the NLO to NNLO K-factor for \(t\bar{t}\) production at the LHC is derived for an initial state dominated by gluon fusion. One could argue it should not be blindly applied to four-quark EFT operators in the \(qq\to t\bar{t}\) production mode.

For four-quark operators in \(t\bar{t}\) production, it was shown in Ref. [32] that an EFT calculation at NLO in QCD (with SMEFTatNLO-NLO) was crucial for obtaining the most precise bounds, as the EFT parameterisation at NLO accuracy was often not contained within scale variations of the EFT parameterisation at LO. This is explained by the opening of the \(qg\)-initiated channel at NLO. This same channel is also of interest for top+X processes, since one may then find virtual electroweak bosons in the \(t\)-channel diagrams. This is known as the "embedding of scattering amplitudes"4 and is highly relevant to the concept of energy growth, as discussed in Ref. [33]. For top+X processes, the generation of samples at NLO accuracy in QCD can however be prohibitively expensive (especially if off-shell effects are accounted for). Fortunately, it was shown in Ref. [34] that the generation of the corresponding process with merging of one additional parton was in most cases a good approximation of the NLO topology.

Footnote 4: An important SM example is that of \(t\bar{t}Wj\) production at LO in QCD. This was expected to be a subleading electroweak correction to the \(t\bar{t}W\) process, however the calculation has shown that the effect is of the order of 10%

## 5 Measurements harmonisation

This chapter describes the motivation and approaches to harmonisation of the physics objects definitions (objects reconstructed in the detector), the harmonisation of the phase-space region definitions and the harmonisation of the modelling uncertainties. The harmonisation of the objects is a useful step in any simultaneous extraction of the parameters from different processes as it allows for a consistent treatment of the statistical correlations and the correlations of the systematic uncertainties. The harmonisation of the phase-space region definitions simplifies the removal of any statistical overlaps between those regions, which then simplifies the statistical approach to the EFT contribution extraction. If the harmonisation of the phase-space regions is not performed, significant statistical overlap between different measurements would be present which would require more complicated statistical treatment, usually involving more approximations.

### Harmonisation of the physics objects

Each physics object in ATLAS has several available sets of definitions utilising different detector information (e.g. tracking, calorimetry) and different techniques. Each object definition also has several "working points" (WPs) which are optimised for different purposes, e.g. large selection efficiency or large background suppression. Measurements targeting different processes and different final states choose object definitions and WPs that are optimised for their selections. However, this can cause difficulties for combining these processes due to two reasons. Firstly, different object definitions (e.g. lepton isolation) make statistical overlaps between regions more difficult to resolve; different event selection criteria might contain the same events if the object definitions are not identical. Secondly, each object definition and WP comes with its own calibration and a set of uncertainties. It is not determined at this time whether treating these parameters as uncorrelated or fully correlated in the fitting framework is the more conservative choice, and neither is technically correct. Although some correlations of the systematic uncertainties for different WPs can be traced, in general, these correlations are not known a-priori and a dedicated effort would be needed to find them and treat them coherently in the statistical analysis. Thus, it is advised (and easier) to harmonise the definition of the objects and the chosen WPs in order to simplify the combinations of multiple processes, even at a cost of decreased precision for individual processes. If the impact of the harmonisation would be significant for some of the processes, a minimal divergence in the definition (ideally no more than two object definitions, or two non-disjoint WPs within an object definition) should be considered in the combination to minimise the statistical overlaps and the required tests of the impact of the assumed correlations of the associated uncertainties. This would allow for enough flexibility for the different final states while limiting the amount of additional studies required. Analyses should consider only defining the objects that they use and not consider objects they are not sensitive to (e.g. hadronic taus in a light-lepton focused \(t\bar{t}Z\) analysis) as this could lead to undesirable effects in estimates of the missing transverse momentum. However, not using the same objects can also lead to partial statistical overlaps between some analyses and this could be estimated and understood. If an overlap between analyses is significant, it is recommended to define a "veto" region (e.g. removing events with hadronic taus in a light-lepton focused \(t\bar{t}Z\) analysis). In this case all of the objects used in the veto need to be considered with their uncertainties.

A dedicated effort should be devoted to designing WP definitions and calibrations for objects where a common WP may not be sufficient for the combination, such as the electron and muon isolation WPs. The resulting "combination-friendly" WPs would allow for a choice of different values for the same object, permitting consistent systematic treatment. This definition already exists in ATLAS for the WPs of the \(b\)-tagging algorithm (identification of jets originating from a \(B\) hadron) which provides four different efficiency WPs (and an "un-tagged" WP) that are coherently calibrated with a consistent set of systematic uncertainties. The optimisation and calibration of the ATLAS prompt-lepton improved veto (PLIV) WPs [35] should be carried out for Run 3, as that would support combinations between analyses with significant fake-lepton contribution and analyses requiring a large selection efficiency. Additionally, since several systematic reduction schemes are usually available for the different physics objects, the schemes that enable combinations should be considered for each object, even when the sensitivity to the systematic variation of the object calibration is small for a given process. If the minimal systematic reduction that is valid for a combination leads to a significant increase of the systematic uncertainty for one of the considered processes due to the simplification in the systematic model, a more complex model could be considered. If it is deemed to be necessary, the more complex model could be adopted by all the considered processes. Alternatively, tools that allow to replace the simplified sources of systematic uncertainties with linear combinations of sources from more sophisticated systematic models, see e.g. Ref [36], could be explored.

### Harmonisation of the phase-space regions

Orthogonalisation of the phase-space regions, to avoid statistical overlap, is an important part of the harmonisation effort. Avoiding statistical overlap between the regions on the detector level simplifies the statistical analysis. Statistical overlaps may also be accounted for using bootstrapping techniques, however the implementation at large scales is non-trivial and can be avoided through the harmonisation procedure. It should be noted that harmonisation here does not mean that events will be removed from the measurement selection, but rather that in the case of a statistical overlap between regions a procedure should exist to decide on the split of the events into the combinable regions without removing them from the consideration. For two processes with large statistical overlaps, such as \(t\bar{t}Z\) and \(tZq\), a boundary could be drawn between phase-space regions most representative of each process. Independent analyses may then utilise the events in both regions, by including them as either signal or control regions in their respective analyses as appropriate. No simple selections based on a single kinematic distribution, e.g. number of charged leptons or \(b\)-tagged jets, can separate all of the top+X processes, as demonstrated by a recent CMS measurement [13]. The harmonisation effort is important in both process-specific as well as EFT-focused approaches described in Section 6.

Naturally, several processes occupy the same or similar phase-space regions, e.g. \(t\bar{t}H\), \(t\bar{t}W\) and \(t\bar{t}t\bar{t}\), thus harmonising the regions might result in a decreased sensitivity for the individual processes but it is a necessary step towards a coherent interpretation specifically in the context of a combined fit. Efforts could be devoted to design discriminants using ML to define the orthogonal regions to increase the separation between the processes. Multi-class neural networks could be used to achieve the separation in the regions of phase-space with a large overlap between individual processes. Harmonising the phase-space regions also includes optimisation of the binning and validation of the individual processes separately, enabling different teams to work on separate tasks while not jeopardising the final combination effort. If new processes or final states are considered after the harmonisation of the regions has taken place, the new analyses would need to follow the harmonised regions, or use regions orthogonal to all of the regions used by the combined measurement, in order to be considered for the combination5. In order to harmonise the regions, a list of all analysis regions used in the previous measurements for the considered processes should be made, the overlapping selections could be identified thereby enabling a dedicated optimisation effort. This effort can be split into different phases. In the first phase a harmonisation based on object counting can be used, while in the second phase more sophisticated ML techniques can be used to increase the separation of the processes.

Footnote 5: This is only valid in the assumption that the harmonised regions are utilised in both the combination effort as well as the separate individual analyses. An alternative approach would be to use orthogonal regions for the combination but to decouple this from the high-precision process-specific optimisation performed by individual analyses.

### Harmonisation of the modelling uncertainties

Several sources of modelling uncertainties impact signal or background predictions, and in several cases can even be the leading systematic uncertainties. However, unlike in the case of the experimental uncertainties, the modelling uncertainties often do not originate from dedicated measurements. This makes the harmonisation of the modelling uncertainties more difficult as often different MC generators are used for different processes. Moreover, it is not uncommon to use so-called "two-point uncertainties" that represent an uncertainty on a choice of a given algorithm for the simulation. Nevertheless, to ensure a clear mapping between the modelling uncertainties in different measurements, it is desirable that all analyses adopt uniform choices of the nominal and alternative MC models for a given process. Furthermore, scale choices and their uncertainties could be harmonised between the simulation of different processes to allow for a coherent treatment of these uncertainties. In general, dedicated effort should be devoted to harmonisation and investigation of correlations schemes for simulations of processes resulting in finals states that are close in the phase-space, such as \(t\bar{t}W\) and \(t\bar{t}H\). The situation is simpler for the parton distribution function (PDF) uncertainty, which allows for a coherent treatment between different processes, and should be harmonisedbetween the measurement by using the same PDF uncertainty set and prescription for the uncertainty estimate.

## 6 Proposed steps for a coherent top+X EFT interpretation

This chapter describes the proposed steps towards harmonised combinations in the top+X sector that are suitable for EFT interpretations. The steps are loosely ordered based on their difficulty, expected sensitivity and potential timelines. The steps represent a detector-level EFT extraction and at the final step also the EFT extraction from the multi-signal unfolding. The detector-level extraction is split into different approaches based on the complexity of the analysis and the potential sensitivity to the EFT operators.

### Object-based detector-level fit

The simplest approach for a consistent EFT interpretation in the top+X sector is an object-oriented detector-level fit of simple distributions that separate the data into different detector-level-objects final states. Such an approach has been recently employed by the CMS collaboration in Ref. [13]. These distributions include, but are not limited to, lepton multiplicity, lepton charges, invariant masses of the charged leptons, jet multiplicity, \(b\)-jet multiplicity and photon multiplicities. In this approach, for all contributing processes, the impact of the relevant EFT operators may be included and the impact on the detector-level distributions is parameterised via these operators, as described in Section 3.1. Figure 2 illustrates the object-based detector-level approach. In the figure, three processes are considered in four regions. The regions are defined by counting the multiplicities of detector-level objects to provide some separation between physics processes and consequently EFT operators, but is not optimised for either. In a real scenario, the number of photons would clearly distinguish between \(t\bar{t}\gamma\) and other processes, whilst the number of leptons and number of jets would assist in distinguishing between \(t\bar{t}X\) and \(tXq\) processes. Some processes, such as \(t\bar{t}W\), \(t\bar{t}H\) and \(t\bar{t}\bar{t}\) are not feasibly separable through object multiplicities alone. This approach will thus not be able to separate all of the considered physics processes, but this is not necessarily problematic as achieving high precision cross-section measurements of each SM process is not the goal. Valuable results for a few EFT operators can be extracted in a consistent way. Following an object-multiplicity-based selection, the sensitive directions in the EFT space should be identified using a Principal Component Analysis (PCA). This is a clean and simple to implement step towards a top+X EFT combination which serves as a stepping stone for more complex EFT extractions as the EFT models need to be implemented in all of the considered processes.

### Process-separated extraction at the detector-level

The process-separated extraction utilises process-specific analyses through combining their existing region definitions. These pre-defined regions are optimised for sensitivity to the individual process cross-sections rather than for EFT operators. This methodology benefits from the large amount of work already performed by many analysis teams and could be completed fairly quickly. It is a methodology which will benefit the sensitivity for operators where the impact of the operators has a different parametric impact on the expected yields in a bin (e.g. linear vs quadratic, or linear vs linear with opposing gradients) with respect to different processes. Figure 3 shows an example of the process-separated approach. In the figure, four processes are considered in three regions which have dedicated selections to target them. However as an example processes H and I are not well separated, thus, in this case a dedicated additional effort would need to be devoted to better separate the processes, using e.g. ML techniques. In a real scenario, processes such as \(t\bar{t}Z,tZq,t\bar{t}\gamma\) and \(t\bar{t}H\) are sufficiently separable from one another through careful regions definitions. However, again processes such as \(t\bar{t}W,t\bar{t}H\) and \(t\bar{t}t\bar{t}\) remain difficult to isolate without machine-learning.

It is expected that the process-separated approach will maintain the best control over the non-top-quark processes and their corresponding systematic uncertainties, as compared to the approach outlined in Section 6.1. For future analyses, this is an option to consider but is likely to be beaten in sensitivity by more dedicated techniques described in the following sections. The currently available dataset represents an opportunity to perform a fast EFT re-interpretation which benefits from many completed analyses, but introduces the challenge of dealing with the statistical overlaps between them. This includes the challenges of combining analyses which use different object definitions (e.g. lepton isolation WPs). Strictly speaking, re-utilising different process-specific analyses for this purpose does not require the harmonisation of object definitions. As specified in Section 5 however removing the statistical overlaps by ensuring orthogonality of the regions will greatly simplify the analysis and the treatment of the systematic uncertainties. There are several options for dealing with the statistical overlaps, each with distinct advantages and disadvantages.

Figure 2: Illustration of the object-separated extraction. Four regions are provided representing a selection with different multiplicities of detector-level objects Obj\({}_{1}\) and Obj\({}_{2}\). Contributions from three different processes are shown.

These are listed below, in order of increasing complexity but also increasing gains in sensitivity.

The first approach to deal with the statistical overlap is to remove one or more overlapping regions from the combination. This can be done fast with minimal optimisation required. However, removing overlapping regions works well only if the statistical overlap in the regions is very small or very large (close to 0 or 100%). Otherwise, it is possible that a large loss of available events could significantly degrade the precision of the measurements.

The second approach requires modification of regions to invoke orthogonality. Instead of removing the overlapping regions, the definition of the regions can be modified to achieve no overlap in the events. This also allows to design the regions to maximise the EFT sensitivity. In this approach, no events are lost, however, re-definition of the regions requires redoing the measurements. In case of large overlaps, significant modifications to the regions could lead to a significant drop of precision and the modifications would need to be optimised and tested.

The third approach re-designs regions from scratch with the orthogonality ensured. The optimisation can have different sensitivity goals, direct EFT sensitivity, as described in Section 6.3, or best separation of the processes. This approach is useful for future analyses, but it loses the benefit of re-using existing analyses and expertise.

As the first two approaches rely on the re-implementation of region definitions from existing process-specific optimised analyses, it is conceptually similar to performing a direct combination of those measurements. As such this methodology benefits greatly when object definitions and region definitions are already harmonised. In addition, it is important to consider also the harmonisation of fake- and non-prompt lepton (FNP) background estimations for the analyses from which the regions are replicated. Notwithstanding unusual kinematic dependencies, FNP estimations across different top+X measurements are correlated for individual FNP lepton sources. The rates are determined by the type of the FNP lepton (non-prompt muons, non-prompt electrons, internal / external photon conversions, hadronic photon fakes), the detector efficiency and the object definitions utilised. If process-specific optimised analyses employ harmonised methodologies for FNP determinations (e.g. floating normalisation fakes with dedicated control regions in profile-likelihood fits) then the joint FNP determinations in a process-separated combined EFT extraction is greatly simplified.

### EFT optimised regions at the detector-level

A more complex approach to the top+X EFT interpretations is to improve the separation of the considered top+X processes or the EFT operators by using more complex distributions in the fit. This can be achieved by modern ML techniques that enhance sensitivity to a particular property, such as those described in Ref. [37]. In an EFT-optimised approach the detector-level fit is employed but the region definitions and fitted distributions are outputs of ML algorithms. A basic approach has been recently tested by the CMS collaboration in Ref. [38]. All the harmonisation steps from the previous approach from Section 6.1 should be applied to remove unnecessary complexity from the procedure. Additionally, the ML techniques need to be employed in a such a way that the final discriminants used in the fit are coherently applied for all processes and operators. This can be achieved by using multi-class discriminants where each class represents a category with the best sensitivity to a particular EFT operator, as an example. A sketch of the multi-class discriminant is displayed in Figure 4. In this diagram kinematic event information is provided through input nodes \(I\) and transformed nonlinearly through hidden nodes \(H\). The output nodes define analysis regions with optimised sensitivity to different EFT operators (\(c_{1}-c_{4}\)), containing a mixture of physics processes. In the example given, a balance of 'Process K' and 'Process L' is used for best sensitivity to \(c_{1}\) while 'Process J' dominates for \(c_{4}\). Within these regions, distributions could be defined to further improve the sensitivity and to segregate EFT-insensitive background processes which dilute the measurements. EFT-optimised analyses do not require the pre-existence of process-separated analyses and can be run in parallel to the process-separated approach described in Section 6.2. While potentially achieving the maximum sensitivity, an ML-driven EFT optimised analysis is also the most dependent on the UFO model and EFT basis chosen, as the model is used as the training target. This includes the choice regarding the optional inclusion of \(\Lambda^{-4}\) terms, with \(\Lambda\) usually set to 1 TeV. This dependency is not problematic as long as the considered theory uncertainties are expected to cover the missing higher-order terms, and the assumptions regarding the underlying model are explicitly described in published results.

Figure 3: Illustration of the process-separated extraction. Regions are defined in such a way to maximise the separation of the processes. The processes H and I are not well separated and could benefit from a dedicated machine-learning algorithm.

This is analogous to traditional BSM searches which are more model dependent. Particular EFT models such as SMEFtsim3.0 and SMEFTatNLO[27, 28, 29, 30, 31] are widely used, and tools exist for translating bases between these and other models for future re-interpretations of the same analysis [39]. An ML-optimised approach may hinder future re-interpretations if relevant tools such as Rivet do not fully support ML discriminants, but this problem is not unique to EFT analyses.

### Fully-differential multi-process unfolding

In this approach, the aim is to simultaneously unfold distributions (and/or inclusive cross-sections) for each of the considered top+X processes in a coherent way. The aim of this approach is not to guarantee the most precise results (although this is likely) but to provide results which can be easily reinterpreted, as described in Section 3.2. The result of the unfolding will be one differential distribution for each of the considered processes while properly correlating the common sources of the systematic uncertainties. To achieve this, all the harmonisation steps from the previous approaches should be employed. Moreover, no statistical overlap between the regions is allowed, and the region definitions need to be harmonised as described in Section 5.2. Region definitions from the process-separated approach of Section 6.2 can be used. ML can be used to separate the processes, but the choice of the inputs to the ML procedure needs to be carefully checked to not bias the unfolded distributions. Alternatively, ML techniques can be used to define

Figure 4: Sketch of a multi-class neural network architecture to separate the impact of a given EFT parameter. \(I\) denotes input nodes with kinematic event information, \(H\) denotes hidden nodes which perform non-linear transformations on the inputs. The final output nodes define regions which contain optimised mixtures of different physics processes to maximise the sensitivity to each EFT operator. In this example, a balance of ’Process K’ and ’Process L’ is used for best sensitivity to \(c_{1}\) while ’Process J’ dominates for \(c_{4}\).

detector-level regions which separate contributions from individual bins of the target truth distributions, in order to minimise the migrations to be handled by the unfolding. The choice of the different unfolded distributions on each process should be studied to identify the most powerful distributions. However, if different distributions are chosen for a single process, a new fit needs to be run. Thus, to start with, a single most discriminating distribution for each process should be identified and used in the extraction. The transverse momentum of a given object for each process is expected to be a good candidate for such a distribution. Due to the complexity of the approach, and its partial reliance on the previous results, the fully differential approach should take advantage of the effort spent on the previous top+X combinations.

### Milestones towards more complex EFT fits

Although the final result of each approach is expected to contain all relevant top+X processes, several partial results, providing a proof of concept, are possible and could be pursued:

1. Combination of two processes with small or zero statistical overlap and largely independent analyses but with some overlap of EFT sensitivity e.g. \(t\bar{t}Z\) and \(t\bar{t}\gamma\).
2. Combination of processes with overlapping analyses (e.g. a control region in one analysis is the signal region in another), but without strong statistical overlap, where there is also overlap of EFT sensitivity, e.g. \(t\bar{t}Z\) and \(t\bar{t}W+j\) electroweak.
3. Combination of similar processes with a large statistical overlap and overlap of EFT sensitivity e.g. \(t\bar{t}Z\) and \(tZq\) following the harmonisation recommendations outlined in Section 5.
4. Combination of similar processes with a large statistical overlap and some overlap of EFT sensitivity but also individual sensitivity, e.g. \(t\bar{t}W\), \(t\bar{t}H\) and \(t\bar{t}t\bar{t}\).
5. Combination of as many top+X processes as available, including all those previously described.
6. Combination of as many top processes as available, i.e. top+X and differential \(t\bar{t}\) measurements and measurements of single top quark production.

The gradual increase in the complexity enables splitting the most complex approaches into manageable steps, greatly improving the parallelisation of the required tasks.

## 7 Conclusions

Due to the processes in the top+X sector being intertwined, a coherent EFT interpretation requires careful combinations. This note presents recommendations on how to achieve combinations of top+X results suited for EFT interpretations. In the following, the summary of the most important points is provided.

Dedicated effort has to be devoted to provide working points and calibration for physics objects that allow combinations of analyses with conflicting requirements, such as lepton isolations. Analyses should harmonise the definitions of the object and systematic uncertainty models to allow easier combinations. If some definitions cannot be harmonised, the minimum possible deviation from the common set of definitions should be followed. The phase-space regions should be harmonised for easier combinations. This only refers to the definition of the regions, it does not mean vetoing events. In case of the region orthogonality not being guaranteed by the selection, the overlap should be properly estimated and ideally minimised. The individual analyses can still be optimised as long as all the harmonisation steps are followed, which allows for easy future combinations.

The EFT analyses should proceed in multiple steps, with the first step consisting of the object-separated detector-level fit as this is considered to be the simplest approach. The future measurements should focus on process-separated analyses either on detector-level or using the multi-signal unfolding as well as EFT-optimised analyses.

## References

* [1] W. Buchmuller and D. Wyler, _Effective lagrangian analysis of new interactions and flavour conservation_, Nuclear Physics B **268** (1986) 621, issn: 0550-3213, url: [https://www.sciencedirect.com/science/article/pii/0550321386902622](https://www.sciencedirect.com/science/article/pii/0550321386902622) (cit. on p. 2).
* [2] B. Grzadkowski, M. Iskrzynski, M. Misiak and J. Rosiek, _Dimension-six terms in the Standard Model Lagrangian_, Journal of High Energy Physics **2010** **85** (2010), issn: 1029-8479, url: [https://doi.org/10.1007/JHEP10](https://doi.org/10.1007/JHEP10)(2010)085 (cit. on p. 2).
* [3] A. Falkowski and R. Rattazzi, _Which EFT_, JHEP **10** (2019) 255, arXiv: 1902.05936 [hep-ph] (cit. on p. 2).
* [4] ATLAS Collaboration, _Observation of four-top-quark production in the multilepton final state with the ATLAS detector_, Eur. Phys. J. C **83** (2023) 496, arXiv: 2303.15061 [hep-ex] (cit. on p. 2).
* [5] CMS Collaboration, _Observation of four top quark production in proton-proton collisions at \(\sqrt{s}=13\) TeV_, (2023), arXiv: 2305.13439 [hep-ex] (cit. on p. 2).
* [6] CMS Collaboration, _Measurement of top quark pair production in association with a \(Z\) boson in proton-proton collisions at \(\sqrt{s}=13\) TeV_, JHEP **03** (2020) 056, arXiv: 1907.11270 [hep-ex] (cit. on p. 2).
* [7] CMS Collaboration, _Measurement of the inclusive and differential \(t\bar{t}\gamma\) cross sections in the dilepton channel and effective field theory interpretation in proton-proton collisions at \(\sqrt{s}=13\) TeV_, JHEP **05** (2022) 091, arXiv: 2201.07301 [hep-ex] (cit. on p. 2).
* [8] ATLAS Collaboration, _Measurement of the \(t\bar{t}Z\) and \(t\bar{t}W\) cross sections in proton-proton collisions at \(\sqrt{s}=13\) TeV with the ATLAS detector_, Phys. Rev. D **99** (2019) 072009, arXiv: 1901.03584 [hep-ex] (cit. on p. 2).
* [9] CMS Collaboration, _Measurement of the cross section for top quark pair production in association with a \(W\) or \(Z\) boson in proton-proton collisions at \(\sqrt{s}=13\) TeV_, JHEP **08** (2018) 011, arXiv: 1711.02547 [hep-ex] (cit. on p. 2).
* [10] ATLAS Collaboration, _Search for flavour-changing neutral currents in processes with one top quark and a photon using \(81\,fb^{-1}\) of \(pp\) collisions at \(\sqrt{s}=13\) TeV with the ATLAS experiment_, Phys. Lett. B **800** (2020) 135082, arXiv: 1908.08461 [hep-ex] (cit. on p. 2).