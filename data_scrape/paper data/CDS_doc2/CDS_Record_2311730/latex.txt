# Performance of the ATLAS global transverse-momentum triggers at \(\sqrt{s}=8\) TeV

The ATLAS Collaboration

###### Abstract

The global transverse momentum triggers of the ATLAS experiment at the CERN Large Hadron Collider are designed to select collision events with non-interacting particles passing through the detector and events with a large amount of outgoing momentum transverse to the beam axis. These triggers use sums over the full calorimeter, and therefore can be very sensitive to the average number of interactions per bunch crossing. This note describes the methodology and the performance of the transverse-momentum trigger algorithms that were deployed during the 2012 ATLAS data taking campaign at 8 TeV center-of-mass energy. Improvements over what was done in 2011 resulted in better efficiency in 2012 despite a doubling of the per bunch crossing luminosity.

ATL-DAQ-PUB-2018-001

## 1 Introduction

The ATLAS experiment [1] global transverse-momentum triggers use sums over the full detector solid angle to characterize and select events to be recorded for offline analysis.

The missing transverse momentum (\(E_{\mathrm{T}}^{\mathrm{miss}}\)) triggers [2; 3; 4] are designed to select events with an imbalance in the total measured momentum due to the presence of particles invisible to the detector. These include neutrinos, which are produced in the decay chains of Standard-Model particles such as the \(W\), \(Z\) and heavy quarks. Such events can also arise from the presence of non-interacting particles predicted by theories beyond the Standard Model, such as the lightest supersymmetric particle in R-parity conserving theories.

Even for events in which no non-interacting particles are produced, imperfect calorimeter resolution will give rise to non-zero values of \(E_{\mathrm{T}}^{\mathrm{miss}}\), since measurement fluctuations lead to directional energy imbalances that appear to be \(E_{\mathrm{T}}^{\mathrm{miss}}\). This problem is exacerbated by the multiple collisions per bunch crossing occurring at high luminosities. A note describing the 2011 transverse-momentum trigger performance [4] includes an extensive discussion of how an increase in the average number of collisions per bunch crossing, \(\mu\), results in "pileup", the addition of both positive and negative energy readings to the calorimeter measurement, and therefore an increase in measurement fluctuations. The \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger selects events with measured \(E_{\mathrm{T}}^{\mathrm{miss}}\) larger than some threshold. It is designed to accept events of a type that are likely to have large true (and therefore measured) missing momentum, while discarding the bulk of other events that are more likely to have lower, below threshold, measured \(E_{\mathrm{T}}^{\mathrm{miss}}\). However, since events without true \(E_{\mathrm{T}}^{\mathrm{miss}}\) are much more abundant, regardless of pileup the vast majority of measured high \(E_{\mathrm{T}}^{\mathrm{miss}}\) events arise from mismeasurement rather than non-interacting particles. The challenge is therefore to design an \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger which is as efficient as possible for selecting events with true \(E_{\mathrm{T}}^{\mathrm{miss}}\) (that is, measuring \(E_{\mathrm{T}}^{\mathrm{miss}}\) as large when it is large due to momenta of invisible particles) while minimizing the number of background events selected due to erroneously determined large \(E_{\mathrm{T}}^{\mathrm{miss}}\).

The missing transverse momentum reconstruction performed after data is collected and stored (the "offline" algorithm) uses the full detector information to get an optimum determination of \(E_{\mathrm{T}}^{\mathrm{miss}}\). Instead of just summing calorimeter energy, the best offline algorithm [5; 6; 7; 8] uses separately identified objects (jets, electrons, photons, muons, and taus) along with the best calibration for each of them that takes into account their respective energy measurement responses. At the trigger level, both computing time and access to the detector information are limited, so algorithms cannot be identical to those used offline. Also, since at trigger level the relative importance of background rejection and good resolution may be different from offline, the best trigger algorithm may not be the best offline one, and may not even run offline. A dedicated trigger \(E_{\mathrm{T}}^{\mathrm{miss}}\) algorithm and performance evaluation strategy must therefore be developed. On the other hand, since the trigger selection will typically be followed by offline selection in physics analyses, the full chain of trigger to offline reconstruction must be taken into account in order to maximize efficiency. It is therefore important for the trigger algorithms to be as close as possible, within constraints, to the ones used offline. A potential complication in achieving this is that, unlike the offline algorithms which can vary from analysis to analysis and can be improved during the analysis process, a trigger algorithm must serve all purposes and has only one opportunity to make its selection.

As described in detail in [4], in 2011 the ATLAS trigger used the position of each calorimeter segment (individual calorimeter cells or groupings of cells depending on the trigger level) and the energy deposited in that segment to determine the measured momentum. The sum over all calorimeter segments provides a good approximation of the measurable transverse momentum missing in an event. In this approach, thecomponents of the missing transverse momentum vector, \(\vec{E}_{\mathrm{T}}^{\mathrm{miss}}\), are defined as \(E_{x}^{\mathrm{miss}}=-\Sigma_{i}E_{i}\mathrm{sin}\theta_{i}\cos\phi_{i}\) and \(E_{y}^{\mathrm{miss}}=-\Sigma_{i}E_{i}\mathrm{sin}\theta_{i}\sin\phi_{i}\), where the sum is over all calorimeter segments \(i\), \(E_{i}\) is the energy measured in segment \(i\), and \(\theta_{i}\) and \(\phi\) are the polar and azimuthal coordinates of segment \(i\) (in a right-handed coordinate system whose origin is the center of the ATLAS detector, with \(x\) axis pointing towards the center of the LHC ring, \(y\) axis straight up, and \(z\) axis along the beam direction). The impact of noise and the amount of data that needs to be transferred are reduced with "zero suppression" strategies that omit calorimeter segments with low energy. \(E_{\mathrm{T}}^{\mathrm{miss}}\) is defined as \(\sqrt{(E_{x}^{\mathrm{miss}})^{2}+(E_{y}^{\mathrm{miss}})^{2}}\) and is used to select candidate events for further study.

The scalar sum of the calorimeter cell energies times the projection of their position unit vectors onto the plane perpendicular to the beam axis, \(\Sigma E_{\mathrm{T}}=\Sigma_{i}E_{i}\mathrm{sin}\theta_{i}\), can be used as an indication of a hard scatter having taken place, and is therefore also potentially a useful quantity with which to detect interesting events. Triggers selecting events with high \(\Sigma E_{\mathrm{T}}\) were used in 2011 and 2012, both to search for rare events in proton-proton collisions and to signal that an interaction had occurred in nucleus-nucleus collisions. However, the large average number of interactions per bunch crossing, \(\mu\), of 2012 meant that \(\Sigma E_{\mathrm{T}}\)-trigger thresholds had to be set to higher values for proton-proton studies in this period.

The transverse-momentum trigger algorithms used in 2012 are described and compared with those used in 2011 in Section 2. Section 3 describes the measured and simulated samples used in the studies presented here. The performance of the algorithms, including their efficiencies for signal selection under trigger-rate constraints are described and compared in Section 4. A study of the dependence of algorithm performance on luminosity is presented in Section 5. Comparisons of measured and simulated event transverse momentum properties and trigger algorithm efficiencies are presented in Section 6. Section 7 provides the details of the main transverse-momentum trigger chains used to select data in 2012, including the algorithms used and their respective thresholds at the various trigger levels. A summary of the degree to which the different algorithms were useful in providing data for physics analysis is also included. Finally, a summary and conclusions are presented in Section 8.

## 2 Transverse-momentum triggers

In 2011 and 2012, ATLAS used a three-level (L1, L2, and EF) trigger system [9], with progressively finer input information and more computing power available at each level. The transverse-momentum trigger algorithms implemented at each trigger level are described in this section.

### L1 and L2 triggers

For both 2011 and 2012, the Level 1 (L1) trigger, implemented in custom hardware, used analog sums over cells in "trigger towers". These projective regions were of approximate size \(\Delta\eta\,\times\,\Delta\phi=0.1\,\times\,0.1\) for \(|\eta|<2.5\) and were larger and less regular in the more forward region [9]. L1 processors digitize the analog sums to provide \(x\), \(y\), and total transverse energy. Sums over the full detector are done by Common Merger Modules [10], which then compare the total transverse energy with thresholds for \(\Sigma E_{\mathrm{T}}\) triggers, while using a look-up table to compare the quadrature addition of the \(x\) and \(y\) sums with L1 \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger thresholds. A uniform threshold of about 1 GeV was used for all L1 trigger towers in 2011. At this trigger level energy was recorded in digitized counts, which determine the granularity of the recorded measurements. The approximate value of 1 GeV per count will be used in the rest of this note.

In 2012, increased fluctuations due to pileup meant larger fluctuations in measured energy, especially in the forward region of the calorimeter. The trigger-tower thresholds were therefore increased in this region in an \(\eta\)-dependent manner, thus reducing the number of spurious triggers.

Pileup also causes differences between colliding bunches, even of the same luminosity. Colliding bunches in the LHC come in trains, sets of colliding bunches separated from each other by a fixed interval, 75 ns at the start of 2011, 50 ns for most of 2011 and 2012, and 25 ns at the very end of 2012. The trains are separated by longer gaps of no colliding bunches. For bunches in the middle of a train, the positive energy contribution to the measurement from extra interactions in the current bunch crossing ("in-time" pileup) is canceled on average by the negative energy sum of the tails of calorimeter signals from interactions in previous bunch crossings ("out-of-time" pileup). However, this cancellation does not occur for the first few bunches in a train, for which there are fewer or no previous bunch crossings. As \(\mu\) increased in 2012, it was therefore found useful to introduce an L1 \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger that did not include the first three bunches in each train. Figure 1 shows the \(E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\Sigma E_{\mathrm{T}}\) distributions determined with the various 2012 algorithms for events collected with a random trigger on the first three bunch crossings of a train and on all other bunch crossings. The algorithms other than L1 are explained in the following sections. The fraction of events with high \(E_{\mathrm{T}}^{\mathrm{miss}}\) and with high \(\Sigma E_{\mathrm{T}}\) is larger for all algorithms for the first three bunches; this is especially true at L1. The L1 trigger omitting the first 3 bunches could therefore run unprescaled (as opposed to triggers that run prescaled, for which only M out of N events above threshold are selected by the trigger) at a significantly reduced threshold (35 GeV versus 50 GeV) with improved efficiency that outweighed the loss in luminosity from removing the first few bunches.

In 2011, because of data transfer-rate limitations, the L1 result was also used at Level 2 (L2), the first stage software trigger. Modifications in firmware and software at the end of 2011 allowed a parallel line of data access to a front-end board (FEB) summary information block, providing Level 2 with transverse-momentum sums over up to 128 cells. Noise suppression could also be applied at the front end, so the final results, based on finer granularity than L1, were closer to those of the EF-level algorithms described in the following section. This is particularly the case for events collected with a random trigger, the bulk of which do not contain localized large energy deposits from a hard scatter. For such events, the trigger-tower zero suppression removes many of the low energy contributions which are retained by the FEB algorithm. This can be seen clearly in the difference between trigger-tower and FEB \(\Sigma E_{\mathrm{T}}\) distributions in Figure 1. The FEB algorithm was the default one used at L2 in 2012.

### EF triggers

The highest trigger level for both 2011 and 2012 was that of the Event Filter (EF). At this level complete detector information was available to the algorithms, including the full granularity of \(\sim\)188 000 calorimeter cells. Cell energies were those determined in Digital Signal Processors (part of the calorimeter back-end electronics) with sophisticated algorithms using pulse-shape information [1].

The 2011 EF algorithm determined transverse momentum by summing over the full set of cells that remained after noise suppression. Noise suppression was performed at EF in 2011 by ignoring cells with energy less than 3 times the cell noise distribution width, \(\sigma_{cell}\). At the start of 2011, cell energy fluctuations were characterized solely by what was expected due to electronic noise. In order to deal with the increasing \(\mu\), in the middle of 2011 the constants used in shape parameterization were changed and the \(\sigma_{cell}\) definitions were modified to include a pileup-dependent term. Like the energy deposit due to pileup, the resulting \(\sigma_{cell}\) values varied significantly with cell position. For example, for \(\mu=8\), the hadroniccalorimeter \(\sigma_{cell}\) ranged from tens of MeV in the central part of the calorimeter to hundreds of MeV at \(|\eta|\sim 2\) to several GeV at \(|\eta|\sim 4\).

An algorithm using a "single-sided" noise cut like that used in 2011, which omits all cells or towers with negative noise fluctuations while including those with above-threshold positive fluctuations, causes an offset in the \(\Sigma E_{\rm T}\) distribution and also impacts the \(E_{\rm T}^{\rm miss}\) distribution. The details of the effect strongly depend on the shape of the noise distribution and hence also on the in-time and out-of-time pile-up. As shown in [4], the changes in the parameterization of noise in the middle of 2011 therefore caused differences in the measured \(E_{\rm T}^{\rm miss}\) and \(\Sigma E_{\rm T}\) distributions.

Figure 1: \(E_{\rm T}^{\rm miss}\) (top) and \(\Sigma E_{\rm T}\) (bottom) of events triggered on random bunch crossings as calculated using L1 trigger towers (red circles), the L2 FEB algorithm (orange inverted triangles), the EF cell algorithm with two-sided noise cuts (blue squares), the EF algorithm using clustering (magenta crosses), the EF tclcw algorithm which uses clustering with hadronic calibration (green triangles), and the offline cluster algorithm that corrects energies according to the calorimeter objects contributing to the calorimeter energy (black line). Distributions are shown for events from the first 3 bunches in each bunch train (left) and all other bunches (right).

Two EF-level transverse momentum algorithms were implemented in 2012. One of these, similar to the 2011 EF algorithm, used summation over the full granularity of \(\sim\)188 000 calorimeter cells, but with a different noise suppression scheme. In order to more closely align the trigger with offline algorithms, a "two-sided" noise cut was used in 2012. That is, noise suppression was performed by ignoring any cells with absolute value of energy less than twice \(\sigma_{cell}\). Cells with energy less than -5\(\sigma_{cell}\) were still rejected for the bulk of the 2012 data-taking period. This combination allowed continued protection of the trigger against large negative energy fluctuations while reducing differences with offline algorithms, which also retain negative-energy cells above threshold. Also, cell signal-shape parameters in 2012 were set for \(\mu\) of 20, appropriate to the average conditions expected in that period.

In 2012 it was also possible to institute at EF level the "tclcw" algorithm, which used calorimeter clustering [11] and included a correction for the hadronic energy scale [8]. To allow trigger studies, cluster algorithm results before hadronic calibration were also stored. The algorithm used as cluster seeds cells with energy of magnitude greater than 4 times the standard deviation from noise, and added to the cluster surrounding cells with energy of magnitude greater than twice the standard deviation and all immediate neighbor cells. Though not identical to the offline algorithm, it was much closer to it than the uncorrected sum over all calorimeter cells used at EF level in 2011.

Figure 1 compares the \(E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\Sigma E_{\mathrm{T}}\) distributions of the various algorithms for a sample of events collected with a random trigger. As seen in the \(\Sigma E_{\mathrm{T}}\) distributions, the cell-sum algorithm measures substantially more energy per event than the FEB algorithm. This also results in higher measured \(E_{\mathrm{T}}^{\mathrm{miss}}\). The effect of clustering can be seen by comparing the cell algorithm with the cluster algorithm without hadronic energy scale correction. The impact is most clearly visible in the high energy regions of both \(\Sigma E_{\mathrm{T}}\) and \(E_{\mathrm{T}}^{\mathrm{miss}}\). Finally, as can be seen by comparing the \(\Sigma E_{\mathrm{T}}\) and \(E_{\mathrm{T}}^{\mathrm{miss}}\) distributions of the cluster algorithms with and without hadronic energy scale correction, the hadronic calibration further substantially increases the measured energy. Excluding events collected in the first three bunch crossings, the hadronically calibrated cluster algorithm matches the offline algorithm much better than any of the other algorithms. The result is larger measured \(E_{\mathrm{T}}^{\mathrm{miss}}\) for events with true \(E_{\mathrm{T}}^{\mathrm{miss}}\), but also more random events measured to have high \(E_{\mathrm{T}}^{\mathrm{miss}}\).

### \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance triggers

Even for events in which no non-interacting particles are produced, imperfect calorimeter resolution will give rise to non-zero values of \(E_{\mathrm{T}}^{\mathrm{miss}}\), since measurement fluctuations lead to directional imbalances that appear to be \(E_{\mathrm{T}}^{\mathrm{miss}}\). The \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance (XS) triggers [4] were introduced in 2011 to select events whose \(E_{\mathrm{T}}^{\mathrm{miss}}\) is unlikely to come from measurement fluctuations. XS is defined as the ratio of \(E_{\mathrm{T}}^{\mathrm{miss}}\) in an event to the expected size of \(E_{\mathrm{T}}^{\mathrm{miss}}\) due to measurement fluctuations in the event, determined from the \(\Sigma E_{\mathrm{T}}\) in that event. Studies conducted for 2011 data taking showed that the standard deviation of the components of \(E_{\mathrm{T}}^{\mathrm{miss}}\) in events for which there should not be any real \(E_{\mathrm{T}}^{\mathrm{miss}}\) are well parameterized in the form \(a+b\sqrt{\Sigma E_{\mathrm{T}}}\).

XS triggers were active in 2012, with parameterization as appropriate for the algorithm in use at each trigger level. To protect against spurious large XS values, these triggers required \(E_{\mathrm{T}}^{\mathrm{miss}}>\)10 GeV and \(\Sigma E_{\mathrm{T}}>\)16 GeV. In order for them to be efficient for cases of very large energy deposition, events with \(E_{\mathrm{T}}^{\mathrm{miss}}\) above some threshold were also passed by the algorithms regardless of XS value.

## 3 Data selection and simulation

Events selected by a random trigger on colliding bunches, used throughout 2012, provide the sample used to study the bulk of the transverse momentum triggers, and are referred to as the "zero-bias" data sample. Since these events have little true \(E_{\mathrm{T}}^{\mathrm{miss}}\), they are useful for studying algorithm resolution and high transverse momentum tails. Note, however, that limited statistics and the steep decrease in the number of events with increasing \(E_{\mathrm{T}}^{\mathrm{miss}}\) mean that these events do not necessarily provide a complete description of background at high \(E_{\mathrm{T}}^{\mathrm{miss}}\).

Events rich in \(W\to\mu\nu\) decays are used to study trigger efficiency for events with real \(E_{\mathrm{T}}^{\mathrm{miss}}\). These events, containing high transverse momentum muons, provide a relatively low background sample with which to study trigger behavior. Since measured \(E_{\mathrm{T}}^{\mathrm{miss}}\) depends on the detector response to the full event, its properties will, however, vary with signal type. In particular, the detailed shape of trigger turn-on curves will likely be different for other event topologies.

The \(W\to\mu\nu\) candidate events are selected by triggering on a muon candidate with transverse momentum of at least 15 GeV at L1 and 24 GeV at L2 and EF. The transverse mass of the reconstructed W candidate, \(m_{T}^{W}\), is required to be between 40 GeV and 100 GeV. Here \(m_{T}^{W}\) is defined as \(\sqrt{2\mbox{$p_{T}^{\mu}$}E_{\mathrm{T}}^{\mathrm{miss}}[1-\cos\phi(\mbox{$ \vec{p}_{T}^{\,\mu}$},\mbox{$\vec{E}_{\mathrm{T}}^{\mathrm{miss}}$})]}\), where \(E_{\mathrm{T}}^{\mathrm{miss}}\) is the magnitude of the missing transverse momentum vector \(\mbox{$\vec{E}_{\mathrm{T}}^{\mathrm{miss}}$}\), \(p_{T}^{\,\mu}\) is the magnitude of the muon transverse momentum \(\mbox{$\vec{p}_{T}^{\,\mu}$}\) and \(\phi(\mbox{$\vec{p}_{T}^{\,\mu}$},\mbox{$\vec{E}_{\mathrm{T}}^{\mathrm{miss}}$})\) is the angle in the transverse plane between \(\mbox{$\vec{p}_{T}^{\,\mu}$}\) and \(\mbox{$\vec{E}_{\mathrm{T}}^{\mathrm{miss}}$}\), all as determined offline. The \(W\) selection criteria are chosen to minimize the background when comparing data with simulated events, which do not contain background.

Events are simulated with the ATLAS simulation framework [12]. Minimum-bias events are generated using the PYTHIA8 [13; 14] program, using the ATLAS MC12 A2M tune [15] with the MSTW08 leading order parton distribution functions [16]. These events simulate the bulk of the inelastic proton-proton interactions, and are used to compare with the zero-bias data sample. \(W\) events are generated using POWHEG [17; 18] and PYTHIA 8. The GEANT4 [19] software package is used to simulate the passage of particles through the ATLAS detector. When data and simulation are compared below, the simulated events are normalized to the luminosity of the data.

## 4 Comparison of algorithms

There is no one best metric for comparing the performance of \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger algorithms. For thresholds typically used, events selected by the \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger arise primarily due to a combination of mismeasurement of the total energy of the large number of events without any hard parton scatters, and mismeasurement of one or more jet energies in QCD multi-jet events, both of which can result in large measured \(E_{\mathrm{T}}^{\mathrm{miss}}\). As a result, there are many fewer signal than background events accepted by the triggers discussed here, so the trigger rate and therefore also trigger threshold settings are determined by the fraction of background events which can be recorded. One method of comparing algorithms therefore consists of (1) determining for each algorithm the fraction of background events that are passed by the trigger as a function of threshold value, (2) setting the thresholds for each of the algorithms (typically to different values from each other) such that they reject the same fraction of background events (and therefore result in the same trigger rate), and then (3) comparing the fraction of signal events that are above the respective threshold of each of thealgorithms. Since \(E_{\mathrm{T}}^{\mathrm{miss}}\) is determined from a sum over the full event, the determination with this method of which algorithm is better may depend on signal event characteristics.

However, the purpose of retaining events is to analyze them offline, so what is important is the efficiency of an algorithm for events that are useful for analysis. While some analyses can use most events (even those with offline properties for which the trigger is only partially efficient), others only use events for which the trigger is highly efficient. For these, an algorithm whose efficiency plateaus at a lower value of offline \(E_{\mathrm{T}}^{\mathrm{miss}}\) (the definition of which may vary from analysis to analysis) for the same trigger rate (fixed by the threshold set) may be a better algorithm.

A related property is the difference between the measured online and offline values of the trigger parameter. Since this may be different for signal and background events, the relation of this metric to efficiency is not always simple. For example, an algorithm which has good resolution for low values of \(E_{\mathrm{T}}^{\mathrm{miss}}\) and poor resolution for high \(E_{\mathrm{T}}^{\mathrm{miss}}\) values may have higher efficiency for the same background rejection than an algorithm which greatly overestimates \(E_{\mathrm{T}}^{\mathrm{miss}}\) for a small fraction of background events but has better resolution in general.

A few different comparisons of trigger and offline algorithms are therefore presented in this section.

### Trigger vs offline

As discussed above, Figure 1 shows the \(E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\Sigma E_{\mathrm{T}}\) distributions determined with the various 2012 algorithms for events collected with a random trigger on the first three bunch crossings and on all other bunch crossings. While in general the trigger algorithm distributions are quite different from the offline ones, the tclcw algorithm \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution for events excluding the first 3 bunches in a train are quite close to the offline one.

Figure 2 compares on an event-by-event basis the \(E_{\mathrm{T}}^{\mathrm{miss}}\) for random-trigger events as determined by the trigger algorithms with those determined by the offline algorithms. Not surprisingly, the features observed here are consistent with those seen in the separate distributions in Figure 1. The scale of the 2012 L2 FEB algorithm is much closer to that of the offline one than the trigger-tower algorithm used at L2 in 2011. The scale of the tclcw algorithm used at EF level in 2012 is much closer to that of the offline one than all the others, including the cell algorithm used at EF level in 2011 and the cluster algorithm that does not include hadronic energy scale correction. As can be seen in Figure 1, the effect of hadronic calibration alone is greater than about 50% for these events. Note that, as mentioned above, the impact of this behavior on efficiency is not obvious. For example, the algorithms without hadronic calibration systematically underestimate \(E_{\mathrm{T}}^{\mathrm{miss}}\) of random events compared with the offline determination. To have an identical trigger rate, the thresholds for such algorithms would therefore be set to lower values than that of the hadronically calibrated algorithm. But since these algorithms will also underestimate the missing momentum of events with true \(E_{\mathrm{T}}^{\mathrm{miss}}\), it is not clear from this consideration alone whether the resulting efficiency would be better or worse.

### Efficiencies

Efficiencies of the trigger algorithms are studied here using \(W\rightarrow\mu\nu\) candidate events, selected as described in Section 3. Muon momentum is not included in the algorithms shown here, so measured \(E_{\mathrm{T}}^{\mathrm{miss}}\) due to the \(W\) in these events comes from a combination of the neutrino and the muon. Efficienciesare shown here as the fraction of these events which pass the trigger as a function of offline \(E_{\mathrm{T}}^{\mathrm{miss}}\). Two different offline algorithms are used for this purpose. The best offline \(E_{\mathrm{T}}^{\mathrm{miss}}\) is one described previously, which uses the full set of identified objects in the event to allow for the best momentum calibration for each of them. But as the algorithms described here do not use muons, the muon term of this offline algorithm is not included here. The other offline algorithm is one that uses calorimeter cell clustering and hadronic energy scale correction, but no calibration for calorimeter objects. This version is closer to what is done at trigger level, so comparison with this algorithm avoids other complications. It differs from the trigger-level cluster algorithm in the details of clustering and in the better cell-level information available offline. The efficiency of each of the 2012 algorithms and the full trigger chain for the lowest threshold unprescaled \(E_{\mathrm{T}}^{\mathrm{miss}}\) chain is shown for \(W\to\mu\nu\) candidate events in Figure 3.

The effect of the 2012 algorithm changes can be seen by comparison of the 2011 and 2012 efficiency for the for \(W\to\mu\nu\) candidate events. As discussed above, one approach for doing such a comparison would involve determining the thresholds for each of the algorithms to equalize their trigger rates and then comparing their efficiencies for signal events. This comparison would require using at each level only a set of random events that passed the same fraction of events at the previous trigger level. However, the large amount of random-trigger data required to perform such a study is not available. Instead, we compare in Figure 4 the efficiencies for signal of three algorithm chains whose thresholds were set during data collection according to the constraints at each level. Two of these use a Level 1 trigger-tower algorithm with a threshold of 50 GeV and L2 FEB with a threshold of 55 GeV. One of these two uses the tclcw algorithm with a threshold of 80 GeV at EF level, so the complete chain is identical to one run unprescaled in 2012. The other uses a cell-sum algorithm at EF level with an identical threshold of 80 GeV for comparison. This chain is approximately what would have been run in 2012 if only the 2011 EF algorithms had been available, though it does take advantage of the 2012 L2-FEB algorithm and uses the 2012 EF-level noise cuts. The third chain shown is identical to another one run unprescaled in 2012. For this chain, L1 did not pass events in the first three bunches of a bunch train. As discussed above, in- and out-of-time pileup largely cancel on average for the remaining bunches, so that \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger rates are lower for these bunches for the same threshold. This allowed lowering trigger thresholds at L1 to 35 GeV and at L2 to 40 GeV while keeping EF at 80 GeV, with a significant increase in efficiency, as shown in the figure. The cost of doing this was the loss in luminosity associated with the first 3 bunches in each group, estimated from the \(W\to\mu\nu\) candidate events to be at the level of about 8%.

Comparison of the various algorithm characteristics shown above demonstrates that, because of both the algorithm properties and the thresholds used, the combined full \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger chain (selecting only the events that pass all three algorithms, L1 trigger-tower, L2 FEB and EF tclcw) used in 2012 performed much better than that used in 2011 with respect to retaining with high efficiency events to be used in offline analysis.

Figure 2: Event-by-event comparison of \(E_{\mathrm{T}}^{\mathrm{miss}}\) in data collected with a random trigger on crossing bunches as calculated with trigger level algorithms and the offline cluster algorithm calculation that corrects energies according to the calorimeter objects contributing to the calorimeter energy, for events triggered on random bunch crossings. The trigger algorithms compared are the L1 trigger-tower algorithm (top left), the L2 FEB algorithm (top right), the EF cell algorithm with two-sided noise cuts (middle left), an EF algorithm using clustering (bottom right) and the EF tclcw algorithm which uses clustering with hadronic calibration (bottom left). A few low \(E_{\mathrm{T}}^{\mathrm{miss}}\) bins are empty for the trigger-tower algorithm because of the addition in quadrature of the 1 GeV granularity \(x\) and \(y\) component values.

Figure 3: Efficiencies for \(W\to\mu\nu\) candidate events of the individual algorithms and the full combined trigger chain (selecting only the events that pass all three trigger levels, L1, L2 and EF) for one of the nominal unprescaled \(E_{\rm T}^{\rm miss}\) triggers used in 2012. Events are selected with a trigger requiring a muon candidate with 15\(\,\)GeV transverse momentum at L1 and 24\(\,\)GeV at L2 and EF, and requiring an offline-calculated \(W\) transverse mass between 40 and 100\(\,\)GeV. The chain used here has threshold of 50\(\,\)GeV for the L1 trigger-tower algorithm, 55\(\,\)GeV for the L2 FEB algorithm and 80\(\,\)GeV for the hadronically calibrated cluster (tclcw) algorithm. Efficiencies are shown as a function of offline \(E_{\rm T}^{\rm miss}\) for \(W\to\mu\nu\) candidate events for two offline algorithms: one based on calorimeter clustering with hadronic response correction (left) and one based on a sum of identified and calibrated objects - but not including the offline muon contribution -(right).

## 5 Conclusions

Figure 4: Comparison of efficiencies for \(W\to\mu\nu\) candidate events of three trigger chains: L1 trigger-tower algorithm with a threshold of 50 GeV, L2 FEB algorithm with a threshold of 55 GeV and EF cell-sum algorithm with a threshold of 80 GeV (black); L1 trigger-tower algorithm with a threshold of 50 GeV, L2 FEB algorithm with a threshold of 55 GeV and EF hadronically calibrated cluster algorithm (tclcw) with a threshold of 80 GeV (red); and L1 trigger-tower algorithm not firing on the first three bunches (BG7 in the figure refers to the mechanism ATLAS uses to trigger on particular subsets of bunches) and threshold of 35 GeV, L2 FEB algorithm with a threshold of 40 GeV and EF tclcw algorithm with a threshold of 80 GeV (green). The first trigger chain (black) is similar to, but not identical with, the trigger used in 2011. Events are selected with a trigger requiring a muon candidate with 15 GeV transverse momentum at L1 and 24 GeV at L2 and EF, and requiring an offline-calculated \(W\) transverse mass between 40 and 100 GeV. Efficiencies are shown for two offline algorithms: one based on calorimeter clustering with hadronic response correction (left) and one based on a sum of identified and calibrated objects - but not including the offline muon contribution - (right). The thresholds for these chains were not matched for trigger rates. However, the cell-sum chain is approximately what would have been used had no cluster algorithm chain been developed. The cluster algorithm trigger chain and the chain not selecting events on the first three bunches were the lowest threshold unprescaled triggers of their type used during running.

## 5 Dependence of performance on increases in luminosity

The average instantaneous number of interactions per bunch crossing in 2012 ranged from about 20 to 40. This section discusses the impact of the increase in \(\mu\) on the trigger, including changes in trigger rates, \(E_{\text{T}}^{\text{miss}}\) resolution and efficiency.

### Trigger rates and resolution

The rate of \(E_{\text{T}}^{\text{miss}}\) triggers is overwhelmingly dominated by the fraction of events without true \(E_{\text{T}}^{\text{miss}}\) which pass the trigger threshold requirement because of mismeasurement. The rate of such triggers is therefore what determines the trigger threshold. Increase in luminosity results in higher trigger rates making it necessary to raise thresholds, thus affecting efficiency. As the number of events of any type is proportional to luminosity, linear dependence of trigger rate with luminosity is the usual case for most triggers. For the global transverse momentum triggers, however, additional interactions in the same bunch crossing can skew distributions and give rise to much faster trigger-rate increases. Monitoring of trigger rates and threshold adjustment with luminosity was therefore particularly important for these triggers. The sensitivity of rate to luminosity is therefore an important measure of trigger performance, and is described in this section.

A detailed discussion of \(\Sigma E_{\text{T}}\) distributions, pileup, \(E_{\text{T}}^{\text{miss}}\) resolution, and the resulting \(E_{\text{T}}^{\text{miss}}\) distributions is given in Reference [20]. For a single random interaction, the probability of getting large \(\Sigma E_{\text{T}}\) drops roughly exponentially with increasing \(\Sigma E_{\text{T}}\) above an initial peak whose position is determined in part by the calorimeter zero suppression being used (see for example the \(\Sigma E_{\text{T}}\) distributions in Figure 1, though those are for events with multiple interactions per bunch crossing). To describe the case of multiple interactions per bunch crossing, a convolution of exponential distributions is performed. The resulting Erlang \(\Sigma E_{\text{T}}\) distribution predicts a much faster than linear in \(\mu\) increase in the high \(\Sigma E_{\text{T}}\) trigger rate. Such behavior can be seen in Figure 5, which shows the trigger rate as a function of \(\mu\) for several \(\Sigma E_{\text{T}}\) trigger chains. The thresholds given in the figure are for the EF-level algorithm of each chain; the full set of thresholds at all levels for each chain are provided in Table 4 in Section 7.

Figure 5: The dependence on \(\mu\), averaged over luminosity blocks, of the rate of several \(\Sigma E_{\text{T}}\) trigger chains. The rates are corrected for prescales used. The thresholds shown are those at the EF level of the trigger chains.

For events in which calorimeter energy is not dominated by a hard scatter, the total energy in the calorimeter determines the scale of fluctuations of \(E_{\mathrm{T}}^{\mathrm{miss}}\). This can be seen in Figure 6, which shows the width of the \(E_{x}^{\mathrm{miss}}\) distributions as a function of \(\mu\) for events collected with a random trigger. Because of this increase in the scale of fluctuations of \(E_{\mathrm{T}}^{\mathrm{miss}}\), the \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger rate also increases much more quickly than linearly with \(\mu\), as long as the threshold is low enough that fluctuations in \(\Sigma E_{\mathrm{T}}\) dominate the trigger. At higher threshold values, mismeasurement of QCD jet events begin to dominate the \(E_{\mathrm{T}}^{\mathrm{miss}}\) rate [20]. As the number of such events scales linearly with luminosity, the \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger rate should scale linearly with \(\mu\) at these thresholds. These trends are evident in Figure 7, where the trigger rate is seen to scale linearly with \(\mu\) for high threshold \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers, but to grow much more quickly with \(\mu\) for low threshold triggers. The thresholds given in the figure are for the EF-level algorithm of each trigger chain; the full set of thresholds at all 3 levels for each chain are provided in Table 3 in Section 7. Not surprisingly, the rate-determined thresholds of the 2012 \(E_{\mathrm{T}}^{\mathrm{miss}}\) unprescaled triggers are in the range for which the rate dependence on luminosity is linear.

The increase with luminosity in the width of the \(met\) distribution from any one algorithm means that the resulting trigger rate will be higher and that the threshold may need to be increased for that algorithm. However, care must be taken in comparing the widths of different algorithms. For example, the tclcw algorithm has a larger width at the same \(\mu\) as the cell algorithm. But, since the former includes hadronic calibration, the \(E_{\mathrm{T}}^{\mathrm{miss}}\) values it determines for events with true \(E_{\mathrm{T}}^{\mathrm{miss}}\) are also likely to be higher than those of the cell algorithm, so its threshold can be set higher. The net effect of comparative width on efficiency is therefore not clear. On the other hand, the fact that the tclcw algorithm width grows more quickly with \(\mu\) than that of the cell algorithm may be an indication that it is less robust to changes in luminosity.

For XS triggers, the rates should be \(\mu\) independent as long as the triggers are dominated by \(\Sigma E_{\mathrm{T}}\) fluctuations. For large XS, where these fluctuations are less likely to contribute, the trigger rates could even drop since a fixed value of \(E_{\mathrm{T}}^{\mathrm{miss}}\) is on average less significant for larger \(\mu\) (for which \(\Sigma E_{\mathrm{T}}\) is typically larger). Such trends were visible in 2011 data as presented in [4]. Data for 2012 is presented in Figure 8. Here the XS trigger rates all increase roughly linearly with increasing \(\mu\). The thresholds given in the figure are for the EF-level algorithm; the full set of thresholds at all 3 levels for each chain are provided in Table 5 in Section 7. Figure 9 shows the XS distributions for the EF algorithms as calculated by using the parameters determined during the early part of data collection. These constants were used for the entire data collection period. While the shapes are similar, for the higher \(\mu\) values of 2012 the \(\mu\) dependence of \(E_{\mathrm{T}}^{\mathrm{miss}}\) resolution is apparently no longer negligible.

A more detailed analysis of XS can improve the understanding of this behavior. The XS trigger parameterizes the event-by-event resolution in \(E_{\mathrm{T}}^{\mathrm{miss}}\) in terms of \(\Sigma E_{\mathrm{T}}\). As discussed above, the width of the \(E_{x}^{\mathrm{miss}}\) and \(E_{y}^{\mathrm{miss}}\) distributions for random triggers are a measure of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) resolution. Events are divided into bins of \(\Sigma E_{\mathrm{T}}\), and a Gaussian fit is done to the central part of the \(E_{x}^{\mathrm{miss}}\) and \(E_{y}^{\mathrm{miss}}\) distributions. Sample fits for few values of \(\Sigma E_{\mathrm{T}}\) are shown for the EF cell algorithm in Figure 10. The set of fit widths versus \(\sqrt{\Sigma E_{\mathrm{T}}}\) are then used to determine the dependence of \(E_{\mathrm{T}}^{\mathrm{miss}}\) resolution on \(\Sigma E_{\mathrm{T}}\). Figure 11 shows the width of the \(E_{x}^{\mathrm{miss}}\) distributions as a function of \(\Sigma E_{\mathrm{T}}\) for various values of \(\mu\) for the trigger algorithms. Also shown is a linear fit to the widths of the all-\(\mu\) distributions of the form \(\sigma(E_{x}^{\mathrm{miss}})=a+b\sqrt{\Sigma E_{\mathrm{T}}}\). While the \(\sqrt{\Sigma E_{\mathrm{T}}}\) dependence is the dominant one for the \(E_{\mathrm{T}}^{\mathrm{miss}}\) resolution, the \(\mu\) dependence is also clearly visible.

Table 1 lists the constants determined for XS parameterization during 2012 data taking. In these linear models there is some correlation between the calculated slope and intercept. The effects due to this correlation are small compared with the differences in fit parameters for different algorithms, but are important when comparing \(E_{x}^{\mathrm{miss}}\) and \(E_{y}^{\mathrm{miss}}\) fits. To remove this correlation, a fit was done to a function of

the form \(\sigma(E_{\mathrm{x}}^{\mathrm{miss}})=a+b(\sqrt{\Sigma E_{\mathrm{T}}}-\sqrt{E_{M }})\), where \(\sqrt{E_{M}}\) is the mean value of \(\sqrt{\Sigma E_{\mathrm{T}}}\). As shown in Table 2 the fits to \(E_{x}^{\mathrm{miss}}\) and \(E_{y}^{\mathrm{miss}}\) agree to about 1%.

### Efficiency

Pileup increases the fluctuation in the measurement of \(E_{\mathrm{T}}^{\mathrm{miss}}\), increasing the trigger rate. For the \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers this must be dealt with by increasing the threshold, which results in lower efficiency for signal events. In addition, the smearing of \(E_{\mathrm{T}}^{\mathrm{miss}}\) in itself can also decrease the efficiency. Figure 12 left compares the efficiency of \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers for \(W\) candidate events for different ranges of \(\mu\). The efficiency curve shifts

Figure 6: The dependence of the width of the \(E_{\mathrm{x}}^{\mathrm{miss}}\) distributions on \(\mu\), averaged over luminosity blocks, is shown for \(E_{\mathrm{T}}^{\mathrm{miss}}\) calculated using L1 trigger towers (top left), the L2 FEB algorithm (top right), the EF cell algorithm with two-sided noise cuts (bottom left), and the EF tclcw algorithm which uses clustering with hadronic calibration (bottom right), for events collected with a random trigger on colliding bunches. The red lines show linear fits to the plots, which only roughly describe the behavior.

by 10 to 20 GeV when \(\mu\) changes from about 15 to 25, depending on the offline calculation used. As

\begin{table}
\begin{tabular}{|l|c|c|c|} \hline
**Algorithm** & \(a\) **[GeV]** & \(b\) **[GeV\({}^{\mathbf{0.5}}\)]** & _E\({}_{\mathrm{T}}^{\mathrm{miss}}\) Thresh. [GeV]** \\ \hline L1 trigger tower (period A) & -1.9385 & 1.1305 & 80 \\ L1 trigger tower & -1.866 & 1.15 & 80 \\ L2 FEB & -0.898 & 0.57315 & 80 \\ EF Cell & 4.265 & 0.2966 & 95 \\ EF using cluster algorithm & 1.7035 & 0.60855 & 95 \\ EF hadronically calibrated clusters & 1.0915 & 0.837 & 95 \\ \hline \end{tabular}
\end{table}
Table 1: Constants used for XS in 2012 for parameterizing \(E_{\mathrm{T}}^{\mathrm{miss}}\) component fluctuations as \(a+b\sqrt{\mathrm{z}E_{\mathrm{T}}}\). The stand-alone XS triggers used the L1 trigger tower, L2 FEB, and EF Cell algorithms. Events with \(E_{\mathrm{T}}^{\mathrm{miss}}\) above the threshold were passed regardless of XS value.

Figure 8: The dependence on \(\mu\), averaged over luminosity blocks, of the XS trigger rate of several complete chains requiring passing of algorithms at L1, L2, and EF levels. The rates are corrected for prescales used. The thresholds used to label the data are those of EF level of the chains.

Figure 7: The dependence on \(\mu\), averaged over luminosity blocks, of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger rate for full trigger chains, requiring passing of L1, L2 and EF algorithms, with various thresholds. The rates are corrected for prescales used. The right side plot shows the rate on a linear scale to make clear the linear dependence on \(\mu\) for high threshold triggers. The thresholds shown in the figure are those of the chain’s EF algorithm.

discussed above, the XS trigger rate is designed to depend much more weakly on \(\mu\). On the other hand, since \(\Sigma E_{\rm T}\) increases with increasing \(\mu\), XS will tend to be lower at higher \(\mu\) for the same true \(E_{\rm T}^{\rm miss}\). The efficiency for XS triggers therefore drops significantly with increasing \(\mu\). This can be seen in Figure 12 right, which shows the emulated efficiency of a trigger which uses solely the thresholds of a standard XS chain (thresholds of 3.0, 3.0, and 4,5 at L1, L2 and EF respectively) to determine whether to pass an event. To mitigate this effect, XS triggers also passed events which had \(E_{\rm T}^{\rm miss}\) above some threshold, regardless of the XS value.

Figure 10: \(E_{x}^{\rm miss}\) distributions for two \(\sqrt{\Sigma E_{\rm T}}\) bins for the EF cell algorithm with two-sided noise cuts. A Gaussian fit to the central part of the distribution is shown in red. The distribution and fit mean and width are shown and are in good agreement.

Figure 9: XS distributions for random triggers of the EF cell algorithm, as calculated using the XS parameters used during data taking. Though shapes are similar, a clear dependence on the range of \(\mu\) is visible.

Figure 11: The standard deviation of \(E_{x}^{\rm miss}\) versus \(\sqrt{\Sigma E_{\rm T}}\) shown for a few ranges of \(\mu\) (labeled by their average value) for the L1 algorithm (top left), L2 FEB algorithm (top right), EF cell algorithm (bottom left) and the EF tclw algorithm which uses clustering with hadronic calibration (bottom right), for events collected with a random trigger on colliding bunches. Also shown is a linear fit to the full data set.

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline
**Algorithm** & \(\sqrt{E_{M}}\) **[GeV\({}^{0.5}\)]** & \(\sigma\) **Component** & \(a\) **[GeV]** & \(b\) **[GeV\({}^{0.5}\)]** \\ \hline L1 trigger tower & 5.676 \(\pm\) 0.002 & \(x\) & 4.506 \(\pm\) 0.002 & 1.186 \(\pm\) 0.002 \\  & 5.675 \(\pm\) 0.002 & \(y\) & 4.531 \(\pm\) 0.002 & 1.189 \(\pm\) 0.002 \\ L2 FEB & 9.425 \(\pm\) 0.001 & \(x\) & 4.508 \(\pm\) 0.001 & 0.5559 \(\pm\) 0.0005 \\  & 9.412 \(\pm\) 0.001 & \(y\) & 4.483 \(\pm\) 0.001 & 0.5583 \(\pm\) 0.0005 \\ EF Cell & 10.355 \(\pm\) 0.002 & \(x\) & 7.418 \(\pm\) 0.002 & 0.3403 \(\pm\) 0.0007 \\  & 10.366 \(\pm\) 0.002 & \(y\) & 7.469 \(\pm\) 0.002 & 0.3412 \(\pm\) 0.0007 \\ EF cluster & 9.597 \(\pm\) 0.002 & \(x\) & 7.816 \(\pm\) 0.002 & 0.6066 \(\pm\) 0.0008 \\  & 9.585 \(\pm\) 0.002 & \(y\) & 7.775 \(\pm\) 0.002 & 0.6120 \(\pm\) 0.0008 \\ EF had. calib. cluster & 11.192 \(\pm\) 0.004 & \(x\) & 10.685 \(\pm\) 0.004 & 0.793 \(\pm\) 0.002 \\  & 11.168 \(\pm\) 0.004 & \(y\) & 10.637 \(\pm\) 0.004 & 0.801 \(\pm\) 0.002 \\ \hline \end{tabular}
\end{table}
Table 2: Results of fits to the dependence of \(E_{x}^{\rm miss}\) and \(E_{y}^{\rm miss}\) resolution on \(\sqrt{\Sigma E_{\rm T}}\) parameterized in the form \(\sigma(E_{x}^{\rm miss})=a+b(\sqrt{\Sigma E_{\rm T}}-\sqrt{E_{M}})\). Results show good agreement of \(E_{x}^{\rm miss}\) and \(E_{y}^{\rm miss}\) for each of the algorithms used in 2012: L1 trigger tower, L2 FEB, EF cell sum, EF using calorimeter clustering, and EF using calorimeter clustering and hadronic calibration (tclcw).

Figure 12: Trigger efficiencies for \(W\rightarrow\mu\nu\) candidate events, selected with a trigger requiring a muon candidate with 15 GeV transverse momentum at L1 and 24 GeV at L2 and EF, and requiring an offline-calculated \(W\) transverse mass between 40 and 100 GeV. Left: Efficiency of a full \(E_{\rm T}^{\rm miss}\) trigger chain with L1 trigger-lower, L2 FEB, and EF hadronic cluster (tclcw) thresholds of 50, 55, and 80 GeV respectively, as a function of offline \(E_{\rm T}^{\rm miss}\). Right: Efficiency of an emulated trigger chain which, unlike the XS triggers actually run, uses only XS values to determine whether an event passes. The thresholds used are 3.0, 3.0, and 4.5 for L1 trigger-lower, L2 FEB, and EF cell algorithms respectively. Efficiencies are shown for offline \(E_{\rm T}^{\rm miss}\) calculated from the calorimeter clusters, therefore not including the offline muon contribution. They are compared for \(\mu<10\), between 10 and 20, between 20 and 30, between 30 and 40, and greater than 40.

## 6 Comparison of measured and simulated events

This section presents a comparison of measured and simulated properties of the transverse momentum trigger algorithms. Algorithm resolution and high transverse momentum tails are studied with events having little true \(E_{\mathrm{T}}^{\mathrm{miss}}\), using a random trigger and a minimum-bias simulation. Because of the steep decrease in the number of events with increasing \(E_{\mathrm{T}}^{\mathrm{miss}}\), this approach does not, however, allow comparison of measured and simulated trigger rates. Such a study would require several orders of magnitude more simulated minimum-bias events, and inclusion of other rarer events that contribute at high \(E_{\mathrm{T}}^{\mathrm{miss}}\). Collected \(W\to\mu\nu\) candidate events are compared with a simulated sample to study the trigger efficiency for events with true \(E_{\mathrm{T}}^{\mathrm{miss}}\).

Measured \(E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\Sigma E_{\mathrm{T}}\) distributions in 2011, both for events selected by a random trigger on crossing bunches and on W candidate events, had a larger tail of high transverse momentum events than predicted by simulations [4]. This was attributed mainly to a mixture of use in the those simulations of PYTHIA 6[21], which does not describe minimum-bias events as well as PYTHIA 8, and to imprecise simulation of cell noise. Figures 13, 14, 15 and 16 compare the 2012 \(E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\Sigma E_{\mathrm{T}}\) of events collected with a random trigger on colliding bunches with simulations. The L1 simulated distributions greatly overestimate the tails of the distributions for the first three bunch crossings, but do much better for the rest of the data. This indicates incomplete modeling of the effects at this level of noise, pileup and zero suppression on the first three bunches. The FEB high \(E_{\mathrm{T}}^{\mathrm{miss}}\) tail seen in the data but not the simulation arises from noisy cells which could not be quickly masked at this level. There is better agreement at EF level for both \(E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\Sigma E_{\mathrm{T}}\) distributions, though differences remain. This implies that the physics model for random events is reasonably good, as is the description of calorimeter cell response. Because of the steeply falling spectra, the impact on trigger rates of the remaining differences can be compensated for by shifts of only a few percent in thresholds (a few \(\mathrm{GeV}\) for \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers and a few 10's of \(\mathrm{GeV}\) for \(\Sigma E_{\mathrm{T}}\) thresholds).

Figure 17 compares the measured and simulated efficiency of a full trigger chain (requiring events to pass at all three trigger levels, L1, L2 and EF) as a function of offline \(E_{\mathrm{T}}^{\mathrm{miss}}\) calculated from calorimeter clusters alone. The simulation and the data agree quite well, and predict roughly the same offline \(E_{\mathrm{T}}^{\mathrm{miss}}\) value at which 50% efficiency is reached. The measured resolution appears slightly better than the simulated one, as it results in a curve which is a few \(\mathrm{GeV}\) narrower.

Figure 13: The measured L1 algorithm distribution of \(E_{\rm T}^{\rm miss}\) (top) and \(\Sigma E_{\rm T}\) (bottom) for events collected with a random trigger on colliding bunches compared with simulations for the first 3 bunches in a train (left) and all other bunches (right).

Figure 14: The measured L2 FEB algorithm distribution of \(E_{\rm T}^{\rm miss}\) (top) and \(\Sigma E_{\rm T}\) (bottom) for events collected with a random trigger on colliding bunches compared with simulations for the first 3 bunches in a train (left) and all other bunches (right).

Figure 15: The measured EF cell-sum algorithm distribution of \(E_{\rm T}^{\rm miss}\) (top) and \(\Sigma E_{\rm T}\) (bottom) for events collected with a random trigger on colliding bunches compared with simulations for the first 3 bunches in a train (left) and all other bunches (right).

Figure 16: The measured EF hadronically calibrated cluster (tclcw) algorithm distribution of \(E_{\rm T}^{\rm miss}\) (top) and \(\Sigma E_{\rm T}\) (bottom) for events collected with a random trigger on colliding bunches compared with simulations for the first 3 bunches in a train (left) and all other bunches (right).

Figure 17: Comparison of measured efficiency for \(W\to\mu\nu\) candidate events with simulated efficiency for \(W\to\mu\nu\) events of a full \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger chain with L1 trigger-tower, L2 FEB, and EF hadronic cluster (tclcw) thresholds of 50, 55, and 80 GeV respectively. Efficiencies are shown as a function of offline \(E_{\mathrm{T}}^{\mathrm{miss}}\), calculated from the calorimeter clusters, therefore not including the offline muon contribution. Events are selected with a trigger requiring a muon candidate with 15 GeV transverse momentum at L1 and 24 GeV at L2 and EF, and requiring an offline-calculated \(W\) transverse mass between 40 and 100 GeV.

## 7 Triggers implemented in 2012

This section discusses the full trigger chains, L1 to L2 to EF, that were used in 2012. "Stand-alone" \(E_{\mathrm{T}}^{\mathrm{miss}}\), \(\Sigma E_{\mathrm{T}}\), and XS triggers selected events solely according to their transverse-momentum properties. "Combined" triggers included additional selection, such as the requirement of a lepton trigger. The algorithms used for the stand-alone triggers were the ones described above, and used only calorimeter information. Algorithms used in some of the "combined" chains that required the presence of muon candidates also performed muon-momentum corrections to \(E_{\mathrm{T}}^{\mathrm{miss}}\). Rates for low threshold triggers were set by prescaling them. Luminosity and trigger-rate limitations determined the lowest threshold at which a trigger could run without being prescaled.

The lowest-threshold unprescaled \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers were the ones primarily used in physics analyses. The improvements in the 2012 \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers discussed above allowed for them to be used with good efficiency for a large number of ATLAS physics studies. Analyses used both the trigger chains running on all bunches and those omitting the first three bunches. These triggers were the primary ones used by ATLAS in 2012 for supersymmetry searches and a generic dark matter search, and were also used in searches for various exotic decays. They were used in several Higgs studies, such as \(H\) decay to invisible particles or \(H\to b\bar{b}\) decays when \(ZH\) production is followed by \(Z\) decay to neutrinos. They were also used in conjunction with other triggers, either at the selection level, for example with leptons in searching for \(W\) to \(\tau\) decay followed by \(\tau\to 3\mu\), or in \(W\) tag and probe studies of electron efficiency.

In order to allow fast response to high rates, additional higher-threshold "backup" unprescaled trigger chains ran along with the lowest threshold ones. If rates become too high the lowest threshold triggers could then in principle be prescaled or turned off and thereby replaced by the next lowest threshold unprescaled trigger.

Only a few high threshold stand-alone \(\Sigma E_{\mathrm{T}}\) chains ran, a subset of these unprescaled, mainly to study trigger behavior. These triggers were found to mainly select either energetic QCD multi -jet events or high-pileup events. They did not independently select events interesting for physics analysis.

Although a few stand-alone XS triggers did run, these were always prescaled. Already in 2011 these triggers were not found efficient enough to use on their own for any physics studies. They were used at that time in conjunction with other triggers, for example in selecting \(\tau\nu\) final states. However, though XS triggers combined with others were used to select events in 2012, these triggers were found less efficient than others and were not used in physics analyses.

Some representative stand-alone \(E_{\mathrm{T}}^{\mathrm{miss}}\), \(\Sigma E_{\mathrm{T}}\), and XS triggers are described in Tables 3, 4 and 5. \(E_{\mathrm{T}}^{\mathrm{miss}}\), \(\Sigma E_{\mathrm{T}}\), and XS trigger chains are indicated by "xe", "te" or "xs" at the start of the name, respectively. This is followed by a number referring to the transverse-momentum threshold in GeV (or 10 times the dimensionless XS threshold for XS triggers) for the EF algorithm. Additional terms are appended to indicate information about algorithms used or other special conditions.

## 8 Summary and conclusion

The ATLAS global transverse-momentum triggers in 2012 included those on \(\Sigma E_{\mathrm{T}}\), \(E_{\mathrm{T}}^{\mathrm{miss}}\) and the \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance, XS. Improvements over 2011 were introduced at all three trigger levels. At L1, noise cuts applied to the trigger-tower algorithm took luminosity into account, and a trigger was introduced that

\begin{table}
\begin{tabular}{|l|c|c|c|l|} \hline
**Trigger** & \multicolumn{3}{|c|}{**Thresholds [GeV]**} & **Comments** \\  & **L1** & **L2** & **EF** & \\ \hline xe30 & 20 & 25 & 30 & Prescaled when it ran \\ xe30\_tclcw & 20 & 25 & 30 & Prescaled when it ran \\ xe60 & 40 & 45 & 60 & Prescaled \\ xe70 & 50 & 55 & 70 & Prescaled \\ xe80 & 60 & 65 & 80 & Not prescaled \\ xe80T\_tclcw\_veryloose & 35 & 40 & 80 & Not prescaled; first 3 bunches in a group omitted \\ xe80\_tclcw & 50 & 55 & 80 & Not prescaled \\ xe80\_tclcw\_tight & 60 & 65 & 80 & Not prescaled \\ xe100\_tclcw\_loose & 50 & 55 & 100 & Not prescaled \\ xe100\_tclcw & 60 & 65 & 100 & Not prescaled \\ \hline \end{tabular}
\end{table}
Table 3: Definition of some representative stand-alone \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger chains used in 2012. All of these used the trigger-tower algorithm at L1. Unless otherwise indicated in the name, the L2 FEB and EF cell algorithms were used and all bunches were included in the trigger. The use of “tclcw” in a trigger name means that the EF cluster algorithm with hadronic calibration was used. A “T” in the name following the numerical value means that an L1 trigger was used which omitted the first three bunches in each bunch train. Suffixes such as “tight” or “loose” indicate that the lower trigger-level thresholds were higher or lower respectively than the nominal ones for the same EF-level threshold. The L1 threshold is only approximately in GeV.

\begin{table}
\begin{tabular}{|l|c|c|c|l|} \hline
**Trigger** & \multicolumn{3}{|c|}{**Thresholds [GeV]**} & **Comments** \\  & **L1** & **L2** & **EF** & \\ \hline  & 20 GeV \(E_{\mathrm{T}}^{\mathrm{miss}}\) & 1.5 & 3.0 & Prescaled; used an \(E_{\mathrm{T}}^{\mathrm{miss}}\) rather than XS threshold at L1 \\ xs45 & 3.0 & 3.0 & 4.5 & Prescaled \\ xs60 & 4.5 & 4.5 & 6.0 & Prescaled \\ xs75 & 5.0 & 5.0 & 7.5 & Prescaled \\ xs100 & 6.0 & 6.0 & 10.0 & Prescaled \\ \hline \end{tabular}
\end{table}
Table 4: Definition of the main stand-alone \(\Sigma E_{\mathrm{T}}\) trigger chains used for proton-proton collisions in 2012. These triggers used the trigger-tower algorithm at L1, the FEB algorithm at L2 and the cell algorithm at EF level. The L1 threshold is only approximately in GeV.

\begin{table}
\begin{tabular}{|l|c|c|l|l|} \hline
**Trigger** & \multicolumn{3}{|c|}{**Thresholds [GeV]**} & **Comments** \\  & **L1** & **L2** & **EF** & \\ \hline  & 20 & 25 & 30 & Prescaled when it ran \\ xe30\_tclcw & 20 & 25 & 30 & Prescaled when it ran \\ xe60 & 40 & 45 & 60 & Prescaled \\ xe70 & 50 & 55 & 70 & Prescaled \\ xe80 & 60 & 65 & 80 & Not prescaled \\ xe80T\_tclcw\_veryloose & 35 & 40 & 80 & Not prescaled; first 3 bunches in a group omitted \\ xe80\_tclcw & 50 & 55 & 80 & Not prescaled \\ xe80\_tclcw\_tight & 60 & 65 & 80 & Not prescaled \\ xe100\_tclcw\_loose & 50 & 55 & 100 & Not prescaled \\ xe100\_tclcw & 60 & 65 & 100 & Not prescaled \\ \hline \end{tabular}
\end{table}
Table 3: Definition of some representative stand-alone \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger chains used in 2012. All of these used the trigger-tower algorithm at L1. Unless otherwise indicated in the name, the L2 FEB and EF cell algorithms were used and all bunches were included in the trigger. The use of “tclcw” in a trigger name means that the EF cluster algorithm with hadronic calibration was used. A “T” in the name following the numerical value means that an L1 trigger was used which omitted the first three bunches in each bunch train. Suffixes such as “tight” or “loose” indicate that the lower trigger-level thresholds were higher or lower respectively than the nominal ones for the same EF-level threshold. The L1 threshold is only approximately in GeV.

omitted the colliding bunches that were most severely affected by pileup. A new algorithm, using front-end board sums, was introduced at L2 for all trigger types. At EF level, a cluster algorithm with hadronic calibration was introduced and used for some of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers. These changes provided algorithms which agreed better with offline measurements and resulted in improved signal efficiency, despite the increased LHC luminosity.

Dependence of global transverse-momentum variables and trigger rates on luminosity are consistent with a general description discussed in detail in Reference [20]: increased luminosity results in a linear increase in mismeasured QCD jet events that give rise to large \(E_{\mathrm{T}}^{\mathrm{miss}}\), but also produces more energy in the calorimeter and larger fluctuations in energy measurements. The result is \(\Sigma E_{\mathrm{T}}\) and \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger rates that increase linearly or exponentially with luminosity, depending on trigger thresholds.

Simulated \(E_{\mathrm{T}}^{\mathrm{miss}}\) distributions of unbiased events reproduce the general features of measured global transverse-momentum variables for events collected with random triggers. However, pileup effects, especially in the first few bunch crossings where the impact is most severe, are not perfectly modeled. Simulation models well the measured efficiency of a complete \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger chain for signal events with large offline \(E_{\mathrm{T}}^{\mathrm{miss}}\), as determined with a hadronically calibrated cluster algorithm.

The large pileup effects at high luminosity reduced the usefulness of XS and \(\Sigma E_{\mathrm{T}}\) triggers in 2012. But the improvements made for 2012 enabled the \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers to select events for a wide variety of physics studies.

## References

* [1] The ATLAS Collaboration, _The ATLAS Experiment at the CERN Large Hadron Collider_, JINST **3** (2008) S08003.
* [2] D. Casadei et. al., _The implementation of the ATLAS missing \(E_{\mathrm{T}}\) triggers for the initial LHC operation_, ATL-DAQ-PUB-2011-001 (2011).
* [3] The ATLAS Collaboration, _Performance of the ATLAS transverse energy triggers with initial LHC runs at \(\sqrt{s}\) = 7 TeV_, ATLAS-CONF-2011-072 (2011).
* [4] The ATLAS Collaboration, _The ATLAS transverse-momentum trigger performance at the LHC in 2011_, ATLAS-CONF-2014-002 (2014).
* [5] The ATLAS Collaboration, _Performance of Missing Transverse Momentum Reconstruction in ATLAS with 2011 Proton-Proton Collisions at \(\sqrt{s}\) =7 TeV_, ATLAS-CONF-2012-101 (2012).
* [6] The ATLAS Collaboration, _Performance of missing transverse momentum reconstruction in proton-proton collisions at 7 TeV in ATLAS_, Eur. Phys. J. **C 72** (2012) 1844.
* [7] The ATLAS Collaboration, _Jet energy measurement with the ATLAS detector in proton-proton Collisions at 7 TeV_, Eur. Phys. J. **C 73** (2013) 2304.
* [8] The ATLAS Collaboration, _Local Hadronic Calibration_, ATL-LARG-PUB-2009-001-2 (2009).