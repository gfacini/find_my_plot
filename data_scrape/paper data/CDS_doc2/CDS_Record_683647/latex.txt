**ATLAS Internal Note**

**DAQ-NO-099**

**22 May 1998**

**The Read-Out Crate in ATLAS DAQ/EF Prototype -1**

presented by D. Francis

on behalf of

G. Ambrosini\({}^{\rm a}\), H.-P. Beck\({}^{\rm a}\), D. Francis\({}^{\rm b}\), M. Joos\({}^{\rm b}\), A. Lacourt\({}^{\rm b}\), G. Lehmann\({}^{\rm a}\),

A. Mailov\({}^{\rm c}\), L. Mapelli\({}^{\rm b}\), G. Mornacchi\({}^{\rm b}\), M. Niculescu\({}^{\rm b,e}\), K. Nurdan\({}^{\rm c}\),

J. Petersen\({}^{\rm b}\), D. Pringent\({}^{\rm b}\), J. Rochez\({}^{\rm b}\), R. Spiwoks\({}^{\rm b}\), L. Tremblet\({}^{\rm b}\) and G. Unel\({}^{\rm c}\).

a. Laboratory for High Energy Physics, University of Bern, Switzerland.

b. CERN, Geneva, Switzerland.

c. Bogazici University, Istanbul, Turkey.

e. Institute of Atomic Physics, Bucharest, Romania.

The systems exploitation in phase three covers laboratory systems test and possible use within test beam environments.

The project has been split into four sub-groups:

1. _Detector interfaces_: This group consists of representatives from each of the ATLAS detector groups. It is responsible for establishing the requirements of the detector groups and the smooth integration of the DAQ/EF with the detectors.
2. _DataFlow_: This group is responsible for the moving of data from the detector Read-Out Links to mass storage.
3. _Event Filter_: This group is responsible for the studies related to the architecture of the Event Filter, the usability of off-line code in the Event Filter and the organization of processing power, including its use for detector monitoring and calibration.
4. Back-End: This group is responsible for the software related to configuring, control and monitoring of the DAQ/EF.

Two possible architecture of the prototype are envisaged, one of these architecture is shown in Figure 1.

**The Read-Out crate**

The Read-Out Crate (ROC) is shown within its context in Figure 1. The logical view of the ROC is shown in Figure 2. It consists of a Local DAQ (LDAQ), one or more ROBs, an Event Filter Interface (EFIF) and a Trigger module (TRG). The LDAQ provides the control and monitoring functionality within the ROC and also provides the interface with the Back-End system. The ROB, EFIF and TRG are instances of a logical object referredto as an I/O Module (IOM). The different IOMs have the following functionality:

* The TRG: this module provides the interface to the trigger systems. It receives and buffers data control messages from the trigger system and relays them to other IOMs within the ROC.

* The EFIF: this module receives data control messages from the TRG. It collects and buffers data from the ROBs via a process called Data Collection (DC). This module is also the responsible for the output of data from the ROC to downstream elements of the DAQ/EF prototype -\(1\), namely the event building sub-system. It also provides data to the LDAQ for monitoring purposes.
* The ROB: this module receives data control messages from the TRG and EFIF, and receives and buffers data from the detector electronics. It provides data to the EFIF for data collection and the LDAQ for monitoring purposes.

An alternative prototype architecture is one in which the ROB modules are directly connected to the event building switching network i.e. there is no EFIF module.

The logical model of the ROC also has an _intra-create link_ object, not shown in the figure. This object is used to implement the exchange of data and data control messages between the ROC modules. The data control messages are defined as:

* _Level 2 Reject (L2R)_: This is a message sent from the TRG to the REIF. On reception of this message the REIF collects all the event fragments associated to a single event from the ROBs. The identifier of the event is contained within the message.
* _Region-Of-Interest Request (ROI)_: This message is sent from the TRG to the ROBs. It is a request from the Level 2 trigger system for data. On reception of the message the ROBs emulate the output of data to a level 2 trigger system. The identifier of the event information required is defined within the message.
* _Discard_: This is a message sent by the EFIF to the ROBs. It is used by the EFIF to inform the ROBs that data collection for a specific event has been performed and that the associated event fragments may be removed from the ROB buffers.

### Initial Implementation

As the name suggests, the pre-design implementation of the ROC does not perform all the functionality currently foreseen by the high-level design. A more complete description on the pre-design implementation can be found elsewhere [3].

The hardware implementation was based on various versions of the CES RIO2 [4] running the LynxOS operating system. A TRG, a EFIF and five ROBs were implemented on seven RO2s as follows:

* TRG: CES RIO 8062, PPC@166 MHz.
* EFIF: CES RIO 8061, PPC@96 MHz.

Figure 1: A DAQ/EF architecture.

Figure 2: Logical model of the ROC.

* Three ROBs: CES RIO 8061, PPC@96 MHz.
* Two ROBs: CES RIO 8062, PPC@200 MHz.

All communications between IOMs have been implemented on VMEbus. The VMEbus parameters for the

different types of data control messages and data collection are given in Table 1. In addition VMEbus request level three and fair arbitration were used.

A few points relating to the pre-design functionality are high-lighted here:

* The TRG does not receive data control message from an external trigger system. Instead the messages are generated internally. Emulation of input from external sources is performed using a simple PMC. Dummy data are transferred from the PMC, under the control of the PMC DMA, after 31 data control messages have been processed by the TRG application. The size of the DMA transfer is equal to the size of 31 data control messages (~840 bytes).
* The data control messages generated internally by the TRG are produced with the ratio 1:90:10 (L2A:L2R:ROI).
* The ROB generates event fragments internally and does not transfer any ROB fragments to the Level 2 trigger system.
* EFIF does not transfer any event data to the Event Builder sub-system.
* In addition to the exchange of data control messages, there is also the exchange of messages to ensure synchronisation between the TRG and the ROBs, see [3] for more details.
* The EFIF and ROB applications may run in emulation mode. In this mode all of their normal functionality is suppressed except the receiving of data control messages from the TRG.

### Performance studies

The measurements presented below were made using operating system timing functions and VMETRO PBTM-315 PCI bus and VBT-325 VMEbus analysers. Using these "tools" and by varying the number of L2R messages grouped before transfer to the ROBs and the number of ROBs used, measurements were made of:

1. The frequency and bandwidth at which data control messages were sent by the TRG.
2. The utilisation of PCI bus on the TRG, EFIF and ROB.
3. The utilisation of VMEbus.

The utilisation of PCI bus is indicated by the Transaction Time1 (T) and the Data Transfer Time2 (DT). The Transfer Efficiency on PCI bus is defined as the ratio DT/T and is a measure of how efficiently the system is transferring data.

Footnote 1: The number of bus samples with FRAME OR IRDY active divided by the number of samples.

_TRG with EFIF and ROB emulators_

In Figure 3. the rate at which the TRG sends data control messages using VMEbus as the communications link technology, is shown for up to three ROBs and as a function of L2R group size. In this measurement the EFIF and ROBs were running in emulation mode. It can be

\begin{table}
\begin{tabular}{|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|p{28.5pt}|} \hline  & & & & & \multicolumn{1}{p{28.5pt}|}{**Single cycle**} & \multicolumn{1}{p{28.5pt}|}{**BMA**} \\ \hline
**Type** & **Size (bytes)** & **Source** & **Destination** & **Master** & **WPa** & **RPb** & **(A32/ D32)** & **Chain** & **mode** \\ \hline \hline L2R & 24 & TRG & ROBs & TRG & yes & no & no & yes & D32 \\ \hline ROI & 56 & TRG & ROBs & TRG & yes & no & yes & no & no \\ \hline L2A & 24 & TRG & EFIF & TRG & yes & no & yes & no & no \\ \hline L2A & 24 & EFF & ROBs & EFIF & yes & no\({}^{c}\) & yes & no & no \\ \hline DC & N*1024 & ROBs & EFIF & EFIF & yes & yes & no & yes & D64 \\ \hline \multicolumn{7}{l}{a. Write Posting.} \\ \multicolumn{7}{l}{b. Read Pre-fetch.} \\ \multicolumn{7}{l}{c. A Pre-fetch block size of 16 and 32 words for a RIO2 8061 and 8062 respectively.} \\ \multicolumn{7}{l}{d. Number of ROBs.} \\ \end{tabular} A few points relating to the pre-design functionality are high-lighted here:

* The TRG does not receive data control message from an external trigger system. Instead the messages are generated internally. Emulation of input from external sources is performed using a simple PMC. Dummy data are transferred from the PMC, under the control of the PMC DMA, after 31 data control messages have been processed by the TRG application. The size of the DMA transfer is equal to the size of 31 data control messages (~840 bytes).
* The data control messages generated internally by the TRG are produced with the ratio 1:90:10 (L2A:L2R:ROI).
* The ROB generates event fragments internally and does not transfer any ROB fragments to the Level 2 trigger system.
* EFIF does not transfer any event data to the Event Builder sub-system.
* In addition to the exchange of data control messages, there is also the exchange of messages to ensure synchronisation between the TRG and the ROBs, see [3] for more details.
* The EFIF and ROB applications may run in emulation mode. In this mode all of their normal functionality is suppressed except the receiving of data control messages from the TRG.

\end{table}
Table 1: **The VMEbus parameters for each type of data control message.**

[MISSING_PAGE_FAIL:4]

implemented on a RIO2 clocked at 96 MHz (solid line in figure) and a RIO2 clocked at 200 MHz (dotted line in figure). In the latter case the measurement agrees with the expected behaviour.

### Discussion and outlook

The measurements and model predictions shown in Figure 5. and Figure 6. indicate that the current ROC performance is limited (as expected) by the implementation of data control message exchange and data collection on VMEbus. The measurements show that although VMEbus is being used at the expected maximum capacity, -70%, the maximum bandwidth achieved is only 6 Mbytes/s. This can be attributed to:

* the implemented message passing protocol.
* the additional synchronisation messages.

The first point refers to the data transfers associated with the message passing protocol: two single cycle reads and a single cycle write, for every message exchanged. Referring to Table 2., one can see that these operations can only be achieved at a bandwidth of 2 Mbytes/s. In addition, this version of the ROC uses single cycle writes for sending both the ROI and L2A data control messages. The second factor limiting the achievable performance is the exchange of synchronisation messages between the TRG and ROBs [3]. Basically, each ROB, for each event fragment it generates, performs a single cycle write to the TRG. In other words, the achieved bandwidth on VMEbus is dominated by the bandwidth obtainable using single cycle read or write operations.

In Figure 7., the model predictions for the rate at which the TRG can send data control messages as a function of the number of ROBs and for a L2R group size of 30, are shown for three specific situations. The lower curve represents the expected results based on pre-design implementation (already presented in Figure 6.). The middle curve is the model prediction when the ROI and L2A data control messages are also sent via DMA. For 10 ROBs the use of the DMA for all transfers only improves the TRG send rate by 36%, indicating that the single cycles associated to the message passing protocol and synchronisation are still dominant. The top curve in Figure 7. represents the model predictions without the synchronisation traffic. In this case, again for 10 ROBs, the achieved TRG send rate has increased by 121%. This is simply due to the reduced number of single cycle operations over VMEbus.

In Table 3. the simple model results are presented for the TRG message rate to 10 ROBs, a L2R group size of 30 and with a hypothetical broadcast functionality on VMEbus, which was characterised by the single cycle parameters in Table 2. It can be seen that, in principle, the introduction of a broadcast facility would greatly improve the TRG message sending rate. Indeed, implementing data collection on some other technology and the introduction of broadcast functionality on VMEbus would enable the final ATLAS requirements to be met.

The current draft standard for two edge Source Synchronous Transfers (2eSST) over VMEbus [5] proposes to

Figure 6: Measurement and model comparisons.

Figure 7: Expected results using only DMA and no synchronisation.

use the three-phase address broadcast of the VME64 Extensions [6]. Within the first phase of the address broadcast an extended address modifier code is placed on the bus. The second phase allows the VMEbus master to specify the maximum amount of data that will be transferred. These two phases when applied to 2eSST could allow a broadcast functionality to be included in the 2eSST standard. An extended address modifier code must be assigned to broadcast functionality and the issues of slave response and mapping must be addressed. These issues are being pursued within the 2eSST task group, the VMEbus Standards Organisation and the companies Tundra and CES.

## Summary

The Read-Out Crate (ROC) in ATLAS DAQ/EF prototype -1 has been presented. The performance of an initial implementation consisting of a TRG, EFIF and up to five ROBs has also been presented. The initial implementation was done on CES RIO2 8061 (96MHz) and RIO2 8062 (200MHz) VMEbus CPU modules and VMEbus. Data control messages between IOMs were exchanged via VMEbus, the latter was also used for the Data Collection. External input to the TRG was emulated by a simple PMC interface, while the ROBs and the EFIF had no external I/O via PMCs.

The performance results can be summarised as follows:

* With one and five ROBs the performance was measured to be 122 and 28 KHz respectively. The performance decreases inversely to the number of ROBs in the ROC.
* The performance was limited by the rate at which data control messages could be transferred over VMEbus. A bandwidth of only 6 Mbyte/s was measured and is due to the use of VMEbus single cycles in the message passing protocol and the method of synchronisation between the TRG and the ROBs.
* A simple model indicates that the performance may be improved by a factor of two if all messages are sent using VMEbus block transfers and if the synchronisation is suppressed.

The performance decreases inversely to the number of ROBs in the ROC, reflecting the fact that the TRG sends data control messages sequentially, due to the lack of a broadcast mechanism on VMEbus. Assuming that such a mechanism exists, the simple model has been used to calculate the expected system performance. It showed that in a system where L2R messages are broadcast using D32 block transfer mode, ROI messages using chained DMA and synchronisation is suppressed, a message rate of 83 KHz can be obtained.

It has also been noted that broadcast functionality over VMEbus is possible within the proposed Source Synchronous Transfers standard.

## References

* [1] The ATLAS Collaboration, Technical Proposal for a General purpose pp Experiment at the Large Hadron Collider at CERN. CERN/LHCC/94-43.
* [2] G. Ambrosini et. al., The ATLAS DAQ and Event Filter Prototype "-1" Project, presented at Computing in High Energy Physics 1997, Berlin, Germany. [http://atddoc.cern.ch/Atlas/Conferences/CHEP/ID388/ID388.ps](http://atddoc.cern.ch/Atlas/Conferences/CHEP/ID388/ID388.ps).
* [3] The main data flow in the Read-Out Crate of ATLAS DAQ prototype -1. [http://atddoc.cern.ch/Atlas/Notes/047/Note047-1.html](http://atddoc.cern.ch/Atlas/Notes/047/Note047-1.html)
* [4] RIO2 8061 and RIO2 8062 PowerPC based RISC I/ O Board. Technical Manuals. CES.
* [5] 2eSST (Source Synchronous Transfer) Draft Standard. VITA 1.5-199x. Draft 1.0.
* [6] VME64 Extensions Draft Standard. VITA 1.1-199x. Draft 1.8.

\begin{table}
\begin{tabular}{|c|c|c|} \hline  & \multicolumn{2}{c|}{**TRG message rate**} \\  & \multicolumn{2}{c|}{**(KHz)**} \\ \hline  & **with** & **without** \\  & **broadcast** & **broadcast** \\ \hline \hline With synchronisation and & 31.4 & 19.1 \\ data collection & & \\ \hline With synchronisation and & 35.2 & 20.4 \\ NO data collection & & \\ \hline NO synchronisation and & 83.2 & 30.6 \\ with data collection & & \\ \hline NO synchronisation and & 116.5 & 34.2 \\ NO data collection & & \\ \hline \end{tabular}
\end{table}
Table 3: **Model predictions with a hypothetical broadcast functionality.**