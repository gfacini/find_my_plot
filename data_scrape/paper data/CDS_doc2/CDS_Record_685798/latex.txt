ATLAS DAQ Note 77

18 Sept 1997

R.K.Bock, A.Bogaerts, R. Hauser, C. Hortnagl,

K. Koski, P.Werner, A.Guglielmi, O.Orel

**Benchmarking Communication Systems 1**

Footnote 1: Submitted, in modified form, to _Parallel Computing_

## 1 Introduction

Following the market trend of high-performance computing towards parallel systems available at decreasing cost, we believe that there is a finite chance that much or maybe all of the computing load of level-2 triggers in ATLAS, and certainly all of level 3, can be executed in a farm constituted by commercial parallel systems.

The computer performance evaluation of systems use, among other criteria, results of benchmarking, viz. measured execution times obtained by running a representative job mix, usually without investing substantial effort in optimising for the system at hand. In order to assess, how far parallel systems can contribute to our trigger problem solution, we have designed a comparatively naive set of application-independent _communication benchmarks_; they are documented in ATLAS DAQ notes 48 and 61. The results are, in first instance, large tables of measured communication times.

Our goal was to derive from these detailed results several basic communication parameters. They include obvious ones like bandwidth, but also various overheads associated to switching technologies or arising from interfacing to operating systems, and measures of traffic interference. We will eventually use these parameters for comparing different system possibilities, in particular as input to detailed and full-scale ATLAS modelling. While not replacing more detailed benchmarking of applications, they do give more useful information than the combination of CPU benchmarks with bandwidth numbers.

The tested systems include a number of different architectures from clusters of workstations to tightly coupled massively parallel systems. We also included the technologies that were prominent in the ATLAS demonstrator program, SCI and ATM. The benchmark package includes versions for many different communication technologies and programming interfaces, such as shared memory, MPI, Digital's Memory Channel, Cray T3E Shared memory API and Meiko CS-2.

## 2 Description of the benchmarks

To assess the performance of a number of commercially available parallel systems, we defined a set of _abstract_ basic communication benchmarks, which are not specific to our application. We also added two more _application-oriented_ benchmarks, which represent much simplified versions of the currently proposed second-level trigger solutions [2].

All abstract basic benchmarks are executed for a varying number of packet sizes (minimum, 64, 256, 1024) and a varying number of processors where applicable (2, 4, 8, 16, 32, 64). Packet sizes are restricted to those expected in our application, although some implementations have scanned a wider parameter space.

A more detailed definition of the benchmarks can be found in CERN ATLAS documents2. An example implementation in C is also available from a web site3. Default implementations for MPI and shared memory are available, and the software has also been adapted to several low-level libraries from different vendors, including Cray T3E, Meiko CS2 and Digital's Memory Channel.

Footnote 2: [http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/NOTES/note61/rudi.ps.Z](http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/NOTES/note61/rudi.ps.Z), [http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/NOTES/note48/ATLAS_DAQ_48.ps.Z](http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/NOTES/note48/ATLAS_DAQ_48.ps.Z)

The following benchmark programs have been used (N is the total number of nodes):

_Ping-pong_

One node sends data to another node, and waits for the data to be sent back. The benchmark measures the round-trip time.

_Two-way_

Both nodes are sending and receiving data simultaneously.

_All-to-all_

All nodes are sending to each other simultaneously. Increasing the number of nodes in the system increases the overall throughput.

_Pairs_

\(N/2\) nodes send to \(N/2\) receivers one-directionally.

_Outfarming and Broadcast_

For _outfarming_ one node sends packets to \(N-1\) receivers, while _broadcast_ uses hardware broadcast, if present. Thus in _outfarming_ the data could be different in each send, whereas _broadcast_ sends always the same data.

_Funnel and Push-farm_

In the _funnel_ benchmark \(N-1\) senders send data to one receiver.

The _push-farm_ represents a type of communication, in which the data is sent from \(N-1\) nodes to one receiver, much the same way as in the _funnel_. The difference is that in the _push-farm_ benchmark additional computing cycles can be included; the computing represents analysis of these measurements, and we execute dummy code lasting 0, 100, 400 or 1600 microseconds. Each time before the computing cycle is started, the request for the next data item has been issued, allowing overlap of computing and communication.

_Pull-farm_

_Pull-farm_ represents a type of communication, in which first a control message (64 bytes) is sent from the receiver to \(N-1\) senders, and subsequently an amount of data (1024 bytes) is received back from each sender. Computing cycles can be included the same way as in _push-farm_.

A graphical representation of the benchmark topologies is given in Figure 1.

Of particular relevance to our application are the benchmarks _ping-pong_, _pairs_, _push-farm_ and _pull-farm_. The latter two obviously have been specifically designed to correspond to communication patterns typical for our application. _Ping-pong_ tests the request-acknowledge cycle, which is needed in several kinds of transmissions. The _pairs_ benchmark tests one-way communication performance from point to point, which is characteristic for communication without need for acknowledg

## 3 Implementation

The benchmarks have been implemented in different technologies by designing a separate intermediate layer for each technology, as illustrated in Figure 2. This layer contains the message passing routines, such as sending and receiving the message, initialisation, cleaning up and broadcasting. The routines include non-blocking send and receive operations.

By using this kind of layered approach, the porting of the benchmarks has been made more straightforward. When implementing a version for a new technology, only the intermediate layer has to be changed.

Since two different ATM libraries were used, also two different implementations of the low level interface had to be programmed for that particular technology. Since the application programming interface in different ATM hardware, for example, is usually proprietary, the interface has to be implemented separately for each system.

The _push-farm_ and _pull-farm_ results for ATM, use specific traffic generators as data sources. Additionally, the programs used in these measurements differ slightly from the other benchmarks.

The benchmarks for Mercury RACEway45 system have been designed by using the PeakWare toolkit6, previously known as CapCASE [14].

Figure 1: A graphical representation of the communication benchmarks. The dot indicates, which time is measured.

## 4 Platforms

The measurements have been done in a number of technologies:

* Scalable Coherent Interface (SCI) on a RIO2 80617 embedded processor board using the LynxOS operating system, on PC under LINUX, and on Digital Alphas under Digital UNIX Footnote 7: [http://www.ces.ch/Products/Products.html](http://www.ces.ch/Products/Products.html)
* Digital Memory Channel connecting Digital Alphas
* Asynchronous Transfer Mode (ATM), on RIO2 (and RTPC) embedded processor boards, with the LynxOS operating system, on PC-s under Windows NT, and on Digital Alphas under Digital UNIX
* Cray T3E Shared memory Application Programming Interface
* Raceway Bus, using a Matra Systemes & Information PeakWare toolkit
* T9000 using IEEE 1355 DS Links (GPMIMD)
* Meiko CS-2 communication library

In addition, shared memory and Message Passing Interface (MPI) have been used in multiple systems, as opposed to lower-level API-s. Shared memory has been benchmarked in Digital 8400, Silicon Graphics Challenge and Origin systems. The tested MPI platforms include shared memory multiprocessors, such as Digital 8400 system and Silicon Graphics Challenge, clusters, such as Digital's Memory Channel, and conventional distributed memory systems, such as IBM SP2, Cray T3E and Meiko CS-2.

We will here describe more in detail the test procedure of three of these technologies: Digital's Memory Channel, SCI and ATM.

### Memory Channel

Memory Channel8[5] is a proprietary network technology from Digital Equipment Corporation, is commercially targeted as inter-node transport medium in Digital

Figure 2: The structure of the benchmark implementation. The low level interface has been programmed separately for each technology.

UNIX Tru Cluster configurations. It typically interconnects several AlphaServers with multiple processors each, and thus extends the scalability of Digital's product line to installations with currently up to 96 (\(=8\times 12\)) parallel Alpha processors.

Memory Channel presents the abstraction of a unique shared address space to all processes, regardless of their attachment to remote CPUs. Inter-node communication can be pursued with low overhead because after an initial phase of memory-mapping single user-level CPU store and load instructions suffice to launch communication, and required protection mechanisms are enforced in hardware.

Unlike cache-coherent non-uniform memory architectures (CC-NUMAs) in particular, Memory Channel opts for a slimmer solution which exposes differences of local and remote memories at the application level. It does not provide a strong memory _coherency_ model; thus senders and receivers have to follow software-based protocols for distinguishing between outdated and fresh copies of data. Furthermore its memory mappings are _asymmetric_, i.e. applications must be prepared to use different virtual addresses for reading from and writing to physically unique remote locations.

The quoted results were obtained with the following experimental setup: One AlphaServer 4000 5/300 (299 MHz Alpha 21164 E5, 96 KB + 2 MB off-chip cache) and four AlphaStations 200 4/166 (166 MHz Alpha 21064 EV4.5, 512 KB off-chip cache) were combined in a Digital's UNIX TruCluster. Inter-node connectivity was established over Memory Channel-to-PCI adapter cards (revision 1.5), a Memory Channel hub with five line-cards and copper link cables. All workstations had 128 MB RAM configurations and ran copies of the Digital UNIX 4.0B operating system. The test environment is illustrated in Figure 3.

The tests used a two-copy implementation of the low-level interface's message passing library that ran on top the Memory Channel API library [6]. It is understood that results can be improved by avoiding the second copy, at the price of violating the protocol stack of Figure 2.

### Scalable Coherent Interface (SCI)

SCI [8] is an enabling technology for cache-coherent non-uniform memory architectures (CC-NUMAs). It aims at better scalability for shared-memory multiprocessors than what bus-based schemes can achieve, by using multiple point-to-point links and

Figure 3: Configuration in the Memory Channel tests. Later an AlphaStation 500 system was added into the configuration replacing one of the older stations.

which aggregates links into topologies of ringlets and switches, is effectively hidden from clients behind an interface which resembles backplane buses.

In comparison to Memory Channel, which has been adopted by a single vendor for market-ready solutions, SCI an IEEE 1596 standard, with increasing strong commitments e.g. from Data General9, Sequent10, Sun11 and Siemens-Nixdorf12

Footnote 9: [http://www.dg.com/numaliline/html/sci_interconnect_chipset_and_adapter.html](http://www.dg.com/numaliline/html/sci_interconnect_chipset_and_adapter.html)

Footnote 10: [http://www.sequent.com/numaq/](http://www.sequent.com/numaq/)

For our tests we used Dolphin Interconnect Solutions PCI-SCI cards (rev. B) [7] which conform to the 32-bit PCI local-bus specification. This PCI implementation did not offer SCI's cache coherency; it enabled us to utilise a variety of different nodes, in particular also including embedded processor boards, which is of great importance to some of the conceived parallel applications in our domain.

The cards, attached to 18-DE-200 link cables for a 16-bit parallel, electrical implementation of the physical layer, offer up to 200 MB/s of aggregate bandwidth on the medium. We observed that obtained bandwidths (\(\lesssim 70\) MB/s maximum) were limited by the performance of PCI buses. This restriction is avoided by systems which integrate SCI on the system-bus level, at the obvious price of giving up reusable interface cards.

Our results were obtained with the following equipment: point-to-point measurements refer to our fastest pair of nodes, i.e. an AlphaStation 500/400 (400 MHz Alpha 21164 E5.6, 96 KB + 2 MB off-chip cache) and an AlphaServer 4000 5/300. Tests involving more that two nodes ran on the AlphaServer and a pool of RIO 2 80611 (100 MHz PowerPC 604) VME-embedded processor boards.

Footnote 11: [http://www.sun.com/hpc/tech/interconnect.html](http://www.sun.com/hpc/tech/interconnect.html)

Footnote 12: [http://www.sni.de/public/sni.htm](http://www.sni.de/public/sni.htm)

The low level interface's two-copy message passing library operated on top of an implementation of a draft version of SCI PHY-API [9], whose general aim is to provide a standard for low-level software access to SCI services.

SCI tests have been done using a configuration illustrated in Figure 4.

### Atm

#### 4.3.1 Abstract benchmarks

ATM tests were carried out by using point-to-point connections between a number of different systems. Additionally, a testbed consisting of Digital AlphaStation

Figure 4: Configuration in the SCI tests.

200, RIO2 and RTPC systems and FORE ATM switch was used. The testbed configuration is illustrated in Figure 5. The maximum number of nodes available during the tests was five. In addition new generation Digital AlphaStation 500 and AlphaServer 4000 systems were available for ping-pong tests.

A general problem with the ATM measurements arose for benchmarking from the limited availability of the systems; our testbed was relatively heterogeneous: two different types of system and two different ATM libraries were used. The Digital systems used Digital's ATMOCK library version 1.0, which will become a commercial product [10] (it had not been released at the time of the tests). The RIO2s and RTPC used ATM NicLib library [13], which was an efficient implementation reducing overheads by bypassing the kernel (a library of utility functions was called instead of a device driver) and avoiding data copies (NicStar network interface has a direct access to user buffers). Thus the ATM library implementation running in RIO 2s and RTPC was especially tuned.

Most of the benchmarks, including all the testbed runs, have been run using AlphaStation 200 systems, since newer systems were not available for testing at that moment. The point-to-point results from the newer Digital systems (AlphaServer 4000, AlphaStation 500) were added later.

Minimum overheads correspond to the smallest packet size used in the measurements, which was either 1 byte or 8 bytes. The largest packet size used here was 1024 bytes, although some of the ping-pong tests were additionally run with larger packet size.

In testbed measurements the full optimisation was not used. The point-to-point measurements have been done with full optimisations between two AlphaStation 200, between AlphaStation 500 and AlphaServer 4000, and between RIO2 and RTPC.

The basic implementation of the benchmarks uses Undefined Bit Rate (UBR) connections, and sends in full speed; as ATM has no flow control, the receiver can in some cases loose packets. This is especially apparent in benchmarks _pairs_, _funnel_ and _push farm_. In the _funnel_ benchmarks, when four senders and large packets were used, no meaningful measurements were possible. To avoid the problem of loosing packets, Constant Bit Rate (CBR) connections with reduced bandwidth for each

Figure 5: ATM testbed configuration.

sender could be used instead. Tests using CBR have been carried out in addition [11], [12].

Minimum round trip time divided by two for RIO2 was around 80-100 microseconds in benchmark _ping-pong_ in which both sides receive and send. The same overhead for Digital AlphaStation 200 is around 200 microseconds. For newer Digital systems the ping-pong overhead is close to the one obtained on the RIO2s. The performance difference between the measurements with Digital processors of different generations is surprisingly large, which should be attributed not so much to the faster processor technology (communication benchmarks are not very CPU intensive), but to other architectural changes, which have taken place during recent years.

For receiving only (benchmark _pairs_), RIO2 can achieve a very low receiver overhead, less than 10 microseconds for one byte. Digital Alpha's receiver overhead in _pairs_ is around 20-30 microseconds for one byte. The fast RIO 2 results demonstrate the performance gain, which can been achieved by investing in low-level design work with ATM drivers [4] compared with directly available commercial implementations.

The ATM link speed was nominally 155 Mbit/s. The maximum user bandwidth, e.g. link speed subtracted by the amount of control data, was around 135 Mbit/s. For example, sending in full speed (UBR) one-directionally between RIO2s, already with 1024 bit packet size the link speed is almost entirely used (129.6 Mbit/s). However, since the nominal speed 155 Mbit/s is not very high compared with some other communication technologies of today, it would be interesting to see the effects of ATM connections with higher speed (e.g. 622 Mbit/s).

#### 4.3.2 Application benchmarks: push farm and pull farm

The performance of the receiver processor in the _push farm_ tests was measured on the upgraded demonstrator of the RD31 project [11]. In this system, sender processors are replaced by ATM traffic generators which emulate the senders. The receiver processor establishes a Constant Bit Rate connection with each sender. The bandwidth of this channel is \(1/Number\_of\_Senders\) to avoid congestion in the switching network and the receiver. We measured the maximum event rate that a receiver can handle for various packet sizes, number of messages to be grouped and processing times. For small messages when the total amount of event data does not exceed 2 kBytes the performance of the system is determined by the software and hardware overhead of the receiver \(T_{oh\_push}=30\mu\)s + 8\(\mu\)s \(Num\_of\_Senders\).

For large messages maximum event rate is limited by the usable bandwidth of the 155 Mbit/s ATM links. For example, when each of the four senders sends 1 kbyte of data, the total data transmission time is 250\(\mu\)s and maximum event rate is 4 kHz.

The measurements for the _pull farm_ benchmark were made on the demonstrator for ATLAS described in [12]. For this implementation of the pull protocol, the overhead to handle small events (less than 2 kBytes) in the receiver is: \(T_{oh\_pull}=200\mu\)s + 13\(\mu\)s \(Num\_of\_Senders\)

For events bigger than 8 kBytes the link bandwidth limits the maximum event rate per receiver. For intermediate event sizes no simple formula can be derived. For example, when four senders send 1 kbyte of data, maximum event rate is about 2.66 kHz which corresponds to \(T_{oh\_pull}\) of 376 \(\mu\)s.

For both the _push farm_ and _pull farm_, when the sum of overhead and processing time is larger than the event transfer time the maximum event rate is \(1/(overhead+processing\_time)\). When the data transfer time is dominant the maximum event rate is \(1/transfer\_time\).

Results

### An overview of the parameters

We have condensed the large number of different benchmark results into a few meaningful parameters, as shown in Figure 6. A complete set of results is available from the web site14.

Footnote 14: [http://www.cern.ch/RD11/combench/results.html](http://www.cern.ch/RD11/combench/results.html)

The parameters are the following:

In _ping-pong_ we define one parameter: the overhead derived from dividing the round-trip time for the smallest packet size by two. This parameter represents the latency of the communication.

Latency has also been measured in _two-way_. However, in this measurement the time has not been divided by two, since both the nodes are sending and receiving data simultaneously. Note that the _two-way_ latency is larger than in _ping-pong_, since setup times of both sending and receiving are included.

In the _pairs_ benchmark we define two parameters: overhead and effective bandwidth. Overhead is the one-directional communication speed. The effective bandwidth has been calculated from this benchmark (and not from the previous ones), since in one-directional communication the speed is not limited by waiting for an acknowledgement each time the message is sent. We extract the effective bandwidth using as a reference the packet of one kilobyte. It should be noted that this is in most cases not the upper limit for bandwidth; some systems achieve substantially higher bandwidth only with packets much larger than 1 kbyte.

A parameter describing the broadcasting capabilities of the system has been extracted from the _outfarming_ and _broadcasting_ benchmarks by dividing the outfarming ratio by the broadcasting ratio. The ratios have been calculated by dividing the time for \(2*N\) nodes by the time for \(N\) receiving nodes, from 2 to 4, 4 to 8 and 8 to 16 processors when possible, from which the average is taken. The parameter shows how well the broadcasting has been implemented in each system; the larger the number is, the more efficient the broadcasting is compared to the outfarming performance of the same system.

The _funneling_ (and _push-farm_) benchmark represents a typical data collection approach, in which a number of nodes sends their data to one receiver. A typical example has been chosen: four nodes sending to one node a 1 kbyte packet each. The cycle time to complete the operation, the inverse frequency, is given as a parameter. The results of _funnel_ and _push-farm_ differ from each other only slightly. We present mainly the results from _funnel_, since it has been run on a larger number of systems. Where up-to-date _funnel_ results have not been available, _push-farm_ results were used instead.

_Pull-farm_ represents another type of data collection, in which first a read request is sent. Also here a typical example of four senders and one receiver has been chosen. In _pull-farm_ the packet sizes have been fixed: 64 bytes for the control message requesting the data and 1 kbyte for the actual data. The cycle time, inverse frequency, is presented.

The current implementations of communication benchmarks _push-farm_ and _pull-farm_ allow, in principle, _overlap_ between computation and communication. They use non-blocking communication primitives for starting to gather fragments belonging to the \((n+1)\)-th event, before starting on pre-emptive calculations on the \(n\)-th event. Neither benchmark attempts to request fragments belonging to more than one future event in advance. This is partly because only few implementations of the communication layer allow multiple outstanding send- or receive-operations between two processes at each time.

We parameterised the observed amount of overlap by \(1-a/t_{r,0}\) (in percent), where \(a\) is the (application-specific) communication overhead, and \(t_{r,0}\) is the time spent by the receiver for gathering fragments (communication) with \(d=0\) (no calculation). The overhead \(a\) was obtained as the average of \(t_{r,d}-d\) for different values of \(d\); that is the overall excess over the time \(d\) that is required for calculating alone. We used results from measurements, in which four senders relayed fragments of \(10\,24\) bytes each to one receiver. The overlap presented in the table has been taken from the _push-farm_.

### Discussion of the results

The results provide a large amount of data. The parameters extracted from these results attempt to compress this large amount of benchmark data to a few meaningful numbers, which can be used in comparing the communication performance of different systems.

From the parameters in Figure 6 a number of observations can be made. The

Figure 6: Parameters extracted from benchmark results. _Push-farm_ results were used instead of _funnel_, when either _funnel_ results were not available or _push-farm_ results were considerably newer. The _push-farm_ results have been marked with an asterisk (*). _Push-farm_ and _pull-farm_ results of ATM, marked with double asterisk (**), have been achieved by slightly different measurements, and are described more in detail in section 5.3.2.

latencies vary considerably, from few to few hundreds of microseconds. The same observation applies to overhead in _pairs_ benchmark. The lowest overheads are not necessarily obtained by the tightly coupled shared memory systems, even though that might be expected, since for example Digital's Memory Channel reaches less than 4 \(\mu\)s and Cray T3E 5 \(\mu\)s.

The bandwidths measured by sending a kilobyte packet vary between 3.7 and 59.0 MB/s, although many of the systems can do better with a larger packet size.

MPI results were obtained in multiple systems, in which also lower level API-s were available. The latencies and overheads with MPI, at least with the current MPI versions, is large, typically 3-6 times larger than with lower level API-s. On the other hand, since MPI is available in multiple platforms, it provides portability; it can be debated, whether the difference in performance justifies additional programming work.

The overlap results show the best overlap for the Meiko CS-2, as is explicable from its powerful per-node communication co-processors. The AlphaServer 8400 occupies the other end of the spectrum as a typical SMP multi-processor, with no overlap at all (and low absolute latencies at the same time). Some other observed overlaps must be attributed to artifacts from implementations of intermediate software layers, that stemmed from different authors, and are not so easily explained.

The scalability of the benchmarked systems does not strongly appear from the parameters. This is partly due to the fact that in many of the benchmarked configurations only few processors were available. Few of the systems, Silicon Graphics Origin and Cray T3E, could be tested with large number of nodes. These systems demonstrated quite good communication network scalability up to the tested 44 (SGI) and 64 (Cray) processors.

The large number of benchmark results has been obtained over a time span of more than a year. During that time a number of hardware and software upgrades took place, so that the results do not necessarily represent the most up-to-date situation. In addition, the maximum number of processors presented in some benchmark results depended on access to the systems.

It would be expected that in _pairs_ benchmark the overhead would be constantly smaller than the latency in _ping-pong_, since only receiving setup time is present in addition to the transfer time, and no flow control from the receiver back to the sender is used. However, in some cases the _pairs_ overhead is larger. In one case, this kind of behaviour can be explained by background load (the tested systems could not always be dedicated ), in another there was a configuration change during some of the measurements. The difference is in these cases quite small, and the suspicious times are also from the lower end (mostly less than 10 \(\mu\)s with few exceptions), thus the statistical error of the measurements might also influence the results.

The parameters in Figure 6 represent critical aspects of the systems, but are in no way sufficient to generate all measured benchmark results. They may be seen, however, as parameters that can be used in a model.

The current implementation does a memory copy in each end of the data transfer, which is not an optimal way for some of the technologies, for example for shared memory.

## 6 Summary

Benchmark suites such as Parkbench15 measure the _multiprocessor_ performance of a system by running a set of predefined applications or kernels of applications. Only a small set of the Parkbench suite deals with communication, however. We feel that our benchmark suite can serve as a useful tool in comparing the raw communication performance of parallel systems, as it is available to application-level programs.

There is a large number of results for these benchmarks available. This makes it possible to widely compare the communication performance of different systems. Many of the latest-generation parallel technologies have been measured.

Several of the systems show communication overheads below 10 \(\mu\)s for small packets. Some of the systems additionally have proven a good scalability, which has been tested in some cases with up to 64 processors. Due to the good scalability of some of the systems within the tested range it is predictable that scalability to hundreds of processors will either already now, or at least in the near future, also be quite efficient for example with some of the tightly coupled parallel systems or shared memory systems using NUMA (non-uniform memory access) architecture.

The communication parameter most typical for our application, pull-farm with four senders, is completed in some systems in around 60-90 \(\mu\)s. The best result for the push-farm, for the same number of processors and 1 kbyte data, is around 40 \(\mu\)s. We consider these numbers as promising for our trigger work.

## 7 Future work

The benchmarks described in this document have been run on a large number of different systems. This provides an extensive set of results, from which information about the different communication networks can be extracted. However, new and faster systems constantly arrive at the market; we intend to subject them to the same procedures.

Parallel systems are evolving, too. Many of the main vendors are developing systems based on clustering shared memory systems, in which each node thus is a multiprocessor system itself. This kind of two-level (or more) communication hierarchy creates new challenges for future releases of communication benchmarks.

## 8 Acknowledgements

We would like to thank Digital Equipment Corporation for their close co-operation during this benchmark work.

We would like to thank Center for Scientific Computing (CSC) in Finland for usage of their supercomputer systems.

We would also like to thank Irakli Mandjavidze (DSM/DAPNIA), Andreu Pacheco (CERN), Denis Calvet (DSM/DAPNIA) and the CERN RD31 group for technical assistance, and for the opportunity to use the ATM switch and related hardware in building the testbed for measurements.

In addition, we thank the following persons, who have contributed in running the benchmarks: Igor Zacharov (Silicon Graphics), Raynald Huaulme (Matra Systemes & Information), Iosif Legrand (DESY), Ruud van Wijk (NIKHEF), Roger Heeley (CERN) and John Apostolakis (CERN).

## References

* [1]_ATLAS Technical Proposal._ CERN/LHCC/94-43, 1994.
* [2] Atlas Level-2 Trigger Groups, _Atlas Second-Level Trigger Options._ CHEP 97 conference proceedings16.