# Fast Track Reconstruction for HL-LHC

The ATLAS Collaboration

###### Abstract

At the High Luminosity LHC (HL-LHC) typical average pile-up values between 140 to 200 are expected. For such data taking conditions, track reconstruction is expected to dominate the CPU time required for the ATLAS offline reconstruction. The development of fast track reconstruction algorithms will be crucial to ensure the computing requirements of the experiment are sustainable into the HL-LHC era. In this note a study is presented of the performance of a prototype fast track reconstruction algorithm, based on the current ATLAS track reconstruction software.

## 1 Introduction

ATLAS has an ambitious Phase-2 detector upgrade programme in preparation for HL-LHC in view of the expected rise in instantaneous luminosity and consequently, in the number of simultaneous proton-proton interactions (pile-up). A central part of this programme is the replacement of the current Inner Detector with a new Inner Tracker (ITk) [1, 2] that is optimised for track reconstruction in events with an average pile-up (\(\langle\mu\rangle\)) of 140 to 200 interactions. The reconstruction of complex events with such levels of pile-up is a combinatorial problem. Studies using current ATLAS Run-2 tracking software without modifications on high levels of pile-up indicate a strong rise in the CPU required for track finding [2]. The ATLAS computing model projections for HL-LHC [3] have taken into account the pile-up dependence of the track reconstruction for the projections of the future computing needs of the experiment.

Figure 1 shows the current projection of the total ATLAS CPU requirements for HL-LHC and the breakdown into different processing steps for data and Monte Carlo simulation production. In the ATLAS baseline model a large fraction of Monte Carlo simulation will be produced using fast simulation, so if changes to the offline event reconstruction are not made, it will become the dominant component in terms of CPU required by the experiment. In this note we present a study of a fast track reconstruction chain that would dramatically reduce the CPU required for event reconstruction for HL-LHC.

## 2 The ITk Detector and the default Track Reconstruction Chain

The new tracking detector ITk [1] will feature a Strip Detector with four double sided layers with a small stereo angle. In the forward region a six disk end-cap system, with strip modules mounted on both sides of the petal support structures, will extend the Strip Detector coverage down to \(|\eta|\) of 2.7. A five layer Pixel Detector will cover up to \(|\eta|\) of 4 with at least nine measured hits in the forward region and will be subdivided into three parts: a three layer outer barrel with a central flat section and a section with inclined rings; an outer end-cap with three layers of rings; and a two layer inner section with a flat section and an

Figure 1: **Left:** The ATLAS computing model projection in terms of MHS06 [4] for HL-LHC based on the current offline software. Shown are different options based on producing 75% of the Monte Carlo events with fast simulation, the possible improvements from using fast reconstruction and faster event generators. **Right:** The breakdown into the different contributions to the total CPU budget for 2028, for a model with 75% of fast simulation and standard event reconstruction. Figures are taken from Reference [3].

end-cap section with rings, both within an Inner Support Tube (IST) to make it replaceable. The left side of Figure 2 shows the ITk Layout in an \(R-z\) view of the active components. An important feature of the ring design of the inclined and end-cap sections of the Pixel Detector is that each ring can be placed individually. Positioning adjacent rings closely to each other results in multiple hits per track in a given layer. The ITk Layout has been optimised to reduce the CPU time for track reconstruction, placing multiple hits per pixel layer where needed to keep the number of hits on tracks above the minimal threshold as a function of \(\eta\) and to avoid large extrapolation gaps between hits. The right side of Figure 2 shows the number of hits as a function of \(\eta\) for the ITk Layout for primary tracks coming from the interaction region.

The simulation and performance studies for the upgrade Technical Design Reports were carried out using the Run-2 offline software. The tracking software developed for the current experiment was adapted to the ITk Layout and to the HL-LHC levels of pile-up in order to carry out detailed full simulation studies with Geant4 [5]. The starting point of the study presented in this note is the default reconstruction software used in Reference [2]. All Monte Carlo simulation samples used in this note are using pixel sensors with a pixel size of \(50\times 50\)\(\mu\)m\({}^{2}\).

The default ITk reconstruction chain starts with the decoding of the raw data information. For the ITk Strip Detector the readout chip will transmit already clustered hits that need to be converted into C++ objects in the Strip Cluster Finding algorithm. The Pixel Detector will produce a readout format that compresses the hit information. The Pixel Cluster Finding algorithm carries out a 2D connected component analysis to find clusters of hits and applies an analogue calibration [1] to determine the cluster position. The role of the Space Point Finding algorithm is also different for the Pixel Detector and for the Strip Detector. For the Pixel Detector it is merely a change in C++ object type, providing a copy of the pixel cluster information. For the Strip Detector the Space Point Finding algorithm combines hits from both sides of a barrel stave or an end-cap petal, exploiting the small stereo angle to construct a 2D measurement. For the ITk constructing such hit combinations is complicated by the large distance between the strip modules on both sides of a stave or petal.

The task of the Silicon Tracking Finding algorithm is to reconstruct track candidates from the measured pixel and strip clusters. The pattern recognition starts from combinations (seeds) of either three pixel or strip space points. For each seed a search road is defined that contains the detector modules with all

Figure 2: **Left:** A schematic layout of the “ITk Layout” with a five layer Pixel Detector (red) surrounded by the Strip Detector (blue). Only the positions of the active sensors are shown. **Right:** The total number of pixel and strip hits per track as a function of \(\eta\). Both plots are from Reference [2].

clusters compatible with the estimated trajectory. A combinatorial Kalman Filter is used to find all track candidates compatible with the initial seed that pass loose candidate selection cuts.

The Ambiguity Resolution algorithm selects the final tracks from the set of track candidates provided by the Silicon Track Finding. Bad quality tracks are removed by means of the track fit, as are individual clusters that give overly large contributions to the fit \(\chi^{2}\) (outliers). It also provides the optimal track parameter estimation, making use of the full detector material geometry model. A more robust hole search is re-run in the algorithm, which takes about the same time as the track fit itself. The Ambiguity Resolution algorithm also makes use of the Neural Network cluster splitting [6] to improve the reconstruction efficiency in the core of high-\(p_{T}\) jets and it applies the final track selection cuts.

Table 1 summarises the track selection cuts used in Reference [2]. The selection is done in bins of \(|\eta|\) to allow for the variations in expected resolutions and in hit coverage. The cuts used for the default ITk reconstruction are sufficiently tighter than the Run-2 tracking cuts [7] to suppress effects of the higher number of hits from pile-up interactions. Thanks to the tight tracking settings the rate of fakes is much lower for the ITk than for current Run-2 detector.

Table 2 lists the CPU requirements in terms of HS06 \(\times\) seconds [4] for each reconstruction step for \(t\bar{t}\) Monte Carlo events with \(\langle\mu\rangle=200\) for the ITk Layout [2]. For reference, the results for \(\langle\mu\rangle=20\) using the current Run-2 Inner Detector [1] are listed as well, which include the CPU time for running the algorithms specific to reconstructing the Transition Radiation Tracker (TRT). Figure 3 shows the CPU required to reconstruct a \(t\bar{t}\) event in the ITk with different levels of pile-up. With the preliminary tuning of the default ITk reconstruction as used in the current ATLAS upgrade software release, the total CPU time spent in tracking for events with \(\langle\mu\rangle=200\) is comparable or better than for the current Run-2 detector with \(\langle\mu\rangle=60\) (277 HS06 \(\times\) seconds).

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Requirement & \multicolumn{4}{c|}{Pseudorapidity interval} \\  & \(|\eta|<2.0\) & \(2.0<|\eta|<2.6\) & \(2.6<|\eta|<4.0\) \\ \hline Pixel+Strip hits & \(\geq 9\) & \(\geq 8\) & \(\geq 7\) \\ Pixel hits & \(\geq 1\) & \(\geq 1\) & \(\geq 1\) \\ Holes & \(\leq 2\) & \(\leq 2\) & \(\leq 2\) \\ \(p_{T}\) [MeV] & \(>900\) & \(>400\) & \(>400\) \\ \(|d_{0}|\) [mm] & \(\leq 2\) & \(\leq 2\) & \(\leq 10\) \\ \(|z_{0}|\) [cm] & \(\leq 20\) & \(\leq 20\) & \(\leq 20\) \\ \hline \end{tabular}
\end{table}
Table 1: Set of cuts applied for the default track reconstruction depending on the pseudorapidity interval. Holes are counted if track candidates cross active sensors on which no hit was found. Here, \(d_{0}\) and \(z_{0}\) are defined with respect to the mean position of the beam spot.

\begin{table}
\begin{tabular}{|c|c||c|c|c|c|c|c||c|} \hline Detector & \(\langle\mu\rangle\) & Cluster & Space & Si Track & Ambiguity & TRT+Back & Primary & Total \\  & & Finding & Points & Finding & Resolution & Tracking & Vertex & ITk/ID \\ \hline ITk Layout & 200 & 22 & 6.5 & 78 & 97 & - & 6 & 219 \\ \hline Run-2 & 20 & 1.5 & 0.7 & 23 & 15 & 19 & 0.5 & 64 \\ \hline \end{tabular}
\end{table}
Table 2: The CPU required in HS06 \(\times\) seconds [2] to reconstruct \(t\bar{t}\) for Monte Carlo events with \(\langle\mu\rangle=200\) in the ITk for the different reconstruction steps. Also shown are the results [1] for the current Run-2 Inner Detector for \(t\bar{t}\) events with \(\langle\mu\rangle=20\). An Intel Xeon 7210 was used with 1.3 GHz, 64 core CPU and 116 GB RAM. The measured CPU time is multiplied with the HS06 factor of 3.5 for this processor.

## 3 A Strategy for a fast ITk Track Reconstruction at HL-LHC

The aim of this study is to demonstrate the possible CPU performance improvements achievable optimising the track reconstruction techniques that have been successfully applied to the ATLAS data during Run-1 and Run-2 of the LHC. Starting from the default ITk reconstruction chain several modifications are implemented.

The largest contributor to the overall reconstruction time, as shown in Table 2, is the Ambiguity Resolution. In the default ITk reconstruction about 60 % of the input track candidates pass the Ambiguity Resolution algorithm. For the purpose of this study the Ambiguity Resolution algorithm is omitted from the reconstruction chain. Instead, a tighter track selection is implemented (including limiting the number of shared hits between output tracks) in the Silicon Track Finding to remove duplicate tracks and fakes already at this stage. For the track parameter estimation the fast Kalman Filter track fit is used directly, as applied by the combinatorial filter in the Silicon Track Finding algorithm. The fast Kalman Filter now makes use of the precise cluster calibrations, but uses an approximate material model and approximations in the cluster corrections. Therefore the fit is expected to yield a slight loss in resolution compared to the full offline track fit.

The Silicon Track Finding in the default ITk reconstruction was tuned for robustness and physics performance, primarily to allow for studies of different layout options. Contrary to the Run-2 situation, where the seeding takes about 20 % of the CPU time and 80 % is spent in the road building and the combinatorial track finder, the fraction of the CPU spent in seeding rises to 50 % for the ITk for a sample with \(\langle\mu\rangle=200\). Speeding up the seed finding and improving the purity of the output seed collection by tuning the selection cuts leads to significant gains in the CPU time needed for reconstruction, because this also reduces the subsequent time spent in the road building and combinatorial track finder. The five layer Pixel Detector covers the full range of \(-4<\eta<4\), so only pixel hit combinations are used for the fast ITk track reconstruction version of the seed finding, leaving out the strip seed iteration. In additional, the cuts on the final number of hits and non-shared hits that were previously applied in the Ambiguity Resolution are moved to the Silicon Track Finding to remove additional fakes and duplicate track candidates from the output track collection. The \(p_{T}\) cut in the central region was raised from 900 MeV to 1 GeV and the impact

Figure 3: The CPU required in HS06 \(\times\) seconds to reconstruct a \(t\bar{t}\) event in the ITk as a function of \(\langle\mu\rangle\). The figure shows the total CPU required to reconstruct the ITk, the CPU spent in the Silicon Track Finding and for the Ambiguity Resolution. For comparison the corresponding CPU requirements for reconstruction in the current Run-2 detector are shown for \(\langle\mu\rangle=20\) and 60 events. The plot is taken from Reference [2].

parameter cut was set to \(|z|<150\) mm, to just match the length of the beam interaction region for HL-LHC. The full list of changes in Silicon Track Finding are listed in Table 3. For the purpose of this study the option for the recovery for Bremsstrahlung is temporarily disabled in the Silicon Track Finding.

For this study a minor code refactoring to the code is applied to speed up the Pixel Space Point Finding algorithm, such that only slightly more than 1 HS06 \(\times\) seconds per event is needed at \(\langle\mu\rangle=200\). Since the only use of the space points in reconstruction is for finding track seeds, and because the seeding using space points in the Strip Detector is turned off for the fast ITk track reconstruction, there is no need to run the space point finding on the strip clusters, further reducing the CPU time needed.

In the default ITk reconstruction about 85% of the CPU time is spent in the 2D cluster finding in the Pixel Detector modules that performs a connected component analysis of the individual hits. The ITk Strip Detector, like the current SCT, will read out clusters that need to be converted by the strip cluster creation algorithm. Both algorithms are optimised for CPU speed for the purpose of this study.

Unlike the current detector, the new Pixel Detector is foreseen to use the RD53B readout chip[8] that features an improved data compression. The new pixel readout chip achieves a significant reduction in event size: for a sample of \(t\bar{t}\) events with \(\langle\mu\rangle=200\) the average event size is 580 kB (comparable to the 470 kB for the Strip Detector). It should be expected that unpacking of the ITk pixel and strip raw data will take about the same time per kB as unpacking the current raw data information. The CPU time spent on pixel and SCT raw data decoding is measured for the current detector on a sample of high pile-up events taken in 2017. The results are then scaled by the ITk event size for the pixels and strips raw data for \(\langle\mu\rangle=140\) and 200. On Monte Carlo simulation, the decoding of byte stream raw data has to be replaced by the decoding of the file containing the simulated hits.

## 4 CPU required for fast ITk Reconstruction

To study the CPU performance of the prototype fast ITk track reconstruction, \(t\bar{t}\) samples were used with \(\langle\mu\rangle=140\) and 200. The CPU times are measured on a dedicated machine with two Intel Xeon CPUs (version E5-2620v2) running at 2.1 GHz, with six physical cores each. The machine is rated at approximately 17.8 HS06 per physical core according to [4; 9] for single thread running. For comparison, the samples are also reconstructed on this machine using the default ITk software.

Table 4 summarises the CPU times for both the default and the fast ITk track reconstruction for both \(\langle\mu\rangle=140\) and 200. The timing numbers for ITk byte stream decoding are estimated as outlined in the previous section. As can be seen from the table, the fast version of Silicon Track Finding is approximately

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Requirement & \multicolumn{3}{c|}{Pseudorapidity interval} \\  & \(|\eta|<2.0\) & \(2.0<|\eta|<2.6\) & \(2.6<|\eta|<4.0\) \\ \hline Pixel+Strip hits & \(\geq 9\) (7) & \(\geq 8\) (7) & \(\geq 7\) (7) \\ unique hits & \(\geq 7\) (1) & \(\geq 6\) (1) & \(\geq 5\) (1) \\ shared hits & \(\leq 2\) (no cut) & \(\leq 2\) (no cut) & \(\leq 2\) (no cut) \\ \(p_{T}\) [MeV] & \(>1000\) (900) & \(>400\) (400) & \(>400\) (400) \\ \(|z_{0}|\) [cm] & \(\leq 15\) (20) & \(\leq 15\) (20) & \(\leq 15\) (20) \\ \hline \end{tabular}
\end{table}
Table 3: Changes in tracking cuts used in Silicon Track Finding for fast and default (in brackets) track reconstruction, depending on the pseudorapidity interval. Here \(z_{0}\) is defined with respect to the mean position of the beam spot.

eight times faster than the sum of the default Silicon Track Finding plus the Ambiguity Resolution for \(\langle\mu\rangle=140\) and 200, respectively. The fast track finding is about a factor 1.8 faster for the \(\langle\mu\rangle=140\) sample, compared to the \(\langle\mu\rangle=200\) sample. Adding the CPU needs for the Cluster Finding and the Space Point Finding algorithms, the total CPU requirement for the fast ITk track reconstruction becomes 19 and 31.7 HS06 \(\times\) seconds for \(\langle\mu\rangle=140\) and 200, respectively. The small difference in the CPU time required for the default track reconstruction between Tables 2 and 4 are attributed to the differences between the machines and their installed software. Figure 4 shows the CPU time comparison in a graphical way.

\begin{table}
\begin{tabular}{|c|c||c|c|c|c|c||c|} \hline \(\langle\mu\rangle\) & Tracking & \begin{tabular}{c} Byte Stream \\ Decoding \\ \end{tabular} & \begin{tabular}{c} Cluster \\ Finding \\ \end{tabular} & \begin{tabular}{c} Space \\ Finding \\ \end{tabular} & \begin{tabular}{c} Si Track \\ Finding \\ \end{tabular} & \begin{tabular}{c} Ambiguity \\ Resolution \\ \end{tabular} & 
\begin{tabular}{c} Total \\ ITk \\ \end{tabular} \\ \hline
140 & 
\begin{tabular}{c} default \\ fast \\ \end{tabular} & 1.2\({}^{(*)}\) & 17.1 & 6.0 & 41.1 & 58.2 & 123.6 \\ \hline
200 & 
\begin{tabular}{c} default \\ fast \\ \end{tabular} & 1.6\({}^{(*)}\) & 26.3 & 8.6 & 85.8 & 92.0 & 214.3 \\ \hline \end{tabular} \({}^{(*)}\) Scaled from Run-2, see text.

\end{table}
Table 4: The CPU required in HS06 \(\times\) seconds to reconstruct \(t\bar{t}\) Monte Carlo events with \(\langle\mu\rangle=140\) and 200 in the ITk. Listed are the results for the different reconstruction steps using the default and the fast ITk track reconstruction. An Intel Xeon E5-2620v2 was used with 2.1 GHz and six physical cores per CPU. The CPU time is multiplied with the HS06 factor of 17.8 for single thread running.

Figure 4: The CPU time required in HS06 \(\times\) seconds to reconstruct a \(t\bar{t}\) event for the ITk. Shown is the total CPU time required for reconstruction using the fast and the default ITk tracking software. For comparison, the corresponding CPU requirements for reconstructing Run-2 events are shown for \(\langle\mu\rangle=20\) and 60. The default ITk tracking software and Run-2 results shown here are taken from Reference [2].

## 5 Tracking Performance of the fast ITk Reconstruction

In the following performance plots are presented, using single muon samples to illustrate the technical performance and \(t\bar{t}\) samples with \(\langle\mu\rangle=140\) and 200. The \(t\bar{t}\) sample plots are shown with a \(p_{T}\) cut of 2 GeV on the truth particle to avoid turn-on effects due to the efficiency variation at the reconstruction \(p_{T}\) cut.

Figure 5 shows the efficiency as a function of \(\eta\) for the fast and the default ITk reconstruction on single muon samples. Compared to the default ITk reconstruction, the fast version is losing slightly more than 1% in efficiency at 2 GeV, while for 100 GeV sample the efficiency of the fast version is closer to the default reconstruction. Additional efficiency losses for the fast track reconstruction are visible for \(|\eta|\) close to 4 and in the barrel/end-cap transition region of the Strip Detector (see Figure 2), attributed to the preliminary nature of the tuning of the fast track reconstruction software.

Figure 6 shows the efficiency on \(t\bar{t}\) events with \(\langle\mu\rangle=140\) and 200 as a function of \(\eta\) and \(p_{T}\) for the fast and the default ITk reconstruction. As expected from the single muon results, the fast reconstruction has a slightly lower efficiency at all \(\eta\), with no strong \(p_{T}\) dependence. The efficiency is found to be independent of pile-up.

The number of tracks per event as a function of \(\eta\) is shown in Figure 7. This is an inclusive measure of the rate of additional tracks, fakes or duplicates, compared to the default reconstruction. The fast reconstruction finds slightly less tracks at \(\langle\mu\rangle=140\) and at 200, which is attributed mainly to its lower tracking efficiency compared to the default ITk reconstruction.

The number of pixel and strip hits associated to reconstructed tracks is a measure of the hit association efficiency of the reconstruction. Figure 8 shows a comparison between fast and the default ITk reconstruction. The hit association to tracks is very similar for both versions of the reconstruction, which both use the full detector in the current study.

Figure 9 shows the \(d_{0}\) and \(z_{0}\) impact parameter resolutions, as well as the relative resolution of the inverse transverse momentum for single muon samples of 2 and 100 GeV. The fast ITk reconstruction almost matches the resolution of the default reconstruction. In the multiple scattering regime at 2 GeV a 30% worse resolution is found for inverse transverse momentum. At 100 GeV a larger difference is visible in the \(z_{0}\) resolution for muons at \(|\eta|\) values between 1.5 and 2.5. Those differences are attributed to approximations in the fast Kalman Filter track fit used for the fast reconstruction.

Figure 10 shows the \(d_{0}\) and \(z_{0}\) impact parameter resolutions, and the relative resolution of the inverse transverse momentum, as a function of \(\eta\), for a sample with \(\langle\mu\rangle=200\). The fast ITk reconstruction almost matches \(d_{0}\) and \(z_{0}\) resolutions of the default reconstruction at all \(\eta\). A loss of 30% is seen for the inverse transverse momentum resolution, similar to result shown in Figure 9 for low-\(p_{T}\) muons.

Figure 5: Track reconstruction efficiency for single muon samples with \(p_{T}\) of 2 GeV (**left**) and 100 GeV (**right**) as a function of \(\eta\) for the fast and the default ITk reconstruction. The ratio is given by the efficiency for the fast reconstruction divided by the efficiency for the default reconstruction.

Figure 6: Tracking efficiency as a function of \(\eta\) (**top**) and \(p_{T}\) (**bottom**) for the fast and the default ITk reconstruction. Samples of \(t\bar{t}\) events are used with and \(\langle\mu\rangle=140\) (**left column**) and 200 (**right column**). A \(p_{T}\) cut of 2 GeV is used for the generated particles, to avoid turn-on effects. The ratio is given by the efficiency for the fast reconstruction divided by the efficiency for the default reconstruction.

Figure 8: Number of strip (**left**) and pixel (**right**) hits as a function of \(\eta\) for the fast and the default ITk reconstruction. A sample of \(t\bar{t}\) events is used with \(\langle\mu\rangle=200\). A \(p_{T}\) cut of 2 GeV is used to avoid turn-on effects. The ratio is given by the number of hits on tracks for the fast reconstruction divided by the number of hits on tracks for the default reconstruction.

Figure 7: Inclusive track rate as a function of \(\eta\) for the fast and the default ITk reconstruction. Samples of \(t\bar{t}\) events are used with \(\langle\mu\rangle=140\) (**left**) and 200 (**right**). A \(p_{T}\) cut of 2 GeV is used to avoid turn-on effects.The ratio is given by the number of tracks for the fast reconstruction divided by the number of tracks for the default reconstruction.

Figure 9: Track parameter resolutions for a single muon sample of 2 GeV (**left**) and 100 GeV (**right**) for the fast and default ITk reconstruction. Shown are the \(d_{0}\) (**top**) and \(z_{0}\) (**middle**) impact parameter and the relative inverse transverse momentum (**bottom**) resolutions. The ratio is given by the resolution for the fast reconstruction divided by the resolution for the default reconstruction.

## 6 Summary and Outlook

A study of a fast track reconstruction chain for HL-LHC is presented in this note that runs seven times faster than the default ITk reconstruction. The fast track reconstruction strategy uses the same classical tracking approach as used in ATLAS during LHC Run-1 and Run-2, based on a combinatorial Kalman Filter seeded on combinations of pixel hits. The improvements obtained in this study in the CPU performance of the track reconstruction are a significant step towards sustainable computing requirements for HL-LHC. This is a preliminary study, so minimal efficiency and resolution losses are found with the fast reconstruction chain, when comparing with the default ITk reconstruction. The offline software upgrades in preparation for Run-3 and HL-LHC were not deployed in this study, so further efficiencies can be expected. In particular, ATLAS is investing significantly in the ACTS project [10], an open source software project to modernise the ATLAS tracking software with multi-threading and data locality for the event data model in mind. The modern C++ technologies used in ACTS are expected to further improve the CPU performance of the reconstruction. R&D on track finding algorithms is being carried out to further reduce the CPU time needed for e.g. seed finding, include approaches inspired by machine learning and data science [11; 12]. The aim is for a further improved fast ITk track reconstruction chain to replace the current default reconstruction for HL-LHC for all data and simulation processing.

Figure 10: Track parameter resolution for transverse impact parameter \(d_{0}\) (**left**), longitudinal impact parameter \(z_{0}\) (**right**) and relative transverse momentum \(p_{T}\) (**bottom**) as a function of the true pseudorapidity. Shown are results for the fast and the default ITk reconstruction. A sample of \(t\bar{t}\) events is used with \(\langle\mu\rangle=200\). A \(p_{T}\) cut of 2 GeV is used to avoid turn-on effects. The ratio is given by the resolution for the fast reconstruction divided by the resolution for the default reconstruction.

## References

* [1] ATLAS Collaboration,., _Technical Design Report for the ATLAS ITk Pixel Detector_, Tech. Rep. ATL-COM-ITK-2018-019, CERN, Geneva, 2018. [https://cds.cern.ch/record/2310230](https://cds.cern.ch/record/2310230).
* [2] ATLAS Collaboration Collaboration,., _Expected Tracking Performance of the ATLAS Inner Tracker at the HL-LHC_, Tech. Rep. ATL-PHYS-PUB-2019-014, CERN, Geneva, Mar, 2019. [https://cds.cern.ch/record/2669540](https://cds.cern.ch/record/2669540).
* [3]_ATLAS computing model projections for Phase-2_, [https://twiki.cern.ch/twiki/bin/view/AtlasPublic/ComputingandSoftwarePublicResults](https://twiki.cern.ch/twiki/bin/view/AtlasPublic/ComputingandSoftwarePublicResults).
* Benchmarking Working Group_, [http://w3.hepix.org/benchmarking.html](http://w3.hepix.org/benchmarking.html).

* [6] ATLAS Collaboration, G. Aad et al., _A neural network clustering algorithm for the ATLAS silicon pixel detector_, JINST **9** (2014) P09009, arXiv:1406.7690 [hep-ex].
* [7]_Early Inner Detector Tracking Performance in the 2015 data at \(\sqrt{s}\) = 13 TeV_, Tech. Rep. ATL-PHYS-PUB-2015-051, CERN, Geneva, Dec, 2015. [http://cds.cern.ch/record/2110140](http://cds.cern.ch/record/2110140).
* [8] RD53 Collaboration Collaboration,, _Output Data Format for RD53B, Rev.6_, tech. rep., CERN, Geneva, 2018.
* [9]_CPU Benchmarks_, [https://www.cpubenchmark.net](https://www.cpubenchmark.net).
* [10]_A Common Tracking Software (Acts)_, [http://acts.web.cern.ch/ACTS/](http://acts.web.cern.ch/ACTS/).
* International Workshop on Connecting the Dots and Inteiligent Trackers_, [https://indico.cern.ch/event/742793/](https://indico.cern.ch/event/742793/).
* [12]_TrackingML Particle Tracking Challenge_, [https://sites.google.com/site/trackmlparticle/](https://sites.google.com/site/trackmlparticle/).