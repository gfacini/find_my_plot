ATLAS TDAQ Note - 104

12 june 1998

An ATM based Demonstrator System for the Sequential Option

of the ATLAS Trigger

D. Calvet, J.-M. Conte, R Hubbard, M. Huet, P. Le Du, I. Mandjavidze

_CEA Saclay DAPNIA, 91191 Gif-sur-Yvette Cedex, France_

B. Blair, J. Dawson, J. Schlereth

_Argonne National Laboratory, Argonne, Illinois 60439-4812, USA_

M. Abolins

_Michigan State University, East Lansing MI 48824-1321, USA_

###### Abstract

This paper describes an implementation on a small scale demonstrator of the sequential option for the ATLAS trigger. This demonstrator is built around an ATM switch connecting source modules, destination processors, a supervisor and a monitor. We describe the hardware and software architectures of the system. We investigate the performance of the demonstrator in various configurations and with different sets of input parameters. Finally, we discuss the scaling of the results to the design of a large trigger system for ATLAS.

## I Introduction

Several possible schemes for the selection of events in the ATLAS experiment have been proposed and a certain number of architectural designs for the Trigger/DAQ are currently under study [1]. The relative benefit of each design is being evaluated by means of spread sheet models, computer modeling and simulation, and the development of small scale demonstrators. The demonstrator activity is very important to prove the validity of the concepts proposed, to measure the performance of a real system and to understand technology issues. The measurement of critical parameters provides the necessary feed-back to the modeling activity to make accurate extrapolations.

In the single farm architecture proposed for the ATLAS level 2 trigger system, a farm of event selection processors is connected to the detector read-out buffers by a communication network that transports event data and protocol messages. This architecture exploits a sequential event selection strategy [2] intended to meet the ATLAS physics goals while reducing the data transfer and processing power requirements. Following this strategy, a single processor performs all the trigger algorithms for a given event and makes the final level 2 decision. The event selection algorithms are executed step-by-step; data for the next step is requested only if the analysis is still consistent with at least one set of trigger conditions. A decision can be issued after each processing step to reject background events as soon as possible.

The demonstrator program that we have been carrying was designed to test the concepts of the single farm architecture, measure the performance of a real system, understand certain technology issues, and provide feedback to the modeling activities.

## II Overview and Advantages of the Sequential Selection Scheme

### Sequential selection

The event selection is made by a series of steps. At each step, only the event data which is necessary to validate this step is acquired and analyzed. A given step is executed only after the validation of the event by the previous steps. This scheme is opposed to a parallel selection approach where data for a given event are processed in parallel on a per sub-detector basis before merging the results to determine the global trigger decision. Because it is expected that event rejection can take place before analyzing the event data in various regions of the detector and in all sub-detectors, the sequential scheme has the potential to reduce significantly the demands both on computing power and on network bandwidth. The parallel event selection scheme does not benefit from early event rejection and some of computing power and network resources are wasted.

### Supervision scheme

In the sequential event selection option, a supervisor assigns each event to a single processor. The supervisor has no knowledge on the selection algorithms which are executed entirely by the assigned processor. The sequential selection scheme avoid a central bottleneck in the system and makes the development of a supervisor far less challenging compared to the proposed parallel schemes. Indeed, in these schemes the supervisor allocates a processor per "Region of Interest" in all sub-detectors identified by the first level trigger. This multiply by a significant factor (at least 30 with current estimates) the number of messages the supervisor has to handle at the LVL1 rate. In addition the parallel selection schemes add to the complexity of the supervisor as it has to identify from the event information the location of the data needed for the execution of the algorithms.

### Event data flow

In the sequential event selection option, when a processor receives an event from the supervisor it "pulls" data from the sources, processes the event fragments received, then decides to fetch more data if needed to make the final decision to accept or reject the event. This scheme allows a reduction of the volume of data transferred but adds to the complexity of the event handling mechanism. It allows the application of specific selection criteria that use data from one sub-detector (e.g. the calorimeter) before analyzing data from the others (e.g. the trackers). This is in contrast with the "push" scheme where all the data that might be used is sent by the sources (upon request from the supervisor) without the intervention of the destination processors. The "pull" data flow protocol allows the use of more complex selection algorithms in the sequential scheme. The added flexibility assures a higher background rejection than can be achieved with the parallel schemes. This reduces the requirements on network bandwidth for the event builder and processing power for the event filter.

### Trigger decision latency

With the sequential event selection scheme, event rejection can take place at various stages in the algorithm. It is estimated that a majority of events will not pass the first steps of the selection process (e.g. confirmation of the LVL1 criteria). With the parallel scheme, however, all 'features' need to be collected before a decision can be made. This requires waiting until the completion of the longest algorithm in the slowest detector. Because the processing time in some sub-detectors is expected to be about 5 times longer than in others (e.g. silicon detector versus calorimeter) and because of the additional communication overhead between feature extractor and global LVL2 processors, no significant gain in the event decision latency can be expected from a parallel processing scheme.

The trigger decision latency is a parameter that determines the LVL2 buffering requirements. The current state and trends of the RAM technology indicate that these requirements will be easy to meet, so the trigger decision latency is not a critical parameter.

### Architecture

The architecture proposed uses a single communication network linking all nodes in the system (Figure 1). Each source groups several Read-out Buffers (ROBs) from the same subdetector in order to match the required read-out bandwidth to the speed of the network links. This aggregation of event data from multiple ROBs decreases the influence of the network communication overheads and event fragment handling frequency in destinations. On the other hand, the frequency of requests to the sources is increased when more ROBs are grouped together. The single farm architecture described here proposes to use the same network link to transfer event filter data as well. For all these reasons, the number of interface cards and network ports is reduced compared to the other architectures.

In a similar way, a destination may serve several processors in order to match the network link speed and CPU power. It is responsible for sending data requests and assembling received data fragments before passing them to

Figure 1: Proposed single farm architecture.

the processors. The network ports are bidirectional, so that control information can flow from the processors and the supervisor toward the ROBs. The network transports protocol messages as well as event data. This approach makes the system homogeneous and flexible (all-to-all connections). Alternative architectures use smaller separate networks for event data transfer, in some cases with different technologies, and a dedicated control network to transfer protocol messages. The latter requires a very complicated network that is able to deliver the supervisor messages selectively only to those ROBs that contain RoI data. An alternative approach would be to construct a broadcast control network but this would place heavy demands on the ROBs because they would have to analyze every incoming message at the LVL1 rate.

Unlike the other schemes proposed, which are designed only for high luminosity operation with RoI guided algorithms, the Single Farm Architecture remains unchanged and provides a smooth upgrade path from low to high luminosity running.

## III Description of the Demonstrator

### _Hardware Configuration_

The configuration of the demonstrator is presented in Figure 2. It is built around an ATM switch (FORE ASX 1000) connecting 4 event data sources, 4 destination processors, a supervisor and a monitor. The switch is equipped with 12 ports at 155 Mbit/s and 6 ports at 25 Mbit/s. It can accommodate many different types of interfaces, with up to 10 Gbit/s aggregate bandwidth. The ATM network interface cards are based on the NicStar segmentation and re-assembly chip from IDT. We use 100 MHz PowerPC VME single board computers (CES RIO2) running LynxOS to emulate the detector data sources and 200 MHz PentiumPro PCs (DELL) running WindowsNT to act as destination processors. In fact, any host platform can be a source, destination, monitor or even a supervisor emulator because we have developed a portable code to allow flexibility in the configuration of the system. For reasons of performance and because of mechanical constraints, LynxOS VME platforms are preferred for the source modules and the supervisor. The PCs provide a cost effective solution for the implementation of the destination processors and the monitor.

We implemented an Ethernet based run control system with a graphical user interface to set-up and start the demonstrator from a workstation. The configuration of the system and the set of input parameters for a given run is stored in a parameter file that is accessed by all nodes at start time. At the end of the run, each node optionally dumps a log file on the screen or on disk for off-line analysis.

### _Structure of the Demonstrator Software_

We had to ensure software portability across platforms and operating systems and some level of independence on the networking technology because the demonstrator is a distributed application running in an heterogeneous environment. The software has a layered structure and consists of more than 20,000 lines of plain C code split into three major blocks (Figure 3).

_The core of the application_ contains those parts that are common to all types of nodes. This block does not depend on the particular operating systems. New types of nodes can be created quickly because they all use the same core part of the software.

_The host-platform and operating-system specific modules_ implement thread creation and manipulation routines, a mechanism for synchronization and mutual exclusion, and timer routines. At present, this specific part is

Figure 2: Configuration of the demonstrator.

supported on Digital UNIX for DEC workstations, on LynxOS for PowerPC VME SBC-s and on WindowsNT for PCs.

_The networking-technology specific modules_ implement a certain number of functions to set-up and close connections and to send and receive messages across the network. Currently the demonstrator can use either ATM or UDP/IP. The same functions could be implemented for other networking technologies. The ATM layer is based on the optimized library described in [4] to achieve a minimum message passing overhead by using zero-copy transfers. The same functions (without the zero-copy feature) have been implemented over WinSock2 and DEC ATM socket API to support commercial ATM products. By supporting UDP/IP, the demonstrator software can run on any cluster of machines interconnected via Ethernet, or even on a single workstation (to test soft real-time functionality). To further facilitate software development, the "File IO network emulation" has been implemented. It allows any demonstrator node to run in a stand-alone mode in which outgoing network messages are written to one file and incoming network messages are read from another file.

Network byte order can be chosen by the user. The demonstrator software runs with either big or little endian byte ordering. Functions have been defined to convert internal representations of structures to or from their network representations.

## IV Operation and Performance of Nodes

### _ATM networking technology studies_

We have largely profited from the experience of the RD31 project in ATM networking [3]. ATM technology seems to be adequate to handle the data traffic for large event building applications. Simulation studies of the single farm architecture confirm that specific features of ATM allow the construction of high performance networks capable of transporting simultaneously the various types of traffic characteristic of this application [2]. Congestion avoidance mechanisms implemented with industrial ATM components minimize the influence of network contention on system performance. Furthermore, ATM supports multicast services efficiently. These features make ATM an attractive candidate for a network that could transfer both level 2 and event filter data, as well as data collection protocol and monitoring traffic.

We have developed program an optimized ATM library has been developed for network interface cards that use the IDT NicStar segmentation and re-assembly chip [4]. The library runs on LynxOS and WindowsNT. It avoids the context switches between user and kernel modes used in traditional network drivers, thus reducing the communication latency and increasing the throughput. It also implements a true zero-copy protocol, placing a minimal load on the host CPU for data movement. This library exploits the full bandwidth of the 155 Mbit/s link for messages larger than 100 bytes (2-3 ATM cells) due to its low data transfer and receive overheads of \(\sim\)10 us and \(\sim\)20 us respectively. This can be compared with the performance of commercial ATM products from DEC with \(\sim\)20 us transmission and \(\sim\)50 us receive overhead, that allow full bandwidth utilization only for messages larger than 1 kbyte [5]. Nonetheless, the trend in high speed networking technologies indicates that future host platforms equipped with commercial network adapters will be able to make efficient use of the bandwidth offered by these technologies.

### _Supervisor_

A detailed description of the supervisor can be found in [6]. This unit consists of an arbiter/router and one or several processors (referred to as "RoI CPU") connected to the ATM switch (Figure 2). The supervisor is a scal

Figure 3: Organization of the Demonstrator “C” software.

able unit; we have deployed only a minimum configuration (i.e. 1 RoI CPU) to demonstrate its operation with the rest of the system.

The arbiter/router implements a hardware mechanism to gate the information from the next event accepted by the level 1 trigger to a RoI CPU. The functional description of the RoI CPU is shown in Figure 4. Its task is implemented in a single thread. Polling is used to check for input from the router and the network. The RoI CPU assigns the event to a destination processor that can accept it, collects trigger decisions, packs them and multicasts them to the sources. This is done via the ATM network. We implemented a credit based flow control mechanism to avoid overloading any of the processors, and a time-out mechanism to detect and isolate faulty processors.

In the normal operation mode, the supervisor receives events from a separate event generator. For test purposes, the RoI CPU can also run in an emulation mode without the event generator and the router/arbiter. It generates dummy events internally at the desired rate. In this mode a desktop PC can be used.

In the normal mode, we found that the supervisor with 1 RoI CPU (PowerPC 100 MHz) is able to handle events at ~8 kHz. When used in emulation mode, this rate increased to ~16 kHz. A 200 MHz PentiumPro PC in emulation mode runs up to event rate of ~22 kHz. We think that with several faster RoI CPU-s and some optimization, the target rate of 100 kHz can be reached.

### Source Module

The structure of the source modules is depicted in Figure 5. A source consists of a variable number of Read-out Buffers (ROBs) [7] connected via a shared media (e.g. PCI bus) to a ROB to Switch Interface (RSI). The ROBs receive the event data from the detector at the level 1 trigger rate and buffer it during the event selection process. The RSI is in charge of servicing the requests coming via the network.

When a request for event data is processed by the RSI (Figure 6) it allocates a data buffer and posts a request to each of the ROBs concerned. The ROBs fill the buffers with the requested event data. When all ROBs have replied, the RSI prepares and links corresponding header information with the data buffers. The Event data response is then sent to the processor that requested it. A time-out mechanism ensures that a malfunctioning ROB does not cause requests to hang up. The RSI also distributes to the ROBs the trigger decisions received from the supervisor.

Figure 4: Functional diagram of RoI CPU.

Figure 5: Source module schematic view.

At present, the design of the ROBs is not completely finalized, and we have not integrated the ROBs in the sources. Instead, we emulate the ROBs function in the RSI processor (single thread application). This approach is sufficient to test the demonstrator system but it does not give an accurate view of the ROB/RSI interaction.

We present in Figure 7.a the maximum rate of data requests, \(\mathrm{F_{src}}\), that can be serviced by a source versus the packet size of the response message (with no data pre-processing in the RSI).

The limitation due to the saturation of the output link is reached for packets larger than 1 kB. On a 100 MHz PowerPC, the source code executes in:

\[\mathrm{T_{src}}=30\;\mathrm{\mu s}+2.5\;\mathrm{\mu s}\;\mathrm{*}\;\mathrm{nb \_of\_ROB\_requests} \tag{1}\]

When a certain amount of data pre-processing is performed by the source, the maximum sustainable data request rate is reduced (Figure 7.b). Because there are no physical ROBs connected to the sources, more conservative numbers have to be considered for realistic sources. Investigations of the ROB/RSI interaction are in progress.

### Destination Processor

The destination processor is in charge of making the final LVL2 decision for the events assigned to it by the supervisor. At each step of the selection procedure, the processor issues data requests to the relevant sources to get the data it needs for the execution of the algorithm. While waiting for the arrival of data for a given event, it can process previous events, issue data requests for other events or become idle if there are no tasks to be executed.

The processor task is implemented on a multi-thread application (Figure 8). On the network side, the Rx thread is in charge of handling incoming packets (interrupt driven), and the Tx thread is in charge of transmitting messages. A certain number of processing threads are in charge of running the selection algorithms (one event per thread).

When a new event is received (flow 1 on Figure 8) it is passed to one of the processing threads (flow 2). This thread posts a request (flow 3) to the Tx thread and becomes idle until data is delivered. When a small number of sources are concerned, an individual message is sent to each source. When a group of sources is concerned (e.g.

Figure 6: Processing of data request in the source module.

Figure 7: Performance of the source.

the whole calorimeter for missing energy calculations, or all sources for full event building), the Tx thread sends a single message on a multi-cast channel to the group of sources.

When the requested data has been completely gathered by the Rx thread, the block of data is posted to the processing thread (flows 4 and 5) that executes the next step of the selection algorithm. This sequence is repeated until the decision whether to keep or discard the event can be made (flow 6). To make the system more robust, a time-out thread periodically scans the list of pending requests posted to the sources. If any of the sources did not reply after a certain delay, the processing thread is informed that its data request has failed. At present, incomplete events are discarded. In the demonstrator, the selection algorithm is emulated by a dummy processing of the desired duration.

The destination software can execute on single processor platforms as well as Symmetric Multi-Processor machines. It could be easily adapted to run on a cluster of processors because the processing threads are independent of the Tx, Rx and time-out threads.

We present in Figure 9.a the amount of time needed for handling one event, T\({}_{\text{dst}}\), versus the size of the event fragment sent by each source (single step algorithm with no processing of the data).

On a 200 MHz PentiumPro, the code executes in:

\[\text{T}_{\text{dst}}=215\text{ }\upmu\text{s}+45\text{ }\upmu\text{s }\text{* nb\_of\_source\_requests} \tag{2}\]

For blocks larger than \(\sim\)2 kB the data transfer time is dominant. The maximum event rate that can be sustained by a destination is 1 / T\({}_{\text{dst}}\). For example, a destination can accept short events with data spread across 4 sources up to a \(\sim\)2.5 kHz rate.

When processing is introduced, the amount of time to handle an event is increased (Figure 9.b). When the processing time is dominant, the maximum event rate is given by:

\[\text{F}_{\text{dst}}=1\text{ }/\left(\text{T}_{\text{dst}}+\text{T}_{\text{ algo}}\right) \tag{3}\]

Figure 8: Functional model of the destination node.

Figure 9: Performance of the destination processor.

When the data transfer time is dominant, the speed of the link determines the maximum event rate:

\[\mathrm{F_{dst}}=1\:/\:\mathrm{T_{transfer}} \tag{4}\]

The selection algorithm re-formats the data received, then executes the selection code itself. We found that the speed for copying the data received is \(\sim\)30 MB/s on our PCs and VME hosts.

Figure 10 presents the maximum event rate versus the size of the event when several events are pipelined to a destination (three sources, single step algorithm, no processing of data).

For small size events the maximum frequency can be increased significantly by assigning more than one event to a destination. This proves that the destination model described above can handle efficiently overlapping selection processes for multiple events. The event rate does not improve, however, when more than eight events are pipelined to a destination; the 3 kHz limit is due to software overhead. For large events the maximum frequency is determined by the 155 Mbyte/s bandwidth of the network links according to equation (4).

For the algorithms that are envisioned, a correct match of the CPU power of the destination and the speed of the link should be made to avoid wasting of network bandwidth or processing resources. Assuming a typical algorithm processing \(\sim\)4 kB of data spread across 4 sources in \(\sim\)100 \(\upmu\)s, and an overhead of 400 \(\upmu\)s derived from equation (2), the data transfer rate would be 80 Mbit/s, well adapted to a 155 Mbit/s link.

### Monitor

The monitor is in charge of checking regularly that all nodes are working properly and taking the appropriate action when a node does not respond. It is also in charge of gathering relevant statistics periodically and making these results available to a visualization program. The monitor itself does not include a graphical user interface and is therefore able to run on any platform. A separate visualization program communicating with the monitor via shared memory could be used to provide a more user-friendly interface. When used without a graphical output, the monitor prints the results on the screen at regular intervals and dumps the final statistics at the end of the run. The monitor can also accept input from the user to modify the parameters of the run dynamically. On-line monitoring and gathering of statistics are done via ATM.

The real time constraints placed on the monitor are relatively loose (human reaction time) and the performance of this node is not a critical issue.

## V System Performance Analysis

The results presented in the previous sections show the performance of each node taken separately. We now present the demonstrator performance when all nodes are placed together as shown in Figure 2. We measured the time \(\mathrm{T_{dec}}\) from the event allocation by the supervisor until the return of a decision by the destination processor (event decision latency). For each event, a processor fetches 1 kB of data from each of the 4 sources. The algorithm runs in a single 100 \(\upmu\)s step. The event rate is set to 3.6 kHz, corresponding to \(\sim\)50% utilization of the destination processors and \(\sim\)20% utilization of the link bandwidth. The distribution of \(\mathrm{T_{dec}}\) is shown in Figure 11.a.

The average trigger latency amounts to 1.2 ms, and 99% of decisions come in less than 1.4 ms. The latency of the communication between the supervisor and the destination processor is \(\sim\)200 \(\upmu\)s. The latency introduced by the destination is \(\sim\)500 \(\upmu\)s (sending 4 requests plus a 100 \(\upmu\)s algorithm). The transmission of the requests to the sources takes \(\sim\)100 \(\upmu\)s. The source adds \(\sim\)60 \(\upmu\)s and the transfer of 4 kB of data over a 155 Mbit/s link takes \(\sim\)250 \(\upmu\)s. The total amounts to 1.1 ms which is the minimum latency observed. Additional delays are due to queuing in various places, operating system scheduling, etc.

Figure 10: Pipelining of several events to a destination processor.

The measurement of T\({}_{\text{dec}}\) does not include the time needed to transfer the decision from the supervisor to the sources, then to the ROBs. The event decision latency at the ROB level is a parameter that determines the depth of its event buffer. However, assuming that 8 MB of memory are placed on the ROB and that this buffer is filled at 100 MB/s, this provides a comfortable 80 ns storage capability, and event decision latency is not a major issue.

We investigated the behavior of the system when the event rate was increased. The average value of T\({}_{\text{dec}}\) is plotted against event rate in Figure 11.b. It can be seen that the system operates safely at a rate of 1.5 kHz per destination (i.e. 6 kHz event rate). The saturation point is around 1.9 kHz per destination. The limitation for this set of input parameters that we have selected comes from the destination processors. This number is compatible with the destination model given in equation (3):

\[\text{F}_{\text{dst}}=1\ /\ (\ (215+4\ *45+100)\ 10^{-6})=2\ \text{kHz} \tag{5}\]

We made tests with algorithms that have several steps with different execution times and numbers of sources involved. A test algorithm is presented in Figure 12.a.

The histogram of T\({}_{\text{dec}}\) for each step is plotted in Figure 12.b. The event rate is 3.35 kHz (50% load on the system). The first step executes at a priority higher than that of subsequent steps so that new events are not delayed by a small fraction of events with longer processing times. This test demonstrates the implementation of sequential selection and shows the capability of handling several events concurrently in destination processors. It also demonstrates the capability of using a single network to carry different types of traffic: requests, event data, monitoring and statistics. We use Unspecified Bit Rate (UBR) channels to transfer data for the first and second step of the algorithm and Constant Bit Rate (CBR) channels for the last step (full event building). The monitoring uses low priority and bandwidth CBR channels. This bandwidth allocation scheme is sufficient to avoid network congestion and cell losses.

## VI System Scalability

An important issue is the scalability and the performance of a larger system. The operation of the supervisor does not depend on the size of the system. The performance of the supervisor is expected to scale linearly with the number of RoI CPU-s.

Figure 11: Performance of the system.

Figure 12: Sequential processing.

For a source module, the important parameter is how frequently it participates in an event. This depends on the sub-detector, the number of RoBs per source, their granularity... It is estimated that the most active sources will participate in up to 10% of the events, so they will have to handle a 10 kHz request rate. Although this number is compatible with the measurements we have made, some improvements on the source performance will probably be needed.

The number of destination processors can be adjusted to cope with the rate of incoming events. The first step of event selection in ATLAS is the confirmation of primary Regions of Interest (\(\sim\)1 such RoI per event). The amount of data to be collected is \(\sim\)4 kB and is spread across 4 to 8 sources. Our model predicts that one of today's processor running a 100 \(\mu\)s algorithm should be able to process events at \(\sim\)1.5 kHz. The computing power that would be needed for the first step of event selection at a 100 kHz trigger rate is \(\sim\)66 processors. A farm of a few hundred processors seems realistic for the ATLAS trigger.

On our demonstrator, with four source modules, we could study scalability by adding up to four destination processors to a small scale event selection system. Figure 13 shows that sustained event rate increases linearly with the number of destinations. However, the influence of network contention on system scalability can not be observed on such a small setup.

For the switching network, several vendors offer 10 Gbit/s ATM switches at a moderate cost (\(\sim\)1 kUSS per 155 Mbit/s port). Larger switches (up to a few 100 Gbit/s) are available for the telecommunication market. At present, we estimate that a switch in the 40-80 Gbit/s range will be adequate for ATLAS and affordable. Simulation and measurements indicate that the bandwidth allocation scheme used in the demonstrator should be adequate to avoid network congestion in a large system [8].

## VII Summary of Results

The operation of a demonstrator for the sequential event selection option of the ATLAS trigger has been presented. The request-response data collection protocol was implemented. The possibility of performing sequential event data transfers and selection using partial event data or full event data has been demonstrated. The capability of handling several events simultaneously in a processor has also been shown. Several mechanisms for error detection and recovery were used to ensure the correct operation of the demonstrator. Merging different types of traffic in the same ATM network has been shown. On-line statistics gathering and monitoring via ATM have been implemented. A certain number of parameters and simple formulas that characterize the system were derived.

Several important points have not been addressed by this demonstrator. The implementation of a real algorithm in the destination processors needs to be investigated. The ROB/RSI interaction and performance has to be studied. The 100 kHz operation of the supervisor unit with multiple RoI CPU-s is also a subject for future developments.

We think that the demonstrator results presented in this paper are an important step toward proving the validity and the feasibility of the sequential event selection scheme for ATLAS. We believe that the principles of the architecture proposed are realistic and that ATM technology is suited for this application.

## VIII Acknowledgments

The authors wish to thank D. Laugier and C. Rondot from CPPM, Marseille for their help in the implementation of the run-control and J.P. Dufey from CERN for support.

Figure 13: Demonstrator system scalability.