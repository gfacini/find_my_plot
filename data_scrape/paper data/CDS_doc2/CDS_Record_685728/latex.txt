# Modelling of local/global architectures for second level trigger at the LHC experiment+
Footnote †: This work has been supported in Poland by KBN grants _115/E-343 SPUB-20693_ and _2 P302 047 06_

Zbigniew Hajduk, Wieslaw Iwanski, Krzyszt of Korcyl

Institute of Nuclear Physics

30-055 Krakow, ul. Kawiory 26 A, Poland

Partially supported by RD-13 project at CERN-Geneva

20 February 1994

###### Abstract

We have simulated different architectures of the second level triggering system for experiments on LHC. The basic scheme was local/global system with distributed computing power. As a tool we have used object-oriented MODSIM II language.

Introduction

The collisions rate at LHC creates a formidable challenge for designers of the triggering and data acquisition systems. At top luminosity events arrive with speed of \(10^{9}/sec\). However, interesting physics is rare and data reduction of 10 orders of magnitude is necessary. The final data reduction with application of sophisticated physics criteria takes place off-line, but still the triggering system should entertain the reduction coefficient such as to allow comfortable speed for permanent data storage media i.e few (ten) herz's. One of the ways to realise such a system is to make it hierarchical i.e. multilevel where on each subsequent level less data is treated with more and more refined algorithms. We have assumed three level triggering (figure 1) and worked on possible solutions for the second level. To keep the pipelines and front-end buffers relatively short the first level 'decision-taking-time' (latency) should not exceed 1-2 \(\mu\)secs. It is evident that applied algorithms must be rather simple and can be performed only by dedicated, specialised processors (realised in ASIC's form). The second level works on incoming rate of 100 kHz and its latency (again to keep buffers size reasonable) should not be longer than 1-2 msecs. This level might have a chance to be implemented in commercial processors. Whether they are special systems ex. HDTV type or normal programmable processors depends on the detector and/or envisaged architecture. The third level will most probably perform full physical reconstruction of events and thus can be realised only on a farm of powerful and fast commercial computers.

In this note we deal only with architectural problems of the second level system.

The system components have been modelled as MODSIM [1] objects having some limited resources which could have been granted only to one event in a given time instant. In such a way a queing of events is realised. Some resource objects could accept requests with different priorities. Request with higher priority could interrupt current resource ownership. In this scheme the interrupt system has been implemented.

## 2 New concepts for second level trigger

General structure of studied architectures is based on 'local/global' idea [2]. Since tasks performed by the system can be split basing on their 'generality' we split the system into local and global parts. A local one is located close to the data and naturaly implements parallelism at that level. It performs the partial algorithms which can be done on limited data. The parts producing final decision correlating local results is called global. There are two new concepts in organisation of triggering at level two which are the ways to reduce the load, bandwidth and computing power needed :

* **Regions of interest**
* **Feature extraction**

One of the ways to reduce load on second level system is to implement ideology of _Region of Interest (RoI)_. _RoI_ is an area on the detector where _level ONE trigger (T1)_ has given a positive notation. We assume that _level TWO trigger (T2)_ makes use of that geographical information and concentrates its activity only in _RoI's_. Thus dealing with much less data, it can implement refined algorithms and be run on commercial processors.

The _Feature Extraction_ idea is to trigger on a very well defined, relatively simple physical features of interesting events like : high energy lepton(s), high \(P_{t}\) muons, high energy jets, their specific topological situations etc,etc. Such a feature is well localised and usualy contained in RoI. We see that concept of activities within RoI which we than call feature extraction naturally matches a definition of local activities. To allow for relative 'openness' of the trigger, the local (detector dependent) criteria should not be very strict. Then to keep reduction factor high one should not allow a single detector take a decision and thus only correlation of the features should give positive trigger decision.

## 3 Functional parts of the second level trigger

Conclusion which we can draw from introductory remarks is such that T2 can still be split functionaly into three areas :

1. _Feature Extraction_; performed within _RoI_ and detector (local detector activity)
2. _RoI Object Building_; where features from different detectors are correlated/merged within one _RoI_ ( local _RoI_ activity)
3. _Trigger Event Building_; where analysis and correlation of all _RoI Objects_ gives _Trigger Event_ with decision (global activity)

Applying nomenclature of local/global architecture we see that points 1,2 can be called 'local' where point 3 is evidently 'global' part of the system.

All this activities are differentiated only for purpose of the further studies - one can easily imagine situation where certain levels are merged and/or performed in one step.

We have split our studies into two steps:

* Local Simulations; detailed model of the processing board itself, embedded into very simple global system
* Global Simulations; more attention paid to the general architecture with parameters of the board established by former studies;

We have studied two architectures implementing different approaches to 'local/global' ideology. The first one is a full implementation of three clearly different processing levels, the second one merges _Feature Extraction_ and _Object Building_ into one process. Figure 5 represents simulated solutions.

## 4 Local part simulations

One of the first practical concepts solving the problems of the local/global architectures was to bring down the processing power to the place where data is located. The natural granularity is given by the granularity of the detector front-end electronics. We have started with the calorimetric system since there in chosen coordinates the granules are squares. To each front-end module (which might be a fanned-in collection of smaller units) we connect a processing system able to perform the _Feature Extraction_. In most cases the _RoI_ surface does not match a surface of the module. Then we have to provide a mean of horizontal communication i.e. between processors to ensure the data exchange for full _RoI_ coverage.

Former studies shown [3] that the data from events accepted by _T1 trigger_ should be stored within the level two system i.e. data are transferred from the front-end electronics to the _T2 buffer_ immediately after every positive _T1_ decision. Thus, the basic building block of the system contains a _Front-end communication controller_, a _T2 buffer_ and a _Board Supervisor_ (see fig.2). The latter one works as a _Board Manager_ receiving _T1_ and _T2_ decisions, decoding _RoI_ messages, programming _Front-end communication controller_, managing _T2 buffer_ and as a _Feature Extractor_ processing the data locally. The TMS320C40 DSP processor[4] has been chosen as a board supervisor due to its computing power and 6 communication links which can support high speed data transfers. Different tasks of DSP are prioritised and executed under control of the interrupt system. The board can communicate and exchange the data via links provided by processor. In such a way a'sliding window' can be formed in order to cover fully the _RoI_.

From such a single block a more complicated system has been build containing a matrix of 20*20 boards forming a'mesh' covering the surface of detector. The system is completed by adding links to global processors which were set to operate with fixed processing time. The system has been fed with randomly incoming events having average frequency of 100 kHz. The _RoI_ messages following each _T1 yes_ decision were broadcasted to each board. The number of _RoI_'s was also randomly generated according to the distribution given by physics simulation (see figure 3 for input distributions used during simulations). Initially, one DSP was used on each board as a supervisor. In this case, a local processing as the lowest priority activity, suffered lack of computing power, being permanently shared with other services. In order to improve overall performance, two processors were used, where extra one was dedicated for _Feature Extraction_ process only. Regardless of it one can observe queue of events waiting for local service which results in unpleasant long tails (see figure 3). Use of more _Feature Extractors_ on the board significantly cuts the tails.

Here are some input parameters used for simulations

* T1-yes frequency \(1\,00kHz\) (exponential distribution)
* number of RoI/event 5 (see figure 3.)
* RoI data collection time/link \(1\,00\mu\,\)secs
* local processing time \(300\mu\)secs
* total latency of T2 2 msec

We have investigated the following issues:

* components configuration on the board from point of view of local processing time;
* average and peak occupancy of the FIFO's, units, links, buffers and processors;
* average and peak 'life time' of the event in the system i.e. time from positive T1 decision until readiness of the event for global processing;
* average and peak processing time for variable number of processors on the board;Simulations results are shown on figure 3.

The following conclusions can be drawn from our studies:

* efficient management of _T2 buffer_ is very important, most likely a dedicated processor should be used
* more than one processor should be used as a _Feature Extractor_ to decrease significantly local queues introduced by statical assignment of the processor to serviced area ;

## 5 Global part simulations

For the detailed studies of the overall architecture a system composed of the following components has been simulated (see figure. 5)

* _T1 trigger_ generating T1 positive decisions;
* _T2 supervisor_ distributing _RoI_ messages, assigning processors to new events, keeping event identifiers bank;
* _T2 buffer_ where in some cases _Feature Extraction_ might be executed1 ;

Footnote 1: model #1 figure 5.

* _Local Switch_ where data was distributed to _Local Farm_;
* _Local Farm_ where both _Feature Extraction_ and _Object Building_ might be performed2;

Footnote 2: model #2 figure 5.
* _Global Switch_ distributing the data to _Global Farm_;

* _Global Farm_ where final trigger algorithm is run.

For each positive _T1_ a number of _RoI's_ has been generated according to the distribution from figure 3. Each new _T2 event_ is assigned an identifying number fetched from the bank of free identifiers. Former tests [3] of event management based on incremental counter idea showed method to be sensitive to long lasting events, which usually shows up in much bigger _T2 buffers'_ capacity.The given number is kept until _T2 decision_ is taken, then number is returned to the bank. A dead-time is introduced to the system when bank of identifiers is empty. Position of the _RoI_ determines the place (_T2 buffer_ board) where data might be processed locally (model #1).

The identifiers of the _Local_ or _Global Farm_ where event is going to be treated are also assigned by _T2 supervisor_. For processing stages a number of processors in the farm and location of the execution of given task were point of interest. The assignment algorithm at _T2 supervisor_ level is static, no 'a-priori' knowledge on farm occupation is introduced. For such a scheme an arrangement of processors is an important issue. In fact, the _Feature Extractors_, _Local_ or _Global Farm_ were realised rather as a farms of a small clusters than farms of single processors. Thus T2 supervisor attaches to the event the cluster's manager identifier only (fixed algorithm), which then selects itself any free processor in own cluster dynamicaly. Size of cluster is a compromise between flexible management feasibility and computer power needed to decrease possible queues caused by static assignment algorithm. Figure 4 shows a comparison of the performance of a certain number of processors grouped in the different ways into farm3.

Footnote 3: The latency caused by the farm management was not implemented

As for the switches we have modelled a switching networks built from C104 transputer element, a commercial HIPPI and ATM-Alcatel switches. The modelled components feature following parameters:

* IMS C104 packet routing switch
* 32 way packet router
* packets of 32 bytes
* packet buffering
* wormhole routing algorithm
* 3.2 \(\mu\)secs for packet transmission
* 1.0 \(\mu\)secs for packet decoding

HIPPI crossbar switch
* 32 way router
* no buffering
* transmission of data bursts
* 100 MBytes/s bandwidth
* 1.0 \(\mu\)secs for burst decoding

Alcatel ATM switching fabrics4 Footnote 4: The ATM-Alcatel has been treated as a ’black box’ and the model was simplified, neither traffic shaping nor loss of data mechanism was implemented
* ATM cell of 53 bytes
* 0.44 \(\mu\)secs for Multi Slot Cell transfer

Figure 6 shows connectivity for C104 or HIPPI based switching matrices.

Basic parameters of modelled architectures used for simulations were:

* feature extraction time 300 \(\mu\)secs5 Footnote 5: In model #2, the feature extraction process is performed in one processor for several detectors, thus feature extraction time is a multiplicity of 300 \(\mu\)secs, due to serial execution
* feature correlation time 100 \(\mu\)secs
* global trigger decision latency 50 \(\mu\)secs* raw RoI data size 500 bytes
* preprocessed RoI data size 32 or 48 bytes
* RoI's per event 5 (distribution see at figure 3.)

Following problems has been addressed during simulations :

* occupancy of the 'free identifiers' bank
* distributions of timings at the different levels of the architecture

Figure 7 presents contributions to the event lifetime from different components of the following architecture (see figure 5.):

* Length of event ID bank _400 identifiers_
* T2 buffers/Feature Extractors _75/5 cpu in each_
* Local Switch (C104 based) _4 layers-8 nodes_
* Local Farm _64 clusters-of-2cpu_
* Global Switch (C104 based _2 layers/2 nodes_
* Global Farm _16 clusters-of-1cpu_

Results presented on figures 8 and 9 show event lifetime distributions collected from simulations of different switch technologies and different architecture models. We see quite different behaviour of the models. The average lifetime of the event is much shorter in case of model 1. However the buffer size which is governed by the tails length should be the same in both models. The narrow distribution of the lifetime for model 2 creates some danger if the buffer size is chosen to close to the limit. It will give very steep increase of the dead time in case the buffers become full.

## 6 Conclusions

We have following conclusions from our studies:

* none of the simulated switches proven any special superiority. The ATM switch exhibits very stable latency characteristics however we did not look at the data loses which are known to exist
* system with free ID bank management for new event identification is simple and allows for efficient _T2 buffer_ management
* system works efficiently even with static processor assignment at the _T2 Supervisor_ level when grouping processors into farm of small clusters6* system with small local farms of processors performs better then system with distributed single processors ( the same number of processors in both cases7) Footnote 7: The latency caused by the farm management was not implemented
* better performance is exhibited by the system where \(Feature\)\(Extraction\) is performed in the farm AFTER the switch i.e. not fully localy. However this system might prove to be unpractical due to significant complication of the switching network.

MODSIM II has proven to be very handy tool for behavioural simulations of quite complicated systems.

## References

* MODSIM II Reference Manual
* [2] D.Crosetto et al.: A local/global architecture for level 2 calorimeter triggers, LHC Workshop, Aachen, 1990
* [3] G.Appelquist, W.Iwanski: Simulation of a calorimeter read-out system using VHDL, EAST note # 92-27, FERMI note # 13, CERN November 1992

- EAST note # 93-11 CERN 13 August 1993