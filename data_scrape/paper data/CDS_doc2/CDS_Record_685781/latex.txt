ATLAS DAQ note 61

R.K.Bock

24 Jan 97

Architecture-free benchmarking in the

demonstrator program

The ATLAS level-2 demonstrator program requires the intended hardware and

software implementations to be compared using a single metric. This note defines

simple basic _technology performance_ measurements for the (very partial) solutions

that will be implemented. The results of such measurements essentially set the scale

for the subsequent combining into different architectural units, by specific

demonstrations or modeling. From there, one will approach the ultimate measure of

comparison: some combination of cost, complexity, commercial availability of

components, etc., which will require extrapolations both in scale and technology.

The principle and the parameters have been discussed with many people in

ATLAS, and modified during discussions in January 1997 (demonstrator meetings

Saclay and CERN); all these contributions are herewith acknowledged.

We concentrate _initially_ on two sets of measurements to assess the performance

limits of multi-processor systems interconnected by a network or switch, these being

at the core of B and C:

- abstract communication benchmarks for parallel systems, and

- a systematic scan englobing various potential ATLAS traffic patterns.

From the results of these measurements, we should be able to identify possible

bottlenecks in the systems, and comparatively easily derive numbers corresponding

to a crude and abstract trigger model, improving substantially the input to 'paper

models' and more detailed computer models. Subsequent measurements using more

detailed architectural models and/or based on more or less complete trigger

scenarios will follow, within the limits set by the time scale. The measurements

have to be supplemented by

- more detailed benchmarks of critical algorithm parts, including those that have

to do with data collection, format changes etc ('preprocessing');

- a separate demonstration of the frequency-critical supervisor part of the trigger.

#### 1) Abstract benchmarking of communications in parallel systems

We deal in all implementations with interconnected parallel processing systems, or

viewed differently, with switching networks with a large number of associated

intelligent nodes.

The first group of measurements, therefore, considers combinations of

communication and computing in abstract, assuming a homogeneous parallel

system. We base ourselves on part of the simple measurements as described in

ATLAS DAQ note 48 (Nov. 1995). That note suggests a sequence of basic

measurements establishing communication parameters and interference between computing and communication, in a way independent of any application. The numbers are end-user-to-end-user, including all overheads of an environment that can be used for hundreds of nodes. The communication benchmarks use constant message sizes 0, 64, 256, and 1024 bytes, the content of messages is ignored. Algorithms have no detector nor physics content.

The benchmarks exist on a file as examples of implementation, see [http://www.cern.ch/RD11/combench.html](http://www.cern.ch/RD11/combench.html). A very brief recapitulation of these communication benchmarks:

One-way: one sender, one receiver. Measured by bouncing back a message (ping-pong), and dividing the time by two. This measures basic one-directional communication.

Two-way: two identical nodes communicate, both acting as sender and receiver. This measures basic two-directional communication.

All-to-all: several processors are simultaneously senders and receivers. This measures switch congestion and some scaling.

Pairs: two groups of processors act as senders and receivers respectively. This measures switch scaling without contention.

Outfarming: one sender sends different data to multiple receivers. This measures the basic data distribution capability.

Multicasting: one sender sends identical data to multiple receivers. This measures the basic broadcasting capability.

Funnel: multiple senders, one receiver. This measures basic event building capabilities.

Three more algorithms are defined in DAQ note 48, mimicking the traffic in different architectures of a trigger system; we suggest to replace these by the measurements more specific to the ATLAS demonstrator architectures (see below).

These benchmarks result in fairly large tables of raw data; a reduction towards few characteristic parameters can be been done in most cases, but some potential problems are also apparent (reference note by Reiner Hauser, in preparation).

#### 2) Abstracted ATLAS traffic: a systematic scan

The traffic patterns of the level-2 trigger can be reduced to tasks in abstracted architectures with several data sources (ROBs) and one or more receivers:

This traffic can be defined by the four parameters

- message length from each sender

- number of messages that get grouped into a 'local event'

- computing load per 'local event', once assembled

- frequencies of 'local events'Demonstrator setups will be suitable to perform a systematic scan over several discrete values for the first three of these parameters; each time one establishes the maximum frequency at which the system saturates due to limits of communication or computing capacity. We must assume that saturation will NOT be caused by the data providers (ROB emulators).

The minimum scan comprises

one-directional measurements, driven from the ROB end (36 measurements):

- message sizes: 64, 256, and 1024 bytes,

- number of messages to be grouped: 2, 4, 8,

- computation per group of messages: 0, 100, 400, 1600 microseconds, and

two-directional measurements, driven from the processor end (12 measurements):

- one type of correlated message: 64 bytes sent from processsor (request),

1024 bytes of data sent back from ROB

- number of data messages to be grouped: 2, 4, 8

- computation per group of messages: 0, 100, 400, 1600 microseconds.

Computations are executed as a wait loop.

In one-directional measurements, the data sources (ROBs) send messages in crude synchronisation, and the receivers group them before entering the wait loop (if any). The mesurement consists of the limiting frequency at which traffic can be maintained without loosing events due to saturation at the receiver end.

In the two-dimensional setup, the'receiving' processors have full cntrol, they issue requests, wait for the incoming messages to be complete, and then execute the wait loop if any. The measurement is the frequency at which this can be maintained.

Some additional notes:

At least in the case of two-way traffic, where a local event and its computing has to wait for all messages being completed both ways, it will be necessary to test how much communication and computing can overlap, by running mutliple threads for different events.

We do not prescribe that multiple receiving processors are available, but interesting additional information would be obtained about possible network interference by running the same or some tests with two or more independent sender - receiver systems, or serving multiple receivers from the same senders.

The results being dependent on the level of software used in processors and network/switch interfaces, these parameters have to be defined in detail when presenting results; the software used must be acceptable for a realistic large system, with no tuning for specific benchmarks.

Computing times are set without specifying a processor; obviously, processor boards of different characteristics will be used, their processor capacity may differ, as will their network interfaces and other characteristics. We can only assume that it will be possible to correct for these differences by suitable scale factors. Again, it will be important to specify the hardware under scrutiny.