**ATLAS Internal Note**

**DAQ-NO-06**

**7 October 1992**

**"A Switched Interconnection Network**

**for Large Scale Instrumentations"**

by

Oddvar Serssen, Frode B Nilsen, Einar Nygard, Joar Martin Ostby

Department of Informatics, University of Oslo

P.O. Box 1080 Blindern, N-0316 OSLO, NORWAY

**1 Introduction**

A general concept for flexible interconnection of cooperating nodes, SWIPP (Switched Interconnection of Parallel Processors), is being developed at the University of Oslo, Department of Informatics [1, 2]. The nodes can be of various kinds: general or special purpose processors, memory modules, storage devices and input devices such as transducers and radiation detectors, figure 1. Bridges to other interconnection networks (e.g. SCI, Scalable Coherent Interface [3, 4, 5]) might be included.

Figure 1: SWIPP, a flexible interconnection scheme of heterogeneous nodes.

**2 SWIPP, general features**

A central part of the SWIPP concept is the powerful interconnection network. Some of the main features of the network are listed below:

* based on packet switching
* speedy: 1 Gbit/s for each channel simultaneously
* flexible, allowing user-defined topologies
* scalable
* using source routing
* optical or electrical interconnection
* ASIC implementation

Each node is connected to the SWIPP network by means of a Protocol Engine (PE). The purpose of the PE is:

* giving an effective and unified connection to the network
* revealing the nodes from communication tasks
* supporting simple layered protocols
* cost-effective ASIC implementation

**3 Interconnection network**

The interconnection network is made up of crossbar switches [6, 7, 8, 9], each one having 16 bidirectional channels, figure 2. The channels can be operated simultaneously, the throughput being limited only by contention. The maximum speed of each channel is 1 Gbit/s.

A pair of optical fibres or electrical cables connect the crossbar switches and interconnect the switches to the PEs [10, 11]. Optical fibres may be used for overcoming distance in a geographical distributed system. In addition noise immunity is obtained. When realizing large switching networks groups of switches might be closely connected using electrical connectors.

The flexible interconnection of crossbar switches allows various user-defined topologies. The topologies can be tailored and expanded according to user needs.

The data-traffic is packet-switched- where the information is divided into dynamically variable length packets having a destination address attached, figure 3[12]. This is a fixed-route addressing scheme, the destination address being put on by the sender PE (source routing). As the packet passes the individual switches in multiple stages, 4 bits of the destination address are used in every switch and the address field is circulated to give the address to be used in the next switch. By doing so, the source address is attached to the end of the field, figure 4. When the packet reaches the receiver, the correct sender address is contained in the packet header. Thus the sender address is generated on-the-fly.

Figure 3: Data is sent as packets.

Figure 2: Crossbar switches connected by optical fibres (or electrical cables).

The switches support worm-hole routing, having flexibly expandable buffers for their input ports, figure 5. The data string is digging its way through the network, just like a speedy, little worm eating dust. When contention occurs the flexible buffers will be filled up to a maximum limit to hold the packet. Control signalling on the return channel chokes the data stream until the forward channel is again free. Then the transmission is resumed, figure 6.

Figure 4: Address manipulation through switches.

Figure 5: The crossbar switch module.

The packets can be of variable lengths, always containing a header and data field. The length to be used is controlled by the PEs. Information about traffic or transmission load can be used by the PEs to alter the packet length. As a help for doing that, status information is put into the packets when passing the individual switches. The status information can contain switch occupancy, bottlenecks etc.

Simple layered protocols can be coded into the data field. The packet is always ended by a CRC word, Cyclic Redundancy Check, for end-to-end error checking.

A special valuable feature included in the switching network is the group access. A packet can have more than one destination, i.e. a whole group. A part of the data field is then used for the address information. When one PE sends the same information to a lot of receivers, everyone is obtaining the sender address, as this is generated automatically. The ultimate case of the group access is broadcasting to everyone.

A special 2-level arbitration scheme is built into the switches to avoid deadlock situations. The lowest level, being a simple speed, fixed port position priority scheme, takes the most common situations. The second level will take care of the rest and prevent deadlocks.

The packets can have different priorities. Urgent control messages will then quickly pass the switches, postponing on-going, low-priority data traffic.

The SWIPP network can conveniently be connected to other networks by gateways containing PEs [14].

The crossbar switches are to be implemented by using up-to-date VLSI technology (i.e. BiCMOS). Thereby the switch contains parts of high density and parts of high speed. The interconnection for the first prototype is done either electrically or through attached off-the-shelves electro-optical components.

## 4 Protocol Engine, PE

To be able to interconnect all kinds of nodes a Protocol Engine, PE, is used [14, 15]. Although the nodes are heterogeneous, the net itself will see a homogeneous environment. The PE interface will be equivalent to a switch interface.

The PEs relieve the connected nodes from all communication tasks. The nodes can then consentate on effective computing or data collecting work.

Figure 6: Flowcontrol.

A simple, layered protocol is being used. This enables all communication to have a common framework for all kinds of messages. Although a protocol hierarchy is defined individual PEs does not need to have the whole protocol implemented, just the necessary parts of it. The primary low-level parts of the protocol are common to all PEs. Those consist of packet assembling/disassembling, putting on the correct receiver address and categorizing incoming packets into various types or belonging to different streams. The PE also generates and checks a CRC at the end of the packets.

The PE interfaces to the computing or input node through some sort of memory or registers, figure 7. This is done by DMA (Direct Memory Access).

The PEs are custom made ASICs, each one containing the primary send/receive functions. In addition the PEs can be tailored to the relevant node requirements.

**5 SWIPP used as a back-stream network to radiation detectors**

Although the SWIPP concept is a general concept for interconnecting cooperating nodes, a promising application might be for large scale instrumentation, i.e. for the LHC experiments [16, 17, 18]. The following discussion is by no means an attempt to solve such a demanding application. However, some of the preliminary parameters have been chosen as examples for evaluating the SWIPP potential. The following examples are from a Silicon tracker detector as being proposed in the framework of RD20 [19, 20].

The main idea is to implement a system for successive data reductions by the different trigger levels. In this example the SWIPP network is used as the low-level interconnection and switching network between all the Detector Modules (DMs) and the next level storage and processing devices. (In principle it has the potential to be used as an interconnection network for all levels in an LHC experiment.) The flexibility of the network allows the crossbar switch topology to fit the system architecture given by the experiment. Optical fibres will help overcoming distance and obtaining a high bandwidth in a noisy environment, thus reducing the interconnection problem in a detector.

The PE can be made very simple and located together with the front-end electronics on the Detector Module, figure 8. The unified protocol framework will allow different packets to go both into and out of the DM, using the same connection through the PE and the Opto Module (OM). Typical activities are off-loading of the heavy 1st level trigger

Figure 7: A Protocol Engine connected to a Compute Engine (CE) through some memory.

data stream, sending out prioritized data as when a Region of Interest occurs, parameter settings, monitoring or testing of the individual DMs.

Figure 9 shows the top level picture of DMs connected to a SWIPP network. The overall dataflow concept might be as follows: A real-time trigger T1, being external to SWIPP, is received simultaneously by all the DMs. The exact trigger and BCO clock are furnished through a separate fibre, figure 8. Following the T1, the related trigger info, being produced by the 1st level trigger system, is broadcasted to the DMs, figure 9. Included in the "data request" packets may be an event identifier and the address of a "Full event buffer", where the relevant event data have to be collected. It should be observed that just by broadcasting this information with the buffer PE as a source, every DM will get the correct return address for the data.

Figure 8: A Protocol Engine connecting a Detector Module to the SWIPP network.

Figure 9: The SWIPP back-stream system.

Each DM stores the event identifier and event buffer address in a number of registers, for later on to be able to concatenate this information with the detector hit data for the corresponding event. The data consist of the channel numbers for the hit channels, the corresponding signal pulse heights and a chip and module identifier. The data will be packed together and sent to the designated buffer (computer farming).

At some occasions Region of Interest (ROI) data are requested by sending prioritized packets over the network. This request packet might bypass the waiting ones in the Detector Module requesting queue, prompting the PE to send out the ROI data immediately, figure 11.

In-between, low speed monitoring and testing data are being sent to selected DMs.

The back-stream data acquisition system collects the 1st level events in a number of buffers, ready for the second level data reduction. The SWIPP network can connect these buffers to the next level of interconnection system through gateways to other networks.

To cope with the data bandwidth in and out of the network an appropriate topology has to be used. Data belonging to the same event has to be smeared out in time by using

Figure 10: Detector Module (DM).

## 6 Detector module

Figure 12 shows a possible configuration of a DM as being discussed in RD20. For collecting data from the Si-strips a set of front-end chips are used. For each bunch crossing data are intermediately collected, awaiting the 1st level trigger. The triggered data are being sent further into the Front-End chips (FE) and a matched filtering algorithm is performed to obtain the most representative analog signal value [21]. After sparsification, data can be read out from the FE chips. The readout is supervised by a Control and Monitor chip (C&M) in cooperation with a simple version of the PE.

The analog signals from the channels which have been hit in a given event are sent out together with the channel addresses. (In addition it is considered also to transmit the analog values of the two neighbouring strips.) The analog values are converted to digital by an ADC and are together with the channel addresses, chip addresses and trigger information packed into SWIPP packets with the relevant network destination address.

The minimum PE supports a subset of the SWIPP protocol, just enough for sending out the data and taking care of the low speed information. The PE is in this context primarily some sort of a finite-state-machine. Data is sent out on the optical fibres when the packet has been assembled. Preliminary studies indicate that the average data speed

Figure 11: Data flow for a request-response communication scheme.

is about 10 Mbytes/s, sustained.

As indicated, a C&M chip contains storage cells for trigger identification and buffer address for the data destination. These buffers run in parallel with the Front-End chip buffers. When a ROI happens, the relevant information may bypass the other requests to give a quick response and short latency. The ROI information may be simplified, for instance OR'ed yes/no data.

The C&M might also contain test and monitoring functions, clock phase adjustment circuitry, D/A for analog parameter settings etc.

Figure 12: The readout part and control unit of the Detector Module:

## 7 Project status and plans

Up to now the SWIPP concept has been developed rather generally by Ph.D and Master students. The achievements obtained are as follows:

The main switch and PE concept has been developed [6, 7, 12, 14, 15]. Various emulations of cooperation principles have been done by using workstations i Ethernet [22]. Recently a 4 node prototype is realized by using off-the-shelves components [23]. This prototype is to be used for further simulations and prototype programming.

In parallel with the mentioned activities ASIC technology has been investigated to see how the critical switch and PE modules might be implemented. The basic system and logical level descriptions have been simulated and specific VLSI designs started. A 4:4 switch is being designed in BiCMOS and the realization of a specialized PE has started.

Relevant applications for the SWIPP concept are constantly being investigated, the large scale instrumentation activities towards LHC being a promising one.

The future plans for this application are to demonstrate critical hardware modules and work out in more details the overall dataflow structure and the requirements and performance needed for the different parts.

The design of PE and C&M which might be used inside the DM will continue. Of special importance in the LHC environment is the power consumption and radiation hardness requirements. The first version does not need rad hardness.

The architecture and structure of the interconnection network is nontrivial for such a large system performance.

## 8 Conclusion

A general concept, SWIPP, has been described to some level of details. It has been suggested how the system philosophy and powerful SWIPP components can be used in large scale instrumentation. Some of the benefits for using the SWIPP concept in this context are:

The crossbar switches allow flexible, expandable topologies to be realized. Optical fibres are used for interconnection simplification. By using a unified protocol frame all kinds of information can be loaded onto or extracted from the connected modules. This is highly programmable and lends itself well for data off-loading, monitoring etc. Handling of the simple protocols are taken care of by specialized hardware as the Protocol Engine. The PEs can conveniently be integrated into the critical Detector Modules, having the pair of fibres as the only necessary contact to its surroundings. Various kinds of nodes can be integrated into the same network and bridges to common computer networks can be included.

**Acknowledgement: This work has been supported by The Royal Norwegian Council for Scientific and Industrial Research (NTNF), project IT0111.21609 "Komplekse Digital-systemer", and NTNF Strategic Technology Programme (90-92): "Microelectronics".**

**References:**

[1]Lundh, Y.: _Skisse av multiprosessorstruktur_. Internal note (in Norwegian), Dep of Informatics, Univ of Oslo, Oct 1987.

[2]Larsen, Liao, Lundh, Sarssen and Ostby: _SWIPP - Switched Interconnection of Parallel Processors_. Dep of Informatics, Univ of Oslo. Int paper 1991.

[3]James, D., Laundrie, A, Gjessing, S and Sohi, G.: _Scalable Coherent Interface_. IEEE COMPUTER, June 1990.

[4]IEEE: _The Scalable Coherent Interface_. IEEE Std 1596-1992.

[5]Gustavson, D.: _The Scalable Coherent Interface and Related Standards Projects_. IEEE Micro, Febr 1992.

[6]Baltzersen, P. M. Lie: _Switsjender for et Pakkesvitsjeet Multiprosessornett_. Master thesis (in Norwegian), Dep of Informatics, Univ of Oslo, Aug 1989.

[7]Karlsen, F. R.: _Et hayhastighets, fibeptisk ferprosessornett_. Master thesis (in Norwegian), Dep of Informatics, Univ of Oslo, Oct 1989.

[8]Liao, G, Ostby, J. M., Sarssen, O., Lundh, Y.: _An Optical Data Link for Multicomputer Systems_. Research Report no. 152, Dep of Informatics, Univ of Oslo, Febr 1991.

[9]Liao, G., Ostby, J. M., Sorssen, O., Lundh, Y.: _Self Synchronizing Data Transfer Scheme for Multicompuers_. Poster for the 1991 International Conference on Parallel Processing, St. Charles, Illinois, Aug 1991.

[10]Ostenstad, J.: _Modul for fiber-optisk kommunikasjon i et hyhastighets multiprosessornetwork_. Master thesis (in Norwegian), Dep of Informatics, Univ of Oslo, 1991.

[11]Olsen, H.: _Optisk prosessorkommunikasjon_. Master thesis (in Norwegian), Dep of Informatics, Univ of Oslo. May 1992.

[12]Ostby, J. M.: _Definition of Some Cuts in the Multicomputer Network: Draft no 2.0_. Internal note, Dep of Informatics, Univ of Oslo, Nov. 1990.

[13]Roseth, O.: _Brookbling mellom et multicomputernettwerk og et lokalt nettverk_. Master thesis (in Norwegian), Dep of Informatics, Univ of Oslo, Aug 1991.

[14]Blekastad, I. and Hagen M.: _Protokollmaskin, en kommunikasjonsenhet for en hyhastighets multiprosessor_. Master thesis (in Norwegian), Dep of Informatics, Univ of Oslo, Jan 1989.

[15]Esvall, R. and Bonde, A. Schreder.: _Protokollmaskinen PE. En nettuawhengig kommunikasjonsenhet tilpasset SWIPP-konseptet_. Master thesis (in Norwegian), Dep of Informatics, Univ of Oslo, Aug 1992.

[16]CERN. _LHC - Large Hadron Collider._ CERN Publications, European Laboratory for Particle Physics, 1211 Geneva 23, Switzerland, June 1990.

[17]CERN, ECFA. _Towards the LHC Experimental Programme - General Meeting on LHC Physics & Detectors._ Evian-les-Bains, France, March 1992.

[18]Ellis, N. et al. _First-Level Trigger Systems for LHC Experiments._ Technical Report CERN/DRDC 92-17, CERN, March 1992.

[19]Weilhammer, P. et al: _R8D proposal - Development of High Resolution Si Strip Detectors for Experiments at High Luminosity at the LHC._ Technical Report CERN/DRDC 91-10, CERN, March 1991.

[20]Hall, G. et al.: _Status report - Development of High Resolution Si Strip Detectors for Experiments at High Luminosity at the LHC._ Technical Report CERN/DRDC 92-28, CERN, May 1992.

[21]Gadomski, G. et al: _The Deconvolution Method of Fast Pulse Shaping at Hadron Colliders._ CERN - PPE/92-24, Jan 1992.

[22]Larsen, O. G.: _Development and emulation of interaction mechanisms for a heterogeneous multicomputer._ Ph.D thesis, Dep of Informatics, Univ of Oslo, Oct 1991.

[23]Nergard, R., Smastuen, S. and Torp, P. H.: _SWIPP'em._ Master thesis (in Norwegian), Dep of Informatics, Univ of Oslo, Aug 1992.