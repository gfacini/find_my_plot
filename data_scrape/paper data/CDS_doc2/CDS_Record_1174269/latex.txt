**Search for the Standard Model Higgs Boson via Vector Boson Fusion**

**Production Process in the Di-Tau Channels**

The ATLAS Collaboration1)

Footnote 1: This note prepared by: A. Ahmad, J. Alison, S. Asai, M. Beckingham, S. Burdin, M. Campanelli, V. Cavasinni, X. Chen, K. Cranmer, S.M. Farrington, G. Gaycken, M. Groh, J. Grosse-Knetter, M. Heldmann, S. Horvat, H. Kroha, K.J.C. Leney, C.L. Mauer, R. Mazini, B.R. Mellado Garcia, N. Mser, S. Mohrdieck-Moeck, J. Monk, G. Nunes Hanninger, A. Patwa, L. Poggioli, W. Quayle, I. Rottlnder, D. Rousseau, C. Ruwiedel, F. Sarri, M. Schmitz, M. Schumacher, J. Tanaka, F. Tarrade, I. Tsukerman, S. Tsuno, C. Valderanis, D. Varouchas, T. Vickey, I. Vivarelli, C. Weiser, N. Wermes, S.L. Wu, S. Xella, S. Yamamoto, Z. Zenonos.

_This note is part of CERN-OPEN-2008-020. This version of the note should not be cited: all citations should be to CERN-OPEN-2008-020._

We outline a search for the Standard Model Higgs boson decaying into a \(\tau\)-pair in association with two jets, which is produced dominantly by the Vector Boson Fusion (VBF) process. The results indicate significant potential for a discovery in the low mass range. We consider fully leptonic, semi-leptonic, and, for the first time, fully hadronic tau decays. Mass reconstruction, central-jet veto, and jet tagging are discussed, and we present an approach to estimate the background from the data. Additional emphasis has been given to trigger issues and the impact of pileup. The results are based on an improved detector description, including misalignments, the most recent reconstruction software, and modern Monte Carlo event generators, including a revised prediction of the underlying event activity.

Introduction

The search for the Higgs boson and the source of electroweak symmetry breaking is a primary task of the Large Hadron Collider (LHC). It has been shown [1] that the ATLAS detector is capable of discovering the Standard Model Higgs boson with masses ranging from the LEP limit of 114 \(\,\mathrm{GeV}\)[2] to about 1 TeV. The low-mass region is preferred from electroweak precision measurements and in this region (\(m_{H}<130\,\,\mathrm{\GeV}\)) the searches for Higgs bosons decaying to taus and photons are the most promising for discovery [3, 4, 5]. Searches for the Higgs boson produced in Vector Boson Fusion (VBF) tend to have reasonably high signal-to-background ratios, making them more robust to systematic uncertainties.

Within the Standard Model, the ability to observe the Higgs boson in multiple production and decay configurations makes it possible to measure the Higgs boson coupling to fermions and vector bosons [6]. Furthermore, the VBF processes provide a tool for measuring the Higgs boson spin and CP properties [7, 8]. In the context of the Minimal Supersymmetric Standard Model, (MSSM), the branching ratio of a Higgs boson decaying to photons is generally suppressed, which makes the search for Higgs boson decaying to taus very important. The complementarity of the coupling of the light and heavy CP-even, neutral Higgs bosons of the MSSM to taus makes it possible to cover most or all of the \(m_{A}-\tan\beta\) plane by reinterpreting the results for a Standard Model Higgs boson decaying into taus in the context of the MSSM [9, 10].

A previous ATLAS analysis outlined the sensitivity to a low mass Higgs boson including the first estimates for the VBF channels [4]. These results were primarily based on a fast simulation that parameterized the results of key detector performance studies performed with a full GEANT simulation. In this note we have considered three decay modes: the lepton-lepton (\(ll\)-channel), lepton-hadron (\(lh\)-channel) and the hadron-hadron (\(hh\)-channel) from VBF \(H\to\tau^{+}\tau^{-}\) signature. The analysis has been done using state-of-the art Monte Carlo generators, full GEANT-based simulation of the ATLAS detector with realistic misalignments and distortions applied to the expected material in the detector, utilization of our current reconstruction algorithms, and, where possible, incorporation of pileup interactions.

This analysis requires excellent performance from every ATLAS detector subsystem; the presence of \(\tau\) decays implies final states with electrons, muons, hadronic tau decays, and missing transverse momentum, while the Vector Boson Fusion production process introduces jets that tend to be quite forward in the detector. Due to the small rate of signal production and large backgrounds, particle identification must be excellent and optimized specifically for this channel. Furthermore, triggering relies on the lowest energy lepton triggers or exceptionally challenging tau trigger signatures. The detector performance aspects so important to this analysis are described in Refs. [11, 12, 13, 14, 15, 16, 17, 18, 19].

### Monte Carlo samples

Estimating the sensitivity of ATLAS to this channel requires the state of the art in Monte Carlo tools. The most challenging aspect of the theoretical calculations is the description of jet activity, an area in which the tools have evolved substantially since ATLAS'first publication on the sensitivity to the VBF processes. Details of the Monte Carlo samples are outlined in Ref. [20]. The signal samples were produced with HERWIG [21] and PYTHIA [22]. The QCD \(Z\)+jets and \(W\)+jets samples were produced with ALPGEN [23], which employs the MLM matching [24] between the _hard process_ (calculated with a leading-order matrix element for up to 5 jets) and the parton shower of HERWIG. The electroweak (ELWK) \(Z\)+jets background was simulated with SHERPA [25]. The \(t\bar{t}\)+jets and diboson background samples were generated with MC@NLO [26]. In all processes with taus, the tau decay was simulated using TAUOLA [27]. Additional photon radiation from charged leptons was simulated with PHOTOS [28]. The production cross-section for the signal is based on the next-to-leading order (NLO) computation and the k-factor (the ratio of the cross-section to that predicted by the lowest order calculation) is around 5% in the target mass range of 100\(-\)150 \(\,\mathrm{GeV}\). Note that the k-factor only involves the QCD corrections.

Because the GEANT-based detector simulation is computationally intensive, an event filter was applied to each sample after the parton shower and hadronization. Most processes were required to have at least one lepton in the final state. For background processes a VBF filter was used to remove events that would fail jet-related requirements. The filter bias has been studied and well-validated, but it affects our ability to estimate background rates early in the analysis cut flow. Furthermore, a significant Monte Carlo sample was produced with the ATLAS fast simulation, ATLFAST [29], without any event filter. These ATLFAST samples are used for systematic studies and to aid in the estimation of background rates (see Section 3.4).

The effect of in-time pileup (i.e. other soft \(p\)-\(p\) collisions in the same bunch crossing), out-of-time pileup (i.e. \(p\)-\(p\) collisions in neighboring bunch crossings), and the underlying event (i.e. multi-parton scattering and soft activity in the \(p\)-\(p\) collision of interest) are all important to this analysis. The underlying event has substantial theoretical uncertainty, and different models' predictions for the underlying event activity vary by large factors when extrapolating to the LHC energy range. Fortunately, the underlying event activity will be one of the first measurements at the LHC and will be well measured by the time the analysis described in this Note is performed. The pileup interactions are incorporated early in the simulation chain, at the time when the detector readout is simulated.

## 2 Event selection

### Triggering

While the ATLAS trigger system provides several possibilities for triggering that take advantage of the signal's complex final state, we restrict ourselves here to simple robust trigger signatures that are expected to have a low rate and an acceptable selection efficiency [12, 13]. For the \(lh\) and \(ll\) final states the events are selected by an isolated electron with \(p_{T}\geq 22\) GeV (e22i) or an isolated muon with \(p_{T}\geq 20\) GeV (\(mu\)20). The entire trigger chain has been simulated with the use of our current trigger algorithms and trigger menus; however, the dilepton triggers composed of isolated muons with \(p_{T}\geq 10\) GeV and isolated electrons with \(p_{T}\geq 15\) GeV considered in Ref. [4] were not used in this study. The trigger efficiency for VBF \(H\to\tau\tau\) (with \(m_{H}=120\) GeV) is 9.0% for events selected by the electron trigger and 9.9% in the case of muons. The trigger efficiencies include detector acceptance and are normalized with respect to the production cross-section for VBF \(H\to\tau\tau\). Additional triggers for the \(lh\) and \(ll\) final states, for instance the combined \(\tau+e\) or \(\tau+\mu\) triggers and triggers which take advantage of the tagging jets of the VBF process, are under study.

The all hadronic mode, or \(hh\)-channel, utilizes a different triggering strategy. Unlike the clean signature of the electron and muon triggers, the single tau trigger is expected to be exposed the large QCD jets background. Therefore only tau trigger in combination with other signatures, like missing \(E_{T}\) or another tau in the event, can be considered. We use L1_TAU30_xE40_softHLT as the primary trigger menu for the \(hh\)-channel in this study. It should be noted that unlike the high \(p_{T}\) single lepton triggers, both the hadronic tau and \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers are based on requirements from the first level of the trigger system with only a loose selection in the high-level trigger [30]. The expected trigger acceptance of L1_TAU30_xE40_softHLT is listed in Table 1 as well as those from e22i and mu20 menus. The trigger efficiency for the signal events (for \(m_{H}=120\) GeV) is 3.7% for L1_TAU30_xE40_softHLT. The disadvantage of the missing \(E_{T}\) trigger is the relatively low efficiency on signal; therefore, alternative menus like double tau menus are now being developed.

### Electron and muon reconstruction and identification

Electron candidates are formed from a cluster of cells in the electromagnetic calorimeter together with a matched track. The electron identification includes information from the shape of the shower, tracking information, and the consistency of the track and cluster. ATLAS provides multiple working points that trade electron efficiency for improved rejection of fakes. In this analysis we use the _Medium_ class electron as it provides sufficient fake rejection and provides a higher signal efficiency. In addition to the standard electron identification, we require that the energy in an isolation cone of radius \(\Delta R=0.2\) around the electron contains less than 10% of the electron's \(E_{T}\)2. The isolation cut is imposed to reject the contamination from hadronic jets.3 The reconstruction and identification efficiency is fairly flat after \(p_{T}\geq 15\)  GeV, where it achieves 69.4 \(\pm\) 0.2% efficiency while keeping the fake electron contamination at the order of 0.1%. In the VBF \(H\rightarrow\tau\tau\rightarrow\mu\mu+4\nu\) signal sample, the probability to reconstruct a fake electron was found to be slightly higher, 0.25\(\pm\)0.03%, reflecting some level of process-dependence.

Footnote 2: Due to a problem with the reconstruction, a correction to the isolation energy in the Tile gap scintillator was required.

Footnote 3: A track-based isolation requirement was also studied and shown to have similar performance.

In ATLAS, muon candidates can be seeded from either tracks in the inner detector or in the stand-alone muon spectrometer. In this analysis we required the highest quality muon candidates, which are formed by extrapolating the track in the muon spectrometer to the interaction point, finding a matching inner detector track, and forming a combined track if the two tracks satisfy various quality requirements [15]. The muon identification is composed of requirements on track quality and hit multiplicity in several muon stations. Similarly to the electrons, we require an isolation condition that the summed \(E_{T}\) within a radius \(\Delta R\) of 0.2 is less than 10% of the muon \(p_{T}\) to reject the contamination from jets. The reconstruction and identification efficiency is fairly flat after \(p_{T}\geq 10\)  GeV, and it achieves 91.9 \(\pm\) 0.1% while keeps the fake muon rejection under 0.005%.

The electron and muon identification criteria are summarized in Table 2. The \(p_{T}\) thresholds for electron and muon identification are chosen to provide stable identification efficiency and sufficient fake rejection. In addition, we require that the \(p_{T}\) of the offline reconstructed lepton must satisfy the \(p_{T}\) thresholds of the corresponding trigger, which is not strictly enforced due to subtle differences between the offline reconstruction and the trigger algorithms.

### Hadronic-tau reconstruction and identification

Approximately 65% of tau leptons decays produce hadrons. The majority of hadronic tau decays are composed of _single-prong_ candidates with one charged pion, which provides a track and a hadronic shower, and potentially associated neutral pions that provide an additional electromagnetic sub-cluster. In addition, _three-prong_ tau decays are also reconstructed, but with a higher rate of fakes from QCD jets. Due to the high momentum of the taus produced in this process, the decay products are collimated into a narrow region. ATLAS currently employs two hadronic tau reconstruction algorithms [31]; both require a calorimeter cluster matching a track; however, one algorithm is seeded by calorimeter clusters

\begin{table}
\begin{tabular}{c c} \hline \hline Trigger menu & Efficiency \(\times\) Acceptance(\%) \\ \hline e22i & 9.08 \(\pm\) 0.03 \\ mu20 & 9.88 \(\pm\) 0.04 \\ \hline L1\_TAU30\_xE40\_softHLT & 3.67\(\pm\)0.02 \\ \hline \hline \end{tabular}
\end{table}
Table 1: The product of efficiency and acceptance for the signal from the e22i, mu20, andL1\_TAU30\_xE40\_softHLT triggers.

and the other is seeded by the track. The two algorithms' efficiencies are complementary in different \(p_{T}\) regimes, and provide rejection strategies for their energy measurement and rejection against jets. The calorimeter-seeded algorithm was used for this analysis.

The calorimeter-seeded algorithm provides a log-likelihood ratio that distills discriminating power from a variety of track quality and shower shape information to discriminate between taus and jets [14]. The discriminating variable is designed to maintain a high tau efficiency while rejecting fake tau candidates from jets, leaving the precise working point to be optimized in the context of a specific analysis. The cuts on the discriminating variable and \(p_{T}\) of the tau candidates were optimized with respect to a simple \(s/\sqrt{s+b}\) performance measure. The background sample included \(Z\)+jets, \(W\)+jets, and \(t\bar{t}\)+jets, which comprises a background sample with a representative mixture of real and fake taus. Our modeling of the jet fragmentation indicates that quark-initiated jets are more collimated and have a 6-8 times higher fake rate than gluon-initiated jets. The relative abundance of real and fake tau candidates depends on the kinematic requirements imposed on the sample, thus the optimization should be performed after the final kinematic requirements described in Sections 2.8 and 2.9. However, the limited size of Monte Carlo samples requires that only a subset of the criteria used in the final event selection are applied during the optimization. Several subsets of the final event selection criterion were evaluated, and the final optimization was found to be reasonably stable and nearly independent of \(p_{T}\). After the optimization, the calorimeter-seeded algorithm's log-likelihood ratio was required to be greater than 4, corresponding to an identification efficiency of 50.0\(\pm\)0.2% and a fake jet selection efficiency of \(\sim\)1% for gluon-initiated jets and \(\sim\)2.5% for quark-initiated jets.

In addition to rejection against jets, an electron-veto was used to reject tau candidates which arise from electrons that have failed the electron identification. This electron-veto was performed by requiring that the tau candidate have at least 0.2% of its energy in the first sampling of the hadronic calorimeter and that the ratio of high-threshold (HT) to low-threshold (LT) hits in the transition radiation tracker (TRT) be less than 20% in the range \(|\eta_{\tau}|<1.7\). This electron-veto procedure suppresses the electron fake rate by 82.5% while retaining 90% of the hadronic tau candidates selected without the veto.

Finally, we present the hadronic tau reconstruction and identification performance in Fig. 1 (a) and the fake-jet tagging rate (b) as a function of \(p_{T}\), respectively. The selection criteria for the hadronic tau identification is summarized in Table 3.

\begin{table}
\begin{tabular}{c} \hline \hline Lepton identification \\ \hline
**Electron ID**: Medium \\ Isolation \(E_{T}(\Delta R=0.2)/p_{T}\leq 0.1\) \\ \(p_{T}\geq 25\)  GeV for trigger electron (e22i) \\ \(p_{T}\geq 15\)  GeV for other electrons \\ \hline
**Muon ID**: Combined muon \\ Isolation \(E_{T}(\Delta R=0.2)/p_{T}\leq 0.1\) \\ \(p_{T}\geq 20\)  GeV for trigger muon (_mu20i_) \\ \(p_{T}\geq 10\)  GeV for other muons \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of the identification requirements for electrons and muons.

Figure 1: Reconstruction and identification efficiency of the hadronic tau (a) and the jet-fake rejection efficiency (b) as a function of \(p_{T}\), respectively.

\begin{table}
\begin{tabular}{l} \hline \hline Hadronic tau identification \\ \hline
**Tau ID**: Calorimeter-seeded \\ \(p_{T}\geq 30\) GeV \\ Track multiplicity : 1 or 3 tracks \\ \(|charge|=1\) \\ \(Log\)_Likelihood Ratio_\(\geq 4\) \\
**Electron Veto**: \\ minimum TRT \(HT/LT\)\(\leq\)0.2 if \(|\eta_{\tau}|\)\(\leq\)1.7 and \(LT\)\(\geq\)10 \\ \(E_{T}^{\text{HAD}}/p_{T}\)\(\geq\)0.002 in matched electron object \\ \hline \hline \end{tabular}
\end{table}
Table 3: Selection criteria for the hadronic tau identification from the calorimeter-seeded reconstruction algorithm.

### Jet reconstruction

#### 2.4.1 Forward tagging jets

The jet activity of the vector boson fusion process is unique in several ways, providing many handles to suppress backgrounds and isolate a sample of signal events with high purity. The most important features of the VBF process are the presence of two high-\(p_{T}\) quark-initiated "tagging jets", which tend to be relatively forward and well separated in rapidity. Furthermore, due to color coherence in this electroweak process, additional QCD radiation between the tagging jets tends to be suppressed and motivates a Central Jet Veto (CJV) [32]. This section outlines the choice of jet algorithms and their performance, the definition of the tag jets, and several issues related to the CJV.

Figure 2 shows the \(\eta\) spectra of the highest and second highest \(p_{T}\) jets in signal and various background samples. Because the VBF jets can be very forward, the jet finding efficiency in this region is important in the analysis. Furthermore, the forward calorimeters (3.1\(\leq|\eta|\leq\)4.9) do not have a projective geometry, which leads to different challenges for jet reconstruction. ATLAS currently provides collections of jets based on two algorithms (a seeded cone algorithm with split-merge and a \(k_{T}\) algorithm), each with two sets of parameters (the cone size and the \(k_{T}\) cutoff scale), applied to two different input representations of the energy deposits in the calorimeter (towers merged to avoid negative energy fluctuations from electronic noise and clusters based on the ATLAS TopoCluster algorithm) [18]. These different jet algorithms and the different calorimeter pre-clustering result in different performances for jets, especially at low \(p_{T}\) and high \(|\eta|\).

Jet identification efficiency and purity are defined to give a quantitative measure of the jet identification. The efficiency and purity were calculated with respect to generator-level jets obtained by running the same jet algorithm on the stable interacting particles after hadronization and before GEANT simulation. To ensure that only hadronic jets are considered, we only use dimuon events, where both taus decay into a muon and neutrinos, or \(Z\)/\(W\) bosons directly decay into muons. This avoids any bias in the jet reconstruction produced by the presence of electrons. A reconstructed jet is considered to be matched if the corresponding generator-level jet is within \(\Delta\)R \(\leq\) 0.15 for jets with a cone size of 0.4. The matching cone size was chosen to avoid a single generator-level jet being matched to more than one reconstructed jet; with the given parameters this effect is at the order of \(10^{-3}\).

The jet reconstruction efficiency in different \(|\eta|\) regions and two different clustering algorithms is shown in Fig. 3 as a function of the generator-level jet \(p_{T}\) and \(\eta\). The reconstruction efficiency rises over 95% for jets with \(p_{T}\) above 50. On the other hand, the efficiency drops at \(|\eta|\sim\) 1.5 and \(|\eta|\)\(\sim\) 3.2 for jets in the range of 20-30. This drop in efficiency is due to the crack region in the calorimeter or large amounts of dead material in the corresponding \(\eta\) region. The jet collections based on calorimeter towers show a drop in efficiencies in the forward region due to a higher seed threshold, while the jet collections based on TopoClusters do not show this loss of efficiency. For this reason, jets based on TopoClusters have been chosen for this analysis.

Correctly identifying the quark-initiated tagging jets from the VBF process is very important for the measurement of Higgs boson spin and CP properties and for making precise correspondence with theoretical calculations [8]. Typically, the tagging jets are found in opposite hemispheres, but there are two approaches to incorporating this requirement in the analysis. One option is to define the tagging jets as the two highest \(p_{T}\) jets in the event, and reject the event from the signal candidates if they are in the same hemisphere (e.g. require \(\eta_{j1}\times\eta_{j2}\leq\) 0). A second option is to define the first tagging jet to be the highest \(p_{T}\) jet in the event and the second tagging jet to be the highest \(p_{T}\) jet in the opposite hemisphere. In this second approach it is not required that the second tagging jet is the second highest \(p_{T}\) jet in the event. These two strategies were compared, and it was found that the first method more reliably matched the quark-initiated tagging jets from the hard process.

The generator-level jets match the hard-scattered quarks nearly 100% of the time above a certainFigure 3: Jet reconstruction efficiency for the Cone jet algorithm with \(R=0.4\) as a function of the generator-level jet \(p_{T}\) for the jets based on TopoClusters (a) and \(\eta\) for Tower- and TopoCluster-based jets (b).

Figure 2: Pseudorapidity of the highest \(p_{T}\) (a) and the second highest \(p_{T}\) (b) jets for the Cone jet algorithm based on TopoClusters with \(R=0.4\) in VBF \(H\to\tau\tau\to\mu\mu\) (\(m_{H}\)=120 ) and background events. Only \(p_{T}\) cuts were applied to jets. Solid (black) histogram is for signal, dashed (red) histogram is for \(t\bar{t}\to WW\to(\mu\mu)\), and dotted (blue) histogram is for Z\(\to\mu\mu\)+n jets.

\(p_{T}\) threshold. To estimate the purity of the tagging jets, we define the efficiency with respect to the generator-level jets. The reconstructed tag jets have a high purity over the entire \(p_{T}\) and \(\eta\) range and do not show a strong dependence on the jet algorithms. Integrated efficiencies and purities for jets with \(p_{T}\geq 20\)  collider that the TopoCluster-based algorithm has better performance for this analysis. Because additional jets often lie in the central detector region, where we wish to employ a central jet veto, jets with smaller cones are favored for the selection. Furthermore, calorimeter noise (including effects from pileup of minimum-bias events) increases with jet cone radius. Thus, we use the cone jet algorithm with \(R=0.4\) running on TopoClusters as the primary jet algorithm in this analysis.

Having converged on a specific calorimeter pre-clustering and jet algorithm, we now present the kinematic properties of the jets that discriminate between the signal and backgrounds. The \(p_{T}\) cuts on the tagging jets are effective at reducing several backgrounds and Fig. 2 shows that the pseudorapidity distributions are substantially different. Instead of relying directly on the pseudorapidity of the tagging jets, Fig. 4 shows that the pseudorapidity gap (a) and invariant mass of the two tagging jets (b) provide substantial background rejection.

#### 2.4.2 Central jet veto

As mentioned above, the color coherence in the VBF Higgs boson production leads to a suppression of QCD radiation between the tagging jets. This color coherence is also found in the electroweak \(Z\)+jets background. In contrast, most of the other backgrounds have a much larger probability for additional QCD radiation in the central region. This is the physical motivation for a central jet veto (CJV). Figure 5 shows the jet multiplicity distribution for the signal and backgrounds after requiring two tagged jets (with \(p_{T}\geq 20\)  collider) in opposite hemispheres. The fraction of signal events with three or more jets is small.

The experimental challenge for the CJV is to provide a cut that is robust against additional minimum bias events (in-time pileup events). The optimization for the central jet veto has been studied in terms of \(p_{T}\) and \(\eta\). The probability to have at least one reconstructed jet with \(p_{T}\geq\)20  collider within \(|\eta|\leq 3.2\) is 1.6% from a single minimum-bias event. In Fig. 6, we present the trade-off of background rejection versus signal efficiency from varying the \(p_{T}\) threshold on the third highest \(p_{T}\) jet (markers indicate

Figure 4: Pseudorapidity gap between tag jets (a) and invariant-mass distributions of tag jets (b) in VBF \(H\rightarrow\tau\tau\rightarrow\mu\mu\) events (\(m_{H}\)=120  collider). A requirement \(\eta_{1}\times\eta_{2}\leq\)0 is used in addition to the cuts on jet \(p_{T}\). Solid (black) histogram is for signal, dashed (red) histogram is for \(t\bar{t}\to WW\rightarrow(\mu\mu)\), and dotted (blue) histogram is for Z\(\rightarrow\mu\mu\)+n jets.

thresholds of 20 and 30 GeV). A veto based on a fixed \(\eta\)-window was compared to a dynamic \(\eta\)-window defined by the \(\eta\) of the two tagging jets. We maintain the previous central jet veto requirement: no jets in \(|\eta|\leq 3.2\) with \(p_{T}\geq 20\) GeV. Figure 7 shows the efficiency of the central jet veto for signal, irreducible, and reducible backgrounds at varying levels of pileup.

The CJV poses significant theoretical challenges as well. At the parton-level, the CJV efficiency is expected to be known quite well with little theoretical uncertainty. However, the current tools that allow for the full parton-shower and hadronization (prerequisite for an analysis based on a GEANT-based detector simulation) show significant uncertainties. We have observed significant differences between the central jet activity in signal events generated with PYTHIA and those generated with HERWIG. Knowledge of the uncertainty on the CJV is needed for setting limits on the Higgs boson cross-section and for making coupling measurements; however, it is not needed directly in establishing a deviation from the background-only expectation (see Section 5.3).

In future studies we will also include a veto procedure using track information; in particular using vertexing information to reduce the impact of jets from in-time pileup. Furthermore, a track-based veto and the use of timing information in the calorimeter will also be studied to reduce the impact of out-of-time pileup.

#### 2.4.3 \(b\)-jet veto

In the \(ll\)-channel, the largest background contribution comes from \(t\bar{t}(+jets)\to l\nu b\,l\nu b\,(+jets)\). By introducing a veto on \(b\)-tagged forward jets it is possible to reduce this background [19]. Because the tagging jets are fairly forward in the detector, the \(b\)-tagging requirement is rather loose, i.e. efficient, and the \(t\bar{t}\) background can be reduced by a factor 2\(\sim\)3. Figure 8 demonstrates the efficiency of the \(b\)-jet veto as a function of the forward jet \(p_{T}\) for the signal and \(t\bar{t}\) background. The cut on the \(b\)-tag weight was optimized to achieve 65.1% reconstruction efficiency for \(b\)-quark jets, while 9.4% mis-identification efficiency for the light flavor jets is retained. Note that the \(b\)-jet veto is only used in the \(ll\)-channel.

### Missing transverse energy

Significant missing transverse energy (\(E_{\rm T}^{\rm miss}\) ) is present in \(H\to\tau^{+}\tau^{-}\) events because neutrinos are always associated with the \(\tau\) decays. The performance of the \(E_{\rm T}^{\rm miss}\) algorithm plays a vital role in this analysis because \(E_{\rm T}^{\rm miss}\) is used in the mass reconstruction of the tau pair. Ultimately, the \(E_{\rm T}^{\rm miss}\) resolution is what limits the \(m_{\tau\tau}\) resolution. Furthermore, the absolute scale of the \(E_{\rm T}^{\rm miss}\) must be well calibrated to correctly reconstruct the Higgs boson mass. In addition to the standard \(E_{\rm T}^{\rm miss}\) algorithm [17], we have made a dedicated correction in the presence of hadronic tau decays. The correction is based on the calibrated tau energy instead of the default treatment of the object that uses a jet calibration. This removes a \(\sim 1\) GeV bias in the \(E_{\rm T}^{\rm miss}\) distribution for the \(lh\)-channel. The sensitivity of the signal efficiency to the absolute energy scale is presented in Section 5.2. By requiring a large \(E_{\rm T}^{\rm miss}\), it is possible to improve the \(m_{\tau\tau}\) resolution and reject many backgrounds that do not contain neutrinos (e.g. \(Z\to ll\)). We require \(E_{\rm T}^{\rm miss}\geq 30\) GeV for the \(lh\)-channel and \(E_{\rm T}^{\rm miss}\geq 40\) GeV for the \(ll\)- and \(hh\)-channels.

### Mass reconstruction

Although there are several neutrinos in the event, it is possible to reconstruct the \(\tau^{+}\tau^{-}\) invariant mass by making the approximation that the decay products of the \(\tau\) are collinear with the \(\tau\) in the laboratory frame. This is a good approximation since \(m_{H}/2ggm_{\tau}\) and hence the taus are highly boosted. This leaves two unknown quantities and two equations: the fraction of each \(\tau\)'s momentum carried away by neutrinos and the constraints from the two components of \(E_{\rm T}^{\rm miss}\). For notational simplicity, consider the \(lh\)-channel and let \(l\) represent the momentum vector for the leptonic visible decay product and \(h\) represent the momentum vector for the hadronic visible decay products. By neglecting the \(\tau\) rest mass and imposing the collinear approximation, we can write

\[m_{\tau\tau}=\sqrt{2(E_{h}+E_{vh})(E_{l}+E_{vl})(1-\cos\theta_{lh})}\quad. \tag{1}\]By introducing the variables \(x_{h}\) and \(x_{l}\), the fraction of the \(\tau\)'s momentum carried away by the visible decay products, we can re-write the invariant mass as

\[m_{\tau\tau}=\frac{m_{lh}}{\sqrt{x_{l}x_{h}}}\qquad\qquad\text{for $x_{l,h}\geq 0 $}\quad. \tag{2}\]

One can easily solve for the \(x_{\tau}\) variables by requiring that the vector sum of the neutrinos coincides with the two measured components of \(E_{\text{T}}^{\text{miss}}\) :

\[x_{h}=\frac{E_{h}}{E_{h}+E_{vh}}=\frac{h_{x}l_{y}-h_{y}l_{x}}{h_{x}l_{y}+E_{ \text{x}}^{\text{miss}}l_{y}-h_{y}l_{x}-E_{\text{y}}^{\text{miss}}l_{x}}=\frac{ N}{D_{h}} \tag{3}\]

and

\[x_{l}=\frac{E_{l}}{E_{l}+E_{vl}}=\frac{h_{x}l_{y}-h_{y}l_{x}}{h_{x}l_{y}-E_{ \text{x}}^{\text{miss}}h_{y}-h_{y}l_{x}+E_{\text{y}}^{\text{miss}}h_{x}}=\frac {N}{D_{l}}, \tag{4}\]

where we have introduced \(N,D_{h}\), and \(D_{l}\) for convenience. If the two \(\tau\)s are back-to-back, then these equations are linearly-dependent and one cannot solve for the \(x_{\tau}\)s. For this reason, we require that \(\cos\Delta\phi_{\tau\tau}\geq-0.9\). Typically the Higgs boson has significant \(p_{T}\) due to the tagging jets. Events that come from the process \(X\to\tau\tau\) with no other sources of missing energy should have \(0\leq x_{\tau}\leq 1\), though resolution effects in \(E_{\text{T}}^{\text{miss}}\) may lead to unphysical solutions with either \(x_{\tau}<0\) or \(x_{\tau}\geq 1\). Equation 2 shows explicitly that cuts on \(x_{\tau}\) will impose constraints on the reconstructed mass for a given event, viz. \(m_{\tau\tau}\geq m_{lh}\), which results in an asymmetric distribution for \(m_{\tau\tau}\).

The sensitivity of \(m_{\tau\tau}\) to a mis-measurement of \(E_{\text{T}}^{\text{miss}}\) depends on the orientation of the \(\tau\)s. This sensitivity can be summarized by a Jacobian factor, \(J\). Neglecting correlation between the mis-measurement of the \(x\)- and \(y\)-components of \(E_{\text{T}}^{\text{miss}}\), one can define the Jacobian as follows

\[J=\frac{\Delta m_{\tau\tau}}{\Delta E_{T}^{\text{miss}}_{x/y}}=\sqrt{\left( \frac{\partial m_{\tau\tau}}{\partial E_{\text{x}}^{\text{miss}}}\right)^{2} +\left(\frac{\partial m_{\tau\tau}}{\partial E_{\text{y}}^{\text{miss}}} \right)^{2}}\quad. \tag{5}\]

Thus, we arrive at

\[J=\frac{1}{2}\frac{m_{lh}\sqrt{x_{l}x_{h}}}{|N|^{3}}\sqrt{\left(x_{l}h_{y}D_{ h}^{2}-x_{h}l_{y}D_{l}^{2}\right)^{2}+\left(x_{h}l_{x}D_{l}^{2}-x_{l}h_{x}D_{h}^{ 2}\right)^{2}}\quad. \tag{6}\]

The final mass measurement is a result of a fit to the \(m_{\tau\tau}\) distribution, and it is important to incorporate both the asymmetry and the fact that the width of the \(m_{\tau\tau}\) distribution is not common for all events. The modeling of the asymmetry and Jacobian scaling of the \(m_{\tau\tau}\) distribution is described in Section 4.2.

### Summary of the event selection for \(ll\)-channel

The event selection for \(ll\)-channel is summarized below, including some kinematic requirements specific to the \(ll\)-channel.

* Trigger: electron trigger e22i or muon trigger mu20.
* Trigger lepton: at least one lepton must have a reconstructed \(p_{T}\) greater or equal to the corresponding trigger requirement.
* Dilepton: exactly two identified leptons with opposite charge.
* Missing \(E_{T}\): \(E_{\text{T}}^{\text{miss}}\geq 40\,\) GeV.

* Collinear approximation: \(0\leq x_{l1,l2}\leq 0.75\) and \(\cos\Delta\phi_{ll}\geq-0.9\). The tighter cut on \(x_{\tau}\leq 0.75\) has been found to provide a better background rejection in the \(ll\)-channel. The shape of the control samples used to estimate the signal sensitivity are obtained after these cuts; additional details of the data-driven background estimation are given in Section 3. In addition to the cuts above, the signal candidates are also required to satisfy the following cuts.
* Jet multiplicity: at least one jet with \(p_{T}\geq 40\) and at least one additional jet with \(p_{T}\geq 20\).
* Forward jets: in opposite hemispheres \(\eta_{j1}\times\eta_{j2}\leq 0\), with tau centrality \(\min\{\eta_{j1},\eta_{j2}\}\leq\eta_{lep_{1,2}}\leq\max\{\eta_{j1},\eta_{j2}\}\) for the two highest \(p_{T}\) jets.
* \(b\)-jet veto: the event is rejected if either tag jet has \(b\)-tag weight greater than 1.
* Jet kinematics: \(\Delta\eta_{jj}\geq 4.4\) and dijet mass \(m_{jj}\geq 700\) for two forward jets.
* Central jet veto: the event is rejected if there are any additional jets with \(p_{T}\geq 20\) in \(|\eta|\leq 3.2\).
* Mass window: \(m_{H}-15\) around the test mass \(m_{H}\).

Table 4 summarizes the cross-section for signal events after each of the cuts described above.

### Summary of the event selection for \(lh\)-channel

The event selection for \(lh\)-channel is summarized below, including some kinematic requirements specific to the \(lh\)-channel.

* Trigger: electron trigger e22i or muon trigger mu20.
* Trigger lepton: at least one lepton must have a reconstructed \(p_{T}\) greater or equal to the corresponding trigger requirement.
* Dilepton veto: exactly one identified lepton (ensures this sample is disjoint from the \(ll\)-channel).
* Hadronic \(\tau\): exactly one identified hadronic \(\tau\) with opposite charge of the lepton.
* Missing \(E_{T}\): \(E_{\rm T}^{\rm miss}\geq 30\).

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline Mass (GeV) & 105 & 110 & 115 & 120 & 125 & 130 & 135 & 140 \\ \hline Cross section (fb) & 394.7 & 372.0 & 341.8 & 309.1 & 266.8 & 225.4 & 180.1 & 135.8 \\ Trigger & 65.6(3) & 65.1(2) & 61.1(2) & 57.2(1) & 51.5(2) & 44.7(1) & 36.5(1) & 28.3(1) \\ Trigger lepton & 56.4(3) & 56.2(2) & 53.2(2) & 49.5(1) & 44.7(2) & 38.9(1) & 31.8(1) & 24.7(1) \\ Dilepton & 5.73(7) & 5.86(6) & 5.80(6) & 5.46(3) & 4.94(5) & 4.30(4) & 3.61(4) & 2.88(4) \\ \(E_{\rm T}^{\rm miss}\ \geq 40\) GeV & 3.41(5) & 3.49(5) & 3.45(5) & 3.17(3) & 2.94(4) & 2.56(4) & 2.17(3) & 1.78(4) \\ Collinear Approx. & 2.34(5) & 2.38(4) & 2.33(4) & 2.15(2) & 1.95(4) & 1.69(3) & 1.46(2) & 1.16(3) \\ N jets \(\geq 2\) & 1.96(4) & 1.97(4) & 1.95(4) & 1.77(2) & 1.61(3) & 1.41(3) & 1.20(2) & 0.95(3) \\ Forward jet & 1.48(4) & 1.49(4) & 1.48(3) & 1.34(2) & 1.21(3) & 1.08(3) & 0.91(2) & 0.73(3) \\ \(b\)-jet veto & 1.26(3) & 1.30(3) & 1.25(3) & 1.16(2) & 1.04(3) & 0.94(2) & 0.77(2) & 0.64(2) \\ Jet kinematics & 0.70(3) & 0.69(2) & 0.70(2) & 0.63(1) & 0.58(2) & 0.52(2) & 0.43(1) & 0.37(2) \\ Central jet veto & 0.61(2) & 0.60(2) & 0.62(2) & 0.56(1) & 0.50(2) & 0.45(2) & 0.38(1) & 0.32(2) \\ Mass window & 0.52(2) & 0.50(2) & 0.51(2) & 0.45(1) & 0.39(2) & 0.34(1) & 0.29(1) & 0.23(1) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Signal cross-section (fb) for the \(ll\)-channel for various Higgs boson masses.

* Collinear approximation: \(0\leq x_{l}\leq 0.75\), \(0\leq x_{h}\leq 1\), and \(\cos\Delta\phi_{lh}\geq-0.9\). The asymmetric treatment of \(x_{h}\) and \(x_{l}\) provides background rejection in the \(lh\)-channel.
* Transverse mass: in order to further suppress the \(W\) + jets and \(t\bar{t}\) backgrounds, a cut on the transverse mass of the lepton and \(E_{\rm T}^{\rm miss}\) \[m_{T}\ =\ \sqrt{2\ p_{T}^{lep}\ E_{\rm T}^{\rm miss}\cdot(1-\cos\Delta\phi)}\ \leq\ 30\ \ \mbox{GeV}\] (7) is required, where \(p_{T}^{lep}\) is the transverse momentum of the lepton in the \(lh\)-channel and \(\Delta\phi\) is the angle between that lepton and \(\vec{E_{\rm T}^{\rm miss}}\) in the transverse plane. The shape of the control samples used to estimate the signal sensitivity are obtained after these cuts; additional details of the data-driven background estimation are given in Section 3. In addition to the cuts above, the signal candidates are also required to satisfy the following cuts.
* Jet multiplicity: At least one jet with \(p_{T}\geq 40\) GeV and at least one additional jet with \(p_{T}\geq 20\) GeV.
* Forward jets: in opposite hemispheres \(\eta_{j1}\times\eta_{j2}\leq 0\), with tau centrality \(\min\{\eta_{j1},\eta_{j2}\}\leq\eta_{lep,\tau}\leq\max\{\eta_{j1},\eta_{j2}\}\) for the two highest \(p_{T}\) jets.
* Jet kinematics: \(\Delta\eta_{jj}\geq 4.4\) and dijet mass \(m_{jj}\geq 700\) GeV for two forward jets.
* Central jet veto: the event is rejected if there are any additional jets with \(p_{T}\geq 20\) GeV in \(|\eta|\leq 3.2\).
* Mass window: \(m_{H}-15\) GeV \(\leq m_{\tau\tau}\leq m_{H}+15\) GeV around the test mass \(m_{H}\).

Table 5 summarizes the cross-section for signal events after each of the cuts described above. With 30 fb\({}^{-1}\) integrated luminosity, about 20 signal events are expected in the mass window.

### Summary of the event selection for \(hh\)-channel

The event selection for \(hh\)-channel is summarized below, including some kinematic requirements specific to the \(hh\)-channel.

* Trigger: a combination of the hadronic tau and missing \(E_{T}\) trigger L1_TAU30_xE40_softHLT.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline Mass (GeV) & 105 & 110 & 115 & 120 & 125 & 130 & 135 & 140 \\ \hline Cross section (fb) & 394.7 & 372.0 & 341.8 & 309.1 & 266.8 & 225.4 & 180.1 & 135.8 \\ Trigger & 65.6(3) & 65.1(2) & 61.1(2) & 57.2(1) & 51.5(2) & 44.7(1) & 36.5(1) & 28.3(1) \\ Trigger lepton & 56.4(3) & 56.2(2) & 53.2(2) & 49.5(1) & 44.7(2) & 38.9(1) & 31.8(1) & 24.7(1) \\ Dilepton veto & 50.0(3) & 49.6(2) & 46.7(2) & 43.4(1) & 38.9(2) & 34.0(1) & 27.6(1) & 21.3(1) \\ Hadronic \(\tau\) & 7.7(1) & 8.1(1) & 8.1(1) & 8.02(7) & 7.4(1) & 6.68(8) & 5.72(7) & 4.53(9) \\ \(E_{\rm T}^{\rm miss}\geq 30\) GeV & 4.8(1) & 5.1(1) & 5.08(9) & 4.96(5) & 4.63(8) & 4.16(7) & 3.51(6) & 2.82(8) \\ Collinear Approx. & 3.19(9) & 3.50(8) & 3.51(8) & 3.34(5) & 3.14(7) & 2.77(6) & 2.37(5) & 1.91(6) \\ Transverse mass & 2.53(8) & 2.70(7) & 2.67(7) & 2.46(4) & 2.26(6) & 1.98(5) & 1.64(4) & 1.29(5) \\ N jets \(\geq 2\) & 2.12(7) & 2.2(7) & 2.21(6) & 2.02(4) & 1.80(5) & 1.60(4) & 1.32(4) & 1.00(5) \\ Forward jet & 1.61(7) & 1.66(6) & 1.73(5) & 1.52(3) & 1.41(5) & 1.20(4) & 1.03(3) & 0.78(4) \\ Jet kinematics & 0.88(5) & 0.86(4) & 0.92(4) & 0.82(2) & 0.73(3) & 0.65(3) & 0.56(2) & 0.42(3) \\ Central jet veto & 0.77(5) & 0.77(4) & 0.81(4) & 0.72(2) & 0.63(3) & 0.55(2) & 0.50(2) & 0.38(3) \\ Mass window & 0.68(4) & 0.68(4) & 0.70(3) & 0.61(2) & 0.52(3) & 0.44(2) & 0.40(2) & 0.30(3) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Signal cross-sections (fb) for the \(lh\)-channel for various Higgs boson masses.

* Hadronic taus: two identified hadronic taus are required \(p_{T}\) above 35 \(\,\mathrm{\,Ge\kern-1.0ptV}\) and 30 \(\,\mathrm{\,Ge\kern-1.0ptV}\) with opposite charge.
* Missing \(E_{T}\): \(E_{\mathrm{T}}^{\mathrm{miss}}\geq 40\)\(\,\mathrm{\,Ge\kern-1.0ptV}\).
* Collinear approximation: \(0.2\leq x_{h1,h2}\leq 1\), \(\,\mathrm{\,and\,\,cos\Delta\phi_{hh}\,\geq\,-0.9}\).
* Di-tau transverse mass: in order to further suppress fake-\(\tau\) candidates from \(W\) + jets and \(t\bar{t}\) backgrounds, a cut on the di-tau transverse mass \[m_{T}^{hh}\,=\,\sqrt{2\,\,p_{T}^{hh}\,E_{\mathrm{T}}^{\mathrm{miss}}\cdot(1- \cos\Delta\phi)}\,\leq\,80\,\,\,\mathrm{\,Ge\kern-1.0ptV}\] (8) is required, where \(p_{T}^{hh}\) is the transverse momentum of the two hadronic tau system and \(\Delta\phi\) represents the azimuthal angle between \(p_{T}^{hh}\) and \(E_{\mathrm{T}}^{\mathrm{miss}}\). This variable has been identified as potentially useful for the analysis. The optimal value of this cut depends heavily on the relative amount of the \(W\)+jets, \(t\bar{t}\) and QCD backgrounds, therefore, the requirement is kept fairly loose.
* Jet multiplicity, forward jets, angular cuts, and central jet veto in the case of the \(lh\)-channel.
* Total \(p_{T}\): to reject events with many jets like \(t\bar{t}\), a cut on the total \(p_{T}\) is applied: \[||\vec{p_{T}}^{\,h1}+\vec{p_{T}}^{\,h2}+\vec{p_{T}}^{\,j1}+\vec{p_{T}}^{\,j2} +E_{\mathrm{T}}^{\mathrm{miss}}||\,\leq\,60\,\,\mathrm{\,Ge\kern-1.0ptV}\quad.\] (9)
* Jet kinematics: \(\Delta\eta_{jj}\geq 4\) and dijet mass \(m_{jj}\geq 700\)\(\,\mathrm{\,Ge\kern-1.0ptV}\) for two forward jets.
* Central jet veto: the event is rejected if there are any additional jets with \(p_{T}\geq 20\)\(\,\mathrm{\,Ge\kern-1.0ptV}\) in with \(|\eta|\leq 3.2\).
* Mass window: \(m_{H}-15\)\(\,\mathrm{\,Ge\kern-1.0ptV}\leq m_{\tau\tau}\leq m_{H}+20\)\(\,\mathrm{\,Ge\kern-1.0ptV}\) around the test mass \(m_{H}\).

Table 6 summarizes the cross-section for signal events after each of the cuts described above. The events used in the analysis have been generated applying a filter that requires two hadronic taus with \(p_{T}\geq\)12\(\,\mathrm{\,Ge\kern-1.0ptV}\) in the final state, produced in \(|\eta|\,\leq\)2.7 and with \(\Delta\phi\leq 2.9\).

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline Mass (GeV) & 105 & 110 & 115 & 120 & 125 & 130 & 135 \\ \hline Cross section (fb) & 394.7 & 372.0 & 341.8 & 309.1 & 266.8 & 225.4 & 180.1 \\ Trigger tau \& MET & 12.4(2) & 12.1(2) & 12.0(2) & 11.4(1) & 10.4(2) & 9.2(1) & 7.93(1) \\
2 Hadronic \(\tau\)s & 1.73(8) & 1.80(8) & 1.93(8) & 1.83(4) & 1.67(7) & 1.52(5) & 1.29(5) \\ \(E_{\mathrm{T}}^{\mathrm{miss}}\,\geq 40\)\(\,\mathrm{Ge\kern-1.0ptV}\) & 1.34(7) & 1.39(7) & 1.50(7) & 1.43(3) & 1.32(6) & 1.17(5) & 0.99(4) \\ Collinear Approx. & 0.91(6) & 1.02(6) & 1.13(6) & 1.03(3) & 1.00(5) & 0.85(4) & 0.72(3) \\ Di-tau Transverse mass & 0.91(6) & 1.02(6) & 1.13(6) & 1.03(3) & 1.00(5) & 0.85(4) & 0.72(3) \\ N jets \(\geq 2\) & 0.77(5) & 0.88(6) & 0.94(5) & 0.86(3) & 0.84(5) & 0.72(4) & 0.61(3) \\ Total \(p_{T}\) & 0.72(5) & 0.84(5) & 0.91(5) & 0.83(3) & 0.80(5) & 0.69(4) & 0.58(3) \\ Forward jet & 0.62(5) & 0.73(5) & 0.75(5) & 0.72(2) & 0.68(4) & 0.58(3) & 0.50(3) \\ Jet kinematics & 0.37(4) & 0.43(4) & 0.41(4) & 0.45(2) & 0.41(3) & 0.36(3) & 0.28(2) \\ Central jet veto & 0.34(3) & 0.38(4) & 0.36(3) & 0.39(2) & 0.35(3) & 0.32(3) & 0.24(2) \\ Mass window & 0.25(3) & 0.35(4) & 0.33(3) & 0.34(2) & 0.29(3) & 0.27(2) & 0.20(2) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Signal cross-sections (fb) for the \(hh\)-channel for various Higgs boson masses. The events used in the analysis had a filter applied at generation that required two hadronic taus with \(p_{T}\geq\)12\(\,\mathrm{Ge\kern-1.0ptV}\) in the final state, produced in \(|\eta|\,\leq\)2.7 and with \(\Delta\phi\leq 2.9\).

Background estimation

### Overview

Despite the advances in theoretical tools and extraordinarily detailed simulation of the ATLAS detector, it is preferable to estimate backgrounds from data instead of relying entirely on Monte Carlo estimates. Below we describe data-driven background estimation techniques for each of the major backgrounds and estimate the systematic uncertainty of the estimates. Each technique has been developed to address the aspects of the background estimation which are most relevant for the analysis: the shape of the \(m_{\tau\tau}\) tail from the irreducible \(Z\to\tau\tau\), the fake tau contribution in the \(lh\)-channel, and the normalization of the QCD backgrounds. Section 4 describes how these techniques are incorporated into the final signal extraction, significance calculation, and mass measurement.

While it is possible to use Monte Carlo to estimate the systematics associated with the data-driven background estimation techniques, we must wait for data until we can employ these methods to produce reliable _estimates_ of the backgrounds. Thus, in a feasibility study such as this one we face the additional challenge of _predicting_ the expected backgrounds with Monte Carlo. The challenge of predicting our background and the associated uncertainties are distinct from the ones that we will face once we have collected \(\sim\)30 fb\({}^{-1}\) of data.

The major challenge in background prediction is related to the limited size of our Monte Carlo samples. Backgrounds from mis-identified leptons are difficult to estimate due to the large rejection factors of the identification algorithms. Even the irreducible \(Z\to\tau\tau\) backgrounds are suppressed by several orders of magnitude due to the kinematic requirements. The \(t\bar{t}\) background requires particularly large sample sizes because it is suppressed by both identification and kinematic requirements. Table 7 summarizes the size of the Monte Carlo samples used in this study and their corresponding luminosity. Despite the large computing investment and generator-level filters4), many background samples were not sufficiently large to estimate rates deep in the analysis where rejection is at the order of \(10^{6}\) - \(10^{8}\). Thus, the "full" GEANT simulation was augmented with a fast simulation sample \(\sim\)100 times larger and a cut factorization method was used to predict the final background rates. While these procedures have large uncertainties, they are only relevant to our ability to estimate our sensitivity and will not plague the analysis once we have data.

Footnote 4): \(\sqrt{\text{BFCut}}:N_{e/\mu}\geq\)1 or 2, or \(N_{\tau}\geq\)1 with \(p_{T}\geq\)10 GeV, \(|\eta|\leq\)2.7 for electron, muon and tau from \(W\) and \(Z\), respectively. For the hadron level jets with cone size 0.4, \(N_{jet}\geq\)2 with \(p_{T}^{1}\geq\)20 GeV for the highest \(p_{T}\) jet, \(p_{T}^{2}\geq\)15 GeV for the second highest \(p_{T}\) jet, and \(|\eta|\leq\)5, \(m_{jj}\geq\)300 GeV, \(\Delta\eta_{jj}\geq\)2.

### Z + jets

While there is some theoretical uncertainty in the \(Z\to\tau\tau\)+jets background [33], the most serious danger of this background comes from the high-side tail in the \(m_{\tau\tau}\) distribution, where we would expect to see the signal. This tail is dominated by instrumental effects; particularly mis-measurement of \(E_{\text{T}}^{\text{miss}}\), which is correlated to instrumental effects related to jet energy mis-measurement. Thus, we have developed a data-driven background estimation technique, which begins with a signal-free \(Z\to\mu\mu\)+jets sample and transfers the dominant instrumental effects to the \(Z\to\tau\tau\)+jets sample. This is achieved by replacing the muons with an equivalent tau, and carefully treating the decay of the tau. This technique is justified because the \(Z\to\mu\mu\)+jets events have identical jet activity and kinematics as \(Z\to\tau\tau\)+jets (before the tau decays) and because the relevant features of tau decays are well understood. We restrict the technique to the \(Z\to\mu\mu\)+jets control sample because muons lose only a small fraction of their energy in the calorimeter, and the effect on \(E_{\text{T}}^{\text{miss}}\) is easier to estimate. After creating the emulated \(Z\to\tau\tau\)+jets control sample, the full event selection is applied. The normalization of the \(Z\to\tau\tau\) background does not require this emulation because it can be estimated directly from the height of the \(Z\)-peak in \(m_{\tau\tau}\) spectrum obtained with the signal candidates.

The first task is to obtain a signal-free dimuon control sample. Both loose and tight control samples have been used in this data-driven estimation technique. The loose control sample is used to estimate the ditau background not only from \(Z\rightarrow\tau\tau\)+jets events but also from \(t\bar{t}\) or di-boson backgrounds, while the tight control sample can be used for the \(Z\rightarrow\tau\tau\) background estimation by obtaining relatively pure \(Z\rightarrow\mu\mu\)+jets events. The loose control sample requires only a minimum sets of cuts from the \(ll\)-channel, hence \(\sim 10\%\) of this control sample includes other processes such as \(t\bar{t}\), diboson or even \(Z\rightarrow\tau\tau\). The tight control sample, in contrast, selects pure \(Z\rightarrow\mu\mu\) events with less than 1% contamination from the other processes. A tighter event selection is used to define this control sample, it is identical to the final event selection in the \(ll\)-channel but excludes of the \(E_{\rm T}^{\rm miss}\), collinear approximation, and mass window cuts. To improve the purity of \(Z\) events, a \(Z\) mass window cut is used: \(m_{\mu\mu}\geq m_{Z}-10\,\) GeV. Due to a strong correlation between the \(m_{\tau\tau}\) and \(m_{\mu\mu}\) distributions before the \(\mu\rightarrow\tau\) conversion, the \(m_{\tau\tau}\) distribution is biased by a cut on \(m_{\mu\mu}\). The lower bound on \(m_{\mu\mu}\geq\ m_{Z}-10\,\) GeV has little influence of the shape in the signal region; however, an upper mass cut causes a large bias. Therefore, no upper bound is placed on \(m_{\mu\mu}\). After obtaining the dimuon events, the reconstructed muons are replaced by the Monte Carlo tau, then decayed and simulated in the ATLAS detector simulation and reconstruction software.

Two different techniques for replacing the muon with the tau decay products were evaluated in this study. One uses the TAUOLA decay package [27] and the other uses a simple re-scaling of the momentum and efficiency of taus, since the technical complexity of re-simulation by TAUOLA are rather difficult in the full detector simulation and reconstruction. A comparison of the two different methods provides a quantitative validation of this procedure. Several comparisons of the \(Z\rightarrow\tau\tau\)+jets control sample emulated from \(Z\rightarrow\mu\mu\) and the true \(Z\rightarrow\tau\tau\) background were performed after imposing the full event selection criteria for the \(ll\)- and \(lh\)-channels. Figure 9 shows the \(p_{T}\) of the leptons and the \(E_{\rm T}^{\rm miss}\) distributions of the emulated sample compared to the true \(Z\rightarrow\tau\tau\to ll+4\nu\) process. Figure 10 (a) depicts the reconstructed visible mass for the true and emulated samples in the \(lh\)-channel and (b) shows the bin-by-bin ratio of these distributions. Excellent agreement between the true and emulated distributions are also observed in each of the \(ll\)-, \(lh\)-, and \(hh\)-channels in the region of interest. The gray horizontal band in the figure represents \(\pm 10\%\) around a ratio of 1, which is used to reflect the uncertainty

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Process & cross-section (pb) & Simulated events & Luminosity (fb\({}^{-1}\)) \\ \hline \(Z\to ll\)+jets & 35.1 & 714,500 & 63.1 \\ \(W\to l\nu\)+jets & 346.3 & 765,000 & 3.43 \\ \(t\bar{t}\)+jets (full) & 450.0 & 1,012,941 & 1.65 \\ \(t\bar{t}\)+jets (fast) & 833.0 & 96,250,000 & 84.3 \\ \(t\bar{t}\) (2-muon, full) & 32.6 & 904,000 & 20.4 \\ \(WW/WZ/ZZ\)+jets & 174.2 & 258,094 & 0.57 \\ QCD di-jets (full) & \(\sim\)1.4\(\times 10^{9}\) & 1,503,250 & \(\sim\)10\({}^{-6}\) \\ QCD di-jets (fast) & \(\sim\)1.4\(\times 10^{9}\) & 80,000,000 & \(\sim\)5 \(\times 10^{-5}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Details of the background samples used in this study. The VBFCut filter is applied for \(Z/W\)+jets events at generator level. The corresponding luminosities for \(Z/W\)+jets and di-boson events are estimated for 2 jets events for \(Z\) and 3 jets events for \(W\), and \(WW\) events for di-boson process respectively. In \(t\bar{t}\) samples, the actual physics events are approximately 70% to the total number of simulated events according to the treatment of the negative weighted events.

in the shape from the tau modeling and sensitivity to the analysis cuts. With this method we are able to accurately model both the shape and normalization of the \(Z\to\tau\tau\) backgrounds for all tau decays.

### QCD background in \(lh\)-channel

The method described above estimates the contribution of taus for all processes, including \(t\bar{t}\), but does not estimate the contribution of the fake taus. The fake rate of leptons is much smaller as compared to the contribution from real leptons from \(W\) and \(\tau\) decays in the \(ll\)-channel. In contrast, in the \(lh\)-channel roughly half of the \(t\bar{t}\) background come from fake hadronic-taus. In addition, the \(W\)+jets background is comparable to \(t\bar{t}\) in the \(lh\)-channel; therefore, estimating the QCD fake contribution to the \(lh\)-channel requires a dedicated procedure. Estimation of the QCD fake rate is determined from a data-driven method.

Figure 10: Reconstructed invariant mass distribution (a) and its bin-by-bin ratio (b) generated from the true and emulated \(Z\to\tau\tau\to lh+3\nu\) events. The gray band represents \(\pm 10\%\) around a ratio of 1.

The technique used here exploits the track multiplicity in a cone around the tau candidate. Real taus typically have one or three tracks, with some spread due to tracking efficiency or the presence of spurious tracks. Electrons have dominantly a single track, while jets have a broad distribution with a higher average multiplicity. Figure 11 shows the track multiplicity distribution for taus, electrons, and jets in a cone of radius 0.7 after removing outlying tracks. It is clear that the track multiplicity can be used to constrain the relative abundance of the three components to the distribution. While Fig. 11 was created from Monte Carlo, the electron and jet track multiplicity distributions can easily be obtained from data.

Given a sample of tau candidates, the relative abundance of taus, electrons, and jets can be found by fitting the track multiplicity distribution with the extended likelihood function

\[\begin{array}{rcl}L_{track}(r_{QCD},r_{tau})&=&\prod_{i}^{N}Pois(n_{exp}^{tot }\times(r_{tau}f_{tau}^{i}+r_{QCD}f_{jet}^{i}+(1-r_{tau}-r_{QCD})f_{lep}^{i})|N_{ obs}^{i})\\ &&\times\;Gaus(N_{obs}^{tot}|n_{exp}^{tot},\sqrt{n_{exp}^{tot}})\\ &&\times\;Gaus(N_{lep}^{measured}|n_{exp}^{tot}(1-r_{tau}-r_{QCD}),\Delta_{lep}n_{ exp}^{tot}(1-r_{tau}-r_{QCD}))\end{array} \tag{10}\]

where \(n_{exp}^{tot}\) is the total number of events estimated by the fit, \(r_{tau}\) (\(r_{QCD}\)) is the fraction of the tau (jet) contribution with respect to the estimated total number of events, \(\Delta_{lep}=10\%\) is the relative uncertainty on lepton measurement, and \(f^{i}\) is the normalized probability for the \(i^{th}\) bin of the track multiplicity distribution. The second term constrains the normalization, and the third term is an additional constraint term for the lepton contribution estimated by an independent analysis. The fit is performed to find the maximum likelihood estimate with minuit[34].

The track multiplicity distribution for the QCD jets is modeled from samples of QCD dijets that produce tau candidates with \(p_{T}\) in the range of 17- 280. No event level selections are applied at this stage. Similarly, the multiplicity distribution of the tau signal and lepton background are modeled with Drell-Yan Monte Carlo samples; however, all analysis requirements up to transverse mass cut are applied. Pseudo-datasets were generated for various luminosities based on the corresponding cross-sections and multiplicity distributions. The highly uncertain QCD multi-jet background was scaled to be five times larger than the estimated rates of \(t\bar{t}\) and \(W\)+jets after event selection. A fit was performed for each of the 2000 pseudo-datasets and the results were used to estimate quantify the performance of the method. The expected error on the fraction \(r_{tau}\) is presented in Fig. 12 as a function of luminosity.

The fraction \(r_{tau}\) in the signal candidates remaining after the transverse mass cut can be measured to within 5% accuracy with 1 fb\({}^{-1}\) integrated luminosity. The largest uncertainty in this method comes from the dependence of the track multiplicity on the jet \(p_{T}\). The systematics were estimated by dividing the jet into two samples; those with \(p_{T}\leq\)70  and those with \(p_{T}\geq\)70. Additionally, the presence of \(E_{\rm T}^{\rm miss}\) in the pure QCD processes is strongly correlated to the event kinematics. Thus, we assign an additional systematic associated with the variation observed when repeating the method with modified track multiplicity distributions with and without the requirements on \(E_{\rm T}^{\rm miss}\). The systematics associated with the QCD shape contribute about 2% to \(r_{tau}\) measurement.

### Cut factorization method

The analysis cuts described in Section 2 have rejections against backgrounds of the order of \(10^{8}\). Only a few tens of events are expected with 30 fb\({}^{-1}\) of data, and the background Monte Carlo samples generally correspond to 5 fb\({}^{-1}\) or less. The lack of sufficiently large Monte Carlo samples requires an approximate procedure to predict the background rate at the end of the analysis. We utilize a cut factorization method in which the analysis cuts are divided into three categories that are roughly uncorrelated so that the rejection can be factorized. The first category are related to the tau decays from the Higgs bosoncandidate (trigger, lepton ID, hadronic tau ID, \(E_{\rm T}^{\rm miss}\), the collinear approximation, and the transverse mass) and the rejection is dominated by detector performance issues. The second category of cuts are related to the tagging jets (forward jets, jet separation, and dijet mass) and the rejection is dominated by the kinematic properties of the events. The third category consists of those cuts which are strongly correlated to both the forward tagging jets and the tau decay products (centrality, central jet veto, and mass window cut). The method itself is simple; the background rejection rate is determined for each of the categories individually, and the product of these rejection rates is used to estimate the total rejection rate. Two variations on this cut factorization procedure were considered. In the first approach, the rejection of the jet-related cuts were calculated without any tau-related cuts. The second approach differed in that it included the lepton and tau identification in order to avoid bias effects from the contribution of fake leptons and taus. The residual correlations between the categories of cuts contributes to an uncertainty in this technique. The uncertainties were estimated with the use of larger samples produced with fast simulation and fully simulated samples with generator-level filters, which enrich the backgrounds in the signal-like region. The factorization process was used for all background processes except for the irreducible \(Z\to\tau\tau\) background.

For the \(W\to l\nu\)+jets and \(Z\to ll\)+jets backgrounds, ALPGEN samples with up to 5-jets corresponding to 4 \(\rm{fb}^{-1}\) integrated luminosity were used. Table 8 shows the average of the jet rejection from the two approaches, which is used as our final prediction. The \(Z\) events in \(ll\)-channel indicate that the jet-related cuts are not strongly correlated with the tau-related cuts, since the rejection rate is the same in the electron and muon modes. The correlation between the categories of cuts was investigated with an ALPGEN \(Z\)\(\to\tau\tau+\rm{n}\) jets sample enriched with the VBFCut filter. The production kinematics are the same, but the enhanced \(E_{\rm T}^{\rm miss}\) due to the \(\tau\) decays lets a sufficient number of events survive all the analysis cuts. The background predictions with and without cut factorization are consistent within the 20% statistical error. Thus, we assign a 20% systematic uncertainty for this evaluation method on this process.

The \(t\bar{t}\) background is the most complicated process in that it includes both irreducible and reducible contributions in all channels. We use \(10^{6}\)\(t\bar{t}\) events produced by the MC@NLO generator. The events do not include the process in which both \(W\)s decay hadronically, thus the sample corresponds to 1.6fb\({}^{-1}\) of integrated luminosity. While the rejection of the tau-related cuts can be reliably estimated, the rejection of the jet-related cuts suffers from the limited sample sizes. The rejection of the jet-related cuts is expected to be very high (\(\times 10^{4}\) for \(t\bar{t}\)), resulting in a \(\sim\)30% statistical error in the final background predictions.

Table 8 summarizes the rejection of the jet-related cuts subdivided into events with a real or fake hadronic tau for the \(lh\)-channel and by lepton flavor for the \(ll\)-channel. Again, the average of the two approaches is used for the final background prediction. Table 8 shows that there is a considerable contribution of fake-tau events for the \(lh\)-channel. In the \(ll\)-channel, the contribution from semi-leptonic decays of \(B\)-hadrons was found to be very small. An estimate of the uncertainty in the method was found by comparing the prediction from cut-factorization to the direct estimate from an enriched sample filtered to require at least two muons in the event. Furthermore, the cut-factorization results were compared to a sample produced with the fast simulation that was approximately 100 times larger. Based on these comparisons, we assign 50% uncertainty in the \(t\bar{t}\) background prediction.

The diboson background contribution was predicted with the same cut-factorization procedure. Even after using the cut-factorization procedure, the statistical uncertainty in the prediction is very large. Fortunately, the diboson production cross-section is much smaller than the \(t\bar{t}\) processes, so the effect of this background is very small. The results of the cut factorization prediction are presented in Table 8, and we assign a large uncertainty of 50% for diboson background rate. While a number of tests were performed to validate the method, it is clear that this is an approximate procedure and that the limited size of the Monte Carlo samples fundamentally limits our ability to predict this background.

### Background for the \(hh\)-channel

In addition to the \(Z\)+jets, \(W\)+jets and \(t\bar{t}\) backgrounds, the \(hh\)-channel also has a background from the pure QCD multi-jet process. The estimation of QCD multi-jets must be made from data. A few handles exist for estimating the pure QCD background. First, one can utilize a sample of same-sign tau candidates to estimate the fake tau contribution since the sign of the tau candidate from QCD is approximately random. Potentially, one can utilize constraints from the track multiplicity distribution as described in Section 3.3. Furthermore, one can loosen the identification requirements on the tau candidates to obtain a sample dominated by QCD fakes, and then extrapolate this background into the signal region using knowledge of the fake tau's likelihood distribution obtained with data. The efficiency of these techniques must also be established with data, but here we assume that the same-sign sample will be able to estimate the QCD background with an uncertainty given by two components. The first component of this uncertainty is the statistical error in the control sample, which scales like \(1/\sqrt{N_{SS}}\), where \(N_{SS}\) is the size of same sign sample coming from QCD backgrounds. The second component is a systematic uncertainty associated with using the same sign sample to estimate the opposite sign sample. Experience from the Tevatron shows that charge correlations can be of order 13% with an uncertainty of order 3% [35, 36]. Given that the final state requires two additional jets, which can alter the contribution of quark and gluon-initiated

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline Acceptance & \multicolumn{4}{c}{\(ll\)-channel} \\ (\%) & \(Z\to ee/\mu\mu\)+jets & \(W\to e\nu/\mu\nu\)+jets & \(t\bar{t}\) (\(ee/\mu\mu/e\mu\)) & \(WW/WZ/ZZ\) \\ \hline Jet kinematics & 2.32(2) / 2.43(2) & 2.4(3) / 1.5(2) & 0.72(8) / 0.52(5) / 0.60(4) & 0.43(5) / 0.56(3) / 0.33(3) \\ Central jet veto & 1.00(1) / 1.02(1) & 1.1(3) / 0.8(2) & 0.11(3) / 0.04(1) / 0.10(1) & 0.26(5) / 0.26(4) / 0.10(2) \\ Mass window & 0.230(5) / 0.202(4) & 0.1(1) / 0.1(1) & 0.019(7) / 0.002(1) / 0.010(3) & – / – / 0.03(1) \\ \hline \hline \multicolumn{4}{c}{} \\ \hline \hline \multicolumn{4}{c}{} \\ \hline \hline \multicolumn{4}{c}{} \\ \hline \end{tabular} 
\begin{tabular}{l|c|c|c|c} \hline \hline Acceptance & \multicolumn{4}{c}{\(ll\)-channel} \\ (\%) & \(Z\to ee/\mu\mu\) & \(W\to e\nu/\mu\nu\) & \(t\bar{t}\) (\(\tan\) / non-tau) & \(WW/WZ/ZZ\) \\ \hline Jet kinematics & 2.60(5) / 2.8(1) & 2.7(1) / 2.6(1) & 0.93(6) / 1.11(6) & 0.2(2) / 0.6(1) / 0.50(3) \\ Central jet veto & 1.14(3) / 1.30(9) & 1.7(1) / 1.3(1) & 0.07(3) / 0.12(3) & 0.1(1) / 0.3(1) / – \\ Mass window & 0.22(1) / 0.11(2) & 0.10(3) / 0.05(2) & 0.015(8) / – & – / 0.03(3) / – \\ \hline \hline \end{tabular}
\end{table}
Table 8: Rejection rates of the jet-related cuts for \(Z/W\)+jets, \(t\bar{t}\) and diboson events in \(ll\)- and \(lh\)-channels, respectively.

jets, we assume a 10% systematic error associated with the charge correlation. In addition, the same sign sample will also include a contribution from \(W\rightarrow\tau\nu\)+jets, where the Tevatron experiments observed a charge correlation that was much higher with a 40% uncertainty [35]. The experience from the Tevatron provides some insight into this approach, but the results are not directly relevant because the LHC is not a \(\bar{p}\)-\(p\) machine.

As in the \(ll\) and \(lh\)-channels we do not have sufficiently large Monte Carlo samples to predict the background for the \(hh\)-channel. We employ the same cut factorization method for the \(Z\)+jets, \(W\)+jets and \(t\bar{t}\) backgrounds as described above. The prediction of the pure QCD multi-jet background from Monte Carlo is hopeless without factorizing the analysis even further. A sample of 80 million QCD dijet events (including \(c\bar{c}\) and \(b\bar{b}\) processes) were simulated with the fast detector simulation. The tau fake rate was parameterized from full simulation as a function of \(\eta\) and \(p_{T}\) and used to re-weight the dijet sample. In order for a pure QCD process to satisfy the event selection, there must be at least four high-\(p_{T}\) jets. Previous studies have shown that the parton shower underestimates the tagging jet requirement by a factor of 2-3 [4]. Therefore, we multiply the prediction from QCD dijets after the forward jet requirement by a factor of 5 to include the underestimate from the parton shower and an additional safety factor. Table 11 shows that the analysis cuts are extremely effective at rejecting QCD backgrounds, but a realistic estimation of the remaining background requires data.

### Summary of the background predictions

Tables 9, 10, and 11 summarize the background predictions for the \(ll\)-, \(lh\)-, and \(hh\)-channels, respectively. The tables also indicate the statistical uncertainty on the estimates from the limited size of the Monte Carlo samples. Note that the mass window cut is set with respect to the test Higgs mass of 120 GeV. The effective cross-sections estimated with the cut-factorization method are marked with an asterisk. Furthermore, the predictions from the \(t\bar{t}\) sample simulated with the fast simulation are shown, where the effective cross-section was normalized to the fully simulated sample after the collinear approximation for the \(ll\)-channel and after the transverse mass cut for the \(lh\)-channel.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline  & \(Z\rightarrow\tau^{+}\tau^{-}\) & jets(\(\geq\)1) & \multicolumn{2}{c|}{\(t\bar{t}\)} & \(Z\rightarrow\ell^{+}l^{-}\) +n jets & \(W\rightarrow\nu\)+n jets & diboson \\  & QCD & ELWK & Full & Fast & (n \(\geq\) 1) & (n \(\geq\) 1) & \(WW/ZZ/WZ\) \\ \hline Cross section (fb) & 168.4\(\times 10^{3}\) & 1693 & 833\(\times 10^{3}\) & 768.6\(\times 10^{3}\) & 8649\(\times 10^{3}\) & 174.1\(\times 10^{3}\) \\ Trigger & 51.5(1)\(\times 10^{3}\) & 230(1) & 209.8(2)\(\times 10^{3}\) & 633.8(4)\(\times 10^{3}\) & 4411(9)\(\times 10^{3}\) & 32.0(1)\(\times 10^{3}\) \\ Trigger lepton & 42.7(1)\(\times 10^{3}\) & 190(1) & 179.1(2)\(\times 10^{3}\) & 588.0(4)\(\times 10^{3}\) & 3815(9)\(\times 10^{3}\) & 28.0(1)\(\times 10^{3}\) \\ Dilepton & 4.25(5)\(\times 10^{3}\) & 19.2(4) & 21.7(1)\(\times 10^{3}\) & 369.9(5)\(\times 10^{3}\) & 2.5(2)\(\times 10^{3}\) & 3.95(6)\(\times 10^{3}\) \\ \(E_{\rm T}^{\rm miss}\geq 40\) GeV & 744(18) & 9.9(3) & 16847 (99) & 2683 (67) & 1148(176) & 1744 (49) \\ Collinear Approx. & 454(14) & 6.2(2) & 1817 (33) & Atlfast & 104(12) & 46(21) & 73(9) \\ N jets \(\geq 2\) & 262( 8) & 5.8(2) & 1722( 32) & 1699(4) & 73( 8) & 14( 6) & 51( 8) \\ Forward jet & 39 (2) & 2.0(1) & 294 (13) & 324(1) & 10( 3) & \(\geq\)1.2(2)\({}^{*}\) & 8( 3) \\ \(b\)-jet veto & 30( 2) & 1.5(1) & 89( 7) & 90.3(9) & 9 (3) & \(\geq\)1.0(2)\({}^{*}\) & 5( 2) \\ Jet kinematics & 2.71(5) & 0.57(5) & 11.8(3)\({}^{*}\) & 26.7(5) & 0.66(3)\({}^{*}\) & 0.19(4)\({}^{*}\) & 0.33(5)\({}^{*}\) \\ Central jet veto & 1.24(3) & 0.43(4) & 1.9(1)\({}^{*}\) & 2.6(1) & 0.27(1)\({}^{*}\) & 0.10(2)\({}^{*}\) & 0.18(4)\({}^{*}\) \\ Mass window & 0.23(1) & 0.04(1) & 0.10(2)\({}^{*}\) & 0.06(2) & 0.058(3)\({}^{*}\) & 0.01(1)\({}^{*}\) & 0.002(1)\({}^{*}\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Summary of the backgrounds for the \(ll\)-channel. An asterisk is used to indicate cross-sections estimated from the cut factorization method. Note that at least one of taus are decayed leptonically in QCD \(Z\rightarrow\tau^{+}\tau^{-}\)+jets process.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline  & \(Z\rightarrow\tau^{+}\tau^{-}\)+jets(\(\geq\)1) & \multicolumn{2}{c|}{\(t\bar{t}\)} & \(Z\to l^{+}l^{-}\)+n jets & \(W\to l\nu\)+n jets & diboson \\  & QCD & ELWK & Full & Fast & (n \(\geq\) 1) & (n \(\geq\) 1) & \(WW/ZZ/WZ\) \\ \hline Cross section (fb) & 168.4\(\times\)10\({}^{3}\) & 1693 & 833\(\times\)10\({}^{3}\) & 768.6\(\times\)10\({}^{3}\) & 8649\(\times\)10\({}^{3}\) & 174.1\(\times\)10\({}^{3}\) \\ Trigger & 51.5(1)\(\times\)10\({}^{3}\) & 230(1) & 209.8(2)\(\times\)10\({}^{3}\) & 633.8(4)\(\times\)10\({}^{3}\) & 4411(9)\(\times\)10\({}^{3}\) & 32.0(1)\(\times\)10\({}^{3}\) \\ Trigger lepton & 42.7(1)\(\times\)10\({}^{3}\) & 190(1) & 179.1(2)\(\times\)10\({}^{3}\) & 588.0(4)\(\times\)10\({}^{3}\) & 3815(9)\(\times\)10\({}^{3}\) & 28.0(1)\(\times\)10\({}^{3}\) \\ Dilepton veto & 38.4(1)\(\times\)10\({}^{3}\) & 171(1) & 156.4(2)\(\times\)10\({}^{3}\) & 216.5(4)\(\times\)10\({}^{3}\) & 3811(9)\(\times\)10\({}^{3}\) & 23.7(1)\(\times\)10\({}^{3}\) \\ Hadronic \(\tau\) & 3062 (42) & 19.3(4) & 5224 (56) & 20250(156) & 32537(1012) & 704( 30) \\ \(E_{\rm T}^{\rm miss}\ \geq\) 30 GeV & 850 (20) & 12.1(3) & 4251 (50) & 468(26) & 21001( 801) & 474( 26) \\ Collinear Approx. & 514 (15) & 7.8(2) & 606 (19) & 17 (3) & 324 (46) & 32 ( 6) \\ Transverse mass & 415 (13) & 6.5(2) & 176 (10) & Atlfast & 11 (2) & 67 (18) & 14 ( 3) \\ N jets \(\geq\) 2 & 235 (7) & 6.0(2) & 162(9) & 167(1) & 8 (1) & 49 (11) & 7 ( 1) \\ Forward jet & 40 (3) & 2.3(1) & 32(4) & 26.1(4) & 1.3(6) & \(\geq\)2.9(3)\({}^{*}\) & 3 ( 1) \\ Jet kinematics & 2.7(1) & 0.72(6) & 1.8(1)\({}^{*}\) & 3.6(1) & 0.10(1)\({}^{*}\) & 0.7(1)\({}^{*}\) & 0.06(1)\({}^{*}\) \\ Central jet veto & 1.2(1) & 0.49(5) & 0.25(4)\({}^{*}\) & 0.43(5) & 0.047(6)\({}^{*}\) & 0.43(6)\({}^{*}\) & 0.02(1)\({}^{*}\) \\ Mass window & 0.11(2) & 0.04(1) & 0.012(5)\({}^{*}\) & 0.03(1) & 0.008(1)\({}^{*}\) & 0.020(6)\({}^{*}\) & 0.001(1)\({}^{*}\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Summary of the backgrounds for the \(lh\)-channel. An asterisk is used to indicate cross-sections estimated from the cut factorization method. Note that at least one of taus are decayed leptonically in QCD \(Z\rightarrow\tau^{+}\tau^{-}\)+jets process.

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline  & \(Z\rightarrow\tau^{+}\tau^{-}\)+jets(\(\geq\)1) & \(t\bar{t}\) & \(W\rightarrow\tau\nu+n jets\) & QCD di-jet \\  & QCD & ELWK & \((n\geq\) 1) & (\(\times\) 5) \\ \hline Cross section (fb) & 40.3\(\times\)10\({}^{3}\) & 1693 & 833 \(\times\)10\({}^{3}\) & 922\(\times\)10\({}^{3}\) & 19.1 10\({}^{12}\) \\ Trigger tau \& MET & 1756(15) & 126(1) & 78177(232) & 39600(400) & \\
2 Hadronic \(\tau\)s & 161(4) & 4.9(2) & 373(16) & 317(33) & 2.756(3) 10\({}^{6*}\) \\ \(E_{\rm T}^{\rm miss}\ \geq\) 40 GeV & 108(4) & 3.7 (2) & 335(15) & 243(29) & 0.97(3) 10\({}^{3}\)* \\ Collinear Approx. & 72(3) & 2.3 (1) & 43(5) & 20(7) & 1.7(2) 10\({}^{2}\)* \\ Di-tau Transverse mass & 72(3) & 2.3(1) & 39(5) & 18(7) & 1.6(2) 10\({}^{2}\)* \\ N jets \(\geq\) 2 & 46(2)* & 2.1(1) & 34(5)* & 8(3)* & 0.86(4) 10\({}^{2}\)* \\ Total \(p_{T}\) & 40(2)* & 1.9(1) & 24(4)* & 8(3)* & 0.75(3) 10\({}^{2}\)* \\ Forward jet & 17(1)* & 1.1(1) & 9(2)* & 3(1)* & 23(3)* \\ Jet kinematics & 1.4(1)* & 0.43(6) & 0.6(2)* & 0.5(4)* & 8(3)* \\ Central jet veto & 0.7(1)* & 0.36(6) & 0.16(9)* & 0.3(3)* & 4(1)* \\ Mass window & 0.08(3)* & 0.03(1) & 0.03(3)* & 0.1(1)* & 1(1)* \\ \hline \hline \end{tabular}
\end{table}
Table 11: Summary of the backgrounds for the \(hh\)-channel. An asterisk is used to indicate cross-sections estimated from the cut factorization method and/or an additional safety factor. Note that both taus are decayed hadronically in QCD \(Z\rightarrow\tau^{+}\tau^{-}\)+jets and \(W\rightarrow\tau\nu\)+jets processes. The \(t\bar{t}\) sample is required to have at least one lepton in the top decay.

Signal sensitivity

### Overview

In this section we outline the method for extracting the signal significance from the data and measuring the Higgs boson mass. In addition to a simple method based on counting the number of events in a mass window, we present a method based on fitting the \(m_{\tau\tau}\) spectrum. Particular care has been given to the incorporation of uncertainty in both the rate and shape for the signal and backgrounds. The fitting strategy constrains the shape of the \(Z\to\tau\tau\) background from the data-driven techniques described in Section 3.2. The data-driven estimates for the \(t\bar{t}\) and \(W\)+jets background are not finalized, thus we rely on Monte Carlo to constrain the shape of those backgrounds and allow for large variation in the shape. In the \(lh\)-channel, the normalization of the fake-tau contribution is constrained from the track multiplicity measurement outlined in Section 3.3. Sections 4.2 and 4.3 describe the parameterization of the signal and backgrounds, while Sections 4.4 and 4.5 describe how the control samples are incorporated to constrain the normalization and shape uncertainties. The expected signal sensitivity is estimated by considering a hypothetical data set given by the median of the signal-plus-background estimate. The normalization of the background predictions are described in Section 3.6. Due to the limited size of the Monte Carlo samples, the shape of the \(m_{\tau\tau}\) spectrum for the \(t\bar{t}\) and \(W\)+jets background was obtained from an earlier point in the event selection, just after the collinear approximation requirement in the \(ll\)-channel and after the transverse mass cut in the \(lh\)-channel (see Sections 2.7 and 2.8). Similarly, the shape of the \(Z\to\tau\tau\) background in the control sample, which can be estimated by the data-driven techniques described in Section 3.2, was obtained from a \(Z\to\tau\tau\) Monte Carlo sample just after the same points in the event selection. Depending on how the analysis evolves, the shape of the \(m_{\tau\tau}\) spectrum from an earlier point in the event selection - which is dominated by backgrounds - may also provide a useful control sample to validate the data-driven background estimation techniques since it is fairly stable in the later stages of the event selection.

For the first time ATLAS has investigated the \(hh\)-channel. While the signal efficiency and \(m_{\tau\tau}\) mass resolution are roughly comparable to the \(ll\)- and \(lh\)-channels, a reliable estimate of the QCD background can only be provided with data. Therefore, we do not report on an estimated sensitivity for the \(hh\)-channel below.

### Shape parameterization for \(H\to\tau\tau\) and \(Z\to\tau\tau\)

The shape of the \(m_{\tau\tau}\) distribution for signal and \(Z\to\tau\tau\) events is dictated by the resolution of \(E_{\mathrm{T}}^{\mathrm{miss}}\) and the kinematics of the collinear approximation. As discussed in Section 2.6, the width of the \(m_{\tau\tau}\) distributions is given by the \(E_{\mathrm{T}}^{\mathrm{miss}}\) resolution scaled by a Jacobian factor and the cuts on the \(x_{\tau}\) variables introduce an asymmetry in the \(m_{\tau\tau}\) distribution. A full solution would include an event-by-event Jacobian scaling of the width and truncation at \(m_{\tau\tau}\geq m_{vis}/\sqrt{x_{1}^{cut}x_{2}^{cut}}\). An approximate parameterization can be constructed with these features in mind. First, we consider three sub-samples of events with \(J\leq 2\), \(2<J\leq 5\), and \(J>5\). Let, \(\langle J\rangle_{i}\) and \(N_{i}\) denote the mean of the Jacobian and the number of events in the \(\bar{t}^{h}\) subsample, respectively. We account for the Jacobian scaling of the width by using a triple Gaussian with identical mean, normalizations according to the ratios of the \(N_{i}\) and widths according to the ratios of \(\langle J\rangle_{i}\). To account for the asymmetry introduced by the \(x_{\tau}\) cuts, we modulate the triple Gaussian by an efficiency envelope derived from the \(m_{vis}\) spectrum. The efficiency envelope reflects the probability that \(m_{\tau\tau}\) is greater than \(m_{\mathrm{vis}}\) and is parameterized as: \(\nicefrac{{1}}{{2}}+\nicefrac{{1}}{{2}}\mathrm{erf}\{[m_{\tau\tau}-\langle m _{vis}\rangle]/\sqrt{2}\sigma_{vis}\}\), where \(\langle m_{vis}\rangle\) and \(\sigma_{vis}\) are the mean and standard deviation of the spectrum for those events which fail the \(x_{\tau}\) cuts. The parameter \(\langle m_{vis}\rangle\) depends on the Higgs boson mass and was linearly parameterized by \(0.576\,m_{H}+60\,\) GeV; the width \(\sigma_{vis}\) was fixed at \(10\,\) GeV. We parametrize the \(m_{\tau\tau}\) distribution as

\[L_{H/Z}(m_{\tau\tau}|m,\sigma_{H/Z})={\cal N}\left[\frac{1}{2}+\frac{1}{2}\,{ \rm erf}\left(\frac{m_{\tau\tau}-\langle m_{vis}\rangle}{\sqrt{2}\sigma_{vis} }\right)\right]\times\sum_{i=1}^{3}N_{i}\;Gaus(m_{\tau\tau}|m,\sigma_{H/Z} \langle J\rangle_{i}), \tag{11}\]

where the resulting function's normalization constant \({\cal N}\) was found by numerical integration with the RooFit package [37].

The \(Z\to\tau\tau\) control sample described in Section 3.2 is used to constrain the mean, \(m_{Z}\), and the overall width of the distribution, \(\sigma_{Z}\), which are the only free parameters in the \(Z\to\tau\tau\) background model. The error bars in the control sample were scaled by 10% to account for the 10% shape uncertainty in the \(\mu\to\tau\) rescaling method and the extrapolation from the control region to the final signal region. Figure 13 shows the result of the fit to the \(Z\to\tau\tau\) and Higgs boson signal in the \(lh\)-channel.

An alternate parameterization was also considered. This parameterization was also composed of three components, but the asymmetry in the shape was modeled with a "bifurcated" or "dimidated" Gaussian [38], in which the width of the lower half was smaller than the width of the upper half of the distribution. This alternate parameterization did not fix the ratio of the widths and normalizations for the three components based on the distribution of the Jacobian; instead, these parameters were themselves parameterized as a piece-wise linear function of \(m_{H}\). Each of the three components required four parameters to model the widths, together with two parameters for normalization constants, and two parameters for the linearity in the Higgs boson mass. In total the signal model was represented by 16 parameters. Results from these two parameterizations were in good agreement.

### Shape parameterization for \(t\bar{t}\) and \(W\)+jets

In contrast to the irreducible \(Z\to\tau\tau\)+jets background, the \(W\)+jets background is dominated by situations in which one of the tau decay products comes from a \(W\) decay and the second tau decay product is a fake from a jet. The \(t\bar{t}\) background is even more complicated because the decay products from top contain a real tau contribution as well as a fake tau contribution. It is difficult to estimate this background using Monte Carlo because one must understand in detail both the jet kinematics for as well as the lepton or \(\tau\) fake rate as a function of \(p_{T}\) and \(\eta\). Instead, it is desirable to estimate this background with data. Since the

Figure 13: Figures (a) and (b) show the result of a fit to a pure Monte Carlo samples of \(Z\to\tau\tau\) and signal (\(m_{H}=120\) GeV) in the \(lh\)-channel, respectively. The dashed lines represent the three components of the model and the dotted curve represents the erf() efficiency envelope. These samples do not include pileup.

data-driven strategy for the \(W\)+jets background is under development, we simply rely on Monte Carlo to provide a control sample for this background. Figure 14(a) shows the shape of the fully simulated \(W\)+jets background and the fast simulation \(t\bar{t}\) background sample after the transverse mass cut. The shapes are consistent within statistical errors. While the shapes from those backgrounds remain stable through the final stages of the event selection, a conservative 50% error is applied to each bin in the combined \(t\bar{t}\) and \(W\)+jets control sample to reflect uncertainty in how this shape changes as the remainder of the analysis cuts are applied.

The shape of the QCD background was parameterized with the following equation

\[L_{QCD}(m_{\tau\tau}|a_{1},a_{2},a_{3})=\mathcal{N}\left(\frac{1}{m_{\tau\tau} +a_{1}}\right)^{a_{2}}\,m_{\tau\tau}^{a_{3}}\quad. \tag{12}\]

The form is motivated by a competition between the parton distribution functions and the matrix element. In the \(lh\)-channel, the normalization of the backgrounds with fake taus can be constrained by using the track multiplicity constraint described in Section 3.3; however, there is an additional uncertainty associated with how well the fake fraction can be extrapolated from the control sample to the signal like region. We apply a conservative 50% systematic on this fraction associated with the extrapolation. Figure 14(b) and (c) show the result of the simultaneous fit to the fake tau background described in the Section 4.5 with (solid) and without (dashed) the signal contribution for the \(ll\)- and \(lh\)-channel, respectively. The variation reflects the magnitude of the shape uncertainty.

### Signal significance neglecting shape uncertainty

For a given hypothesized Higgs boson mass, \(m_{H}\), the mass window has been defined as \(m_{H}-15\ \ \mathrm{GeV}\leq m_{\tau\tau}\leq m_{H}+15\ \ \mathrm{GeV}\). A simple approach to estimating the expected significance of the signal is to count events in this range and calculate the probability for at least this many events from the background-only prediction. An alternate approach is to fit the \(m_{\tau\tau}\) spectrum and use the resulting signal yield as a test statistic. Table 12 shows the significance obtained from number counting assuming a 10% background uncertainty as was done in Ref. [4] and the result from the fitted signal yield. The next subsection provides a final result indicating a more realistic treatment of both normalization and shape uncertainties.

Figure 14: Figure (a) shows that the shapes are similar for these backgrounds and that the shape is stable in the final stages of the cut flow. The \(m_{\tau\tau}\) spectrum for \(t\bar{t}\) and \(W\)+jets backgrounds after all cuts for the \(ll\)-channel (b) and \(lh\)-channel (c) with a fit to the spectrum. The solid and dashed curves show the result of the simultaneous fit to the control sample and signal candidates with and without the signal contribution, respectively.

### Incorporating control samples and shape uncertainty

By fitting the \(m_{\tau\tau}\) spectrum to a model that accurately describes the signal and various backgrounds it is possible to directly incorporate uncertainty in the background shape and take advantage of the shape of the signal within the mass window. In order to constrain the background rate and shape, we simultaneously fit the signal candidates and the background control samples outlined in Section 3. The fit is performed twice, once letting the signal parameters float (the maximum likelihood estimates denoted with a single \(\hat{\phantom{0}}\)) and once constraining the signal normalization to be zero (the conditional maximum likelihood estimates denoted with a double 8). The ratio of these likelihoods is referred to as the profile likelihood ratio, \(\lambda\),

Footnote 8: When constraining \(\mu\geq 0\), the distribution for the background-only hypothesis is modified such that \(-2\log\lambda(\mu=0)\sim 1/2\delta(0)+1/2\chi_{1}^{2}\), and this is taken into account in computing the p-value.

\[\lambda(\mu=0)=\frac{L(data|\mu,\hat{\underline{b}}(\mu),\hat{\underline{v}}( \mu))}{L(data|\hat{\mu},\hat{b},\hat{\nu})}\quad, \tag{13}\]

where \(\mu\) represents the signal strength in units of the Standard Model expectation and \(\nu\) represents the nuisance parameters needed to describe the shape. If the Higgs boson mass is specified, the distribution of \(-2\log\lambda\) ratio asymptotically approaches a \(\chi^{2}\) distribution with the number of degrees of freedom given by the number of parameters of interest [5]. The motivation for \(\mu\) is that it enforces the relationship of the Standard Model branching ratios when combining the individual channels, maintaining the property that the distribution of \(-2\log\lambda\) is \(\chi^{2}\) with one degree of freedom. This improves the power compared to a method which lets the signal in each channel vary independently. If the Higgs boson mass is not fixed, then one must take into account the "look-elsewhere" effect, which is discussed in more detail in Section 6. The likelihood function used in the simultaneous fit is simply a product of the likelihoods from the individual measurements:

\[L(data|\mu,m_{H},\nu) = L_{track}(\text{track multiplicity}|r_{QCD})\] \[\times L_{Z}(\text{Z}+\text{jets control}|m_{Z},\sigma_{Z})\] \[\times L_{QCD}(\text{QCD control}|a_{1},a_{2},a_{3})\] \[\times L_{s+b}(\text{signal candidates}|\mu,m_{H},\sigma_{H},m_{Z}, \sigma_{Z},r_{QCD},a_{1},a_{2},a_{3}),\]

where the \(a_{i}\) are the parameters used to parameterize the fake-tau background and \(\nu\) represents all nuisance parameters of the model: \(\sigma_{H},m_{Z},\sigma_{Z},r_{QCD},a_{1},a_{2},a_{3}\). When using the alternate parameterization

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline  & \multicolumn{2}{c|}{_ll_-channel} & \multicolumn{2}{c|}{_lh_-channel} & \multicolumn{2}{c}{combined} \\ \(m_{H}\) & Counting & Fitted Yield & Counting & Fitted Yield & Counting & Fitted Yield \\ \hline
105 & 2.20 & 2.43 & 2.85 & 3.46 & 3.80 & 4.17 \\
110 & 2.46 & 2.88 & 3.45 & 4.19 & 4.46 & 5.06 \\
115 & 2.86 & 3.26 & 4.18 & 4.96 & 5.32 & 5.96 \\
120 & 2.80 & 3.17 & 4.23 & 4.73 & 5.36 & 5.72 \\
125 & 2.67 & 2.96 & 3.97 & 4.32 & 5.08 & 5.28 \\
130 & 2.42 & 2.73 & 3.54 & 3.88 & 4.62 & 4.77 \\
135 & 2.17 & 2.37 & 3.38 & 3.60 & 4.35 & 4.25 \\
140 & 1.74 & 2.00 & 2.66 & 2.83 & 3.55 & 3.35 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Expected signal significance for several masses based on number counting in a mass window with 30 fb\({}^{-1}\) of data. Results are shown neglecting uncertainty in the background rate and incorporating it with two methods (see text). These results do not include the impact of pileup, which is discussed in Section 4.7.

Figure 15: Example fits to a data sample with the signal-plus-background (a,c) and background only (b,d) models for the \(lh\)- and \(ll\)-channels at \(m_{H}=120\,\)GeV with 30 fb\({}^{-1}\) of data. Not shown are the control samples that were fit simultaneously to constrain the background shape. The fits are performed to the signal and background expectation (histograms), while the overlaid data with error bars are only indicative of a possible data set. These samples do not include pileup.

of the signal, the exact form of Equation 14 is modified to coincide with parameters of that model.

Figure 15 shows the fit to the signal candidates for \(m_{H}=120\)  GeV with (a,c) and without (b,d) the signal contribution. It can be seen that the background shapes and normalizations are trying to accommodate the excess near \(m_{\tau\tau}=120\)  GeV, but the control samples are constraining the variation. Table 13 shows the significance calculated from the profile likelihood ratio for the \(ll\)-channel, the \(lh\)-channel, and the combined fit for various Higgs boson masses with 30 fb\({}^{-1}\) of data. Finally, we present the expected significance as a function of Higgs boson mass in Fig.16.

### Mass determination

The mass parameter \(m_{H}\) and its error can be determined from the fits described above; however, the parameter in the model may not be the best estimate of the physical Higgs boson mass. Similarly, the error on the mass parameter from the fit should be validated with a large number of pseudo-experiments. Figure 17 (a) shows the relationship of the input Higgs boson mass and the reconstructed Higgs boson mass (i.e. the parameter \(m_{H}\) in Equation 11) obtained with 2000 pseudo-experiments per input mass point. Figure 17 (b) shows the \(m_{\tau\tau}\) resolution for the signal as a function of the input Higgs boson mass. When scaling the deviation by the minos errors, the pull distribution of \(m_{H}\) was found to be consistent with the normal distribution \(N(0,1)\). The mass resolution is found to be in the range of 8\(\sim\)10  GeV. A similar mass resolution was found in the \(hh\)-channel when analyzing signal Monte Carlo samples.

### Influence of pileup

The presence of pileup has three major effects on the analysis. First, additional \(p\)-\(p\) interactions can produce hadronic activity in the central region which causes events to fail the central jet veto. Secondly, the presence of pileup generally degrades the \(E_{\rm T}^{\rm miss}\) resolution, which, in turn, reduces the efficiency of the collinear approximation cuts and degrades the \(m_{\tau\tau}\) resolution. Thirdly, pileup degrades the hadronic

\begin{table}
\begin{tabular}{c c c c} \hline \hline \(m_{H}\) & \(ll\)-channel & \(lh\)-channel & combined \\ \hline
105 & 1.95 & 2.41 & 3.10 \\
110 & 2.44 & 3.35 & 4.15 \\
115 & 2.98 & 4.07 & 5.04 \\
120 & 2.92 & 3.87 & 4.85 \\
125 & 2.75 & 3.75 & 4.65 \\
130 & 2.46 & 3.38 & 4.18 \\
135 & 2.21 & 3.32 & 3.99 \\
140 & 1.80 & 2.70 & 3.24 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Expected signal significance for several masses based on fitting the \(m_{\tau\tau}\) spectrum with 30 fb\({}^{-1}\) of data. Background uncertainties are incorporated by utilizing the profile likelihood ratio. These results do not include the impact of pileup.

Figure 16: Expected signal significance for several masses based on fitting the \(m_{\tau\tau}\) spectrum. Background uncertainties are incorporated by utilizing the profile likelihood ratio. These results do not include the impact of pileup.

tau identification. Fortunately, the electron and muon identification have been shown to be quite robust against pileup [15, 16]. While the jet performance is affected by pileup, the analysis is fairly robust against those effects.

The simulation of pileup is technically very challenging since it is performed at a very low level in the detector simulation. Limited samples with pileup were available at the time of writing, and the \(E_{\mathrm{T}}^{\mathrm{miss}}\) and hadronic tau identification algorithms were not re-tuned in this context. The distribution of the log likelihood ratio discriminant for the calorimeter-based hadronic tau identification algorithm is shifted to lower values for both real taus and jet fakes. By simply adjusting the cut on the log likelihood ratio to 0, the same signal efficiency can be maintained with approximately a 50% drop in jet rejection. By re-tuning the discriminant in the context of pileup, improved jet rejection should be possible. In all three channels, the mass resolution is degraded from \(\sim\)9.5 to \(\sim 11.5\)  GeV for \(m_{H}=120\)  GeV due to the degradation of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) resolution. Figure 8 shows that the central jet veto survival probability drops from \(\sim\)88% to \(\sim\)75% at \(10^{33}\) cm\({}^{-2}\) s\({}^{-1}\) and \(\sim\)65% at \(2\times 10^{33}\) cm\({}^{-2}\) s\({}^{-1}\). Studies indicate that the use of tracking and calorimeter timing information can be used to mitigate this loss in signal efficiency.

Given the lack of background samples simulated with pileup and the need to re-optimize the reconstruction and analysis in that context, we do not report signal significance estimates.

## 5 Systematic uncertainties

### Overview

The data-driven background estimation methods described above have been developed so that uncertainty in the background shape and normalization are included directly into the significance calculation. Because the discovery criterion is simply testing the presence or absence of the signal, it is not sensitive to some of the sources of systematic uncertainty. In contrast, measurement of the Higgs boson mass is sensitive to the energy scale of electrons, muons, hadronic taus and \(E_{\mathrm{T}}^{\mathrm{miss}}\). Furthermore, measurement and exclusion of \(\sigma(pp\to qqH)\times BR(H\to\tau\tau)\) are sensitive to the uncertainty on the signal selection efficiency. Below we discuss the impact of these systematics on the analysis.

Figure 17: The linearity of the fitted mass versus the input mass (a) and the mass resolution versus the input mass (b). These results do not include the impact of pileup, which is discussed in Section 4.7.

### Systematic mis-measurement of the signal

First, we consider the purely experimental sources of systematics. The approach here is to assume that once we have data, the estimates of the energy scales, resolutions, and efficiencies might be systematically biased. Estimates of the systematic uncertainty from various sources are given in Table 14. The uncertainty estimates are common for all this series of notes, except for the uncertainties on the central jet veto and forward jet tagging efficiency. There is not yet a dedicated study of the expected uncertainty on these efficiencies from data, thus we assume uncertainty on the reconstruction efficiency to be half the tau identification efficiency and point out that most effects relevant to the jets have already been included in the jet energy scale uncertainty. We use the nominal detector performance as the central value and then manipulate the Monte Carlo signal to reflect these changes. For instance, in the case of the electron energy scale uncertainty, we coherently change all electrons to have 0.5% higher \(E_{T}\), modify the \(E_{T}^{\rm miss}\) vector accordingly, and recalculate the signal efficiency. This is done individually for each source of systematic and upward and downward fluctuations are treated separately. In the case of the jet energy scale, only some elements of the uncertainty are relevant for \(E_{T}^{\rm miss}\). A study of \(E_{T}^{\rm miss}\) projected onto the direction of the reconstructed \(Z\) in \(Z\to ll\)+jets with a subset of the analysis cuts indicated that the \(E_{T}^{\rm miss}\) scale can be measured within 5%; thus we only manipulate the \(E_{T}^{\rm miss}\) vector according to a 5% jet energy scale shift. In the case of systematic uncertainty on resolution, we only considered a degradation in the resolution by the tabulated amount. Finally, for systematics on reconstruction and identification efficiency we assume a 1-to-1 transfer to the uncertainty on the signal efficiency and include a factor of two when the signal efficiency scales as the square (e.g. the electron efficiency in the \(ll\)-channel). Table 14 summarizes the effect of systematic mis-measurement on the signal efficiency.

\begin{table}
\begin{tabular}{l|c|c} \hline \hline Source & Relative uncertainty & Effect on signal efficiency \\ \hline luminosity & \(\pm\)3\% & \(\pm\) 3\% \\ muon energy scale & \(\pm\) 1\% & \(\pm\) 1\% \\ muon energy resolution & \(\sigma(p_{T})\oplus 0.011p_{T}\oplus 1.7\ 10^{-4}p_{T}^{2}\) & \(\pm\) 0.5\% \\ muon ID efficiency & \(\pm\) 1 \% & \(\pm\) 2\% \\ electron energy scale & \(\pm\) 0.5\% & \(\pm\) 0.4 \% \\ electron energy resolution & \(\sigma(E_{T})\oplus 7.3\ 10^{-3}E_{T}\) & \(\pm\) 0.3 \% \\ electron ID efficiency & \(\pm\) 0.2\% & \(\pm\) 0.4\% \\ tau energy scale & \(\pm\) 5\% & \(\pm\) 4.9\% \\ tau energy resolution & \(\sigma(E)\oplus 0.45\sqrt{E}\) & \(\pm\) 1.5\% \\ tau ID efficiency & \(\pm\) 5\% & \(\pm\) 5\% \\  & \(\pm\) 7\% (\(|\eta|\leq 3.2\)) & \\ jet energy scale\({}^{\dagger}\) & \(\pm\) 15\% (\(|\eta|\geq 3.2\)) & \({}^{+16\%}/_{-20\%}\) \\  & \(\pm\) 5\% (on \(E_{T}^{\rm miss}\) ) & \\ jet energy resolution & \(\sigma(E)\oplus 0.45\sqrt{E}\) (\(|\eta|\leq 3.2\)) & \\  & \(\sigma(E)\oplus 0.67\sqrt{E}\) (\(|\eta|\geq 3.2\)) & \(\pm\) 1\% \\ b-tagging efficiency & \(\pm\) 5\% & \(\pm\) 5\% \\ forward tagging efficiency & \(\pm\) 2 \% & \(\pm\) 2\% \\ central jet reconstruction efficiency & \(\pm\) 2 \% & \(\pm\) 2\% \\ \hline total summed in quadrature & & \(\pm\)20\% \\ \hline \hline \end{tabular}
\end{table}
Table 14: Estimated scale of systematic mis-measurements and their effect on the signal efficiency. \({}^{\dagger}\) When varying the jet energy scale, only a 5% mis-measurement of the jet energy was used in manipulating the \(E_{T}^{\rm miss}\) vector. See text for details.

### Theoretical uncertainty

In addition to the effect of systematic mis-measurement on the signal efficiency, theoretical uncertainties also limit our ability to estimate the signal efficiency. Next-to-leading order QCD calculations are now available for the vector boson fusion process. A dedicated study [39] investigated the overall renormalization and factorization scale dependence (2%) as well as the parton distribution function (PDF) uncertainties (3.5%). Next-to-leading order electroweak corrections are also quite large for the vector boson fusion process, giving a 3% uncertainty for the full next-to-leading order calculation [40]. Recently, the dominant next-to-leading order QCD corrections to the Higgs boson plus three jets have been calculated for vector boson fusion, providing a scale uncertainty on the parton-level central jet veto survival probability of 1% [41].

While the parton-level theoretical uncertainties are under very good control and below the level of both the statistical error and measurement-related systematics, the same is not true for the theoretical uncertainty related to the parton-shower and underlying-event. We rely on Monte Carlo simulations that model the parton-shower, hadronization, and underlying event to simulate the detector response. The uncertainty in these calculations is not comparable to the accuracy of the parton-level predictions. The central jet veto efficiency was studied with the signal process generated with PYTHIA (with various tunings), HERWIG  and SHERPA and the fast detector simulation. After the analysis cuts, the different generators differ by 41%. Studies focusing specifically on the matrix element-parton shower matching indicate a substantially smaller uncertainty [33, 42]. We will measure the underlying event [43, 44] and tune the parton shower and hadronization with data, but it is likely that this contribution of the uncertainty will remain significant. Currently there is no estimate of the expected uncertainty related to the parton-shower, hadronization, and underlying event tuning. Clearly, this is an area that deserves attention as such a large uncertainty will hinder exclusions if a Higgs boson does not exist in this mass range and cross-section and coupling measurements if one does. After discussions with the authors of PYTHIA, HERWIG  and SHERPA we feel that the residual uncertainty in the parton shower after tuning to the data will be less than the 18% uncertainty quoted for the jet energy scale. Thus, the uncertainty in the signal efficiency will be dominated by the jet energy / \(E_{\mathrm{T}}^{\mathrm{miss}}\)  scale uncertainty and the precise uncertainty in the parton shower is not relevant. Table 15 summarizes the theoretical uncertainties for the signal production.

## 6 Discussion

The expected signal significances in Table 13 are qualitatively consistent with the results found in Ref. [4]; however, the predicted cross-sections in Tables 9 and 10 are significantly different. In particular, the initial cross-section of the \(Z\)+jets background is smaller by nearly a factor of four and the \(t\bar{t}\) background in the \(ll\)-channel is larger by nearly a factor of 2. Much of this difference reflects the evolution

\begin{table}
\begin{tabular}{l|c|c} \hline \hline Source & Relative uncertainty & Effect on signal efficiency \\ \hline PDF uncertanties & \(\pm\)3.5\% & \(\pm\)3.5\% \\ scale dependence on cross-section & \(\pm\)3\% & \(\pm\) 3\% \\ scale dependence CJV efficiency & \(\pm\) 1\% & \(\pm\) 1\% \\ parton-shower and underlying event & \(\pm\)\(\leq\)10\% & \(\pm\)\(<\)10\% \\ \hline total summed in quadrature & & \(\pm\)\(<\) 10\% \\ \hline \hline \end{tabular}
\end{table}
Table 15: Theoretical uncertainties which affect the estimation of the signal efficiency.

in the Monte Carlo generators for these challenging backgrounds. In the case of the \(Z\)+jets background, differences in the choice of renormalization and factorization scales and parton density functions are the source for part of the discrepancy; however, approximately a factor of two comes from the treatment of soft and collinear divergences. After substantial investigation, we concluded that the \(Z\)+jets background estimate used in Ref. [4] was conservative. In the case of \(t\bar{t}\)+jets, the necessary matching between matrix elements and parton showers had not been developed when Ref. [4] was written. The recipe used to merge the \(t\bar{t}\)+0,1,2 jet samples at that time and the more realistic \(b\)-tagging performance limits our ability to understand in detail the source of the differences. We are confident that MC@NLO and our current detector simulation and offline reconstruction provide a superior prediction of this background.

For the first time, ATLAS has investigated the potential of the \(hh\)-channel. Much of this work has been devoted to the development and study of tau and missing \(E_{T}\) triggers. It now appears that the trigger is feasible, and the reconstruction of the signal maintains an efficiency and \(m_{\tau\tau}\) mass resolution comparable to the \(ll\)- and \(lh\)-channels. The open question for this channel is the size of the QCD background - a question that can only be answered with data.

The results shown in Section 4 are based on a fixed mass hypothesis. If one leaves the Higgs boson mass a free parameter in the likelihood fits, then one must take into account the "look-elsewhere" effect. Naively, one would expect the magnitude of the effect to be rather small for this channel given the \(\sim\)10 GeV mass resolution and the \(\sim 30\) GeV mass range of interest. Detailed study shows that it is a mixture of two effects. First, the distribution of \(-2\log\lambda(\mu=0)\) is not \(\chi^{2}\)-distributed under the background-only hypothesis; it has a longer tail which raises the p-value. Secondly, the median of \(-2\log\lambda(\mu=0)\) under the signal-plus-background hypothesis is systematically larger in the floating mass case because the model can adapt to fluctuations in the signal mean. Figure 19 summarizes the impact of these two effects.

Future work for this channel includes an extensive study of performance in the context of pileup, including optimization of the hadronic tau identification and \(E_{\rm T}^{\rm miss}\) reconstruction. The data-driven background estimation for the \(t\bar{t}\) and \(W\)+jets backgrounds must be further developed. In order to measure \(\sigma(pp\to qqH)\times BR(H\rightarrow\tau\tau)\) at the desired level, the systematics associated with the signal efficiency must be reduced. This requires a technique to measure the central jet veto and forward jet tagging efficiency in data. Additionally, the use of the \(E_{\rm T}^{\rm miss}\) projection method should be further developed to mitigate the impact of the jet energy scale uncertainty.

While the results shown in Section 4 only include the contribution of Higgs bosons produced via vector boson fusion, an additional 10% contribution from the gluon-fusion production process could be expected at \(m_{H}=120\) GeV. Finally, the theoretical uncertainty associated with the parton shower and underlying event, which currently gives the largest uncertainty on the signal efficiency, must be addressed. The tuning of the parton shower, hadronization, and underlying event model will be among the earliest measurements at the LHC. If the residual theoretical uncertainty is not reduced sufficiently, a different strategy may need to be found for the central jet veto. The expected exclusion power based on the uncertainty in the signal efficiency is shown in Fig. 19.

## 7 Summary and conclusion

The sensitivity of the ATLAS detector to a Standard Model Higgs boson produced via vector boson fusion with subsequent decay into taus has been investigated with state-of-the-art Monte Carlo generators, a full GEANT-based simulation of the ATLAS detector and our current trigger and reconstruction algorithms. Particular emphasis has been placed on data-driven background estimation strategies and the estimation of the associated uncertainties in normalization and shape. The impact of pileup has not been fully addressed; however, results without pileup indicate that a \(\sim 5\sigma\) significance can be achieved for a Higgs boson mass in the range 115 - 125 GeV after collecting 30 fb\({}^{-1}\) of data and combining the \(ll\)- and \(lh\)-channels. The Higgs boson mass resolution is approximately 10 GeV, leading to approximately 3.5% mass measurement. The \(hh\)-channel has also been investigated and gives similar results for signal and non-QCD backgrounds as the other channels; however due to the challenge of predicting the QCD background, we do not report on an estimated sensitivity for that channel. Currently, measurement-related systematics and large theoretical uncertainties limit the ability for a measurement of the product of cross-section and branching ratio. Future work is needed to constrain these uncertainties in order to measure the Higgs boson couplings, spin and CP properties.

## References

* [1] The ATLAS Collaboration, "Detector and physics performance technical design report (Volume ii)", CERN-LHCC/99-15 (1999).
* [2] LEP Higgs Working Group, "Search for the Standard Model Higgs boson at LEP", _Phys. Lett._**B 565** (2003) 61.
* [3] The LEP Electroweak Working Group, "A combination of preliminary electroweak measurements and constraints on the Standard Model", hep-ex/0511027.
* [4] S. Asai et al., "Prospects for the search for a Standard Model Higgs boson in ATLAS using vector boson fusion", _Eur. Phys. J._**C 32S2** (2004) 19.
* [5] D. L. Rainwater et al., "Searching for \(H\rightarrow\tau\tau\) in weak boson fusion at the LHC", _Phys. Rev._**D 59** (1999) 014037.