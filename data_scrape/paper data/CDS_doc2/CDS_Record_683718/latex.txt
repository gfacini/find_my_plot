# Emulation of the Sequential Option of the ATLAS Trigger and Event Builder

using the ATM Local Area Network of the RCNP Institute

D. Calvet\({}^{1}\), M. Nomachi, H. Togawa

Research Center for Nuclear Physics, Osaka University, 10-1 Mihogaoka, Ibaraki 567, Japan

J.M. Conte, I. Mandjavidze

DAPNIA/SEI, CEA Saclay 91191 Gif-sur-Yvette Cedex, France

Y. Nagasaka

Nagasaki Institute of Applied Science, 536 Aba-machi, Nagasaki 851-01, Japan

###### Abstract

The computer network at the Research Center for Nuclear Physics (RCNP), Osaka University, has been rebuilt recently using state-of-the-art multi-processor servers and workstations linked by an ATM local area network. This system has been used to emulate a down-sized version of a typical Trigger/Data Acquisition system and event builder as foreseen in future high energy physics experiments. The testbed comprises up to 26 computers representing a total of 60 processors interconnected via the ATM network of the laboratory. We describe the system and measure the real-time performance of the various components. We present a comparison of ATM hardware and software from different vendors. The performance of the testbed is presented and the issues that have been identified are discussed in view of building larger systems.

## I Introduction

Future experiments like those in preparation at the CERN Large Hadron Collider put challenging demands in terms of networking bandwidth and computing power for the selection in real time of physics events out of the dominant background [1, 2, 3]. High level triggers and DAQ of the ATLAS experiment will consist of \(\sim\)1600 detector read-out memories connected to a comparable number of processors. The total effective aggregate bandwidth requirement is over 20 Gbit/s and the number of nodes to inter-connect is about 1000.

The first investigations made by the RD-31 collaboration on the use of ATM technology for T/DAQ applications showed promising results [4] and the research effort has been pursued. A small demonstrator of an ATM based T/DAQ for ATLAS has been reported in [5]. Because this demonstrator included only 10 nodes compared to the one or two thousands that the final system will count, the extrapolation of results is not an easy exercise. Although modeling and simulation are the only means to predict the performance of the full-size system before it is actually built, the operation of larger prototypes have to be shown. Demonstrators with few tens of nodes are foreseen, but a cost effective and complementary approach is the use of an existing network to emulate the system on standard computers. Because local area networking at RCNP is done in ATM, the idea to use the laboratory network and computers for T/DAQ emulation has emerged in [6] and a project was established. This paper presents the results of these investigations.

## II The Computer Network at RCNP

To satisfy the needs in terms of bandwidth and computing power at RCNP, a powerful computer complex and ATM backbone based on Digital technology has been deployed. When the decision to renovate the laboratory computers and network was taken, Digital was selected because it offered a complete, uniform and ready-to-use solution: powerful multi-processor servers and storage devices, state-of-the-art workstations, leading edge ATM networking, complete software, global installation and support. Most of the equipment is rented from DEC and the maintenance of the system is assured by several on-site engineers under the responsibility of two system administrators from RCNP.

A simplified view of the computer system installed at RCNP is presented in Figure 1. Two Alpha servers 8400 (each with 12 CPUs @437 MHz and 4 GB of memory) are used as central file servers. They are connected to an automated tape library that provides up to 10.3 TB of data storage. Four Alpha servers 4100 (each with 4 CPUs @ 400 MHz and 1 GB of memory) are available with a tape library providing 2 TB of data storage. These servers are used for various tasks such as data post-processing for the experiments running on the cyclotron facility of RCNP. All servers are linked by a Memory Channel hub (not

Figure 1: Computer and network system at RCNP.

shown) and are connected to the main ATM core switch via 155 Mbit/s links. Upgrading to 622 Mbit/s links will be done very soon. Four DEC ATM GigaSwitch are deployed in the institute. Each of them has a maximum switching capacity of 10.4 Gbit/s when all slots are populated. The main switch connects all servers, a certain number of Alpha workstations (mail server, web server, and 5-6 machines for general use), the campus network and the university super-computer. It is also connected to bridges to Ethernet devices (modems, PCs and terminals) and to other ATM switches located outside of the main computer room. All inter-switch connections are 622 Mbit/s ATM optical links. Two switches are close to the accelerator ring with four Alpha workstations attached to each of them by 155 Mbit/s ATM links. These workstations are normally used for the experiments. The fourth switch is located in the main building of the institute. It provides a bridge to several tens of Ethernet devices and brings ATM to the desktop in the library, the conference room, the director's office and on a few more Alpha workstations as well as PCs equipped with ATM interfaces. The total number of machines attached directly to the ATM network is ~42, while the total number of Ethernet devices is ~300. The use of LAN Emulation makes the presence of ATM transparent for most users.

The system is in operation since 1996 and is used by the laboratory staff, students and visitors for the usual daily scientific and office work.

## III Network Based T/DAQ

Several architectures for the second level trigger have been proposed for ATLAS [7] and we will focus on the "sequential option". Although this work is intended for ATLAS, the basic concepts can be applied to other experiments that have similar needs, like Phenix [8].

### _Description of the T/DAQ model_

The model of a T/DAQ system that we investigate is depicted in Figure 2. It includes four main components linked by a common network: the _sources_, _destinations_, _supervisor_ and _monitor_.

The _sources_ are the elements that provide the necessary buffering for the event data coming from the detector before interfacing to the rest of the system. Because we consider that only a small fraction of the raw data will be sent through the network, a concentration of several detector read-out links can be made via a backplane bus (e.g. PCI) to best match the capacity of a single network link.

The _destinations_ are the elements that execute the event selection algorithms by processing the detector data coming from the sources. The destinations could be single or multi-processor machines or even clusters of processors. A link to data storage devices is attached to all or part of the processors.

The _supervisor_ makes the interface with the first level of triggering for the distribution of information on accepted events to the destination processors.

The _monitor_ is in charge of the run control; it provides the human interface for interaction with operators. Means for system configuration, global monitoring, etc are not shown.

### _Principle of operation_

After an event has been allocated by the supervisor to one of the destinations, the processor in charge of the selection of the event controls the entire flow of data from the sources. It performs the selection algorithm by a series of steps, requesting from the sources the data it needs to complete the current processing step. Algorithm steps are designed to reject uninteresting events as soon as possible to save network bandwidth and computing resources. This enables more sophisticated algorithms to run on the remaining events and makes an optimal use of the given global amount of resources.

## IV Software for T/DAQ Emulation

The software running on each node was originally developed for the small demonstrator reported in [5]. This software is designed to maximize the flexibility of operation and provides portability across platforms, operating systems and networking technologies. These goals are achieved by organizing the software both in modules and layers with a clearly defined Application Programming Interface (API) between them. Libraries are extensively used to make the software modular. For example, the application can run over native AAL5 ATM or UDP/IP (Ethernet or others) by linking the application with the appropriate library. The operating system abstraction layer allows the application to run under WindowsNT, LynxOS or Digital Unix without any modification.

The software implements all the functionality for the sequential transfer and processing of event data from the sources to the processors; means for basic error detection and recovery, monitoring and on-line statistics gathering. However, many functionality that make a real T/DAQ system operational are outside of the scope of this demonstrator: configuration database, error reporting system, integration with a real detector and back-end software, physics performance monitoring, detector calibration, interface to mass storage, etc. These aspects are covered in the "DAQ Prototype -1" project [9]. In contrast, the present demonstrator is not designed to be a functional T/DAQ system but is dedicated to architecture and protocol studies, performance measurements and investigations of network behavior under various traffic patterns. The system can operate in one of the four following modes:

* "event selection only" All processors are assigned to the task of event selection and

Figure 2: Model of a network based T/DAQ.

perform the same type of algorithms with sequential data transfer and processing. This corresponds to the Level-2 trigger of ATLAS.

\(\bullet\) "event builder only"

In this mode, the complete event data from all sources is gathered in the destinations at once. This corresponds to the Level-3 trigger and data acquisition in ATLAS.

\(\bullet\) "event selection and filter - split farms"

Two types of destinations are distinguished: those for event selection and those for event filtering. When an event has been accepted by a processor of the selection farm, the full event data is transferred to a different processor within the event filter farm. This scheme is the closest to the model of the ATLAS technical proposal though in this model the same network is used to transfer data for the two farms.

\(\bullet\) "event selection and filter - single farm"

All processors have the same functionality. A given processor starts with the sequential event selection then proceeds with full event building and event filtering when needed. This mode of operation is proposed for a combined Level-2 and Level-3 in ATLAS.

## V Basic Performance Tests

Before we show the operation of the complete system, we present several simple tests to evaluate the performance of the different ATM interfaces that are installed at RCNP.

### _Performance of ATM interfaces and low level software_

In a network based T/DAQ, hosts must be able to exchange messages at a high rate and make efficient use of network resources while placing a minimum load on local CPUs. We measure the overheads for sending and receiving messages i.e. the amount of CPU time for filling a message and completing a "send" function call (T\({}_{\text{snd}}\)) and a "receive" function call (T\({}_{\text{rev}}\)).

All tests are made at the AAL5 level on three platforms:

\(\bullet\) DEC Alpha workstation - 333 MHz (Digital Unix) equipped with the ATMWorks 350 (or 351) network interface card and running with DEC native ATM API,

\(\bullet\) "Pentium II PC - 300 MHz (WindowsNT) equipped with the FORE PCA 200 ATM interface accessed via Winsock2,

\(\bullet\) "Pentium II PC - 300 MHz (WindowsNT) equipped with the IDT NicStar card running our custom made true zero-copy driver/library [10]. It can be seen in Figure 3 that a significant reduction of CPU utilization is achieved with the use of the zero-copy library. Performance with the FORE card under Winsock2 is very disappointing. This is partially explained by the rather long time to perform system calls in WindowsNT (about 50 \(\upmu\)s) and by the time spent in the kernel for multiple data copy.

Off-the-shelf software, DEC ATM software and FORE Winsock2 library, provide the operation of legacy applications over LAN Emulation as well as the easiest access to native ATM, but for this access, superior performance is achieved with our zero-copy library. These improvement are done at the cost of flexibility and require to develop non-trivial software. At present, the performance of DEC products is almost sufficient to avoid a custom optimization. On the other hand, the use of the optimized library is fully justified on the PC platforms because the performance of the Winsock2 ATM support software that we tested is very low.

We measure T\({}_{\text{lb}}\), the amount of time to send and receive in loop-back mode a message of 64 bytes over ATM. We circulate 10\({}^{7}\) packets. The probability that T\({}_{\text{lb}}\) exceeds a given value is plotted in Figure 4.

The average T\({}_{\text{lb}}\) on DEC and PC platform (zero-copy library) is less than 100 \(\upmu\)s (220 \(\upmu\)s with Winsock2). The maximum value is 2 ms on the PC (zero-copy or Winsock2 library) and several 100 ms on Digital Unix hosts. Some kernel settings in Digital Unix could improve responsiveness but these were not made to avoid the risks of interference with RCNP users because these optimizations require super-user privilege for setting the correct values of parameters and also for the execution of the application program.

### _Gathering several data packets in a processor_

In this test a destination sends a request message to a variable number of sources and gathers the response blocks of data. We present in Figure 4(a) the amount of CPU time needed to request, gather and re-format blocks of 1 kB coming from a variable number of sources (T\({}_{\text{dst}}\)). Linear scaling is observed and the advantage of the zero-copy library is apparent. This test is made with 19 machines acting as sources. We perform the same test with a system that has only 6 sources. The comparison of T\({}_{\text{dst}}\) is shown in Figure 4(b) where the lower curves correspond to the 6 sources testbed. It can be seen that the

Figure 4: Loop back test.

Figure 3: Message handling CPU overheads.

amount of time to gather blocks of data from a given number of sources depends on the total number of sources in the system when using socket based interfaces (DEC API or Winsock2).

The use of the "select" function call can explain part of the degradation, but the scaling of performance when an application uses several hundred sockets is an issue that we identify. On the other hand, no degradation is observed with the zero copy library. Knowing the detailed structure of this library, we have good reasons to think that no significant degradation of performance will occur when the number of sources is increased.

### _Memory access speed measurements_

An important parameter to characterize the performance of a network based application is the speed of memory access for read, write and copy operations. Even when a true zero-copy library is used, the actual message content has to be written in some memory locations and when receiving messages from different sources, at least one memory copy is made at the application level to format data. In order to support in a transparent way network devices that implements the means for zero-copy as well as standard networking protocol stacks, we developed a centralized buffer manager component to achieve a de-coupling between the management of memory resources and other services. The buffer manager pre-allocates pieces of memory, contiguous, cacheable and locked.

Buffers can be chained to allow for protocol encapsulation without memory copy as well as a direct interface to scatter/gather DMA capable devices. Both the virtual address and the physical address of the pieces of memory are needed to support true zero-copy transfers, but a simplified version of the buffer manager that does not provide the information on physical addresses is also provided for a seamless integration with standard device drivers that do not exploit the potential benefit of this feature. From the application point of view, the linked list of buffers is hidden by a series of macros that make this rather complex structure appear to be a flat, contiguous piece of memory. We present in Figure 6 the speed of memory access for read and write operations for different types of data and byte ordering. Care must be taken when the measurements are made to make sure that the data is effectively read from or written to the actual memory and not simply in the cache. It can be seen that the performance is optimal when the quantities that are manipulated match the size of the data path of the machine (32 or 64 bits) and that using a byte order which is not the machine native ordering can have a significant impact (e.g. reading 32 bit quantities on a PC in Big Endian mode).

## VI System Operation In T/DAQ Emulation

We could use the majority of machines and servers connected via ATM at RCNP for the emulation of a T/DAQ. The 6 multi-processor servers, 5 Alpha workstations and one PC are used to emulate a total of 12 destinations. We use 12 Alpha workstations to emulate the sources (though one workstation does not operate properly and is seldom used). One PC is used as a monitor and a second one as a supervisor. This makes a demonstrator with 26 machines and a total of 60 CPUs. Point-to-point and multi-cast Permanent Virtual Connections (PVCs) are established in the four ATM switches. Performance measurements are made in the early morning to avoid the interference caused by the daily work of all RCNP users.

### _Operation in "event selection" mode_

We test the operation of the system with a simple sequential algorithm. All steps of the algorithm are identical and consist in getting 1 kB of data from 4 sources randomly selected among 11, copying this data in contiguous memory locations and performing a 100 \(\upmu\)s dummy processing. There is no rejection after each step to measure the per-step protocol performance. We measure T\({}_{\text{dst}}\), the amount of CPU time needed to complete this sequence. We find that T\({}_{\text{dst}}\) scales linearly with the number of steps: 800 \(\upmu\)s per step (tested up to 16 steps).

We measure T\({}_{\text{dst}}\) for a 16-step algorithm using several types of machines for the destination and we analyse the different component of T\({}_{\text{dst}}\). This is shown in Table 1.

The advantage of the zero-copy library is again apparent,

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Platform & Winsock2 & DEC & Zero-copy \\ \hline \hline Total T\({}_{\text{dst}}\) (ms) & 25 & 15 & 9 \\ \hline \hline Components of T\({}_{\text{dst}}\): & & & \\ \hline algorithm & 6\% & 11\% & 18\% \\ \hline data copy & 12\% & 26\% & 33\% \\ \hline sent requests & 23\% & 17\% & 7\% \\ \hline get responses & 26\% & 13\% & 3\% \\ \hline application, OS calls, etc & 33\% & 33\% & 38\% \\ \hline \end{tabular}
\end{table}
Table 1: Destination CPU utilization for a test algorithm of 16 steps.

Figure 5: Gathering data from several sources in a destination.

Figure 6: Memory access speed measurements.

but the most important to note is that, even in the most favorable case, no more than 20% of the CPU time is spent in the algorithm itself. The majority of CPU cycles are spent in operating system calls, data copy and various overheads. In order to cope with a target event rate of 100 kHz, this 16-step algorithm would require ~1800 processors to run (assuming 50% occupancy). Given the evolution of CPU performance, this number can probably be divided by 3 or 4 in the future, but we stress that sequential data transfer and processing has a significant cost in terms of CPU cycles. Therefore algorithms steps should be designed to reject events as soon as possible.

### _Impact of sequential processing on trigger decision latency_

We measure the trigger decision latency \(\mathrm{T_{dec}}\), that is the amount of time from event allocation by the supervisor until the supervisor gets a decision from the destination processor. The distribution of \(\mathrm{T_{dec}}\) for the 16-step algorithm previously described is plotted in Figure 7a. The 11 sources, 11 destinations system operates at an event rate of 320 Hz (i.e. 50% of the maximum that can be achieved). The average latency is 52 ms and the maximum observed latency is several 100 ms (each destination CPU usage is 15 ms per event).

We investigate the impact of this trigger decision latency profile on the amount of buffering needed in the sources by means of a Monte-Carlo simulation. In the model, events are added in the source buffers at regular intervals. The lifetime of each event is drawn from the distribution of \(\mathrm{T_{dec}}\) that we measured. If the supervisor forwards each trigger decision immediately to the sources, an event is removed from the source buffer when its lifetime has expired ("no packing"). If the supervisor packs a number of trigger decisions before it sends all of them at once to the sources, a block of N events is removed from the source buffers when the lifetime of N events has expired.

The distribution of the source buffer occupancy is shown in Figure 7b for a simulated event rate of 100 kHz. Assuming that the source buffers are filled at 100 MB/s, the amount of buffering needed after each detector read-out link does not exceed \(\sim\)6 MB. This is easily achieved with today's memories. We think that there is no major issue in tolerating trigger decision latencies of several tens of milli-seconds and that in rare cases it can reach one or several seconds. On the other hand, the algorithm duration should be kept to a minimum because this parameter has a direct impact on the number of processors needed to cope with a given event rate.

### _Impact of system size on response time_

We measure the distribution of \(\mathrm{T_{dec}}\) on two different systems: the first one consists of 8 machines (3 sources and destinations, 1 supervisor and 1 monitor) connected by an ATM switch isolated from the network of the institute, while the second one has its 24 nodes (11 sources and destinations, 1 supervisor and 1 monitor) connected by the LAN of RCNP (with all the normal background traffic and machine load of users). The task of each destination is identical in both cases: gather 1 kB of data from 3 sources, copy this data, then emulate a 100 \(\upmu\)s processing. The tests were made at 50% load, generating \(10^{7}\) event at a rate of 1.8 kHz for the 8-node isolated testbed and 5.5 kHz for the 24-node testbed. The probability that \(\mathrm{T_{dec}}\) exceeds a given value is shown in Figure 8.

The average value of \(\mathrm{T_{dec}}\) is 2.4 ms and 3.8 ms for the small and larger system respectively. It is not very surprising that the response time of the system is degraded when the size of the system is increased but part of the degradation comes from the normal use of RCNP network and machines. For large dedicated systems, a spread of response time is expected. As previously explained, it seems possible to accommodate the additional buffering requirements, but long latencies may have some impact on the correct operation of the system (e.g. ambiguities due to timer or counters roll-over errors). A precise modeling and simulation is probably the best method to investigate the dynamic behavior of large systems, but we try to propose in the next section a simple way to get very rough estimates of the distribution of response times.

### _Getting estimates of response time distributions_

We showed that the average amount of time to gather data from a certain number of sources grows linearly with the number of fragments to collect. Similarly, we measured that the average processing time for an algorithm sequence of identical steps is proportional to the number of steps. We investigate if what applies to average values remains valid for distributions. The method consists in measuring various probability density functions on a testbed and use these measurements as an input to a Monte-Carlo simulation program to predict response time profiles.

We measure the distribution of \(\mathrm{T_{dec}}\) for a simple processing task in the destination: get 1 kB from 4 sources, perform a data copy then a 100 \(\upmu\)s processing (50% system load). We use this distribution in a simulation program that emulates the execu

Figure 8: Dynamic of small isolated testbed versus larger system.

Figure 7: Trigger decision latency and source buffer occupancy.

tion of a 10 step algorithm as shown in Figure (a)a. The output of the simulation is the latency profile of \(\text{T}_{\text{dec}}\) shown on Figure (b)b, simulation.

In a second phase, this algorithm is run on the testbed (always at 50% system load) and the measurement of \(\text{T}_{\text{dec}}\) is compared to the simulated profile. This is shown in Figure (b)b, measure. It can be seen that simulation is in rather good agreement with the measurement. However, in general, this is not the case: with a more complex algorithm sequence scenario, the simulation result diverge quickly from the measurement. We think that this method is too approximate to get usable results but it can be used to get a first guess of response times.

### _Using multi-processor machines_

The use of multi-processor machines for the destinations is being considered. There is a benefit over mono-processor machines for algorithms that are longer than the event data transfer time (i.e. when the link to the network is not the bottleneck). The typical algorithm that we use to illustrate this fact consists for a destination in collecting 1 kB from 4 sources, recooping the data and performing a dummy processing of a given duration \(\text{T}_{\text{alg}}\). We measure that for \(\text{T}_{\text{alg}}\) less than 1 ms, there is no significant improvement when multi-processor destinations are used. When \(\text{T}_{\text{alg}}\) is 30 ms or more, the ratio \(\text{T}_{\text{alg}}/\text{T}_{\text{ds}}\) is close to 1 with mono-processor machines, 3.5 with the 4-CPU servers and around 8 with the 12-CPU servers. Multi-processor machines provide better performance in some cases but may not be more economical than several mono-processor machines.

When using a combination of destinations of different speeds and types (e.g. mono and multi-processors machines), it is needed to achieve a correct load balancing to take the full benefit of available resources. The supervisor implements a credit based scheme for the distribution of events to the destinations. A certain number of credits (possibly different for each destination) are initially stored in the supervisor. Events are distributed to the destinations following a fixed cycle and the corresponding credit count of each destination is decremented. It is incremented when the processor returns a decision to the supervisor. If a credit count is zero, the destination is skipped. This scheme provides an automatic limitation of the event rate in case of surcharge and an isolation of faulty processors, but it does not provide a correct load balancing in most cases as shown in the following test.

The destination task is to collect 1 kB from 4 sources among 11, format the data and perform a 10 ms processing. The destinations are 5 workstations and 6 SMP servers. The maximum event rate that can be sustained by the system is 2 kHz. We measure the trigger decision latency \(\text{T}_{\text{dec}}\) for an event rate of 1 kHz (Figure (a)a). When the simple credit based scheme is used, the average \(\text{T}_{\text{dec}}\) is 75 ms. The peak at 200 ms corresponds to the events allocated to the workstations that are saturated. When operated at 50% system load, all destinations get roughly the same fraction of events as shown in Figure (b)b ("credit based scheme"). The credit scheme is only a flow control mechanism (i.e. active in case of surcharge or failure), but at 50% system load, the allocation of destinations is made following a fixed cycle and a destination is rarely skipped because its credit counts seldom reaches zero.

We improve the event distribution scheme by adding to the supervisor a schedule table that explicitly gives the sequence of destinations for event assignment. This table is built so that some destinations get scheduled more often than others (twice as many times for the SMP servers compared to workstations in our test). The supervisor first consults this table to get a proposed destination, then checks its credit count and makes the appropriate decision for event distribution. When this explicit event allocation scheme is used, the servers get a larger fraction of events (Figure (b)b, "enhanced scheme") and \(\text{T}_{\text{dec}}\) is reduced to 14 ms because a better load balancing is achieved.

With the enhanced credit based scheme, load balancing was achieved statically, for a given algorithm duration. In practise, the event distribution mechanism should take into account many parameters of the destinations: CPU power and speed, link bandwidth capacity and usage, etc. The design of such a scheme requires investigations that are beyond the scope of this paper, but we show the importance of achieving a correct load balancing.

### _Operation in "event builder mode"_

For each event the destinations multi-cast a request to all sources and gather all event data (11x4 kB in this test). It is known that this simultaneous concentration of messages toward single network outlets is a traffic pattern that can create congestion in the switching network and several schemes for congestion avoidance have been proposed [4, 11]. We mea

Figure 10: Load balancing.

Figure 9: Estimation of latency profile.

sure the maximum frequency of event building \(\mathrm{F_{eb}}\) versus the attempted event building rate \(\mathrm{F_{ar}}\). We also measure the percentage of corrupted events due to data loss. We make three tests. Firstly, Unspecified Bit Rate (UBR) channels are used and each source sends data at full speed to the destinations.

It can be seen in Figure 11a that the system operates correctly at low rate, but when \(\mathrm{F_{ar}}\) is increased, the number of corrupted events sharply increases (Figure 11b) and network performance collapses. When \(\mathrm{F_{ar}}\) is further increased, the situation worsens and the credit based flow control mechanism of the supervisor performs an automatic event rate reduction to avoid a catastrophic behavior. The second test is made with Constant Bit Rate channels (CBR): each source can send to a given destination using at most 1:11 of the link bandwidth (fair share for each of the 11 destinations). In fact only 90% of the link bandwidth could be used for all CBR channels due to some settings of ATM interfaces. The performance is sharply increased, the effective event building rate \(\mathrm{F_{eb}}\) follows the attempted rate \(\mathrm{F_{ar}}\) until a plateau is reached. Because network congestion is avoided by the use of CBR channels, there is no cell loss. The system was operated at its maximum speed during 1 hour; more than 400 GB of data were transferred without any data loss. The last test is made with DEC proprietary available bit rate scheme called FlowMaster [12]. The system reaches higher performance than in the test with CBR channels because only 90% of the link bandwidth was allocated to CBR channels while FlowMaster uses 100% of that bandwidth. No data loss is observed when using FlowMaster. Nonetheless, packet losses due to bit errors in any component of the network can still occur because all transfers are made at the AAL5 level which does not provide a reliable end-to-end transmission.

This test shows that a congestion avoidance mechanism is mandatory for the correct operation of an event builder; the use of CBR channels provides a standard solution in ATM while proprietary hardware based flow control mechanisms like Digital FlowMaster can be also adequate. Other technologies can solve the issue in various ways, at the level of the source of the traffic (like in our case), inside the switching network, or in both places using any combination of standard or custom hardware/software.

We measure for the different platforms the maximum link bandwidth usage on the destination side. For this test, we use a testbed configuration with 11 sources and only 1 destination to make sure that saturation occurs on the destination side. The sources use CBR channels (up to 80% of total link bandwidth). It can be seen in Figure 12 that for event fragments of 4 kB, the destination link capacity is correctly exploited on DEC platforms and on the PC running the zero-copy ATM library. With Winsock2, the limitation seems to come from the software overhead and less than 70 Mbit/s of data flow can be absorbed. For a fragment size of 1 kB, performance is dominated by software overheads on all platforms.

These results need to be confirmed on an isolated system using more efficient sources without the constraints on maximum bandwidth allocation for CBR channels that we face with DEC products.

We measure the maximum frequency of event building \(\mathrm{F_{eb}}\) when the number of destinations is varied. The test is made using CBR channels. The destinations gather data from the 11 sources, reformat data but do not run any processing. It can be seen in Figure 13 that the event builder throughput scales linearly with the number of destinations. The maximum throughput is 120 MB/s. This corresponds to 70% of the links capacity.

The target performance for the ATLAS event builder is 1 GB/s with a system that has several hundred sources and destinations. This goal seems realistic if we consider that the present testbed has already 1:10 of the target performance while being less than 1:60 of the final system in size.

For the Phenix experiment, the event builder has 32 sources and 32 destinations and a target performance of 500 MB/s. Our testbed is 1:3 of the final system in size and has 1:4 of the required performance. We think that with up to date PCs (e.g. 100 MHz mother board) and using a zero-copy ATM library, the goal can be achieved.

### Operation in "event selection and filter" modes

A series of test is performed running a sequential algorithm followed by full event building in the single farm and split farms modes of operation. For this test, we use 11 workstations

Figure 11: Event building.

Figure 12: Maximum destination link bandwidth usage.

Figure 13: Event builder throughput.

to act as sources and we have two possible configurations for the 12 destinations. In the "single farm" test, all destinations have the same role. After performing a series of selection steps, full event building for accepted event candidates is made, followed by an emulation of a filtering process. In the "split farm" test, two types of destinations are distinguished: the "selection farm" composed in our case of 6 workstations perform only the sequential selection algorithm part then transfers accepted candidates (via the supervisor) to one of the 6 SMP servers dedicated to the event building phase and the "filtering" process. The algorithm sequence that we use for this test is shown in Figure 14.

We measure the distribution of \(\mathrm{T_{dec}}\) (from event allocation until the end of the sequential algorithm, i.e. Step 4 on Figure 14) on the split farm configuration. The event rate is 1.1 kHz corresponding to a 50% utilization of the "selection farm". We measure on the single farm configuration the distribution of \(\mathrm{T_{dec}}\) (from event allocation until the end of the complete algorithm). The 6 SMP servers are not used in this test to have the same set of destinations for the two different test. The event rate is 0.5 kHz (50% system load). The distribution of \(\mathrm{T_{dec}}\) for these two test is shown in Figure 15a.

The peak of latency around 20 ms for the single farm test corresponds to the fraction of events that went through the event building phase. This time is decomposed as: -6.6 ms for the sequential part of the algorithm, -4.3 ms for the transfer of 44 kB of event data at -80 Mbit/s to the destination processor, and 10 ms for the pseudo processing of the event. An additional peak around 200 ms is also observed though we have no clear explanation for it. Abstraction made of this peak, the latency profiles in both tests can be understood.

We investigate the influence of performing or not the event building phase on the trigger decision latency \(\mathrm{T_{dec}}\) measured at the end of the sequential process (Step 4 on Figure 14). In the first test, no event building is made at all after the sequential algorithm. This corresponds to "no EB" on Figure 15b. The event rate is 1.1 kHz. The second test is made on the "split farm" configuration with event building performed in the farm of 6 SMP servers. The distribution of \(\mathrm{T_{dec}}\) is "EB in EFT" on Figure 15b. The event rate is unchanged (1.1 kHz). The last test is made in the "single farm" mode without using the SMP servers. The event rate is reduced to 0.5 kHz to keep the 50% system load constant ("EB in SEL" on Figure 15b). Without surprise, it can be seen that the average \(\mathrm{T_{dec}}\) is increased when the event building traffic is added; it is further increased in the single farm approach. These measurements are presented to illustrate the operation of the system in the "single" farm and "split farm" mode but no conclusion on the potential advantage/disadvantage of both of these approaches can be drawn at this stage. A more realistic sequential algorithm and filtering process as well as more precise evaluations of bandwidth requirements and real-time constraints are needed for this study. Conceptually, the single farm approach is simpler, though in practise it may be difficult to implement especially if a totally different code is run for the sequential selection algorithm and the event filter process. Nonetheless, we validate the protocol of operation in both modes and these preliminary results show that a common network can handle both the traffic for the event selection and for the event builder.

## VII Discussion And Summary

We described the ATM based computer system of RCNP. We successfully used this system to make an emulation of a typical network based T/DAQ. We presented the performance of several ATM cards and software and showed a potential performance scaling issue with socket based APIs. We emulated sequential algorithms and investigated the impact of trigger decision latency on the amount of buffering needed in the sources. We suggested that maximum latencies of several 100 ms can be tolerated. The use of multi-processor destinations was tested. The distribution of events to the destinations in a way that achieves a correct load balancing is a problem that we have identified. The operation of the system in event builder mode was shown. The necessity and the effectiveness of a congestion avoidance mechanism was demonstrated. The performance is compatible with the requirements of two future experiments.

Work will continue to build larger dedicated systems with source and supervisor modules closer to what will be used in an experiment; physics algorithms will be run in the destination processors instead of running dummy code.

Figure 14: Sequential algorithm + Event building phase.

Figure 15: Single and split farm tests.

## VIII Acknowledgments

The authors wish to thank RCNP for allowing this unusual use of the laboratory computer network. We also thank K. Hayakawa, T. Nagano and Y. Shimizu from DEC for their efficient support.

## IX References

* [1] ALICE Collaboration, Technical Proposal, CERN LHCC/95-71, 1995.
* [2] ATLAS Collaboration, Technical Proposal, CERN LHCC/94-43, 1994.
* [3] CMS Collaboration, Technical Proposal, CERN LHCC/94-38, 1994.
* [4] M. Costa et al., "Results from an ATM-based Event Builder Demonstrator", IEEE Trans. on Nuclear Science, vol. 43, No 4, June 1996, pp. 1814-1820.
* [5] D. Calvet et al., "Operation and Performance of an ATM based Demonstrator for the Sequential Option of the ATLAS Trigger", in IEEE Trans. on Nuclear Science, vol. 45 No 4, August 1998, pp. 1793-1797.
* [6] M. Nomachi et al., "Performance Measurements of Mixed Data Acquisition and LAN Traffic on a Credit-based Flow-controlled ATM Network", in IEEE Trans. on Nuclear Science, vol. 45 No 4, August 1998, pp. 1854-1858.
* [7] ATLAS Level-2 Trigger Groups "Options for the ATLAS Level-2 Trigger", in Proc. IEEE Conf. on Computing in High Energy Physics, Lichtenberger Congress Center, Berlin, Germany, 7-11 April 1997.
* [8] B. Cole et al., "The Phenix Event Builder", in Proc. X\({}^{\text{th}}\) IEEE Real Time Conference, Beaune, France, 22-26 September 1997, pp. 179.
* [9] G. Ambrosini et al., "The ATLAS DAQ and Event Filter Prototype -1 Project", in Proc. IEEE Conf. on Computing in High Energy Physics, Lichtenberger Congress Center, Berlin, Germany, 7-11 April 1997.
* [10] D. Calvet et al.,"Performance Analysis of ATM Network Interfaces for Data Acquisition Applications", in Proc. Second International Data Acquisition Workshop on Networked Data Acquisition Systems, Osaka, Japan, 13-15 November 1996, World Scientific Publishing 1997, pp. 73-80.
* [11] D. Calvet et al.,"Evaluation of a Congestion Avoidance Scheme and Implementation on ATM network based Event Builders", in Proc. Second International Data Acquisition Workshop on Networked Data Acquisition Systems, Osaka, Japan, 13-15 November 1996, World Scientific Publishing 1997, pp 96-107.
* [12] K. Suruga and K. Hayakawa, "No Cell Loss: Digital's ATM Flow Control", in Proc. Second International Data Acquisition Workshop on Networked Data Acquisition Systems, Osaka, Japan, 13-15 November 1996, World Scientific Publishing 1997, pp. 81-85.