# ATLAS Level-2 Trigger SCI Demonstrator Evaluation Report

A.Belias, A.Bogaerts, D.Botterill, F.Giacomini, R.Hauser, R.Middleton, P.Werner, F.Wickens

###### Abstract

Measurements have been made on the ATLAS Level-2 Trigger Demonstrator based on SCI interconnect technology. The system comprises a cluster of 20 (mostly) dual CPU Pentium II machines of speeds ranging from 300 MHz to 450 MHz and running Red Hat Linux 6.0. Each is equipped with a PCI/SCI adapter and is connected to a 16 port modular SCI switch. In a few cases some machines share a switch port. Each port is capable of 400 Mbytes/s raw data rate and each adapter has been shown to deliver about 80 Mbytes/s sustained performance.

Substantial experience in operating the system has been gained and many results have been compiled. Component performances have been measured: each Supervisor Emulator is able to deliver 11kHz sustained rate; the ROB emulator output over SCI supports 13kHz request rate for 1kByte records; the Steering processor can gather fragments each of 1kByte from 4 ROBs at \(\sim\)4kHz; the network traffic generated by the application is well within the limits of SCI.

Scheduling of threads by the operating system has a major impact on performance and on scalability. Careful choices affecting scheduling were required to obtain optimum performance and use of system resources.

[MISSING_PAGE_EMPTY:2]

[MISSING_PAGE_EMPTY:3]

## 1 Introduction

The ATLAS Level-2 Trigger SCI Demonstrator forms part of an on-going R &D exercise within the Level-2 Trigger Group of the ATLAS experiment at CERN. The purpose of this work is to establish both an optimised, flexible architecture and appropriate computing and networking technologies to fulfil the needs of a complete system for the final experiment.

To achieve these goals a pilot project was set up within the Level-2 Trigger Group to

* establish a coherent software infrastructure (the Reference Software),
* develop data selection (trigger) algorithms,
* explore candidate networking technologies (SCI, ATM and Fast/Cigabit-Ethernet being selected),
* verify viability of commodity hardware (PC) and operating systems,
* demonstrate scalability to full size systems through modelling,
* define an integration strategy to bring together both Level-2 system components and to place the complete Level-2 system in its wider context of the ATLAS experiment.

These goals have been pursued through the development of the Reference Software [1] and algorithms and their deployment on PC clusters interconnected by a networking technology to form a demonstration platform. Of necessity, the clusters are small (typically 20 to 40 nodes) on the scale of the final system and can only give a small measure of performance. However, they provide the basic parameters for large scale modelling exercises to demonstrate the possibilities for full size systems.

This report presents an evaluation of a demonstrator running the Reference Software on a (~20 node) PC cluster interconnected with SCI. This development of SCI infrastructure and software for this application, and their demonstration also formed part of the EU-funded SICI project (Esprit project 23174).

Each node in the SCI demonstrator is equipped with a Dolphin D321 PCI/SCI network adapter [2] and is connected to a 16-port SCI switch constructed from 6 Dolphin D515 modular switch units. The PC nodes are a mixture of 300, 350, 400 & 450 MHz Pentium II machines (BX chipset), most of which are dual CPU, but a few are single CPU. This inhomogeneity in CPU performance has been taken into account when configuring test set-ups and is also a reason why some plots in this report can not be compared directly.

Measurements have been made using a message passing layer implemented over the SICI API library [3].

### Document Overview

In Section 3 there is a very brief overview of the software design and implementation and a description of the core applications built with the Reference Software. In Section 4 the performance results are presented and in Section 5 some conclusions are drawn.

### Acknowledgements

The authors of this report gratefully acknowledge the support of the SICI project (Esprit project 23174) by the EU and the help of the SICI partners which was essential to the success of this work.

The authors also wish to acknowledge the work of many people outside the SICI project for their contributions to the development of the ATLAS Level-2 trigger. However, particular thanks are due to Haimo Zobernig, Jim Schlereth and Michel Huet and other members of the Level-2 Trigger Reference Software Development Group of the ATLAS project who have worked on many aspects of the generic Reference Software.

## 2 AILAS Level-2 Trigger Demonstrator

### Design Overview

The design of the ATLAS Level-2 Reference Software has been presented in [1] and will not be repeated in detail here.

Figure 2-1 shows the software functional units within the LVL2 system. To the left of the dashed line are systems external to LVL2 (i.e. LVL1, Event Builder and the ATLAS detector readout subsystem). The system implemented combines the functions of pre-processing, feature extraction and global processing into a single unit called the Steering process (named as such as it steers through the various processing steps).

#### ReadOut Buffers (RoBs)

Each readout buffer must be able to manage blocks of memory for storage of data coming from the detectors. It must be able to process requests for data fragments (parts of events) from the processing farm nodes and for full events to be passed to the Event Builder sub-system. Requests to delete data must be processed either when an event is rejected by LVL2 or when, having been accepted, an event has been successfully passed to the Event Builder.

#### Global Processing

The global processing step receives RoI pointers from the Supervisor and steers processing by requesting formation of event features through Feature Extraction. The resultant features are combined into objects which are then compared with options given in a trigger menu. An overall recommendation for an event is formed and sent back to the Supervisor.

#### Feature Extraction (FEX)

Upon receipt of a request from the global processing step, full RoI data is requested from RoBs (indexed by 'geographical' position on the detector). Feature extraction is then performed on the

Figure 2-1 Functional view of Reference Softwarereturned data to transform tracking information into candidate particle tracks and energy deposition measurements into energy clusters. The resultant features are returned to the global processing step.

#### 2.1.4 Preprocessing

Optional preprocessing can be done on partial RoI data contained in a single RoB. The preprocessor can be a special-purpose processor attached to a RoB via an internal connection or PCI. It is also possible to delegate the task to a conventional processor e.g. a Feature extractor. Algorithms may either request preprocessed or raw data.

#### 2.1.5 The Supervisor

RoI pointers are received from LVL1 (at high rate) and distributed to the global processing step. Trigger recommendations are received back from Global, and depending on the setting of certain flags (e.g. to override certain recommendations) a decision for an event is taken. Accepted events are signalled to the Event Builder sub-system. Buffer clear messages are sent to RoBs.

#### 2.1.6 Messages

The details of the messages can be found in [1]. It is important to note that, after combining pre-processing, feature extraction and global processing into a single entity, messages are only a few tens of bytes, except for the RawDataResp message from the RoB to Steering, which contains data for trigger processing and is expected in the final system to be 1 to 2 kbytes.

#### 2.1.7 Software Implementation

Figure 2-2 depicts the software layering model used. It provides for levels of abstraction and hiding of implementation throughout the system for each of the applications built (see 2.3). The operating system services are hidden behind a uniform interface and the network is hidden behind a proxy interface, which in turn is isolated from specific network technologies. Further details are given in [1].

Specific applications have been built using these services and have been optimised through the use of multiple threads and queues.

### Implementation Issues

Overall, the design has been very successful and very few major problems have been encountered. Although it was the first object-oriented project for many in the group, this has not been a significant problem and existing core expertise has been of considerable benefit.

The code has been developed with some parts under Microsoft Visual C++ (version 6.0) on Windows-NT and other parts under egcs (version 1.1) on Linux. This has been positive and with the large difference between Linux and Windows-NT (hidden by the OS abstraction layer), has helped to ensure wide portability. The code has been tested on one or two other UNIX platforms and is known to build on at least Solaris and Digital UNIX. Code has been managed with CVS and distributed access (from Linux and Windows-NT) has been achieved using AFS.

The use of proxies has been very successful in a number of ways. It has :-

\(\bullet\) fully isolated applications from all aspects of networking and inter-process(or) communication;

\(\bullet\) hidden the scheduling of processing nodes from the system Supervisor;

\(\bullet\) hidden the location of RoB(s) in which to find data.

Demonstrator configurations have been operated by a mixture of command scripts (using remote shell) and simple custom built run control software. Node configuration and application specific parameters have been stored in ASCII parameter files shared over AFS. Errors can be reported from any level of an application to a central error logger accessible via TCP/IP and viewed by client applications of the logger.

Monitoring of status information and collection of histograms for subsequent analysis have been via the monitoring system, for which a client display application (written in Java) exists. Histograms are easily imported from file into Microsoft Excel, thus facilitating further analysis.

Care has been given to achieving optimum performance where possible. However, since the networking layers have to support multiple technologies, it has not been possible to optimise fully buffer allocation. Similarly, data copying in and out of network technology specific buffers could be optimised further if only a single technology was to be supported.

Use of SCI has been optimised to gain the highest possible performance. This has been achieved by tuning buffer use (maintaining a buffer pool), by pipelining messages (permitting multiple outstanding buffers) and by adoption of a simple messaging protocol (described in [3]).

### Component Applications

Using the libraries which form the basis of the Reference Software a number of applications have been built to supply the principal components of various testbeds. Of principal relevance to the results presented here are the skeleton Supervisor, Steering and RoB programs. These perform the minimum necessary (i.e. no data processing tasks) to manage the emulation, transmission and control of data through a complete slice through the system. The message formats and sizes used between processes are those of a complete system.

Each of the programs are characterised by being multi-threaded with an input thread dedicated to handling incoming messages. Further processing of the message is based either on a transaction ID (if it is the response to a previously issued request) or message type. In either case a handler routine is called to deal with details of the processing, though typically the message will be put on a queue for handling by another worker thread. In the lower software layers of the worker thread the message headers are stripped off and the data payload is copied into the appropriate data object for processing at application level.

Outgoing messages are sent directly from the thread issuing them through the proxy interface described in [1]. At the lower software layers data object contents are packed in to a message buffer and despatched.

#### 2.3.1 The Supervisor Emulator

This program manages the generation and processing of events in the system. For each event, specific pointers to so called Regions-of-Interest (RoI) are generated. In a full system these would be used in processing nodes to locate the necessary data for processing. The RoI's are packed in to a message which is sent to a Steering process. Both round-robin and random scheduling of Steering processors have been implemented. Simulated results are received back and processed. The Supervisor keeps tally of how many events there are in the system at any time in order to regulate traffic and prevent run-away.

It is possible to run the Supervisor either from a set of pre-loaded data from a file or by internal generation of data based on parameters in a configuration file.

#### 2.3.2 The Steering

The Steering program has one or more worker threads, each dedicated to the processing of an event. Each worker waits on the input queue for a request to arrive. When a message arrives, one of three processing strategies can be selected;

1. No RoI Processing : in which the content of the request message is ignored and a response message returned immediately;
2. Generate : in which the content of the request message is once again ignored and instead a single RoI is generated internally and used to locate and fetch data from appropriate RoB(s);
3. Simple 2 Step : in which the request is processed in two stages. At each stage RoI information is fetched from the appropriate RoB(s). After the first stage 9 out of 10 events are rejected and a decision message returned to the Supervisor immediately. The remaining 1 in 10 events go on to the second stage when again 9 out of 10 events are rejected. Thus, only 1% of events are accepted, thereby simulating the data pattern expected in the final system.

Each worker processes the RoIs for that event sequentially and hence blocks at each request for data from a RoB. CPU utilisation is thus maximised by having a number of concurrent workers.

#### 2.3.3 The Read-out-Buffer (RoB)

The RoB is a provider of data for processing. Since no algorithms are executed in the skeleton Steering, it is not necessary to deliver proper data, but instead empty data buffers of appropriate size are transferred on request. There is just one worker thread serving data and in addition one handling requests to clear used data from the RoB. Each RoB is responsible for serving data for a different part of the detector.

### Testbeds

A number of testbed configurations are supported by the Reference Software. The three outlined below have been used for development, testing and performance evaluation.

#### 2.4.1 Single Node

A single node testbed enables standalone development and testing of code. All functionality is encapsulated within one machine with all component applications (Supervisor, Steering and RoB) interacting over internal TCP or UDP paths.

#### 2.4.2 Reference Testbed

The Reference Testbed configuration permits the development of message passing protocols and other infrastructure to run the Level-2 system. In addition, it should provide an environment to optimize code. Features of the testbed are that:

* all 'data sources' connect to all processors;
* a single physical network (10baseT ethernet) is used for data collection and control;* processors are used for combined pre-processing, feature extraction and global processing.

#### 2.4.3 SCI Cluster Testbed

The SCI Cluster testbed is as for the Reference testbed, but uses SCI and incorporates a SCI switch.

This testbed is shown in Figure 2-3. From a hardware point of view there are only three types of units: Supervisor, Steering (trigger processors) and Read-out-Buffers (RoB). As for the Reference Testbed the Steering implements the functionality of pre-processing, feature extraction and global processing.

Figure 2-3 The SCI Cluster testbed

## 3 Integrated System Performance

The Level-2 Trigger Demonstrator is a complex system with many possibilities for interplay between components. For the measurements presented here, each component of the system (or component type) has been made in turn the focus of analysis. To do this, system configurations and parameters have been chosen to ensure that other components do not become limiting factors. In this way the performance of individual components has been extracted.

The first sub-section focuses on the real-time performance and hence measures attributes of the operating system. Subsequent subsections cover Supervisor, RoB, Steering and Network performance respectively.

### Real-time Performance

It is important that the ATLAS demonstrator is able to show a predictable performance which will scale to much larger systems. In particular, the Supervisor is a time critical component which has great influence over the whole system. First measurements on the system running on the Linux operating system indicated that the performance goals would not be met. In particular, Figure 3-1 shows that as more supervisors were added to the system the throughput did not rise linearly in proportion, but tailed off.

A more detailed analysis of the problem (Figure 3-2) shows, that in measuring the event latency for different numbers of events in the system, a second peak appears, the distance of which from the first peak varies linearly with the number of outstanding events in the system (Figure 3-3). The events contributing to the second peak will reduce the measured throughput.

The version of Linux used did not incorporate any real-time extensions and so of course the above does not come as a surprise. Further analysis shows that, if in each of the applications the thread handling network inputs yields the processor after receiving and internally enqueueing a message (and if there are no more messages from the network outstanding) then predictable and scaling behaviour is restored (Figure 3-4). Previously, the input thread had continued to the end of its time-slice in a polling loop (even though a higher priority thread was ready to compute). Thus the secondary peaks of Figure 3-2 can be interpreted as resulting from delays introduced by the input thread completing a time-slice before giving up the processor to worker threads.

Figure 3-1 System Performance (with no thread yield)Figure 3-4 System Performance (with and without thread yield)

Figure 3-2 Event latency distributions for differing number of events in the system

Figure 3-3 Delay between first \(\&\) second latency peaks

Figure 3-4 System Performance (with and without thread yield)

### Supervisor Performance

In order to characterise the system Supervisor the configuration shown in Figure 3-5 was adopted. Tests were performed where the number of Supervisor Emulators in the system was increased from 1 to 5. A SteerSink program was used instead of the full Steering. This is a much simplified Steering which is optimised to respond to processing requests with dummy replies as fast as possible. It does not make any requests to RoBs for data. Sufficient SteerSinks were included to ensure that there was adequate processing capacity to handle all requests coming from the supervisors. Thus, the overall system throughput is only determined by the supervisors and not limited by other components.

#### 3.2.1 System Capacity

To ensure that the system is adequately filled with processing requests a scan over the number of events outstanding in the system at any one time was made in order to find a suitable operating point for other measurements. The test was performed with one each of Supervisor Emulator, and RoB and sufficient SteerSinks to respond to all requests made. The number of events permitted in the system at a time was varied.

It can be seen (Figure 3-6) that the system saturates above 3 concurrent events in the system and gives a peak rate of just over 11 kHz. From this distribution it is possible to ensure, for other measurements,

Figure 3-6 System capacity in terms of number of concurrent events in the system

Figure 3-5 Baseline configuration for Supervisor measurementsthat the system is kept busy, though account has to be taken of the number of processing nodes in the system (Steering processes) and of how many worker threads are run in each.

#### Scaling

Figure 3-7 shows the effect of adding supervisors to the system. A scan is done over the number of SteerSink processes and for each configuration the point at which the Supervisor(s) limit can be seen. Taking the system at saturation for each supervisor configuration, Figure 3-8 shows that the throughput scales linearly as the system is expanded.

#### Supervisor Latency

In order to measure the latency of an event in the system a configuration was set up with adequate SteerSinks and RoBs to ensure that for a given number of Supervisors the system would run at maximum rate. However, to avoid queuing of events anywhere in the system only 1 event outstanding per supervisor was permitted at a time. Event latency is defined, for the purposes of the measurements presented here, as the round-trip time of an event in the system as measured by the Supervisor. The time starts when the processing request leaves the Supervisor and stops when a decision has been received back.

Figure 3-9 shows the event latency is independent of the number of supervisors and is about 140 us.

Figure 3-8: Performance scaling with number of Supervisors

Figure 3-7: Onset of maximum system utilisation

### ROB performance

The basic I/O performance of the RoB skeleton has been evaluated using configurations based on Figure 3-10. The aim is to see how well the RoB performs under concurrent load from one or more Steering nodes and to see how this varies as a function of RoI data size. Steering and Supervisor nodes were added to the system to ensure that they were not limiting the overall system performance. Each Steering node ran with \(3\) worker threads to ensure maximum Steering process efficiency.

#### 3.3.1 Fragment Size

The system throughput was recorded as a function of the fragment size for different numbers of Steerings. It can be seen clearly that the performance falls off as the fragment size increases, indicating that data copying and network transmission times become dominant.

#### 3.3.2 Number of Steerings

Figure 3-11 shows the variation of system throughput as a function of the number of Steering nodes for different fragment sizes and is based on the same data as used in Figure 3-12. For small (64 byte) fragments the RoB performance scales approximately for up to 3 Steerings, but then begins to break above this. Scaling is preserved for up to 2 Steerings for fragments sizes of 1 kbyte, but beyond this the RoB is clearly not able to cope. For 16 kbyte fragments, the ROB is fully occupied handling even a single request and other requests have to queue.

Since the RoB skeleton is very simple and merely returns a fragment of appropriate size, it is very likely that at large fragment sizes data copying and network transmission times dominate.

Figure 3-9: Invariance of event latency with system sizeThe predicted operating point of \(\sim\)1 kbyte for the final system implies the current implementation would be able to support 6 approximately concurrent requests (1 from each of 3 worker threads in each of 2 Steerings) without performance loss. However, this does not take into account that in a real system the data must be located and possibly fetched from remote memories.

### Steering Processor Performance

This section is devoted to evaluating both processor and I/O performance of the Steering sub-system. A typical configuration is shown in Figure 3-13, where a single Steering process is subjected to sufficient requests from one or more supervisors to keep it fully busy. No data processing algorithm is run in the Steering process, thus focusing on the maximum potential I/O performance. The number of RoBs, the size of data transferred and the level of concurrency in handling data requests are all explored. To keep the measurements simple to interpret, there is only a single RoI in an event.

#### 3.4.1 Processor Performance

The ability of the Steering process to utilise fully a particular processor is quantified. In particular measurements have been made of context switching rates.

The Steering process is made up of a single input message handling thread and one or more worker threads. Processing request messages are taken from the network and placed on a work queue. Each

Figure 3-11 RoB performance with number of concurrent requesters

Figure 3-12 RoB performance with data size transferred

worker thread waits on this queue until an event is available for processing. A worker thread takes the event processing through to completion before attempting to read another request from the queue. In the course of processing a worker thread will block one of more times pending the receipt of data from a RoB. In the final stage, a decision record is constructed and returned to the originating supervisor.

As the number of worker threads increases, it is expectated that there will be more context switches, ultimately possibly leading to a non-optimal situation in which the threads 'thrash' against each other impeding efficient forward progress. It is also to be expected that moving from a single CPU machine to one equipped with two CPUs should reduce the context switching load. The load on the input thread depends on how many requests for data to RoBs are made for each event. For the 'Electron' data set used here there is on average much less than one RoB data request per event.

Figure 3-14 and Figure 3-15 show the results for single and dual CPU machines respectively. Unlike all other measurements presented in this report, the results in these two plots were recorded on Windows-NT, where suitable tools to make the measurements are integrated into the operating system. Since the handling of thread priorities and associated sheduling algorithms are different for Windows-NT and Linux, it is not easy to extrapolate directly in to the Linux environment.

For a single CPU, it can be seen that in the case of one worker thread a processing request message arrives at the input thread and is placed on the input queue. A context switch happens and the worker thread takes up the processing. In a relatively small number of cases the worker makes a request for RoB data (thereby blocking until the input thread receives the reply), delivers up the result and yields to the input thread for receipt of the next processing request.

In the case of 2 worker threads, the opportunity is present for them to 'fight' between each other and with the input thread. In practice since the number of context switches per event is the same for the worker threads and the input thread it can be seen that there is no switching between the worker threads. However, as the number of worker threads increase the number of context switches per event does increase up to a maximum of just over 5 context switches per event. This is not understood, but is believed to a consequence of the scheduling algorithm intrinsic to Windows-NT.

The maximum context switching rate in this test is just over 10,000 per second. A simple test program, which does nothing but context switch between threads, was used to measure the context switch time as ~\(5~{}\upmu\)s (for a 400 MHz Pentium II, single CPU). Although this was measured under

Figure 3-13 Baseline configuration for Steering processor measurements

Linux it is reasonable to assume a similar number for Windows-NT. Thus, for 3 worker threads and above, 50 ms is spent each second in context switching and amounts to a 5% overhead.

The results for a dual CPU are similar, but the rate of context switches per event is 4-5 times higher than for a single CPU and there are now more context switches per event in the worker threads than the I/O thread (provided that there is more than a single worker thread). At present it is not understood why this should be, except that it is clearly rooted in the scheduling algorithm employed on a SMP machine.

#### 3.4.2 Number of Worker Threads

Worker threads, at any one time are only responsible for handling a single event and can block pending retrieval of data from a RoB. Thus, to get maximum efficiency out of a processor, it is necessary to find a suitable operating point. Figure 3-16 shows a scan over the number of worker threads. The rate, in this particular test, is seen to increase from ~2kHz to just under 3kHz. Optimum efficiency is gained at 5 or more threads, where the CPU is used to maximum effect.

Figure 3-15 Context switching - dual CPU

Figure 3-14 Context switching - single CPU

#### 3.4.3 RoI Data Size

The performance of the Steering will be affected by its ability to handle different size data returned by a RoB. In the result below, the Steering has been operated with a single worker thread (to simplify interpretation of results) and a single RoB.

It can be seen (Figure 3-17) that the throughput is largely invariant up to data size of 1Kbyte, but falls off above this size, with a corresponding increase in event latency. The expected operating point for a final system would be with data sizes of 1 to 2 kbytes, indicating that there should not be any significant performance penalty. Using multiple worker threads, similar shaped curves are obtained, but at much higher rates - e.g. 6.6 kHz for an RoI size of 64 bytes.

#### 3.4.4 Fragment Building

It is quite possible that data for a particular RoI is split across the boundary of two or more RoBs, necessitating the requesting of data from multiple sources for a single RoI and the piecing together of the acquired information. This is the process of fragment building.

In the tests, a single Steering (with 6 worker threads) was kept busy by requests from one or more Supervisors and the number of RoBs and the RoI size were varied. The area of coverage for each RoB was set to cover the whole detector, thereby triggering requests for data to all RoBs for each RoI being dealt with by the Steering. Hence, the number of RoBs in use was equivalent to the number of fragments for an event.

Figure 3-18 shows a nearly linear falloff in throughput as the number of RoBs (and hence fragments) triggered is increased. The results are consistent with multiple data fragments being serialised at the network interface and with an increase in time to handle larger data fragment sizes.

Figure 3-17: Effect of fragment size on system performance

#### 3.4.5 Uni-processor versus dual-processor

To check the effect of uni-processor versus dual-processor systems, tests were run with six identical machines (450 MHz dual Pentium III), but 3 running a uni-processor Linux kernel and the other 3 running a multi-processor Linux kernel. In each case the three machines were configured as one Supervisor, one Steering and one RoB, The RoB data size was set to 64 bytes and there was no simulated algorithm delay in the Steering. Most importantly the input thread in the Steering contained a thread-yield in the polling loop after each failed attempt to receive a message. It was observed that the rate of events was limited by the Steering processor, with the Supervisor and RoB having idle times of 20% or more. When the Steering processor ran on a uni-processor kernel all of the spare cpu time was used in the input polling loop and an event rate of 3.79 kHz was observed using a single worker thread. Under these circumstances increasing the number of worker threads gave no increase in the event rate (in fact a small decrease was observed). However, in the multi-processor kernel whilst the polling-loop used all of the spare cpu time on one of the cpu's the second cpu remained available. In this case the event rate for a single worker thread was 3.53 kHz, but this increased to 6.78 kHz and 7.68 kHz as a second and third worker thread was added, by which time the second cpu was also fully used. Thus under these circumstances it was possible to obtain double the event rate by using a dual-processor.

### Network Performance

This test is designed to explore the SCI network data transfer limit (Figure 3-19). The load on the Steering and ROB nodes is fixed while the network traffic is increased. Starting with a single Steering - RoB pair measurements are made, increasing the number of pairs. At some point, the throughput should start to limit and event latency should start to rise, indicating that the network is starting to saturate. In order to exercise all routes the Steering code randomly selects from the available ROBs and there should be sufficient Supervisor nodes to ensure that the throughput is unchanged by adding one. In order to keep the analysis simple the number of RoIs and the number of fragments are both set to one.

#### 3.5.1 Throughput

Figure 3-20 shows the system throughput as a function of the number of Steering - RoB pairs. Up to the limit of available equipment it can be seen that scaling is preserved and no network limits are reached. Of course, the system throughput varies as a function of RoI data size.

Based on the data rates shown in Figure 3-20 and knowing the message sizes and flow patterns, it is possible to estimate the overall system data rates. These are shown in Figure 3-21 and indicate that even for 16 kbyte fragment sizes the total rate is only 60 Mbytes/s, well within the capacity of the SCI switch. Analysis for different components show that the rates are also well within the capacity of any single SCI adapter.

This provides theoretical verification that the SCI network does not limit system performance.

Figure 3-18 Variation of throughput with number of fragments in an eventFigure 3-21 Overall System Data Throughput

Figure 3-19: Baseline configuration for Network Scaling measurements

Figure 3-20: Overall System Data Throughput

## 4 Summary

Many measurements have been taken encompassing a wide range of operating conditions of the SCI Demonstrator. The overall performance is in line with expectation and in the details there are few effects which are not understood. The performance of individual components has been characterised, thus providing valuable input to the overall trigger modelling exercise.

SCI has proved to be a very good vehicle for evaluating the performance of the other sub-systems. This is largely because the intrinsic SCI performance is much in excess of that required by the current scale of demonstrator, but also SCI circumvents the need for much protocol stack processing which other networking technologies can not avoid.

Of minor concern has been the impact of non real-time aspects of Linux. However, with judicious management of thread scheduling, it has been possible to restore a more deterministic performance, which has been shown, at least within the limitations of the Demonstrator, to preserve scaling.

Also of minor concern, is the ability of a process to utilise fully both CPUs in a dual CPU system. Examination of the Linux kernel source code indicates that thread scheduling in an SMP environment has been optimised for multi-tasking and multi-user use rather than for maximum throughput.

The following points summarise the principle findings of the evaluation :-

* Deterministic and scaling performance requires idling threads to yield the processor.
* Careful use of the system scheduling algorithm has enabled full use of both CPUs on dual-CPU nodes to be demonstrated.
* The Supervisor Emulator can achieve ~11 kHz sustained and 17 kHz peak performance (provided that the system is configured to operate with 3 or more concurrent outstanding events for each Supervisor.
* With the addition of Supervisor Emulators, system performance scales to at least 5 Supervisors, the latter giving a sustained performance of 55 kHz (assuming Steerings and RoBs are not limiting factors).
* The Supervisor Emulator latency (using SteerSink and measured using application code in a pseudo ping-pong mode) is invariant to increasing numbers of Supervisors.
* Data copying and/or network transmission times dominate RoB performance for large RoI sizes.
* The RoB can handle ~6 concurrent requests for fragment sizes of ~1 kbyte.
* Optimum processor utilisation is achieved for 5 or more Steering worker threads.
* Fragment building in the Steering becomes an increasing load for larger fragments and for greater fragment multiplicity.
* RoB pairs.
* Context switching costs 5% CPU overhead for 3 or more worker threads.

## References

* [1] The ATLAS Level-2 Reference Software, ATLAS internal note, ATL-DAQ-2000-019
* [2] Dolphin D321 PCI/SCI Adapter User Manual
* [3] Implementation of the Message Passing Software Layer over SCI for the ATLAS Second Level Trigger Testbeds, ATLAS internal note, ATL-DAQ-2000-028