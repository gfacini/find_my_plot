Prospects for measuring top pair production in the dilepton channel with early ATLAS data at \(\sqrt{s}=10\) \(\mathrm{\,Te\kern-1.0ptV}\)

The ATLAS Collaboration

###### Abstract

We present prospects for measuring the \(t\bar{t}\) production cross-section with the ATLAS detector at a center-of-mass energy of \(10\)\(\mathrm{\,Te\kern-1.0ptV}\) with two leptons in the final state (\(ee\), \(\mu\mu\) and \(e\mu\)). The measurement aims to reestablish the top signal at the LHC with first data and gain confidence with signatures that involve leptons, jets and missing transverse energy. We introduce data-driven background estimation techniques for the dominant non-top Standard Model backgrounds and define control samples for the top dilepton sample. Assuming the Standard Model and an uncertainty of 20% in the early luminosity determination, we expect a measurement of the cross-section for an integrated luminosity of \(200\)\(\mathrm{\,pb^{-1}}\) in the combined channel with a relative uncertainty of \(3.1(\mathrm{stat})^{+9.6}_{-8.7}(\mathrm{syst})^{+26.2}_{-17.4}(\mathrm{ lumi})\)%.

Introduction

The measurement of the top quark pair production cross-section (\(\sigma_{t\bar{t}}\)) at LHC in its different sub-channels is interesting for various reasons. Uncertainties on the theoretical predictions are now at the level of \(10\%\) and a comparison with measurements will allow to test the expectations from Standard Model (SM) QCD. The abundant sample which will be available is also well suited for use as a calibration tool of the reconstructed objects in the detector and algorithms, in particular at the beginning of data taking. Furthermore \(t\bar{t}\) events are often an important background in the searches for the Higgs boson or physics beyond the SM and it is thus of paramount importance to study this process in detail. Finally, cross-section measurements are also an important test of possible new production mechanisms, as a non-SM top quark production can lead to a significant increase of the cross-section. New physics may affect the cross-sections differently in various decay channels.

In this note we study the potential of the ATLAS detector for a measurement of the \(\sigma_{t\bar{t}}\) with final states with two leptons (\(e\) and \(\mu\)). It is assumed that the branching ratio \(BR(t\to Wb)=1\) and leptonic \(\tau\) decays are included in the signal definition. The branching ratio [1] for decay modes to \(ee\), \(\mu\mu\) and \(e\mu\) final states is 6.4%. For the first run we assume a center-of-mass energy \(\sqrt{s}=10\)\(\mathrm{\ Te\kern-1.0ptV}\) and an integrated luminosity of 200 \(\mathrm{pb}^{-1}\).

Top-pair production at the LHC can occur well beyond threshold giving us an opportunity to study the top quark in large quantity as well as broad phase space. A significant fraction of the \(t\bar{t}\) production therefore originates from higher order processes and the availability of calculations for higher order QCD correction is vital. The LO and NLO [2, 3, 4] inclusive \(\sigma_{t\bar{t}}\) calculations [5] have been extended to higher order processes. Improvements on the NLO \(\sigma_{t\bar{t}}\) calculation have been achieved by including soft-gluon corrections by re-summation of large Sudakov logarithms to NLL accuracy [6]. Recently, the prescription was extended to include terms of NNLL accuracy and based on this result, "approximate" NNLO QCD predictions have been derived [7]. The uncertainty due to the mass of the top quark is small, now that the precision of the top mass measurement at the Tevatron is about 1%. It is also noted [7], that the sensitivity of the total cross-section to soft-gluon emission is somewhat different at Tevatron and LHC as the production at Tevatron is largely dominated by parton kinematics close to threshold.

For the first LHC data, the beam energy is expected to be lower than the designed \(7\)\(\mathrm{\ Te\kern-1.0ptV}\). For this study we assume the beam energy to be \(5\)\(\mathrm{\ Te\kern-1.0ptV}\). This reduces the \(t\bar{t}\) cross-section at \(\sqrt{s}=10\)\(\mathrm{\ Te\kern-1.0ptV}\) by 55% compared to \(\sqrt{s}=14\)\(\mathrm{\ Te\kern-1.0ptV}\), giving a value of 400 pb with an uncertainty of approximately 11 % at NLO and 6 % at NNLO, dominated by renormalization and factorization scales and knowledge of the parton distribution functions. For an integrated luminosity of \(200\) pb\({}^{-1}\), we expect about 27 thousand single-lepton events and 5 thousand dilepton events.

## 2 Object Reconstruction and Event Selection

The general selection of \(t\bar{t}\) dilepton (\(ee\), \(\mu\mu\) and \(e\mu\)) candidate events requires a positive decision of a single high-\(p_{\mathrm{T}}\) lepton trigger (Section 2.1). The events are required to have two oppositely-charged lepton (\(e\) or \(\mu\)) candidates with a high transverse momentum of \(p_{\mathrm{T}}>20\)\(\mathrm{\ Ge\kern-1.0ptV}\) (Sections 2.2 and 2.4, respectively). The \(t\bar{t}\) dilepton events are expected to have large missing transverse energy \(E_{\mathrm{T}}^{\mathrm{miss}}\) (Section 2.5), whereas the Drell-Yan events (one of the largest background in the \(ee\) and \(\mu\mu\) channels) are expected to have low \(E_{\mathrm{T}}^{\mathrm{miss}}\). We also require an event to contain at least 2 jets (Section 2.3) with \(p_{\mathrm{T}}>20\)\(\mathrm{\ Ge\kern-1.0ptV}\) that mainly arise from the hadronised b-quarks. The object selections were determined carefully so that the analysis will not rely on variables that are difficult to calibrate with a small amount of data.

### Trigger

The low luminosity conditions that are expected to be prevalent during the early running period at a center-of-mass energy of 10 TeV are favorable for efficiently triggering on top-quark events with leptons in the final state. Single-lepton triggers with low threshold and loose selection requirements can run without any pre-scaling and can by themselves provide a high acceptance \(t\bar{t}\) trigger.

During the early running period, a large fraction of the 200 \(\mbox{pb}^{-1}\) of data is expected to be collected with peak luminosities of around \(10^{32}\) cm\({}^{-2}\)s\({}^{-1}\). The cross-section analysis described in this paper hence assumes the collection of the \(t\bar{t}\) signal events using a single-electron and a single-muon trigger with a \(p_{\rm T}\) threshold of 15 \(\,\mbox{GeV}\). The electron candidates considered for the trigger are required to exhibit a good shower shape and to have an associated inner detector track but there is no explicit isolation requirement.

The trigger efficiencies for the single-lepton triggers can be measured from the data itself using \(Z\) boson decays to leptonic final states. One method of trigger extraction from data is tag-and-probe, where one obtains an unbiased "probe" electron exploiting the \(Z\) peak [8]. To be applied to \(t\bar{t}\) events, the efficiencies should be measured as a function of \(p_{\rm T}\), \(\eta\), hadronic activity of the event, isolation of leptons. The total uncertainty is expected to be of the order of 1% during the early running period. The cross-section analysis in this paper uses the single-lepton trigger efficiencies as predicted from the trigger simulation (Fig. 1).

### Electrons

Electrons are identified in the electromagnetic calorimeter by matching high momentum inner detector tracks to high-energy EM clusters. A cut-based quality requirement identifies signal electrons with high efficiency while keeping the misidentification ("fake") rate from hadronic jets low; the electrons are required to satisfy the medium purity cuts [8]. We require electron \(E_{\rm T}>20\)\(\,\mbox{GeV}\); an additional requirement of isolation \(E_{\rm T}~{}<6\)\(\,\mbox{GeV}\) in the \(\Delta R_{\eta-\phi}\) cone of 0.2 further reduces the fake rate. To maintain uniform efficiency across the accepted regions, we require \(|\eta|<2.47\). We remove electrons in a region (\(1.37<|\eta|<1.52\)) where a significant amount of dead material in front of the calorimeter is

Figure 1: Electron trigger efficiency as a function of \(p_{T}\) (on the left) and \(\eta\) (on the right) for the signal events. The shaded area shows the \(p_{T}\) and \(\eta\) distributions of reconstructed electrons that match true electrons from W decays. The efficiencies are measured with respect to the offline event selection (Section 2.2).

present, and the efficiency is generally lower while the fake rate is higher. Electron efficiency plots are shown in Fig. 2.

### Jets

The ATLAS "cone" algorithm with radius \(\Delta R_{\eta-\phi}=0.4\) was used to reconstruct jets based on the calorimeter tower input calibrated using "H1" cell weights [8]. Jets are required to have \(|\eta|<2.5\) and a transverse momentum of at least 20 \(\mathrm{\ Ge\kern-1.0ptV}\). We do not require jets to be \(b\)-tagged since it has been shown that \(b\)-tagging is not required for initial measurements of the \(t\bar{t}\) cross-section [8], and it will take some time to understand and calibrate with the initial data. Since identified electrons are also reconstructed as jets, those jets near selected electrons are removed to avoid double-counting. For this we remove a jet if there is an identified electron within \(\Delta R<0.2\).

### Muons

We require muons to be reconstructed by both the muon chambers and the inner detector in the region of \(|\eta|<2.5\). A transverse momentum cut at 20 \(\mathrm{\ Ge\kern-1.0ptV}\) is applied to avoid the trigger turn-on region and the isolation \(E_{\mathrm{T}}\) is required to be smaller than 6 \(\mathrm{\ Ge\kern-1.0ptV}\) in a \(\Delta R_{\eta-\phi}\) cone of 0.2. The main source of fake muons are the muons originating from heavy-flavor decay, so we require that there is no identified jet within \(\Delta R_{\eta-\phi}<0.3\) of a muon. Muon efficiency plots are shown in Fig. 3.

### Missing Transverse Energy

Missing transverse energy (\(E_{\mathrm{T}}^{\mathrm{miss}}\)) is associated with particles that escape detection. For example, \(E_{\mathrm{T}}^{\mathrm{miss}}\) is the signature of weakly interacting neutral particles such as neutrinos. It can also come from mismeasurement of the true \(E_{T}\) of the objects, or from backgrounds such as cosmic rays or beam halo. Missing \(\mathrm{E_{T}}\) (\(\vec{E}_{T}\)) is defined by \(\vec{E}_{T}=-\sum_{i}E_{T}^{i}\hat{n}_{i}\), where \(\mathrm{i}\) is the calorimeter tower number for \(|\eta|<4.9\), and \(\hat{n}_{i}\) is a unit vector perpendicular to the beam axis and pointing at the \(\mathrm{i}^{th}\) tower. We define the magnitude \(E_{\mathrm{T}}^{\mathrm{miss}}=|\vec{E}_{T}|\). Corrections are made to the \(E_{\mathrm{T}}^{\mathrm{miss}}\) for the reconstructed objects, including electrons, muons and jets. In addition, the \(E_{\mathrm{T}}^{\mathrm{miss}}\) is also corrected for dead material in the cryostat.

Figure 2: Electron reconstruction efficiencies as a function of electron \(p_{T}\) (left) and \(\eta\) (right) for signal events as predicted from the simulation. The shaded area shows the \(p_{T}\) and \(\eta\) distributions of true electrons from W decays.

The optimization at \(\sqrt{(}s)=14\;\;\mathrm{TeV}\)[8] for the best significance \(S/\sqrt{S+B}\) determined a cut of \(E_{\mathrm{T}}^{\mathrm{miss}}>35\;\;\mathrm{GeV}\) which rejects a large fraction of \(Z\to ee\) (\(\mu\mu\)) events for the \(ee\) (\(\mu\mu\)) channel. To further suppress Drell-Yan background for the \(ee\) and \(\mu\mu\) channels, the invariant dilepton mass is required to be inconsistent with the \(Z\) mass, \(|m_{Z}-m_{\ell^{+}\ell^{-}}|>5\;\mathrm{GeV}\). The \(e\mu\) channel is expected to yield the best signal-to-background ratio, since it does not suffer from Drell-Yan background. The \(E_{\mathrm{T}}^{\mathrm{miss}}\) cut was optimized [8] for the best significance and found to be \(E_{\mathrm{T}}^{\mathrm{miss}}>20\;\;\mathrm{GeV}\).

## 3 Background and Signal Expectation

Several processes can mimic the \(t\bar{t}\) dilepton signal by producing the same signature in the detector (two leptons, \(E_{\mathrm{T}}^{\mathrm{miss}}\) and jets), but with one or more misidentified objects. One example is \(Z\to ee+\mathrm{jets}\) or \(Z\to\mu\mu+\mathrm{jets}\) events, which can have significant \(E_{\mathrm{T}}^{\mathrm{miss}}\) because of resolution effects or mismeasured jet energies (Section 3.1.1). Another example is jets misidentified as leptons in QCD and \(W+\)jets events, which can mimic the \(t\bar{t}\) dilepton signature (Section 3.1.2).

Additionally, a few processes can produce two real leptons, a neutrino, and a number of jets and thus get mistaken for the \(t\bar{t}\) dilepton signal. Events with two gauge bosons (\(WW\), \(WZ\) and \(ZZ\)) or single-top (\(Wt\)-channel) production are an example. These backgrounds are considered small and are calculated based on MC efficiencies (Section 3.2). We also describe the \(t\bar{t}\) acceptance calculation in Section 3.2.

### Data-Driven Methods for Background Determination

We take a data-driven approach to some backgrounds due to difficulties in the MC simulation associated with modeling them, for instance the QCD and the Drell-Yan (DY) contributions. We also cannot trust the MC simulation alone to predict instrumental effects and backgrounds involving the tails of distributions. In this Section we describe procedures to estimate background to dileptons from the DY events and from jets misidentified as leptons.

Figure 3: Muon reconstruction efficiencies as a function of muon \(p_{T}\) (left) and \(\eta\) (right) for signal events as predicted from the simulation. The shaded area shows the \(p_{T}\) and \(\eta\) distributions of true muons from W decays.

#### 3.1.1 Drell-Yan Background

Requiring large \(E_{\rm T}^{\rm miss}\) and a dilepton mass outside the \(Z\) window significantly cuts away the DY background, but does not completely eliminate it. The DY events in the tail of the \(E_{\rm T}^{\rm miss}\) distribution and away from the \(Z\) peak still pass our event selection. We use a data-driven technique to estimate the number of these events; the DY contribution to our signal region is estimated by scaling the MC prediction to match the observed number of events in sideband regions of the data. The method is applied separately for \(ee\) and \(\mu\mu\) events.

Using the grid shown in Fig. 4, we define the columns by the \(Z\) mass window between \(86\,\,\mathrm{GeV}\) and \(96\,\,\,\mathrm{GeV}\). We select dilepton \(ee\) and \(\mu\mu\) events using the selection criteria for leptons described in Sections 2.2 and 2.4. The rows are defined by \(E_{\rm T}^{\rm miss}\) values of \(15\,\,\mathrm{GeV}\) and \(35\,\,\mathrm{GeV}\). We populate the nine bins using a sample of pseudo-data (which contains \(t\bar{t}\), single top, dibosons and \(W\)/\(Z\)+jets) and Alpgen \(Z\) MC samples. The relative admixture of physical processes are different in the different bins. For instance, the \(Z\rightarrow\ell\ell\) signal falls largely in bin "H," whereas \(t\bar{t}\) dilepton events contribute to bins "A", "B" and "C". Using the formulas in Eq. 1, we estimate the amount of DY background in our signal region ("A" and "C") by scaling the MC DY distribution to match the pseudo-data in both the low-\(E_{\rm T}^{\rm miss}\) region as well as in the \(Z\) peak:

\[A_{Est}=G_{Data}(\frac{A_{MC}}{G_{MC}})(\frac{B_{Data}}{H_{Data}})(\frac{H_{ MC}}{B_{MC}}),\hskip 28.452756ptC_{Est}=I_{Data}(\frac{C_{MC}}{I_{MC}})( \frac{B_{Data}}{H_{Data}})(\frac{H_{MC}}{B_{MC}}) \tag{1}\]

In the above equations, the subscripts "Data" and "MC" correspond to the pseudo-data and inclusive \(Z\) MC samples, respectively. The total DY background estimate is given by \(A_{Est}+C_{Est}\). We obtain a final measurement as well as a systematic uncertainty on the estimate by varying both the \(E_{\rm T}^{\rm miss}\) boundaries and the \(Z\) window (see Section 5.8).

#### 3.1.2 Jets Misidentified as Leptons

\(W\)+jets and QCD events with jets misidentified as leptons can mimic the \(t\bar{t}\) dilepton signature, and therefore contribute a significant background to our selection. Due to difficulties in accurately simulating events with jets misidentified as leptons, we estimate this contribution to our data sample in a data-driven

Figure 4: Diagram of \(E_{\rm T}^{\rm miss}\) versus \(M_{\ell^{+}\ell^{-}}\) regions used for Drell-Yan data-driven background estimates.

way. There are two types of dilepton events with fake leptons: events with both a real and a fake lepton (\(W\)+jets) and events with two fake leptons (QCD). Events with two fake leptons typically have small \(E_{\rm T}^{\rm miss}\), so for this analysis they are neglected. The probability for a jet to fake a lepton can be determined by defining an auxiliary loose lepton selection, and measuring the efficiencies of both this loose and the default (tight) lepton selection in two independent event samples, dominated by real and fake leptons respectively.

We define a selection of loose leptons, such that most loose lepton candidates are actually misidentified jets and only a small fraction are signal leptons. We define probabilities for a real and a fake lepton, \(\epsilon\) and \(f\), respectively, to be reconstructed as a tight lepton

\[\epsilon=\frac{N_{\rm tight,real}}{N_{\rm tight,real}+N_{\rm loose,real}},f= \frac{N_{\rm tight,fake}}{N_{\rm tight,fake}+N_{\rm loose,fake}} \tag{2}\]

We measure \(\epsilon\) using the tag-and-probe method in events with two leptons with \(M(\ell,\ell)\) within 5 \(\mathrm{Ge\kern-1.0ptV}\) of the \(Z\) boson mass and \(E_{\rm T}^{\rm miss}<15\ \ \mathrm{Ge\kern-1.0ptV}\). We measure \(f\) separately in two samples dominated by fakes. The first one is a "low \(\Delta\phi\)" sample requiring one tight or loose lepton, \(E_{\rm T}^{\rm miss}>15\ \ \mathrm{Ge\kern-1.0ptV}\), the angle between the lepton and the \(E_{\rm T}^{\rm miss}\), \(\Delta\phi<1\ \mathrm{rad}\). The second one is a "low-\(E_{\rm T}^{\rm miss}\)" sample requiring one tight or loose lepton and \(E_{\rm T}^{\rm miss}<15\ \ \mathrm{Ge\kern-1.0ptV}\). These two selections are orthogonal and are useful for estimating our systematic uncertainty associated with applying the fake rate measured in the single lepton sample to the dilepton sample.

The "low \(\Delta\phi\)" selection rejects 75% (80%) of \(W\)'s in an inclusive \(W\) MC sample for the muons (electrons). The "low \(E_{\rm T}^{\rm miss}\)" selection rejects 98% of \(W\)'s in an inclusive \(W\) MC sample. A signal subtraction is employed to remove the remaining signal events from these samples. We use the "low \(E_{\rm T}^{\rm miss}\)" sample for our fake predictions, the results are shown in Figure 5. Note that while the final estimates for the predicted number of fake events passing the dilepton selection criteria should not be a sensitive function of the choice of loose lepton definition, the tight fake probabilities are sensitive to this choice.

We extract the number of fake events in our selection using the efficiencies and fake probabilities. We consider three different types of events at the reconstruction level: events with two tight leptons, TT, events where the highest \(p_{\rm T}\) lepton is tight (loose) and second lepton is loose (tight) TL (LT). At the truth level there are three different sources of events: events with two real leptons, RR, events where the highest \(p_{\rm T}\) lepton is real and the second lepton comes from a jet, RF, and events where the highest

Figure 5: Tight fake probabilities \(f\) measured as a function of \(p_{\rm T}\) for muons (left) and electrons (right) in a multijet sample.

lepton comes from a jet and the second lepton is real, FR. The two sets of events are related to each other with an efficiency matrix:

\[\left[\begin{array}{c}N_{\rm TT}\\ N_{\rm TL}\\ N_{\rm LT}\end{array}\right]=\left[\begin{array}{ccc}\epsilon_{1}\epsilon_{2}& \epsilon_{1}f_{2}&f_{1}\epsilon_{2}\\ \epsilon_{1}(1-\epsilon_{2})&\epsilon_{1}(1-f_{2})&f_{1}(1-\epsilon_{2})\\ (1-\epsilon_{1})\epsilon_{2}&(1-\epsilon_{1})f_{2}&(1-f_{1})\epsilon_{2}\\ \end{array}\right]\left[\begin{array}{c}N_{\rm RR}\\ N_{\rm RF}\\ N_{\rm FR}\end{array}\right] \tag{3}\]

where \(\epsilon_{1}\) and \(\epsilon_{2}\) (\(f_{1}\) and \(f_{2}\)) are the efficiencies for real (fake) first and second leptons to be reconstructed as a tight lepton.

This equation when inverted yields the number of events coming from each source of events. All events that are reconstructed with two tight leptons and do not come from two real leptons are considered to be fakes. This yields the following expression:

\[N_{\rm Fake}=\left[\frac{f_{2}(\epsilon_{2}-1)}{\epsilon_{2}-f_{2}}+\frac{f_{1 }(\epsilon_{1}-1)}{\epsilon_{1}-f_{1}}\right]N_{\rm TT}+\frac{f_{2}\epsilon_{ 2}}{\epsilon_{2}-f_{2}}N_{\rm TL}+\frac{f_{1}\epsilon_{1}}{\epsilon_{1}-f_{1} }N_{\rm LT} \tag{4}\]

As expressed above, this technique assumes a constant efficiency and tight fake probability. However, it is likely that these will be dependent on various kinematic variables (such as \(p_{\rm T}\) and \(\eta\)). The technique is easily extended if the efficiencies (\(\epsilon_{1},\epsilon_{2}\)) and fake probabilities (\(f_{1},f_{2}\)) are measured as functions of kinematic variables (such as \(p_{\rm T}\) and \(\eta\)). In this case, rather than deriving the fake estimate by weighting event counts (\(N_{\rm TT},N_{\rm TL},N_{\rm LT}\)), the fake estimate is expressed as a sum of weighted events, using the coefficients of (\(N_{\rm TT},N_{\rm TL},N_{\rm LT}\)) in Eq. 4 as the weight for TT, TL or LT events, respectively. This approach has the advantage of providing a coherent prediction in all kinematic variables, rather than a prediction for a specific histogram. For example, one can describe the fake estimate as a function of the number of reconstructed jets in each channel, see Fig. 6.

The technique outlined above should be sufficient for determining the jets faking lepton background to our selection. Only once we are able to look at the control samples in real data it will become apparent what the fake rates are in reality, and what the systematics on these fake rates are (Section 5.9). Once these are available the outlined technique can be used to predict this background.

### Monte Carlo Based Determination of Signal and Background

The expected contributions from signal and from remaining background processes (single top, diboson, \(Z\to\tau\tau\), \(Wb\bar{b}\)) are calculated using MC methods. We also estimate contributions from W+jets and DY MC samples, for which systematic uncertainties are obtained from the data-driven methods described in Section 3.1. The MC sets used in this analysis have passed through a full simulation of the ATLAS detector. Tables 1-3 summarize the estimated contributions for the \(ee\), \(\mu\mu\) and \(e\mu\) channels.

Many of these processes have two high-\(p_{\rm T}\), prompt leptons which are reconstructed. Several of them, however, may produce misidentified leptons, from light- or heavy-flavor jets. For example, production of \(Z\) events in association with jets can produce such an event if one of the \(Z\) decay products is lost and a jet is misidentified as a lepton. This portion of the contribution is already described by the data-driven estimate (Section 3.1).

To avoid double counting, we require the \(t\bar{t}\) dilepton signal and the MC-driven portion of the background to have two real prompt leptons in the truth-record, matched to the reconstructed leptons. In particular, for the \(t\bar{t}\) dilepton acceptance calculation the two leptons are required to be generated from a \(t\to W\to\ell\) or a \(t\to W\to\tau\to\ell\) decay chain. Figure 7 shows that the angular distance between reconstructed and true leptons is very small; we chose a matching distance of \(\Delta R<0.05\).

With this procedure the background contribution in Tables 1-3 have to be increased by those events in the \(t\bar{t}\) dilepton sample that did not pass this truth-matching cut. On average \(\sim 2\%\) of the selected \(t\bar{t}\) dilepton events were found to be in this category.

Figure 6: Fake predictions for \(ee\) (top left), \(e\mu\) (top right), and \(\mu\mu\) (bottom) as a function of the number of jets.

The backgrounds are estimated using theoretical cross-sections. The calculated number of expected background events from a process is given by

\[N_{expected}=\mathcal{L}\times\sigma_{process}\times\frac{N_{passed}}{N_{total}} \tag{5}\]

where acceptance \(A=\frac{N_{passed}}{N_{total}}\) is a ratio of events which pass selection criteria. We take into account weights \(w_{i}\) of the MC events, \(N_{passed}=\sum_{i\in\{passed\ events\}}w_{i}\), \(N_{total}=\sum_{i\in\{all\ events\}}w_{i}\). For real data we will also correct for efficiency differences between MC and data. This is implemented by weighting MC events with appropriate object identification and trigger efficiencies scale factors. The scale factors are defined as \(\epsilon_{\text{data}}^{i}/\epsilon_{\text{MC}}^{i}\), where \(\epsilon_{\text{data}}^{i}\) are identification (ID) and trigger efficiencies measured in data from \(Z\) events, and \(\epsilon_{\text{MC}}^{i}\) are efficiencies measured in inclusive \(Z\) MC. The acceptance is a convolution of kinematic, geometric and reconstruction efficiency contributions.

Dilepton branching ratios include electrons, muons and leptonic tau decays of the \(W\) bosons in the top pair decay. We estimate BR(\(t\bar{t}\to ee\)) = (1.67 \(\pm\) 0.05)%, BR(\(t\bar{t}\rightarrow\mu\mu\)) = (1.64 \(\pm\) 0.05)% and BR(\(t\bar{t}\to e\mu\)) = (3.40 \(\pm\) 0.10)%. We also estimate top dilepton acceptances to be A(\(ee\)) = (16.5 \(\pm\) 0.4)%, A(\(\mu\mu\)) = (26.1 \(\pm\) 0.4)% and A(\(e\mu\)) = (26.5 \(\pm\) 0.3)%.

Based on MC simulation for signal and background, we present the expected kinematic distributions for an integrated luminosity of \(200\) pb\({}^{-1}\). Figures 8-10 show the purely MC based signal and background \(E_{\text{T}}^{\text{miss}}\) and jet multiplicity distributions for each sub-channel. The corresponding Tables 1-3 contain the full list of event types for each MC sample. The uncertainties are due to the limited number of events in the MC samples. Statistical uncertainties for different luminosities are shown in Table 4. These statistical uncertainties assume a Poissonian error on the total number of observed events whereas the error on the background is taken from the MC estimation. The table shows the combined statistical uncertainties.

Figure 7: Minimum angular distance in \(\Delta R\) between reconstructed leptons and leptons from the truth record which originate from \(t\to W\to l\) decays. Overflow is shown at \(\log(\Delta R)=1\).

Figure 8: Missing transverse energy distribution (left) after requiring two opposite signed leptons and jet multiplicity distribution (right) after all cuts (except the \(N_{jets}>1\) cut) for \(ee\) dilepton signal and MC based background estimations after all cuts. The samples are normalized to \(200\,\text{pb}^{-1}\).

Figure 10: Missing transverse energy distribution (left) after requiring two opposite signed leptons and jet multiplicity distribution (right) after all cuts (except the \(N_{jets}>1\) cut) for \(e\mu\) dilepton signal and MC based background estimations after all cuts. The samples are normalized to \(200\,\text{pb}^{-1}\).

Figure 9: Missing transverse energy distribution (left) after requiring two opposite signed leptons and jet multiplicity distribution (right) after all cuts (except the \(N_{jets}>1\) cut) for \(\mu\mu\) dilepton signal and MC based background estimations after all cuts. The samples are normalized to \(200\,\text{pb}^{-1}\).

## 4 Analysis Strategy

### Cross-Section Determination

The expected integrated luminosity in the first year is \(200\,\mbox{pb}^{-1}\) and it will be known with a relatively large (\(\sim 20\%\)) uncertainty. It is important that in estimating the expected uncertainty on the cross-section,

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|} \hline  & lepton sel. & inv. mass cut & \(E_{\rm T}^{\rm miss}\)cut & jet cut & trigger & true & fake \\ \hline \(t\bar{t}\) dilepton & 351 & 322 & 261 & 220 & 214\(\pm\)6 & \(209\)\(\pm\)6 & 4 \\ \(t\bar{t}\) other & 15 & 13 & 10 & 9 & 8\(\pm\)1 & \(0^{+0.1}_{-0}\) & 8 \\ single top & 27 & 25 & 18 & 9 & 9\(\pm\)2 & \(7\pm\)1 & 2 \\ \(Z\to ee\) & 68231 & 16283 & 24 & 14 & \(13^{+2}_{-1}\) & \(11^{+2}_{-1}\) & 2 \\ \(Z\to\tau\tau\) & 156 & 154 & 10 & 7 & \(7^{+2}_{-1}\) & \(6^{+2}_{-1}\) & 1 \\ \(W\to e\nu\) & 126 & 118 & 56 & 7 & \(7^{+4}_{-2}\) & \(0^{+4}_{-0}\) & 7 \\ \(W\to\tau\nu\) & 7 & 7 & 7 & 1 & \(1^{+4}_{-1}\) & \(0^{+4}_{-0}\) & 1 \\ diboson & 145 & 73 & 33 & 3 & 3\(\pm\)1 & 2\(\pm\)1 & 1 \\ \hline sum bkg & 68707 & 16673 & 157 & 51 & \(49^{+8}_{-3}\) & \(54^{+11*}_{-3}\) \\ \hline S/B & 0.0 & 0.0 & 1.7 & 4.3 & \(4.3^{+0.7}_{-0.3}\) & \(3.9^{+0.8}_{-0.3}\) \\ \(S/\sqrt{S+B}\) & 1.3 & 2.5 & 12.5 & 13.4 & 13.2 & 12.9 \\ \hline \end{tabular}

* Including all fakes.

\end{table}
Table 1: Expected number of selected events for the \(ee\) channel selection for \(200\,\mbox{pb}^{-1}\). The errors are only shown for the last two columns for readability, but they are fully taken into account in the summation and the S/B calculation. The column labeled ”true” represents the effect of applying truth-matching cuts. The column labeled ”fake” shows the number of events which failed the truth-matching cuts.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|} \hline  & lepton sel. & inv. mass cut & \(E_{\rm T}^{\rm miss}\)cut & jet cut & trigger & truth & fake \\ \hline \(t\bar{t}\) dilepton & 530 & 490 & 400 & 343 & 332\(\pm\)8 & \(327\pm\)7 & 6 \\ \(t\bar{t}\) other & 9 & 8 & 6 & 6 & 5\(\pm\)1 & \(0^{+0.1}_{-0}\) & 5 \\ single top & 38 & 35 & 27 & 12 & 12\(\pm\)2 & \(11\pm\)2 & 1 \\ \(Z\to\mu\mu\) & 109331 & 24716 & 113 & 49 & \(47^{+3}_{-2}\) & \(44^{+3}_{-2}\) & 4 \\ \(Z\to\tau\tau\) & 263 & 262 & 19 & 11 & \(10^{+2}_{-1}\) & \(10^{+2}_{-1}\) & 0 \\ \(W\to\mu\nu\) & 14 & 14 & 7 & 0 & \(0^{+4}_{-0}\) & \(0^{+4}_{-0}\) & \\ \(W\to\tau\nu\) & 1 & 1 & 0 & 0 & \(0^{+4}_{-0}\) & \(0^{+4}_{-0}\) & \\ \(Wbb\) & 1 & 1 & 1 & 0 & \(0.4^{+0.2}_{-0.1}\) & \(0^{+0.2}_{-0}\) & \\ diboson & 211 & 100 & 45 & 5 & 5\(\pm\)1 & 5\(\pm\)1 & 1 \\ \hline sum bkg & 109868 & 25137 & 218 & 84 & \(81^{+8}_{-3}\) & \(87^{+11*}_{-3}\) \\ \hline S/B & 0.0 & 0.0 & 1.8 & 4.1 & \(4.1^{+0.4}_{-0.2}\) & \(3.8^{+0.5}_{-0.2}\) \\ \(S/\sqrt{S+B}\) & 1.6 & 3.1 & 16.1 & 16.6 & 16.4 & 16.1 \\ \hline \end{tabular}

* Including all fakes.

\end{table}
Table 2: Expected number of selected events for the \(\mu\mu\) channel selection for \(200\,\mbox{pb}^{-1}\). The errors are only shown for the last two columns for readability, but they are fully taken into account in the summation and the S/B calculation. The column labeled ”true” represents the effect of applying truth-matching cuts. The column labeled ”fake” shows the number of events which failed the truth-matching cuts.

our approach incorporates various systematic uncertainties expected in the first year.

The measurement is based on a simple counting experiment, thus we model the observed count \(N^{obs}\) as being Poisson distributed about some expectation \(N^{exp}_{tot}\). The \(tot\) subscript indicates that there are several contributions: i. e. the signal and various backgrounds (indexed by \(k\)).

\[Pois(N^{obs}|N^{exp}_{tot})=Pois(N^{obs}|\sum_{k\in\{sig,bkg\}}N^{exp}_{k}) \tag{6}\]

For each of these contributions, the expectation is simply a product of the integrated luminosity, \(\cal L\), the

\begin{table}
\begin{tabular}{|l|r|r|r|} \hline statistical error \(\Delta\sigma/\sigma\) [\(\%\)] & \(ee\) & \(\mu\mu\) & \(e\mu\) \\ \hline \({\cal L}=50\,\)pb\({}^{-1}\) & 21.7 & 15.7 & 9.9 \\ \(100\,\)pb\({}^{-1}\) & 13.2 & 9.9 & 6.4 \\ \(200\,\)pb\({}^{-1}\) & 8.5 & 6.6 & 4.3 \\ \hline significance \(S/\sqrt{S+B}\) & \(ee\) & \(\mu\mu\) & \(e\mu\) \\ \hline \({\cal L}=50\,\)pb\({}^{-1}\) & 6.6 & 8.2 & 12.3 \\ \(100\,\)pb\({}^{-1}\) & 9.3 & 11.6 & 17.4 \\ \(200\,\)pb\({}^{-1}\) & 13.2 & 16.4 & 24.6 \\ \hline \end{tabular}
\end{table}
Table 4: Statistical error and significance on the cross-section \(\sigma\) for the different sub-channel from MC based signal and backgrounds. The error on the number of observed events is Poissonian and the error on the background estimation is taken from the statistical error in the MC sample.

\begin{table}
\begin{tabular}{|l|r|r|r|r|r|r|} \hline  & lepton sel. & \(E_{\rm T}^{\rm miss}\)cut & jet cut & trigger & truth fake \\ \hline \(t\bar{t}\) dilepton & 908 & 845 & 715 & 698\(\pm\)11 & \(683\pm\)11 & 15 \\ \(t\bar{t}\) other & 24 & 21 & 21 & 20\(\pm\)2 & \(0_{-0}^{+0.1}\) & 20 \\ single top & 59 & 55 & 26 & 25\(\pm\)3 & \(23\pm\)3 & 2 \\ \(Z\rightarrow\mu\mu\) & 110 & 15 & 4 & \(4_{-0}^{+1}\) & \(0_{-0}^{+1}\) & 4 \\ \(Z\rightarrow\tau\tau\) & 421 & 97 & 28 & \(27_{-2}^{+3}\) & \(25_{-2}^{+3}\) & 2 \\ \(W\to e\nu\) & 14 & 11 & 2 & \(2_{-1}^{+4}\) & \(0_{-0}^{+4}\) & 2 \\ \(W\rightarrow\mu\nu\) & 165 & 144 & 20 & \(17_{-4}^{+6}\) & \(0_{-4}^{+4}\) & 17 \\ \(W\rightarrow\tau\nu\) & 11 & 9 & 2 & \(2_{-1}^{+4}\) & \(0_{-0}^{+4}\) & 2 \\ \(Wbb\) & 1 & 1 & 0 & \(0.4_{-0.1}^{+0.3}\) & \(0_{-0}^{+0.2}\) & \\ diboson & 135 & 111 & 11 & 10\(\pm\)1 & 10\(\pm\)1 & 0.2 \\ \hline sum bkg & 939 & 464 & 113 & \(108_{-6}^{+10}\) & \(123_{-6}^{+12*}\) \\ \hline S/B & 1.0 & 1.8 & 6.3 & \(6.5_{-0.3}^{+0.6}\) & \(5.6_{-0.3}^{+0.6}\) \\ \(S/\sqrt{S+B}\) & 21.1 & 23.4 & 24.9 & 24.6 & 24.1 \\ \hline \multicolumn{6}{l}{\({}^{*}\) Including all fakes.} \\ \end{tabular}
\end{table}
Table 3: Expected number of selected events for the \(e\mu\) channel selection for \(200\,\)pb\({}^{-1}\). The errors are only shown for the last two columns for readability, but they are fully taken into account in the summation and the S/B calculation. The column labeled ”true” represents the effect of applying truth-matching cuts. The column labeled ”fake” shows the number of events which failed the truth-matching cuts.

cross-section for the process, \(\sigma_{k}\), and several efficiencies (indexed by \(j\)), \(\varepsilon_{jk}\)1). Thus we have:

Footnote 1): Here we include geometrical acceptances among the \(\varepsilon_{jk}\).

\[N_{k}^{exp}=\mathcal{L}\sigma_{k}\prod_{j}\varepsilon_{jk}\,. \tag{7}\]

Clearly, the most likely value of \(\sigma_{sig}\) will be the one which realizes \(N^{obs}=N_{tot}^{exp}\). This leads to the familiar result:

\[\hat{\sigma}_{sig}=\frac{N^{obs}-\sum_{k\in\{bkg\}}N_{k}^{exp}}{\mathcal{L} \prod_{j}\varepsilon_{j\,sig}}\,, \tag{8}\]

where \(\hat{\sigma}\) denotes the maximum likelihood estimate of \(\sigma\). The intuitive form of Eq. 8 motivates techniques based on background-subtraction; however, that approach makes it less clear how to estimate the uncertainty on the measured value of \(\sigma\). These complications become more severe when systematics are included (for example, the large uncertainty on \(\mathcal{L}\), which appears in the denominator, will no longer be symmetric and it is not uncorrelated to \(\sigma\)). For these reasons, it is preferred to model the observation directly (i. e. Eqs 6 and 7) without background subtraction. Regardless, Eq. 8 remains valid with or without background subtraction.

To incorporate uncertainty on the luminosity, the likelihood function is extended to

\[L(\sigma_{sig},\mathcal{L})=Pois(N^{obs}|N_{tot}^{exp})\times Gaus(\tilde{ \mathcal{L}}|\mathcal{L},\Delta_{\mathcal{L}})\,, \tag{9}\]

where \(\mathcal{L}\) is interpreted as the true, unknown integrated luminosity, \(\tilde{\mathcal{L}}\), is the nominal estimate of the luminosity, and \(\Delta_{\mathcal{L}}\) is the uncertainty on that estimate.

Similarly, the likelihood function can be extended to incorporate uncertainties on the efficiencies \(\varepsilon_{jk}\). This is achieved in two steps. First, we group the sources of systematics, \(\alpha\), such that the corresponding variations in \(\varepsilon\) are expected to be uncorrelated. Next, we vary the sources of the systematics (e. g. jet energy scale, trigger efficiencies, etc.) by the \(\pm 1\sigma\) variations and determine the \(\varepsilon_{jk}(\alpha_{j}^{\pm})\) for each signal and background. A change in the source of the \(j^{th}\) systematic will cause a totally correlated variation among the contributions (indexed by \(k\)). Thus we describe the efficiency as a piece-wise linear function \(\varepsilon_{jk}(\alpha_{j})\), and this parametrized efficiency is used to in place of the nominal efficiency. Additional Gaussian terms are added to the likelihood function to represent the constraints on the \(\alpha\)\({}_{j}\) derived from auxiliary measurements or our assumptions about the uncertainty in the Monte Carlo modeling. This modeling of the likelihood function is repeated for each of the channels, and the total likelihood function is simply the product of the individual likelihood functions. Several of the \(\alpha\)\({}_{j}\) are shared between the channels (for example, the jet energy scale uncertainty), which explicitly introduces correlations between the channels. Thus, we arrive at the final likelihood function:

\[L(\sigma_{sig},\mathcal{L},\alpha_{j})=\prod_{l\in\{ee,\mu\mu,e\mu\}}\left\{ \prod_{i\in bins}\left[Pois(N_{i}^{obs}|N_{i,tot}^{exp})\,Gaus(\tilde{ \mathcal{L}}|\mathcal{L},\sigma_{\mathcal{L}})\,\prod_{j\in syst}Gaus(0| \alpha_{j},1)\right]\right\}\,. \tag{10}\]

### Extracting Measurements from the Profile Likelihood Ratio

The likelihood function can be maximized to determine the maximum likelihood estimate of all the parameters \(\hat{\sigma}_{sig},\hat{\mathcal{L}},\hat{\alpha}_{j}\). We then consider the likelihood ratio

\[r(\sigma_{sig})=\frac{L(\sigma_{sig},\hat{\mathcal{L}},\hat{\alpha}_{j})}{L( \hat{\sigma}_{sig},\hat{\mathcal{L}},\hat{\alpha}_{j})} \tag{11}\]and the profile likelihood ratio:

\[\lambda(\sigma_{sig})=\frac{L(\sigma_{sig},\hat{\hat{\mathcal{L}}},\hat{\hat{ \alpha}}_{j})}{L(\hat{\sigma}_{sig},\hat{\mathcal{L}},\hat{\alpha}_{j})} \tag{12}\]

where \(\hat{\hat{\mathcal{L}}}\) and \(\hat{\hat{\alpha}}_{j}\) represent the conditional maximum likelihood estimates of \(\mathcal{L}\) and \(\alpha_{j}\) holding \(\sigma_{sig}\) fixed. Note, the profile likelihood is always greater than the likelihood ratio, except at the maximum likelihood estimate where they are equal. This means that the curve of \(-2\log\lambda\) is broader than \(-2\log r\), and the difference in the intervals can be attributed to systematics.

Wilks' theorem states that under certain conditions, which are satisfied in this case, the distribution of \(-2\log\lambda(\sigma_{sig}^{true})\) is asymptotically distributed as a \(\chi^{2}\) distribution with one degree of freedom. We utilize this relationship to establish 68%, 90%, and 95% confidence intervals. In order to estimate the expected uncertainty on the signal cross-section measurement we used a hypothetical dataset in which the number of events after all cuts was exactly the Standard Model prediction. The estimates of parameters and their uncertainties derived from hypothetical dataset have been shown to be good estimates of the mean of their respective distributions created from repeated pseudo-experiments.

## 5 Systematic Uncertainties

Systematic uncertainties are calculated by varying a given parameter within its uncertainty and redoing the cross-section measurement. Each systematic is described below. For illustrative purposes, in this Section we count the signal and background events to estimate \(\Delta\sigma/\sigma\) separately for every systematic parameter (Eq. 13).

\[\sigma_{t\bar{t}}=\frac{N_{obs}-N_{bgd}}{A\times\epsilon\times\mathcal{L}} \tag{13}\]

where background estimates \(N_{bgd}\) are MC based \(N_{bkg}^{MC}\) (Section 3.2) and data-driven \(N_{bkg}^{DD}\) (Section 3.1): \(N_{bgd}=N_{bkg}^{MC}+N_{bkg}^{DD}\).

However, to extract the uncertainty on the measured \(\sigma_{t\bar{t}}\), we are using the likelihood method described in Section 4, which takes into account all correlations between different systematic uncertainties.

### Luminosity (\(\mathcal{L}\))

We conservatively expect a 20% luminosity systematic uncertainty for the early data. The systematic uncertainty affects the MC based part of background estimates \(N_{bkg}^{MC}\) and luminosity in the denominator of Eq. 13.

### Trigger/ID Efficiencies (\(\alpha_{\epsilon(e/\mu)}\))

The efficiencies will be extracted from tag-and-probe method from data, and their systematic uncertainties will be determined from varying \(M_{\ell\ell}\) requirement used to define \(Z\rightarrow\ell\ell\) sample. We quote 1% uncertainty on both trigger and the offline lepton identification. We conservatively consider these two efficiencies to be fully correlated, although the exact amount of correlation is yet to be estimated.

### Lepton and Jet Energy Scale (\(\alpha_{e/\mu ES}\) and \(\alpha_{JEWS}\))

We estimate the uncertainty on the lepton energy scale (LES) to be \(\pm\) 1%. The uncertainty due to the limited knowledge of the jet energy scale (JES) is determined by varying the energy of reconstructed jets by \(\pm 5\%\) for all signal and background samples. As a first realistic scenario, the JES variation is doubled for jets with \(|\eta|>3.2\). We correct the \(E_{\rm T}^{\rm miss}\) for the LES and JES variations. The effect of LES and JES on the cross-section measurement is calculated by using the prediction of the background numbers and the selection efficiency from the LES and JES varied samples, but using the total number of observed events from the nominal sample.

### PDF Uncertainties (\(\alpha_{PDF}\))

The uncertainties due to the parton distribution function (PDF) are examined via a reweighting scheme which uses Monte-Carlo truth information about the partons that participate in the hard process [8]. The basic procedure is to evaluate the probability for a certain event with the hard partons of flavors \(f_{1}\) and \(f_{2}\) that have momentum fractions \(x_{1}\) and \(x_{2}\) and a momentum transfer \(Q\). The probability is calculated with the PDF that was used for generation \(P_{0}\) and again calculated with the new PDF \(P_{N}\) and the weight \(w\) for a particular event is:

\[w=\frac{P_{N}(f_{1},x_{1},Q)\times P_{N}(f_{2},x_{2},Q)}{P_{0}(f_{1},x_{1},Q) \times P_{0}(f_{2},x_{2},Q)}\]

The PDFs used for this systematic study are CTEQ6mE (20 parameters), which was used for generation, MRST2001E (15 parameters), CTEQ6.6 (22 parameters) and MRST2006nnlo (15 parameters). Only the resulting variation of the signal acceptance was propagated to the cross-section measurement. The effect on the background was neglected due to the large S/B ratio. The results are summarized in Table 5.

The final uncertainty is the largest of the uncertainties from the errors set or the difference between two central value PDFs from different PDF collaborations (here CTEQ and MRST). Since CTEQ6.6 and MRST2006nnlo are updates of the PDFs used for generation, only these uncertainties from these two PDFs are taken into account. Therefore the PDF uncertainty is largest with CTEQ6.6.

### Initial and Final State Radiation (\(\alpha_{IFSR}\))

Initial (ISR) and final state (FSR) radiation can affect the jet multiplicity of both signal and background events. To study this, we can either compare different models or vary the parameters of one model. The latter strategy has been studied and \(\Lambda_{QCD}\) and \(p_{\rm T}\) cut off for ISR and FSR have been identified as relevant parameters. The parameters were varied in correlation in such a way to maximize and minimize the effect on the measured top mass [8]. The effect of ISR/FSR variation was estimated by calculating signal acceptance with samples generated with varied parameters. The effect on the background was

\begin{table}
\begin{tabular}{|l|c|c|c|} \hline \(\Delta\epsilon/\epsilon\) (=\(\Delta\sigma/\sigma\)) [\(\%\)] & \(ee\) & \(\mu\mu\) & \(e\mu\) \\ \hline statistical \(\Delta\epsilon\) & 2.0 & 1.6 & 1.1 \\ \hline CTEQ6mE & 3.0 & 2.0 & 2.4 \\ MRST2001E & 1.1 & 0.7 & 0.8 \\ CTEQ6.6 & 2.2 & 1.5 & 1.7 \\ MRST2006nnlo & 0.6 & 0.3 & 0.4 \\ \hline \multicolumn{3}{|c|}{difference between PDFs} \\ \hline MRST2001E to CTEQ6mE & 2.0 & 1.3 & 1.5 \\ CTEQ6.6 to CTEQ6mE & -0.3 & -0.3 & -0.2 \\ MRST2006nnlo to CTEQ6.6 & -0.5 & -0.3 & -0.5 \\ \hline \end{tabular}
\end{table}
Table 5: PDF systematics on the selection efficiency \(\epsilon\) for the different sub-channels. For comparison the Monte-Carlo statistical error on the efficiency is also given.

neglected due to the large S/B ratio. The systematic uncertainties are estimated to be 5.2% for the \(ee\), 4.0% for the \(\mu\mu\) and 2.2% for the \(e\mu\) channel.

### Monte-Carlo Model (\(\alpha_{MC\ model}\))

The uncertainty on kinematic distributions predicted by Monte-Carlo simulation can alter both signal and background acceptance. To estimate this uncertainty, we compare predictions from different generators, MC@NLO, AcerMC (LO), and Alpgen. We estimate the uncertainty on the acceptance due to different Monte-Carlo models to be 10% for all the channels.

### Theoretical Cross-Section (\(\alpha_{cross\ sec.}\))

While data-driven techniques are preferred for background estimation whenever possible, some background processes are difficult to estimate without MC simulation. These processes mimic the signal very closely, and therefore it is difficult to obtain a control region suitable for data-driven estimation. In such cases, the MC generators are important for the estimation of both shape and the normalization. The shape uncertainties are accounted for in the previous section by comparing several generators. For the normalization, we rely on published studies.

The \(Wt\) single top process is studied in [9] where scale uncertainty is estimated to be \(\sim 3\)%. However, it is also shown that there is a larger uncertainty due to the \(p_{\mathrm{T}}\) cut used for b-jet veto. As we do not use a b-jet veto in our analysis, we estimate the uncertainty from the variation due to change in b-jet veto over the range of cuts studied in the reference. The PDF uncertainty for \(Wt\) was calculated to be \(\sim 2\)%. We estimate the overall uncertainty on \(Wt\) to be 8%. For diboson processes [10], the scale uncertainty is estimated to be \(\sim 3\)%, the PDF uncertainty is \(\sim 4\)%.

To calculate the uncertainties on our measurement due to theoretical uncertainty on the cross-sections, we vary all Monte-Carlo driven processes by \(5\)% except \(Wt\) for which we vary its cross-section by 8%. As some parts of the systematics such as PDF uncertainty can be correlated, we conservatively vary the uncertainties for all processes in a fully correlated manner.

### Drell-Yan Background (\(\alpha_{Zee/Z\mu\mu}\))

Systematic uncertainty for the DY background (Section 3.1) is obtained by varying both the \(E_{\mathrm{T}}^{\mathrm{miss}}\) boundaries and the \(Z\) window (separately and simultaneously). We move the lower \(E_{\mathrm{T}}^{\mathrm{miss}}\) bound to \(10\,\mathrm{\ Ge\kern-1.0ptV}\) and expand the \(Z\) window to cover 85-97 \(\,\mathrm{\ Ge\kern-1.0ptV}\). These two separate variations give two new estimates of the DY background, and making the shifts simultaneously gives a fourth estimate. We take the average of the largest and the smallest of the four numbers as our estimate, and half the difference between the two as the systematic uncertainty on this estimate. With this approach the systematic uncertainty covers all the values we obtain for different values of \(E_{\mathrm{T}}^{\mathrm{miss}}\) / dilepton mass cut. We conclude that the systematic uncertainty is about 15% for both the \(ee\) and \(\mu\mu\) channels for \(200\,\mathrm{pb}^{-1}\). The systematics is sensitive to the number of events in different \(E_{\mathrm{T}}^{\mathrm{miss}}-M_{Z}\) regions, and therefore we expect it to be higher for smaller datasets (\(50\,\mathrm{pb}^{-1}\), \(100\,\mathrm{pb}^{-1}\)).

### Jets Misidentified as Leptons (\(\alpha_{fake}\))

The largest systematic associated with jets faking leptons arises from measuring the fake rate in control samples and extrapolating to the signal region. Fortunately, we have two different control samples, and we can look at the variation between the two control samples to estimate how much the fake rates varies in different types of events. The difference between the control samples may be smaller than the difference between the control samples and the signal region. Out of caution we take twice the difference between the two predictions as our systematic.

Based on the current studies with QCD Monte-Carlo samples with limited statistics we estimate an uncertainty in the fake rates of 100% for the early data (\(50\,\text{pb}^{-1}\)) for both electrons and muons; and of 50% for the muons and 100% for the electrons for \(100\,\text{pb}^{-1}\) and \(200\,\text{pb}^{-1}\).

### Pile-up Effects

The exact beam conditions for first data are largely unknown, but if we assume that \(200\) pb\({}^{-1}\) of data is produced then it is likely that the LHC will have reached luminosities on the order of \(10^{32}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\). At this luminosity pile-up effects will become noticeable. To estimate the pile-up effects we assume luminosities of \(10^{32}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\), and bunch spacings of 450ns, as these conditions produce the largest average number (\(\sim\) 4) of proton-proton interactions per bunch crossing expected in early data. We compare fully simulated MC datasets that include hits from cavern background and multiple proton-proton interactions to the same datasets without pile-up effects. The effect of pile-up on the signal acceptance is estimated to be 8% for the \(ee\) channel, 5% for the \(\mu\mu\) channel, and 2% on the \(e\mu\) channel. However, there are several effects which compensate and we can't assess it with precision, so we don't include the pile-up systematics to our final estimates of the total systematic uncertainty.

## 6 Results

We present results based on the "Asimov dataset" [8], which represents the expectation of an ensemble of pseudo-data sets drawn from our nominal signal and background expectations. Here we have partitioning of the MC-based background and the "fake" background contributions as if we had a data-driven background estimate for background sources with fake leptons. All cross-section uncertainties are taken to fluctuate in a totally correlated manner; this does not include the signal nor the fake backgrounds, which will be extracted from data driven techniques. The measurements presented are based on the number of events with two or more jets in each channel. We consider an integrated luminosity of \(200\,\text{pb}^{-1}\) with a 20% uncertainty.

Table 6 shows the individual contributions to the relative uncertainty on the cross-section for each of the channels individually and in combination for 200 pb\({}^{-1}\). The first row indicates the statistical uncertainty, the second row indicates the dominant uncertainty from the luminosity uncertainty, and the next several rows indicate uncertainties from individual sources of systematics (determined by fixing all other systematics to their nominal values). The final two lines indicate the effect from all systematics except the luminosity and all systematics together.

While the systematics are grouped such that the sources are uncorrelated, their impact on the cross-section measurement cannot be simply added in quadrature. For instance, if the luminosity is actually higher and the jet energy scale is lower than their nominal values, then the expected number of events may not be very different than the nominal prediction. The correlated effect on the measurement is summarized by a correlation matrix in the fitted parameters of the model (see Table 7).

The log-likelihood curves obtained from fitting each channel individually and combined are shown in Fig. 11. Note the asymmetric nature of the profile likelihood curve introduced by the systematics.

## 7 Conclusion

In this note, we studied the prospects for \(t\bar{t}\) cross-section measurement using the dilepton final states. The emphasis was on the analysis of early data and therefore the object and event selection strategy was kept as simple as possible. The basic strategy was to use a well identified lepton pair and the remaining

[MISSING_PAGE_FAIL:19]

background was removed using the dilepton invariant mass, the \(E_{\rm T}^{\rm miss}\) and the jet multiplicity. The overall selection efficiency (S/B ratio) is \(16.5\%(4.1)\), \(26.1\%(3.8)\) and \(26.5\%(5.5)\) for the \(ee\), \(\mu\mu\) and \(e\mu\) channels respectively.

We studied data-driven methods for the estimation of background. In particular, strategies to determine Drell-Yan and fake background were developed. Uncertainties related to the methods were also estimated. The signal and Monte-Carlo based background were defined carefully to avoid any overlap with the data-driven components.

A range of potential uncertainties were studied in addition to the ones related to the data-driven methods. In particular, uncertainties on jet energy scale, lepton efficiency and Monte-Carlo model turned out to be the largest contribution to the systematics after the uncertainty on luminosity, which is by far the leading constraint on the measurement. On the other hand, with high selection efficiency and the large expected cross-section, statistical uncertainty will not dominate the final uncertainty once several tens of \(\text{pb}^{-1}\text{s}\) of data will be accumulated.

All uncertainties were combined by constructing a likelihood function for each channel. They were fit on the nominal prediction from Monte-Carlo samples and the final sensitivity was obtained from a profile likelihood ratio. The three channels were finally combined by performing a simultaneous fit incorporating the correlations between uncertainties.

Figure 11: The log-likelihood curves for \(ee\), \(\mu\mu\) and \(e\mu\) channels. The solid blue curve is the log of the profile likelihood ratio \(-\log\lambda(\sigma_{sig})\), which includes all sources of systematics. The dotted red curve is the log of the likelihood ratio \(-\log r(\sigma_{sig})\), which can be considered as including only statistical uncertainties. The horizontal green lines indicate 68%, 90%, and 95% thresholds (from bottom to top).

In conclusion, the \(t\bar{t}\) cross-section can be measured realistically using the first data expected from the LHC even at the \(10\;\mathrm{TeV}\) collision energy. While the exact sensitivity depends on the systematic uncertainty extracted from the data, we can expect a competitive result from the first-year physics runs.

## References

* [1] C. Amsler et al. _Phys. Lett. B_, 667:1, 2008.
* [2] P. Nason, S. Dawson, and R.K. Ellis. _Nucl. Phys._, B303:607, 1988.
* [3] W. Beenakker et al. QCD Corrections to Heavy Quark Production in \(p\bar{p}\) Collisions. _Phys. Rev._, D40:54, 1989.
* [4] W. Bernreuther et al. _Nucl. Phys._, B690:81 [arXiv:hep-ph/0403035], 2004.
* [5] M. Cacciari, S. Frixione, L.M. Mangano, P. Nason, and G. Ridolfi. Updated predictions for the total production cross section of top and of heavier quark pairs at the Tevatron at the LHC. _JHEP_, 09:127 [arXiv:0804.2800], 2008.
* [6] N. Kidonakis and R. Vogt. Next-to-next-to-leading order soft-gluon corrections in top quark hadroproduction. _Phys. Rev._, D68:114014, 2003.
* [7] S. Moch and P. Uwer. Theoretical status and prospects for top-quark pair production at hadron colliders. _Phys. Rev._, D78:034003 [arXiv:0804.1476], 2008.
* [8] ATLAS Collaboration. Expected Performance of the ATLAS Experiment, Detector, Trigger and Physics. _arXiv:0901.0512_, CERN-OPEN(2008-020).
* [9] J. Campbell and F. Tramontano. NLO corrections to Wt production and decay. _Nucl. Phys. B_, B276, hep-ph(0506289):109-130, 2005.
* [10] M. Grazzini. Soft-gluon effects in WW production at hadron colliders [arXiv:hep-ph/0510337].