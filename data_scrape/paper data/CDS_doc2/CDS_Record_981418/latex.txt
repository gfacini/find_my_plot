# Physics Analysis Workshop 2006 Summary Report

 Shoji Asai, Ketevi A. Assamagan, Sebastien Binet,

Markus Bischofberger, Paolo Calafiura, James Catmore,

Kyle Cranmer, Kaushik De, Andrea Dell'Acqua,

Amir Farbin, Daniel Froidevaux, Fabiola Gianotti,

Sueh Hou, Osamu Jinnouchi, Peter Loch,

Tadashi Maeno, David Malon, Akira Shibata,

Junichi Tanaka, Monika Wielers, Huaqiao Zhang

Corresponding author:ketevi@bnl.gov

###### Abstract

The third annual Physics Analysis Tools workshop was held in May 2006 at the University of Tokyo, Tokyo Japan. An analysis tutorial was organized for the Asia-Pacific ATLAS groups; and there was a feedback discussion between users and developers. Salient issues in various aspects of the analysis domain, such as interactive analysis in ATHENA, analysis Event Data Model (EDM), Transient and Persistency separation, AOD thinning, structured Athena-Aware NTuple, distributed analysis, were discussed. In this paper, we summarize the various analysis-related issues and the proposed solutions, including new developments up to the time of the release of this document.

###### Contents

* 1 Introduction
* 2 Analysis Tutorial
	* 2.1 Interactive ATHENA
	* 2.2 Event Selection
	* 2.3 Event Display
	* 2.4 Distributed Analysis
	* 2.5 Monte Carlo Truth
	* 2.6 Trigger-Aware Analysis
	* 2.7 Analysis with the EventView
* 3 Feedback from Users
	* 3.1 Analysis Model in Australia
	* 3.2 Analysis Model in China
	* 3.3 Analysis Model in Japan
	* 3.4 Analysis Model in Taiwan
* 4 Distributed Analysis
	* 4.1 Distributed Analysis Use-cases
	* 4.2 LCG User Experiences
	* 4.3 User Issues
* 5 User Requirements
* 6 Interactive ATHENA
	* 6.1 Current Status
	* 6.2 Use cases for more Interactive Tools
* 7 Analysis EDM
	* 7.1 ESD/AOD: Creating A Common Interface
	* 7.2 Unique Identification Numbers for EDM objects
	* 7.3 ILink
	* 7.4 ParticleView (aka UserParticle)
* 8 Transient and Persistent Separation
* 9 AOD Thinning* 10Structured NTuple
	* 10.1 Rationale for Structured NTuple
	* 10.2 Requirements for Adding User Classes
	* 10.3 Adding ESD/AOD objects to Structured NTuple
	* 10.4 Toward Common Classes for Structured NTuple
	* 10.5 Adding ROOT Objects
	* 10.6 Concrete Example of Adding Objects to Structured NTuple
* 11I Navigable4Momentum Issues
	* 11.1 Present Design and Implementation
	* 11.2 Problems with the Design
	* 11.3 New Navigable Design
* 12Conclusions

List of Figures
* 1 An example Event View analysis showing the use of some of the EventViewBuilder tools.
* 2 The Taiwan Tier2, 3 service model.
* 3 The AOD Electron and ESD egamma EDM objects and their constituents. The red (black) lines signify navigable (back-navigable) links.
* 4 Transient/Persistent separation of a class. Two files contain two different persistent versions of a given transient class. When reading the old file, the POOL converter will call the right method to convert the persistent Pers_p0 instance into a Trans instance. The method dispatching is based on the GUID of the persistent classes. Note that there is only one single conversion method to the most recent persistent version of the transient class.
* 5 Evolution of I/O for the McEventCollection class with and without T/P separation. Various cases have been benchmarked: the normalization (no McEventCollection I/O), without T/P separation, T/P separation (as in release 12.0.0), T/P separation replacing doubles by floats (not in production). These data points have been obtained by running 100 jobs writing (and then reading) 1000 \(t\bar{t}\) Pythia events within release 12.0.0 and finally taking the mean of the distribution.
* 6 Static class diagram for the present Jet design.

An example for a class structure using wrappers for Jets. **Note:** this figure does not show the actual implementation of the new Jet class design. It is simplified for clarity and to illustrate the basic concept only.

List of Tables
* 1 ATLAS Participation to 2006 PAT workshop at the University of Tokyo, Tokyo Japan.
* 2 User requirements for distributed analysis versus LJSF and Ganga 22
* 3 Evolution of the disk size for 1000 \(t\bar{t}\) events for various T/P separation schemes: no T/P separation, with T/P separation, with T/P separation replacing doubles by floats in the persistent class.

The agenda consisted of a 2-day tutorial for users, a 0.5-day user feedback discussion session between users and developers, and a 2-day core PAT workshop devoted to issues in Physics Analysis Tools activities. The tutorial, attended by users and developers, covered the following areas:

1. Event Selection with the TAG
2. Event Selection Using the ATHENA-Aware NTuple
3. Event Display
4. Interactive Analysis within ATHENA
5. Distributed Analysis
6. Monte Carlo Truth Tools
7. Trigger-Aware Analysis
8. Analysis with the EventView

This workshop was the first time that the ATLAS Asia-Pacific community (Taiwan, Japan, China and Australia) got together with the aim, first of all, to establish relationships that aren't exclusively based on e-mail and, secondly, to get exposed to the ATLAS software infrastructure

\begin{table}
\begin{tabular}{l|r} \hline Regional Group & Number of Participants \\ \hline Australia & 5 \\ Canada & 1 \\ China & 6 \\ CERN & 4 \\ Europe & 7 \\ Japan & 32 \\ Taiwan & 3 \\ USA & 11 \\ \hline \end{tabular}
\end{table}
Table 1: ATLAS Participation to 2006 PAT workshop at the University of Tokyo, Tokyo Japan.

in general and to physics analysis tools in particular. This workshop wasn't a course or a conference, but a series of work and discussion sessions in which all of them had something to contribute. Indeed, during the feedback session, there were reports from many individuals and groups from the Asia-Pacific region about their local efforts to get themselves going with accessing and analyzing ATLAS data. We also heard about users' own analysis models, i.e., how they integrate pieces of the ATLAS data formats (ESD, AOD, TAG) [3, 4], PAT tools and Distributed Analysis tools to arrive at an analysis model that works for them. There was a constructive discussion among participants about the baseline model for analysis and the necessary additions to the ESD and AOD based on some representative analysis use-cases. The feedback session was an opportunity for the developers to hear frank and constructive comments from users.

The core PAT session followed the approach of the previous PAT workshops held at the University College London, UK (2004) [5] where we defined the Event Data Model (EDM) of the AOD, and at the University of Arizona, Tucson, AZ USA (2005) [6] where we proposed the fundamentals of the EventView. Core PAT and Architecture Team members surveyed the current situation of PAT, discussed and proposed solutions to the salient topics for future PAT developments and integrated requests and comments from users. The gory details of the technicalities were kept at the level which invited users to partake in the discussions, without loss of "depth." Two major points were tackled during the core discussions, namely the evolution of distributed analysis tools and the issues with our current analysis EDM. For the distributed analysis, the status of current available tools was reviewed, i.e., how to locate and access data over the grid, pathena (Panda) on OSG systems, the Light Job Submission Framework (LJSF), and Ganga. Then there was a discussion on how to integrate these various tools so as to present the user with a single user interface for distributed analysis, etc. In the analysis EDM, a review of the major issues was presented by various people, with discussions and concrete proposals. Some the issues affecting the analysis domain and tools include:

1. How to do schema evolution so that evolution and changes in the EDM do not prevent us from reading old data.
2. How to do partial reading of the ESD/AOD so as to gain speed during analysis.
3. How to maintain a common interface in the analysis domain (ESD, AOD), so that analysis codes can run in a transparent way on the ESD/AOD, accessing relevant information on demand.
4. How to use the ATHENA analysis EDM classes in ROOT [7], so that reconstruction and analysis codes developed in ROOT [7], codes that could be of interest to a large number,can be easily ported back and integrated into ATHENA for the benefit of all.
5. How to protect our analysis EDM against evolution in external software such as ROOT, evolution that may affect us, as was the case recently with the ROOT [7] version 5 migration.

In the following sections, we will present a summary of the tutorial, followed by detailed report of the main core Physics Analysis Tools topics discussed during the workshop. Wherever relevant an update is presented to reflect the evolution or progress since workshop proceedings up to the release of this report.

## 2 Analysis Tutorial

An overview of the ATHENA software architecture was given followed by hands-on exercises in the following topics.

### Interactive ATHENA

ATHENA and Python provide an interactive analysis environment, known as Interactive ATHENA. Pylcgdict enables access to C++ classes from Python and is used to create Python EDM classes. All Python commands are available in an interactive session and PyROOT can be used for all ROOT functionality and visualization. The following topics were demonstrated:

* How to retrieve EDM objects from StoreGate
* How to use back-navigation
* How to run user-defined Python scripts
* How to visualize distribution of variables using plot/fill commands
* How to use ROOT commands in interactive ATHENA
* How to browse the contents of POOL with PyPoolBrowser

### Event Selection

ATLAS TAG database is intended to be an event-level meta-data system. The TAG is intended to support efficient identification and selection of events of interest for a given analysis. The expectation is that, in practice, first-level cuts will be possible (and could greatly reduce candidate sample sizes) via queries that are supportable in relational databases. The nominal space budget for the TAG in the Computing TDR is 1 KB/event. The TAG is to be producible from AOD, though TAG databases contain or are linked to sufficient navigational information to allow retrieval of event data at all production processing stages, i.e., AOD, ESD, Raw Data (RDO).

By default, the standard reconstruction, RecExCommon, produces the TAG together with the ESD and AOD, in one step but the TAG is actually produced from the AOD. Since the AOD files are small in number of events, the AOD will be merged before producing the TAG from the merged AOD files. The thus produced TAG is in ROOT format, i.e., it is a ROOT file just like the ATHENA-Aware NTuple [8] is a ROOT file. The idea being that, in a subsequent step, the ROOT files of the TAG will be loaded into the master TAG database and replicated at sites that hold full copies of the AOD (Tier 2 and lower). One can make selections by querying the TAG databases or the TAG ROOT files. This means that there are 2 persistency formats for the TAG:

* ROOT: ordinary ROOT files that can be opened in ROOT just like NTuples
* Relational: TAG content in Relational Databases. The supported database flavors are MySQL [9, 10] and Oracle.

The first attempt to the define the content of the TAG appropriate for fast selection of interesting events was made by the AOD/ESD Definition Task Force; their final report is accessible in [3]. This TAG content definition was used during the Rome Physics Workshop and for the Tier-0 Exercise up to the releases 11.0.X. More recently, the TAG content was reviewed by the TAG Working Group and its final report is available in [4]. This is the TAG content that is implemented from the software _Release 11.4.0_.

One makes selections via queries on the variables (or attributes) that have been defined and appear in the TAG. If the attribute is not defined in the TAG, you cannot use it to make selections. One can have access to the events that pass the selection criteria at the AOD (default), ESD and RDO levels. But the only input data files in the job options are TAG ROOT files or the TAG database. How does the system know where to find the AOD, ESD, and RDO for the selected events? An appropriate file catalog must be provided. To use the TAG for selection and thereby navigate to the data of interest, the AOD, ESD, and RDO files that you require must be registered in a file catalog to which your job has access-in a local _PoolFileCatalog.xml_, or in a site-wide or other shared file catalog.

The TAG definition and content is unique for all ATLAS. And one can only make queries on the variables or attributes that are in the TAG. In some cases, one may want to define some personal (or group) analysis dependent TAG attributes and make queries on them: for example, suppose that you are studying the Missing \(E_{T}\) tail and you have a variable in your ATHENA-Aware NTuple [8] which is the difference between the reconstructed and the truth missing \(E_{T}\):there is no such variable in the common definition of the TAG. The ATHENA-Aware NTuple provides a way for the user to extend the TAG by making queries on the variables that the user has put him/herself in his/her own NTuple: this means that the ATHENA-Aware NTuple is just like the user's version of the TAG but it is also a ROOT Tuple. So one can use the ATHENA-Aware NTuple in ATHENA jobs for event selections in the same way as the TAG: this is what makes the ATHENA-Aware NTuple "ATHENA aware". Making queries on the ATHENA-Aware NTuple was also demonstrated during the tutorial, the details of which can be found here [11].

### Event Display

Atlantis [12] is a Java-based event visualization program. JiveXML [13] is the package containing the ATHENA Algorithm/AlgTool that accesses the event data to be displayed by Atlantis and puts it in an XML format that Atlantis can read. The XML data can be either stored in files (one per selected/processed event) or sent directly to Atlantis via the network (or both). In the process of your analysis, you might want to inspect some interesting or problematic events -- which one can select with the TAG or the ATHENA-Aware NTuple -- in the Event Display.

One may put the event data in the JiveXML files and in a subsequent step load the JiveXML files in Atlantis. Alternatively, one may use Interactive ATHENA, as described above, to pass the event data directly to Atlantis (one may still produce the JiveXML files in the process for later inspection).

How to select the interesting events with the TAG or the ATHENA-Aware NTuple, how to pass the selected events directly to the Event Display using Interactive ATHENA (or how to create the JiveXML files and read them in the Event Display) and how to inspect the events were demonstrated and exercised in this tutorial where various tools, namely the event selection with the TAG or the ATHENA-Aware NTuple, the Interactive ATHENA, and the Event Display are integrated into a single analysis session [14].

### Distributed Analysis

It is a mandatory role of the PAT group to connect end-users to distributed analysis activities. There are two major use-cases when one use the grid for her/his analysis: One use-case is to access datasets which have been produced in the grid, and the other use-case is to submit analysis jobs to the grid.

The dq2 end-user tools allow users to access datasets over the grid. Those tools are designed to insulate grid complication from users. There are four tools; dq2_get, dq2_ls, dq2_put and dq2_poolFCjobO. One can use dq2_get to copy datasets from the grid, use dq2_ls to list datasets in the grid, use dq2_put to register private datasets to the grid, and use dq2_poolFCjobO to build a PoolFileCatalog.xml.

When one submits analysis jobs to the grid, the following procedure is required. First, one customizes ATHENA on the local computer for her/his analysis, e.g., modifying job-options and/or source code. Second, the customization needs to be exported to the grid somehow because that is not included in the distribution kit. Finally, the customized ATHENA runs in the grid. pathena has been developed to simplify the above procedure. It provides an easy interface consistent with ATHENA and the user can submit jobs to the grid in the same run-time environment of ATHENA. Therefore, the user doesn't have to learn so much.

We demonstrated how to use the dq2 end-user tools in the first half of the tutorial, and how to submit jobs using pathena in the second half.

### Monte Carlo Truth

This tutorial aimed at exercising a few classes of the Monte-Carlo area. The idea was to present how one can interact with the non-ATLAS classes like HepMC[15] and how they fit into the ATHENA framework. We used the UserAnalysis package and customized the AnalysisSkeleton algorithm for the sake of the exercise.

In the first part of the tutorial, we demonstrated how to retrieve the main collection of Monte-Carlo particles which are organized as a tree of HepMC::GenParticles connected by HepMC::GenVertex, from the StoreGate service. Using the MCVtxFilterTool class, we filtered the AOD Monte-Carlo collection to keep only a few particles, based on some decay vertex pattern matching, and stored the new collection into StoreGate, allowing downstream algorithms to use it. We tried to emphasize the fact that these vertex patterns can be modified at the job option level without requiring any C++ code modification or recompilation. We also demonstrated how to iterate over particles of a HepMC::GenEvent, fetch their decay vertex and fill some histogram distribution.

The second part of the tutorial was dedicated to the TruthParticle class and its purpose. Indeed, this class has been created to allow ATHENA reconstruction algorithms written for reconstruction which are using the specific ATLAS classes to run on Monte-Carlo objects. The HepMC classes don't have the same interface as the ATLAS ones, so one would have to write and keep in synchronization two versions of a given algorithm. By adapting the interface of the HepMC classes to the so-called INavigable4Momentum interface, one can overcome this issue and allow a physicist to run her very same piece of analysis code over either reconstruction or Monte-Carlo particles by just a StoreGate key switch. We introduced the TruthParticleCnvTool class which captures the technical issues and presents a simple method to convert a HepMC::GenEvent1 into a TruthParticleContainer.

### Trigger-Aware Analysis

This part of the tutorial was aimed at encouraging physicists to include the trigger response in their physics analyses. In a typical physics analysis the user wants to know if the event was accepted by the trigger. He wants to be able to ask the following question

* Did the event pass the trigger selection?
* Did the event pass the selection of a given trigger menu item?
* Which trigger menu items accepted the event?

This information is stored in the TriggerDecision object. In this tutorial (see [16]) the user was shown how to re-evaluate the trigger decision for the single electron trigger from the objects stored in the AOD or ESD [17] and how to interrogate the TriggerDecision class to find out if an event has been accepted or rejected at the different trigger levels.

The first part of the tutorial was intended to familiarize users with the trigger reconstruction steering and how to re-do the trigger selection for the single electron trigger at the second level trigger step (Level-2). This will be needed in the future if a user wants to use the latest set of optimization cuts, to debug the algorithms, or to set-up new trigger menu items. This is done by re-running the Trigger Steering requesting only to run the hypothesis algorithms. Looking at the debug output the user was able to follow the different steps in the trigger chain. In the second example the user learned how to retrieve the TriggerDecision and find out how many of the top events are accepted by the single object trigger which selects efficiently isolated electrons with \(E_{T}>25\) GeV after the different trigger levels. In the final system this will be the main trigger information stored in the AOD/ESD. The TriggerDecison has been fully functional since release 12.0.1. For the tutorial the TriggerDecision was created 'by hand' for a few electron triggers.

This tutorial was the first of its kind in ATLAS and will be updated and expanded in the future to reflect the changes in the implementation.

### Analysis with the EventView

The concept of EventView was discussed in the PAT workshop in 2004 [5] and it is implementation was proposed in the next workshop in 2005 [6], shortly after which the first implementation followed. The EventView as a framework consists of several parts: the EDM class EventViewwhich holds pointers to the AOD objects and the related user data calculated during the course of analysis; and EventViewBuilder which consists of tools necessary to build EventViews. The EventView class acts as a proxy to StoreGate and the objects stored in StoreGate, thereby enabling users to construct a consistent "view" of an event with overlaps removed between the objects inside that view. The EventView tutorial done in this workshop went through these concepts as an introduction and sections of the tutorial covered more of the later additions to the framework as described below.

In addition to the building of a consistent view and the use of UserData as a useful container to store variables, EventView framework offers a further possibility in our analysis model, namely, modular analysis. Modular analysis helps us in various ways.

* It enables us to produce generalized tools, which reduces duplicated efforts in the collaboration;
* Modules can be passed around and plugged into other analyses, providing a means of communication;
* Common tools are debugged more thoroughly than private tools and
* Modular analysis is a more intuitive and productive environment to design your analysis.

The first stage of building an EventView for an event is done by inserter tools each of which applies pre-selection to a given set of AOD objects and inserts them into an EventView after overlap removal. Inserters are AlgTools which are scheduled by an EventViewToolLooper algorithm in the specified order. Once the insertion is complete, further analysis can be done by using the same ToolLooper, scheduling tools that manipulate EventView contents. Each tool can be made self-consistent so that they can be used more than once independent of the context of the analysis to be in line with the modular analysis concept.

Recent developments in the EventView framework were focused on this aspect. EventViewBuilder now consists of several sub-packages in addition to the EventViewToolLooper and EventViewInserters, including the following:

* EventViewCombiners, which includes tools for combinatorics;
* EventViewConfiguration, in which python classes for new configuration scheme are defined;
* EventViewDumpers, tools to output EventView including screen output and NTuple output;
* EventViewSelectors, for applying cuts;* EventViewTransformation, used for operations that modify EventViews including calibration and
* EventViewUserData, which includes calculator tools to store data in UserData and associator tools which associate one object in an EventView to one or more other objects.

The figure 1 illustrates the usage of some of these tools in the context of an EventView analysis. In analysis with the EventView, particles are inserted into EventView and then some of the properties of the inserted electrons are calculated including information on their associated tracks. SimpleCut tool is used to chose the events with some requirements which triggers the JiveXML tool to dump the event. Next step in the example is an EVModule which consists of a collection of tools packaged in a form of a python module. This is a recent addition to the framework which was shown in the EventView tutorial in this workshop.

The addition of EventViewConfiguration was inspired by the forthcoming "configurable" scheme which is planned to replace the current job steering method. Introduction of configurable allows us to include much more structure and ability to pass around configured objects in our job configuration. EventView framework is an ideal place to apply these new possibilities where a number of tools are highly general and configurable and each of the tools may do very little work by itself. The concept of creating modules out of available tools thus enables us to create an object which is more representative of a broader part of the analysis under construction.

Overall, the current analysis model provided by EventView framework at the highest level is to form analysis mostly based on python through the use of EVModules by configuring general tools written in C++. Additional functionalities can be easily added by users by writing their own tools. However, previous use cases are also supported and will continue to be supported, so one can choose not to use the new configuration or not even the EventViewBuilder tools and use EDM EventView object solely with user code.

## 3 Feedback from Users

### Analysis Model in Australia

In Australia, the particle physics groups of the University of Melbourne and the University of Sydney are part of the ATLAS collaboration. The ATLAS group in Melbourne consists of 3 staff members (academics and post-docs), 3 PhD students and 4 diploma students. In Sydney, 2 staff members and 2 PhD students are working for ATLAS. Given the rather small size of the groups and the remote location of Australia, there is traditionally a strong collaboration between the groups of the two universities. Both Sydney and Melbourne are also involved in the BELLE

[MISSING_PAGE_EMPTY:14]

\((H\to 4e)\) and SUSY Higgs searches \((H/A\to\tau\tau)\), searches for SUSY particles (\(\tilde{t}\), mSUGRA co-annihilation region) and others. In general, there is a tendency toward a specialization on electrons and missing transverse energy in the group that is guiding the choice of physics analyses.

Up to now, our analysis work is mainly based on CBNT, however, people are slowly starting to work with AOD and ATHENA-Aware NTuples. Few people have also started to use EventView, however, so far mainly as an easy means to create ROOT NTuples from AOD. Diploma students, who generally do ATLFAST studies, will keep using CBNT for the moment since working with these requires less computing knowledge.

Local clusters in Melbourne and Sydney, where some recent kit releases of ATHENA have been installed, are used for ATLFAST and full reconstruction of private data samples, as well as for running physics analyses on AOD. Software development and validation work, that has to be done with the nightly ATHENA releases, is done on LXPLUS. Due to the slow network connection to CERN and the limited storage space, it would however be preferable to provide a local environment for this work. The usability of AFS is currently tested, however, there is still some effort needed to achieve reasonable speed. Further, as soon as funding is assured, an Australian Tier2 facility will be built in Melbourne which will be connected to the Tier1 in Taiwan and provide access to a large fraction of the available AOD.

### Analysis Model in China

There are several institutes and universities involved actively in ATLAS collaboration in some areas. Institute of High Energy Physics (IHEP), CAS is studying in ttH, H->WW for direct measurement of Yukawa coupling, based upon full simulation AOD. Under a very positive collaboration with Centre de Physics des Particles de Marseille (CPPM) of France, the signal of ttH,H->WW (2 leptons and 3 leptons final states) and main backgrounds (ttbar, ttW, ttZ) Rome samples are ready, and preliminary result is finished. CSC samples preparation and related analysis are ongoing: With experience at Tevatron/D0, University of Science and Technology of China (USTC) aims at physical channel as e \(\mu\) final state, for Lepton Flavor Violation (LFV) search, and di-boson study. They are also interested in high \(p_{T}\) lepton trigger and reconstruction efficiencies etc. Beside, they collaborate with the University of Michigan on Muon installation and calibration; Shan Dong University (SDU) is doing ttbar full simulation AOD analysis for spin correlation study, with collaboration of CPPM, France, and they collaborate with Israel (Weizmann Institute) on TGC installation and calibration; people in Nan Jing University (NJU) plan to do SUSY physics.

Computing sources are developing in China. Beijing Tire-2 prototype is well set up and LCG2 CA assured by IHEP is available, EUChinaGrid International collaboration goes well. USTC has a small LCG farm -- up to 20-30 CPU -- under construction. There is a small farm which plans to make up to 20 CPU is available in SDU; and they work on LCG working environment Installation and Monitoring. In NJU, there are 128 CPU available, another cluster is under construction.

### Analysis Model in Japan

The analysis working group in Japan consists of several universities and the research labs. In principle, there is no boundary between the institutes, and information and work are transparent among the groups. Current (wo)men power (as of May 18th, 2006) dedicated for the analysis work are estimated roughly as \(7\) (staff level, including post-docs), \(\sim 10\) (students) out of roughly \(70\) people in ATLAS Japan society.

For the moment, because analysis based on ROOT is faster than one on ATHENA, CBNT based analysis is suitable for the analysis, where we iterate the compilation of the codes and the execution of the programs. We have an original simple framework for CBNT based analysis, that is, to retrieve CBNT trees and to fill them as objects (C++ classes). On the other hand, some users move to AOD/ESD based analysis since contents of AOD have improved, particularly, to access outputs of full simulation. We also try to use ATHENA Aware NTuple, which makes it simple to access RDO/ESD/AOD of the interesting events. In the future, we will use ATHENA, for example EventView, for the pre-selection to provide AAN. Then the analysis is carried out on the AAN using ROOT or ATHENA. This is one of the prospective candidate schemes for our analysis.

Regarding the usage of various PAT tools in our group, first of all, most of the current work on AOD/ESD is based on the direct access by C++ implementation. So far the usage of the Event View has been very limited, and our group has not been migrated to use this tool yet. We hope the practical tutorials like during this workshop, good use cases and examples will give us enough motivation to officially use this in the end. For the other tools, such as, interactive ATHENA, TAG selection, Event Display, and distributed analysis, up to now, our usage has been limited, however these tools will be necessary items for any physics analysis in coming years, and we definitely need to learn them. Thus the tutorials were very useful as an introduction for PAT beginners.

### Analysis Model in Taiwan

At Academia Sinica in Taiwan, we are constructing the LHC Asia Tier1 Grid and a Tier2 for both ATLAS and CMS experiments. In addition, we maintain a remote CDF analysis facility (CAF) that is affiliated under the Open Science Grid. In order to have a streamlined management, we integrate Tier2 with CAF, and meanwhile construct a Tier3 User Interface (UI) for user access to Grid facilities. We discuss issues which requires further guidance by ATLAS offline management.

The user level coding practice of our members has been relying on direct login to CERN Linux cluster (lxplus) where the user has the correct ATLAS software and access to afs file system. However, occasional network slowness and limited storage are serious problems.

Our goal for the local service is to provide an lxplus-like login environment, and in addition, a platform for job submission to Tier2, 3 clusters with file access and storage via Grid tools. These tasks can be conducted on the UI nodes where registered users login in a similar manner as to lxplus. A few configuration problems were encountered which may be easily tailored by the ATLAS offline team. The Tier 2,3 service model at Academia Sinica is illustrated in Figure 2. The service components are:

1. A common Condor batch system accepting jobs from LCG (Tier2), OSG gatekeepers, and jobs from CDF head-node through Condor-glide mechanism. Local users enjoy a higher priority by direct submission to Condor.

Figure 2: The Taiwan Tier2, 3 service model.

2 User Interface nodes where users login and setup to CDF or ATLAS environment. Users share a common home area and a large NFS storage. An afs client is running for access to CERN afs. Grid tools required for file copy are available via gatekeeper installation.
3 Gatekeepers, head-nodes are private to each Grid and experiments, such that the individual experiment flavor does not interfere with each other.

Currently our users do CBNT analyses on their desktop. It is foreseen that their analyses will migrate to ATHENA, on desktop or the newly instrumented UI machines. A uniform installation instruction for Tier2, 3 service is necessary for streamlined management and easy access to users who need Grid tools.

## 4 Distributed Analysis

The discussion sessions on the distributed analysis tools consisted of an overview of the use-cases for distributed analyses -- making the distinction with distributed production --, a report on the user experience and feedback on the currently available distributed analysis tools and the possible integration of the various tools so as to present users with a single, common, integrated user-interface to distributed analyses.

### Distributed Analysis Use-cases

The first concrete use-case that was discussed can be summarized as follows: assuming a large chunk of physics events becomes available from managed production. AOD files are available on grid. The user wants to run analysis on 1000 AOD files, i.e., one million events. There are several scenarios:

* Possible Scenario 1: browse all production tasks to find the datasets containing 1000 files. The files are copied using dq2_get to local workstation and the user runs ATHENA interactively to analyze files. **This is simple use of grid and it is possible today. However, one needs lots of local storage and a long time to run analysis on the 1000 AOD files.**
* The above scenario can be improved upon: the user runs **pathena** over the 1000 files. The advantage of doing this is that no local computing resource is necessary, and the execution time is shorter. However, it is not easy to understand job failures and it takes time to repeat this process after each code change by the user.

* Further improvement can still be made to the above scenarios: the user runs **pathena** over the 1000 files to create ATHENA-Aware NTuples. In this case one only runs once then analyzes the NTuples. Still a robust **pathena** service is needed.

For all the scenarios, the user should test the analysis code on a few files. Running on a large number of files should takes place only a few times for final results.

The second use-case is the following: ten million physics events become available from managed production. Raw data (RDO) files are available on grid, 200000 files. The user wants to run analysis on 20,000 RDO files (1M events). The following scenario may be considered:

* The user requests new task to create AOD files (through physics groups). Once the AOD are produced, this reduces to the previous use-case discussed above. It is possible to do this to day, one only needs access to a web browser. However, one must wait for task to be completed by managed production.

**The impossible scenario would be the user runs pathena or any other tool to analyze RDO files.**

The third distributed analysis use-case is this one: the user needs a small private Monte Carlo signal production, about one thousand events. The jobs can be run using standard transformations in the official release. One may run Ganga or any other client tools. This is possible today. However, many different tools are not easy to learn, and there is a competition for resources used by managed production.

The fourth use-case discussed during the workshop can be summarized as follows: The user needs a private Monte Carlo signal production, more than a thousand events. The situation for regional production is similar. The following scenarios may be considered:

* The user requests new task through physics groups. This is possible today. However, one must go through the production or the physics coordinators to request tasks.
* In the future, each physics group will have a production manager. Users will be allowed to make task requests to the physics group production managers.

**The difficult scenario would be to run pathena/Ganga/Atcom/ARC or any other tool to create private samples. This is technically possible, but not encouraged!**

Since the workshop, the following additional use-cases have been discussed:

* Event selection with the TAG and collection of the events of interest with subsequent analysis done only on those events. It requires as output the AOD, or the ESD (or in some cases the RDO) of the selected events. It is not clear how this would be realizedor managed in the distributed analysis environment as there are issues on both the input and the output of this task: on input, although the input dataset is the TAG database or the TAG ROOT files, one needs to pull in the original AOD and ESD to access the selected events, i.e., issues of back-navigating the AOD or the ESD of the selected events in the Grid environment. On the output, clearly, one does not expect (or should not allow) users to produce private datasets of AOD or ESD of the selected events since this looks like some sort of a mini-production. Restricting the TAG selections to the AOD level and not outputting the AOD of selected events appears to be the more concrete option in distributed analysis today. However, the possibility to make first pass selections so as to reduce the candidate size -- or to extract into personal files a fraction of the datasets -- and subsequently analyze only these candidates should be realized in distributed analysis somehow.
* The user finishing analysis in ROOT on the ATHENA-Aware NTuple. She notices a few problematic/interesting events and wants to access full data (ESD) of those events by creating an ESD file of those events for detailed inspection such as in an Event Display. Here, the selection is done directly with the ATHENA-Aware NTuples which constitutes the input dataset. This case presents the same issues in the distributed analysis environment as ones discussed above in the context of the TAG based event selection.

### LCG User Experiences

Grid tools are now widely used across the physics groups to retrieve production system data for feasibility studies. An increasing number of users are also using Grid computing power (principally the LCG) to run analysis code directly, thereby avoiding the staging of large files onto their local systems and also taking advantage of the huge resources available. Additionally, distributed computing is also being used for small scale specialist Monte Carlo productions (using, for instance, new event generator models or reconstruction algorithms) which are not covered by the production system.

Distributed analysis and production inevitably involve the submission of multiple jobs and the management of large numbers of files. Low-level Grid commands are not suitable for this as the submission of the jobs and the retrieval of the results would be laborious. Instead for all practical purposes a job management tool must be used. As far as the LCG is concerned, two tools are currently being used - the joint LHCb-ATLAS tool GANGA (Gaudi And Grid Alliance) [18], and the ATLAS product LJSF (Light Job Submission Framework) [19]. The performance of these two tools was compared and contrasted during the workshop.

When distributed analysis is used "for real" the data will be in the form of tagged files which will be identified by the user through the AMI database. Currently all the "data" is from Monte Carlo productions and is identified through the use of centrally managed dataset numbers. A non-exhaustive list of requirements a job management tool would need to meet in order to be useful to the physics community might be as follows:

* The tool should be straightforward, reliable and should work "out of the box", enabling a new user with some prepared analysis code to begin work within an hour or two. In the event of a failure it should certainly not churn out incomprehensible (and dispiriting) error messages, but should give a clear and coherent diagnosis of the problem.
* There should be an adequate means of handling large numbers of jobs, which means the automatic generation of multiple JDL (job descriptor language) files which are ultimately submitted to the LCG. It also requires that some means of job splitting must be provided, such that files with large numbers of events can be broken down into smaller pieces to make the most efficient use of the resources.
* It must be simple for users to patch in their own ATHENA algorithms and run-time job options.
* There should be means for the user to monitor the progress of the submitted jobs.
* There should be an automated means of collecting the output of the jobs and placing the log files into a single space on the user's local machine.
* Large output files should be staged smoothly and transparently to Grid storage elements.
* The framework should not be dependent on a single facility.
* If it can be arranged technically, users should be able to access the computing power and storage capacity of all of the grids, not just the LCG.

Thus far, the physics groups have used the **Light Job Submission Framework** for performing distributed analysis and Monte Carlo production. This is a set of shell and Python scripts which are specifically designed for the running of ATHENA jobs, through the Job Transformations mechanism. JDL files are automatically generated and submitted to the Grid according to the user's requirements, and are simultaneously registered on a database at INFN in Rome. This database permits the monitoring of the progress of the submitted jobs either through a command line interface, or through a web browser. The jobs are then collected and validated with another automated script, the validation amounting to a check on the grid output status, and the number of properly registered output files. Large files are moved either to the storage element closest to the site where the job ran, or alternatively copied to a specific facility. The logical file name is registered on the LFC and the log files are moved into the user's local space.

With the LJSF, an analysis job is submitted for every AOD file in question. The NTuples thus produced are then staged to the user's machine, where they are merged and inspected through the use of the ROOT TCChain mechanism.

The second tool, **Ganga**, is a more general product which is ultimately envisaged as the software which will be used for running distributed analysis when data taking begins. It operates either through a Python interpreter or via a graphical user interface. A number of "plug-ins" are provided for various tasks, including the running of ATHENA Monte Carlo and analysis jobs. Ganga hides more from the user than does the LJSF, and is in general easier to operate from scratch. At the time of the meeting in Tokyo a number of criticisms were raised with respect to inadequate documentation, especially with regards to the ATLAS plug-ins, but since then these problems have largely been addressed. The job splitting and monitoring capacity of the tool has also undergone considerable development since PAT2006, and Ganga and LJSF are at the time of writing largely equivalent in their functionality. Nevertheless a number of features are still missing from both tools - particularly access to other Grids. Table 2 lists the requirements as stated at the start of this section, and gives an assessment of how the two tools meet these needs.

### User Issues

GANGA and pathena integration is an important issue because users desire a common tool to submit jobs to any grid flavor. GANGA has an interactive interface, a nice GUI and an all

\begin{table}
\begin{tabular}{|c|c|c|} \hline  & LJSF & Ganga \\ \hline Easy for a first-time user & No & Yes \\ Works “out of the box” & Yes & Yes \\ Multiple JDL generation & Yes & Yes \\ Job splitting & Yes & Yes \\ User code/job options & Yes & Yes \\ Automated output collection & Yes & Yes \\ Automated validation & Yes & No \\ Job monitoring & Yes & Yes \\ ATHENA-specific features & Yes & Yes \\ Smooth staging of files to SE & Yes & Yes \\ Independent of a single facility & No & Yes \\ Access to non-LCG grids & No & No \\ \hline \end{tabular}
\end{table}
Table 2: User requirements for distributed analysis versus LJSF and Ganga purpose mechanism for job submission via plug-ins. The interactivity of GANGA is essentially an interactive session of Python. However, the interactive ATHENA also provides an interactive environment based on Python, so it could be extended for distributed analysis. GUI is not so important because it is heavy and slow. One possible scenario for the integration would be for pathena to use the plug-ins so that pathena could be used for all grid flavors.

An analysis job is split to many sub-jobs, so one has to check all sub-job status to confirm completion of the analysis job. It would be nice if the distributed analysis system provides some notification mechanism for job completion. Completion of a job is essentially completion of the output dataset. New DQ2 site service sends a callback (HTTP request or e-mail) as soon as a dataset is completed. This mechanism may be used for this purpose.

pathena_util has been included in ATHENA release since 12.0.0. pathena records job history using the Python shell module and pathena_util provides several book-keeping functionalities, such as re-submitting jobs, killing jobs, and listing jobs.

People have requested a capability to merge AAN. There are two approaches. One approach is to copy AAN from the grid to a local computer using dq2_get, and then to merge them. addAANT is a ROOT macro and has been introduced to ATHENA release since 12.0.0. The other approach is to submit a job to merge AAN in the grid, which is possible once 12.0.0 is installed in the grid.

## 5 User Requirements

Five complex analysis use-cases have been proposed, based on discussion in the User's Task Force (UTF) [20]. The first three are real analysis use-cases whereas the last two are important calibration and alignment use-cases that must be catered to with high priority when data taking starts. Details on the requirements and how to get started can be found here [20]:

1. Extraction of \(W/Z\) cross-sections. Ultimate goal is to test distributed analysis tools, use TAG for pre-selection, and use luminosity blocks to obtain integrated luminosity. Use CSC (Computing System Commissioning) filtered di-jet and \(Z\to ee\) samples. The requirements consist of the following: * Use complex Monte Carlo datasets (thousands of AOD files with 1000 events per file for background, AOD files from MCNLO with event weights for signal). * Access the TAG to select e.g., sub-sample of background AOD containing two electrons. * Access luminosity blocks to actually compute normalizations between samples.

* Use diagnostic tools for distributed analysis (e.g. job summary tools providing partial and final summaries).
2. Perform a complex analysis using CSC data and the tools available today to help converge on the infrastructure and tools needed: * Suite of C++ analysis tools to extract reasonably complex signal above background and to estimate uncertainty on background from "data." * Assess currently proposed approach based on EventView tools to go from general AOD format to user-specific format in ATHENA-Aware NTuple. * If left appropriate, evaluate pros and cons of somewhat different approaches (in principle the whole spectrum is possible, from AOD-based analysis using ROOT only to make plots to ROOT-based analysis using AOD only as input to construct user-specific ROOT tree).
3. Carry out complex associations at the AOD level, to make sure the AOD EDM (Event Data Model) is adequate for initial physics. Use \(W/Z\to leptons+jets,\,\gamma+jets,\,jet-jet,\)\(top\) and \(\tau+MissingET\) samples. One should study on the AOD complex overall energy scale and object identification issues, such as: * Final decision on electron/muon identification and electron/muon energy, removal of jet-electron/muon overlaps in space, final computation of jet energies, depending perhaps on jet flavor and choices of labeling of heavy-flavor jets. * Final computation of ETmiss based on above decisions and on event topology. Fit resonance objects to expected shape to extract absolute mass scale from various data streams (\(J/\psi,\,\Upsilon,\,W/Z\to leptons,\,W\to jj\) from top, \(Z\to\tau\tau\)).
4. LAr calorimeter calibration from OFC (Optimal Filtering Coefficients) to cell and cluster corrections. Ultimate goal is to provide reliable TopoClusters, Electron/Photon objects and jets in ESD and AOD. Use \(Z\to ee\) samples, inclusive electrons, \(W\to\tau\nu\) with \(\tau\to\pi\nu\). The requirements are as follow: * Analyze standard data formats (ESD and perhaps AOD) where OFC are dealt with in the ROD and calibrations and corrections are provided before ESD at the appropriate level. * Analyze also non-standard raw data files (with 5+ energy samples in byte stream) with the goal of extracting the relevant calibration parameters and feeding them back to Conditions database.

5. Calibrate and align Inner Detector and Muon Spectrometer. Goal is to provide final validated constants to CondDB (Conditions Database). The data to use would consist of specialized streams for The Inner Detector (minimum bias events) and the Muon Spectrometer (inclusive muon RoI selected at Level-2), muons from inclusive stream for the Inner Detector and the Muon Spectrometer, and eventually muons from \(W/Z\) decays: * Analyze large amounts of specialized data from alignment streams. * Validate complete process from extraction of constants, to storage in CondDB, to retrieval from conditions databases for final processing. * Compare results obtained with different sets of constants. Use specific tools to extract, compare, validate a chosen set of constants in the database.

Volunteers are needed to exercise these examples to help improve the current analysis model. Progress is to be reported and discussed during analysis model meetings and performance meetings.

## 6 Interactive ATHENA

The current status of the tools for interactive analysis in ATHENA was discussed. The need for more interactive analysis tools was also acknowledged.

### Current Status

There is a strong request to develop interactive analysis tools for ATHENA. ATHENA and Python provide an interactive environment to access EDM objects. Objects are retrieved from StoreGate, so that the user may access both persistified and transient objects transparently. Pyclgdict provided Python-binding for C++ objects till the software release 11. However, PypROOT(ROOT5) takes over the role following the SEAL and ROOT integration. Actually the major change for the release 12 was the migration to ROOT5. The migration has finished and the interactive tools are fully functional in the release 12.

Currently users have to use ROOT for AAN, ATHENA for POOL, and a grid tool for datasets, respectively. This means there are three kinds of sessions, i.e., three kinds of user interfaces. An important goal for the interactive ATHENA is to provide a unified user interface to the user. Python enables that by gluing all types of sessions to ATHENA. Here are some advantages:

* ROOT and ATHENA share run-time environment. For example, cmt sets up the LD_LIBRARY_PATH which includes directories for dictionaries. As a result, one can load dictionaries using relative path instead of absolute path.

* Analysis through tools Workshop Summary
* Analysis environment transfers from ROOT to ATHENA smoothly. From the user's point of view, there is only one session in spite of two internal sessions.
* Each internal session is executed in a separate shell with fresh environment variables. Although several variables conflict between ATHENA and grid tools, this problem can be solved naturally.

### Use cases for more Interactive Tools

The following is the list of use-cases based on the analysis of test beam data [21]. The Liquid Argon Calorimeter uses two kinds of Identifiers to identify cells or channels. The "Offline-Identifier" encodes the geometrical position of a cell, e.g., layer, \(\eta\), \(\phi\) and region. The "Online-Identifier" encodes the position in the readout chain, e.g., FEB (Front End Board) channel number, FEB slot number, Feed-through number. Both types of identifiers are internally 32-bit integers. Special Helper Classes are used to encode or decode them. The LArCablingService provides conversion between on-line and offline identifiers.

Being able to access and decode these Identifiers in an interactive ATHENA session would be useful in many cases:

* Interactively plot cell-energies using geometrical cuts, e.g., a histogram of all cells belonging to the LAr-EM Middle compartment -- Calorimeter Cells do have a pointer to a Detector Descriptor Element that contains this and more detailed geometrical information about each cell.
* Interactively decode Identifiers and convert on-line to offline identifiers and vice versa. During the test beam analysis, one frequently hits cell or channel identifiers and wants to know where this cell is geometrically located or to what FEB it belongs. Up to now this was done by looking up large tables more or less manually.
* Interactively access calibration quantities, e.g., a histogram of all pedestals of a certain gain and FEB as they are stored in the database. This is maybe more complicated since LArConditions objects have a complex sub-structure but if it works, it could almost do the job of a Conditions Database browser. Electronic calibration constants are also stored together with on-line Identifiers.

**It was noted that the development of more interactive analysis tools, as needed for the analysis use-cases listed above, necessitates the generation of dictionaries for ATHENA AlgTools and Services, and that this is best done by the sub-system software responsible as in the case of the Event Data Model (EDM) packages. Itwas also recommended that the other sub-detector people come up with use-cases similar to the ones listed above of the Liquid Argon Calorimeter.**

## 7 Analysis EDM

The ESD contains data from the full detector and combined reconstruction chain, namely: hits and cells from byte-stream processing, full Monte Carlo truth information, trigger decision and physics objects, clusters and tracks from pattern recognition, and the results of combined reconstruction (egamma, CombinedMuon, TauObject, Jet, Missing \(E_{T}\), Track, TrackParticle, CaloCluster, etc). Nearly all of the objects stored in the ESD inherit from the INavigable4Momentum interface providing some standardization.

The AOD contains mirror versions of high level data in the ESD, namely Electron, Photon, Muon, TauJet, ParticleJet, TrackParticle, CaloCluster, and Missing Et objects. For the most part these objects inherit from the IParticle interface, which inherits from INavigable4Momentum but adds additional information such as PDG ID and origin to meet analysis requirements. These objects are filled using information in the ESD and are back-navigable (but no longer navigable) to their partner ESD objects: "Back navigation" is navigation to upstream data, i.e., data written in an earlier processing stage, and hence not directly available in one's immediate input files. Back navigation requires two things that simple navigation does not:

* the upstream data file must be accessible and registered in an accessible catalog,
* and back navigation must be explicitly enabled in job options.

The EDM evolution has led to recent extensions of the AOD to enhance analyses. Figure 3 illustrates the inclusion of clusters into the AOD and the possible addition of Electron cells to AOD. Clearly the AOD Electron is rapidly evolving to a more complete mirror image of the ESD. Such changes have prompted some very recent EDM "rewiring" so that AOD objects are navigable to their constituents rather than to their ESD partners.

As a result, it now appears that while the AOD was originally designed for light-weight cross-slice (VG photons and jets) data manipulation during analysis, it is also suitable for many tasks traditionally foreseen as ESD use cases. The obvious attraction here is that these tasks can be completed more rapidly on the AOD, short-cutting the limited ESD access and ESD to AOD reprocessing schedule.

### ESD/AOD: Creating A Common Interface

The combined reconstruction (e.g., gamma in the ESD) and the analysis (e.g., Electron in the AOD) representation of physics objects provide different interfaces to particle-specific information. Therefore ESD \(\leftrightarrow\) AOD migration of client code is often non-trivial. As a consequence users must decide between an AOD- or ESD-based analysis and can be locked-in by their commitment.

This situation may be alleviated by a policy of always accessing the combined reconstruction particles (ESD) via back-navigation from the particle objects (AOD). The particle objects can be quickly made when reading an ESD file so that a user may run the same AOD-based analysis on the ESD file. Then users access information beyond the particle objects by fetching the specific piece of additional information from the combined reconstruction object. However, care must be taken to build mechanisms to build particle objects from new combined reconstruction objects created within the same job.

A more dramatic but promising alternative is to migrate to an EDM built from unique representations of every physics object. In this scheme, the only distinction between the particle and combined reconstruction objects is that a sub-set of the ESD information is available in the AOD. The SplitStore implementation of CaloClusterMoments may be considered as a prototype of such a design. An EDM based on this concept would permit seamless input change between AOD and ESD, can provide simple mechanisms for promoting/demoting data between these layers, and may lead to additional common interfaces and implementations, both between slices (e.g., egamma and jet) and layers (AOD, ESD).

Ideally the implementation would provide a migration path from the old to new EDM, allowing slices to independently migrate. However, many subtleties require consideration. For example, some selections are applied at ESD \(\rightarrow\) AOD conversion and the present division of

Figure 3: The AOD Electron and ESD egamma EDM objects and their constituents. The red (black) lines signify navigable (back-navigable) links.

egamma into Electron and Photon will likely lead the merger of the latter.

The consensus at the PAT workshop was that the CaloClusterMoments infrastructure should evolve to enable support migration, while the new design is proposed to the combined reconstruction groups.

### Unique Identification Numbers for EDM objects

The process of copying particle objects during calibration, kinematic fitting, or AOD thinning, necessitates client code to able to resolve overlaps between different versions of the same particle. While it is possible to keep track of the lineage of a particle through maps of C++ (unsafe) or persistifiable pointers (e.g., DataLink or ElementLink as in AssociationMap), such mechanisms require additional data to be passed around with the particles to clients who will then need to actively ensure good book-keeping. Considering that StoreGate usually requires objects to be locked (constant), a separate such map will be necessary for particle copy steps implemented in separate ATHENA algorithms.

As an improvement, such a scheme can be implemented as a "service" which will keep persistifiable maps between objects. Clients will either need to register their object copy actions, or ask the service for copies of objects. This service provides mechanisms for checking overlap between objects.

A simpler approach would be to assign internal Unique Identification (UID) numbers to all particles and define rules when these UID are shared. Such a mechanism may be integrated with existing identifiers for detector elements such as hits or cells. An overlap check between two objects will then involve comparing the UID of the object and possibly its constituents.

In order to avoid complicating the EDM inheritance tree, this internal UID can be stored in the existing INavigable4Momentum class hierarchy. In addition, checking overlaps between two objects and their constituents may be performed using Navigation. Ideally a single algorithm or tool can be configured via job options to generate the AssociationMaps between any pairs of objects: egamma and Jet, Electron and ParticleJet,...

Some care would be necessary in generating UIDs. At start of each event, a static counter in the EDM object base-class would need to be properly initialized to start above the highest UID of any object in the job input. A Universally Unique Identifier (UUID) mechanism might be a more attractive alternative to static counter, but is likely to be slower and consume more space.

### ILink

The ATLAS EDM implementation provides 2 different types of persistifiable links between objects: DataLink for references to objects stored directly in StoreGate and ElementLink for ref erences to objects stored in containers (DataVectors) in StoreGate. Since most EDM objects are stored in containers and most references in the EDM are sufficiently well-defined, the choice between DataLink and ElementLink is usually clear.

The choice is much more difficult for general purpose EDM objects, like the EventView, where three different links are presently necessary:

* a ElementLinkVector to store references to final state particles, usually the particle objects in the AOD,
* a vector of DataLinks to store references to inferred particles created in the course of analysis, for example a Z or Higgs, and
* a DataLink to parent EventView.

These choices limit EventView, for example making it impossible to refer to a Z particle in a container and forcing EventViews to be stored directly in StoreGate.

A simple solution to this problem is to create a common interface (ILink) for references, from which DataLink and ElementLink may then inherit. Additional implementation of ILink can provide references to objects which are not in StoreGate, for example using simple pointers. A minor complication is that because setting a link depends on the type of link, ILink cannot provide a common interface for setting. However, the interface for dereferencing a link can be placed in ILink. The ILink interface can also provide a means of converting DataLink and ElementLinks to ROOT streamable pointers, which is necessary for storing EDM objects in ATHENA-Aware NTuples. 2

Footnote 2: Another approach, not discussed during the workshop would be to reimplement DataLink as a special case of ElementLink (say, an ElementLink to an object whose indexing policy is NONE would be a DataLink, and only the ”key” and not the ”index” part of the ElementLink would be needed to identify the object). This may be simpler than introducing an ILink, although without additional work it doesn’t solve the ”transient pointer” problem.

### ParticleView (aka UserParticle)

The EventView was originally designed to store references to particle objects after removal of overlaps between the output of the combined reconstruction algorithms. The fundamental issue is that energy deposits in cells or hits in the tracking system are independently interpreted by the different combined reconstruction algorithms leading to multiple reconstructions (or representation) of the same physics particle. Therefore a single "true" electron, for example, often appears in the outputs of the Egamma, Tau, and Jet reconstructions.

Currently, the EventViewBuilder tools provide a means of building EventViews where ideally every "true" physics object appears once. However, this process requires each physics particle to be considered with a unique representation in a given EventView, though it is possible to consider different representations by building multiple EventViews of the same event.

A complementary means of book-keeping overlaps between particles is the proposed ParticleView \((\mathrm{PV}),\) an object which retains references to all representations of a physical particle. This object would ideally inherit from IParticle, and provide the following features:

* Store a container of ILinks intended for keeping references to all representation of a single physical particle. This container can present a similar interface as the final state container in the EventView, providing features like labeling and a means to retrieve the individual representations.
* ParticleView may be manipulated as a particle, presenting the kinematics of a user-selectable representation to clients.
* ParticleView may store any data related to the physics object in a UserDataBlock.

Though ParticleView \((\mathrm{PV})\) can be used independently of EventView, the PV can be easily created, filled, and stored using the existing EventView infrastructure. These PVs may then be passed through tools which attach observables such as isolation or multi-variate (e.g., neutral-network) discriminants as PV UserData.

The PV may also be very useful for kinematic fitting where additional constraints are typically exploited to achieve better resolution of observables. Here, refitted representations of particle can be simply appended to the PV representation and any additional data accumulated during the kinematic fit (e.g., a covariance matrix) may be attached as UserData.

A further possibility is the addition of a container of constituent ILinks to PV. In this case, the PV can also serve as a composite particle. The benefit here is that the combinatorics and kinematic fitting algorithms and tools may then treat all particle objects using the same interface to add/fetch representations and UserData from final state and composite (or inferred) objects.

## 8 Transient and Persistent Separation

The transient and persistent (T/P) separation [23] is a strategy to handle data persistency. It separates a given data model into two different ones:

* the main transient data model,
* the persistent data model, which encapsulates the state of the transient model.

With this separation, one will have two different classes Trans and Pers_pX 3. This separation scheme lets the transient class evolves at its own pace and still allows the reading of old data by implementing custom converters for each version of the persistent classes. This feature is sketched in Figure 4.

Footnote 3: The actual implementation of the two classes can be identical, but one has to be able to distinguish both classes, hence the different C++ types.

In the absence of T/P separation, the inability of a persistence technology such as ROOT to handle complex C++ data models (e.g., "double-diamond virtual inheritance", see section 11.1) has already caused ATLAS software developers considerable effort and contributed to substantial delays, particularly of Release 12. The introduction of a state representation layer frees event data model developers from limitations imposed by the persistence technology -- one is free to do arbitrarily complex things, as long as one can encapsulate one's state in the simpler _p1 objects that will be written to persistent storage.

Separating transient and persistent implementations allows also to optimize the layout of the persistent class to address disk space storage and disk I/O speed. At the Japan workshop, the T/P separation of the HepMC classes has been presented as well as its impact in terms of performance. The outcome of this exercise was a \(\sim 20\%\) improvement in disk space occupation. Despite the transient-to-persistent (and _vice versa_) overhead, one also observed a \(\sim 4\%\) and \(\sim 9\%\) improvements in writing and reading McEventCollection.

A more detailed study to assess the I/O performances (Figure 5) as well as disk space (Table 3) has been performed. Results confirm the previous figures.

Figure 4: Transient/Persistent separation of a class. Two files contain two different persistent versions of a given transient class. When reading the old file, the POOL converter will call the right method to convert the persistent Pers_p0 instance into a Trans instance. The method dispatching is based on the GUID of the persistent classes. Note that there is only one single conversion method to the most recent persistent version of the transient class.

Since the workshop, the transient/persistent separation has been implemented for the Truth-ParticleContainer class and work is in progress for ElectronContainer.

There are however a few issues that are coupled to the T/P separation. As long as the transient class contains POD (_Plain Old Data_), the separation can easily be performed. When the transient class contains a persistent pointer (_e.g.:_ElementLink<T>) or another class which is

Figure 5: Evolution of I/O for the McEventCollection class with and without T/P separation. Various cases have been benchmarked: the normalization (no McEventCollection I/O), without T/P separation, T/P separation (as in release 12.0.0), T/P separation replacing doubles by floats (not in production). These data points have been obtained by running 100 jobs writing (and then reading) 1000 \(t\bar{t}\) Pythia events within release 12.0.0 and finally taking the mean of the distribution.

\begin{table}
\begin{tabular}{c|c c} \hline T/P Mode & size (Mb) & improvement (\%) \\ \hline No T/P separation & 55.9 & 0\% \\ T/P (release 12.0.0) & 42.2 & \(\sim\) 25\% \\ T/P (doubles/floats) & 28.6 & \(\sim\) 50\% \\ \hline \end{tabular}
\end{table}
Table 3: Evolution of the disk size for 1000 \(t\bar{t}\) events for various T/P separation schemes: no T/P separation, with T/P separation, with T/P separation replacing doubles by floats in the persistent class.

also T/P separated, the persistent version must contain the most up-to-date persistent version of the contained class. This can trigger undesirable chain effects when a rather basic class (like one of the DataModel package) upgrades its persistent representation: all the persistent classes which uses this class must be also updated and their version number updated. The work that must be done is straightforward, but it is widespread, tedious, and error-prone. Prototyping of scripts to assist in this kind of maintenance has been commissioned since the time of this workshop.

Another issue is the handling of containers holding polymorphic objects. The main problem in this case is to use the correct converter for the concrete object hidden by its interface class. Since the workshop, a strategy to handle heterogeneous containers has been introduced, along with template classes to assist in the process.

These issues are currently discussed by the Architecture Team and the EMB (Event Management Board).

To conclude, the decoupling between transient and persistent classes allows to optimize the layout of the persistent class without disturbing the transient class with storage details. This separation also gives more control to the developer (zero suppression, double-to-float conversion,...) and increases flexibility in reading old data through custom converters. A study performed on the McEventCollection class shows that both speed and disk space performances are improved, even with the overhead of the transient-to-persistent conversion.

## 9 AOD Thinning

One method to improve significantly AOD access speed while working in the standard AOD processing environment is to _thin_ the AOD data objects. Specifically Frank Paige who pioneered this technique, has achieved a factor \(\approx 10\) gain in access speed and \(\approx 50\) reduction in AOD size by e.g. removing the vast majority of track objects in the AOD track collections. The problem he faced was to remove M Tracks from a TrackCollection leaving the references (EL) to the remaining \(N-M\) Tracks valid. His approach was to copy the tracks into a new TrackCollection and then painstakingly updating all objects referring to those tracks via Data/ElementLinks.

During the workshop, and later in meetings of the Architecture Team we discussed possible approaches to thinning that should make the life of developers writing the thinning code much simpler. The first step common to every approach is the ability to record a new thinned container in StoreGate using the same key as the original container. This requires some new StoreGate infrastructure to be able to access the old or the new version of the container as needed, while making sure that all downstream clients will retrieve by default the new thinned version of the container.

To make thinning work, one also needs an API to specify which elements of a container need to be kept. This API is being designed at the time of this writing but basically it should allow while looping over the original container to mark the elements one wants to keep.

Once the thinned container has been recorded to StoreGate and committed to disk, one needs a way to provide transparent access to all the remaining elements of the container. This implies that iterating over the container should only return the remaining elements, "skipping over" the thinned ones. At the same time direct indexed access to the container elements either via operator [] or crucially via ElementLinks should hide as much as possible the fact that the container has been thinned: the index of each remaining element should appear to be unchanged with respect to the original container; while an attempt to access a "thinned" element should fail in a controlled way (returning a NULL pointer or throwing an exception). There is a prototype of DataVector that behaves like that [24].

## 10 Structured NTuple

It is possible to add complex objects to the user's own AthenaAwareNTuple (AAN) [8]. ROOT [7] version 5 or higher is required, so this does not work for the software releases 11.0.X, rather 11.5.0 or higher. Instead of saving \(p_{T}\), \(\eta\), \(\phi\), \(e\), etc as individual branches on the AAN, one would save more complex (and compact) objects that contain the \(p_{T}\), \(\eta\), \(\phi\), \(e\), and more. The user can therefore create his/her own classes, fill the corresponding objects during some ATHENA analysis and add the objects to the AAN. This also brings up the possibility of adding the AOD objects (Electron, Muon, Photon, etc) to the user's AAN. We are therefore talking about structured NTuples.

### Rationale for Structured NTuple

Creating structured AAN has several advantages. It allows the user to use the same classes in ROOT and ATHENA analyses. In doing so, it would be easier to port pieces of code back and forth: imagine that you develop a complicated electron identification procedure in ROOT, and that piece of code could be useful to a great many people in ATLAS. With the structured NTuple, it would be easier to port back that piece of code into ATHENA without having to modify it significantly to fit within the ATHENA structure (thus, requiring a re-validation). Another advantage of structured AAN is that we would be able to easily compare and share pieces of code between different analyses (be it in ATHENA or in ROOT) since the fundamental objects are the same even in ROOT. With the structured NTuple, we achieve some transparency in the ATHENA-ROOT interplay.

[MISSING_PAGE_FAIL:36]

can now open your structured NTuple and start using it: You read the entire objects from the AAN, access their methods, etc, as you would normally do in ATHENA.
* Running ROOT Without ATHENA: Here we assume that you have installed ROOT on your laptop and you have copied the structured NTuple to your laptop. You want to analyze those AAN in ROOT -- no references to ATHENA whatsoever:
* If you are running ROOT on Linux with the same version and compiler as used in ATHENA to generate the shared libraries mentioned above, then you can simply copy also the shared library to your laptop. Once you start a ROOT session, you would load the shared libraries and start manipulating the AAN. In this case, you must load both the libXyzDict.so and libXyz.so files in ROOT. Due to ATHENA dependencies while creating the shared libraries, you may need to copy one or 2 more libraries from ATHENA: ROOT will tell you.
* Your Linux version or Linux compiler is different from the one used to build the ATHENA software, or your platform is Mac, Windows, etc: in this case, you CANNOT load the libXyz.so and libXyzDict.so files in ROOT. You would simply copy your user classes to your local machines and compile them directly, locally in ROOT. After that, you can process the NTuple in ROOT. Since you may need to compile your classes locally on your platform, you do not want your classes to depend upon the ATHENA infrastructure or external libraries as we mentioned above.

### Adding ESD/AOD objects to Structured NTuple

Adding ESD and AOD objects to structured AAN is straight forward and possible today since the dictionaries for the ESD and AOD classes are already available: they are the same dictionaries used for POOL persistency and interactive ATHENA sessions in Python. However, there are several complications, mainly on the ROOT side when the user needs to process in ROOT the structured NTuple that contains ESD and AOD objects:

* AOD objects contain smart pointers known as ElementLink. These are ATHENA specific tools that do not map in a straightforward way in ROOT: this means that on the ROOT side, one cannot for example, ask the AOD Electron for its associated track and get back a valid pointer to the track. The issue of ElementLink also affects the Navigation aspects within the AOD or ESD, e.g., navigating from the AOD Electron to the Calorimeter cells associated to the Electron (assuming those cells are in the AOD as well).
* If you launch ROOT within ATHENA, i.e., after having setup ATHENA, then the LD_LIBRARY_PATH defines the path to all the **libXyzDict.so** files, so you just need to load the relevant dictionaries in ROOT, e.g., the dictionary for the AOD Electron classes, libElectronPhotonIDEventDict.so for instance, and the dictionaries of the dependent objects such egamma, **libegammaEventDict.so**, etc: the number of the libraries to load in ROOT to be able to define completely all the AOD objects, is finite, of the order of twenty. In a ROOT macro, one can pre-define all the **libXyzDict.so** to load.
* If you launch ROOT without ATHENA, on your laptop for example, and if you are using the same version of Linux and compiler as in ATHENA, you will have to copy almost all the ATHENA libraries, **libXyz.so** and **libXyzDict.so** to your laptop and load them in ROOT, not just the relevant libraries for the AOD or ESD because of system dependencies: this is tedious but straightforward.
* If you launch ROOT without ATHENA, on laptop with a different version of Linux and compiler, a different platform such as Mac and Windows, you cannot copy the ATHENA libXyz.so and libXyzDict.so. You will have to copy a lot of the ATHENA classes and compile them locally: this is prohibitive.

### Toward Common Classes for Structured NTuple

As mentioned above, it is possible today for each user to create his/her own classes, fill them in ATHENA analyses, add the objects of these classes to structured ANN, then access the AAN in ROOT. Doing the same for AOD and ESD objects is also possible but with limited functionalities on the ROOT side: the issue of the ElementLink is expected to be resolved by the release 13. Even when it becomes possible to access the AOD/ESD in ROOT, through the structured AAN, doing so would make sense only when the user starts ROOT within ATHENA as explained above. Thus, adding AOD objects to structured Athena-Aware NTuple is NOT recommended. However, in addition to the case of adding user-defined classes, we need to have common classes across ATLAS to be able to create common structured NTuple for everybody in ATLAS. Members of PAT have been working toward such common classes for structured NTuples for the release 13. These classes will be very similarly to the ESD/AOD classes, they will be light, they will be used in ATHENA and ROOT and they will be portable across different platforms.

### Adding ROOT Objects

If the object that you want to add to the AAN is a ROOT object such as a _TOject_ or _TL LorentzVector_, etc, you do not need to generate dictionaries as ROOT understands those. You can just add them directly to your AAN.

### Concrete Example of Adding Objects to Structured NTuple

In the ATLAS CVS repository, under PhysicsAnalysis/AnalysisCommon/, There is an example of adding user-defined classes to structured NTuples. The details are described in [26].

## 11 INavigable4Momentum Issues

### Present Design and Implementation

All reconstruction and analysis objects in Athena providing a four-momentum and supporting object navigation implement the same INavigable4Momentum interface. Default implementations have been provided for the 14Momentum part of the interface, with various storage options for the kinematic variables, see Event/FourMom package and its attached documentation. Typical implementation classes are P4PxPyPzE and P4EEtaPhiM, with \((E,p_{x},p_{y},p_{z})\) and \((E,\eta,\varphi,m)\) cached in the data object, respectively. Both are complete implementations of the 14Momentum interface, while the corresponding P4PxPyPzEbase and P4EEtaPhiMBase classes only implement the functions for the derived (non-cached) variables.

The INavigable part of the INavigable4Momentum interface can also be implemented using the Navigable\(\langle\)CONTAINER,PARAMETER\(\rangle\) template, at least for simple composite data objects with only one constituent type. This generalization provides the default implementations for the handling of the NavigationToken\(\langle\)OBJECT,PART\(\rangle\) visitor actually navigating the objects and collecting pointers to the requested objects of type OBJT and accumulating the optional relational parameters of type PART along the branches of the relational tree. In addition, it also implements useful methods to insert, remove, reweight, access, and test the constituent data objects. A special implementation NavigableTerminalNode is provided to implement navigation for singular (non-composite) objects. This assures that these objects can be navigated to using the standard visitor. Naturally, there is no navigation from these objects, as they represent terminal nodes in a relational tree.

The typical examples for client implementations are Jet and CaloCluster for composite objects, and CaloCell for singular objects:

class Jet:  virtual public INavigable4Momentum,  public Navigable\(\langle\)INavigable4MomentumContainer,double\(\rangle\)  public P4PxPyPzE {...}

Jet has constituents of the (generic) INavigable4Momentum type, stored in a (storable) INavigable4MomentumContainer, and contributing to the Jet with a kinematic weight of type double.

The static class diagram in Figure 6 depicts the design of Jet

class CaloCCluster: virtual public INavigable4Momentum, public Navigable(CaloCellContainer,double), public P4EEtaPhiM {...} Here the constituent type is CaloCCell, stored in a CaloCCellContainer, and the relation between CaloCCell and CaloCCluster is again described by a kinematic weight of type double. class CaloCCell: virtual public INavigable4Momentum, public NavigableTerminalNode, public P4EEtaPhiMBase {...}

CaloCCell is an example for the implementation of a singular data object. It provides all access methods for its internal four-momentum representation \((E,\,\eta,\varphi,\,m),\) where \(\eta\) and \(\varphi\) are extracted from a database (read-only for clients), \(m\) is hardcoded as \(m=0,\) and \(E\) is the only variable which can be set by clients.

### Problems with the Design

The problem with the design discussed above is the complexity of the inheritance tree, as shown in Figure 6. In particular, there are two routes to the implementations of the INavigable and I4Momentum interfaces, one through the INavigable4Momentum inheritance, the other through

Figure 6: Static class diagram for the present Jet design.

the implementation classes. This forces the inheritance of all interfaces to be virtual for an unambiguous implementation. With the introduction of root version 5 as the persistency provider service in Athena, the client objects became literally unpersistifiable with this design, at least at first. Only the introduction of a very specific workaround deep in the corresponding core software allowed to actually save basically all data objects again.

Also, the _double-diamond_ shaped inheritance tree is somewhat close to the boundaries of object-oriented design, as supported by the compilers, see remarks in [27], for example. The resulting possible intrinsic instability, and even more the possibility that this design is not at all suported by future compiler versions anymore, let to the development of the new design discussed in section 11.3 below.

### New Navigable Design

A first attempt to break the complex multiple (virtual) inheritance structure has been presented at the workshop [28]. It has been significantly modified since that time, due to weaknesses discovered in the first implementation attempt. The design evolution can be followed on the document pages [29]. Here we only describe the basic underlying idea, which has not changed since the time of the workshop.

The solution lined out at the workshop is based on the idea to replace inheritance of implementation by encapsulation of the specific objects providing the same implementation. This

Figure 7: An example for a class structure using wrappers for Jets. **Note:** this figure does not show the actual implementation of the new Jet class design. It is simplified for clarity and to illustrate the basic concept only.

can easily be done by introducing wrappers, where methods of a wrapped class are duplicated into the wrapper class, which then forwards requests to wrapped implementation. In this model the wrapper itself does not need to implement any particular interface, in particular there is no need for it to inherit from the interface implemented in the wrapped class - even though it is technically implementing all methods from this interface!

The advantage of the wrapper is obvious. For example, the Jet class shown in Figure 6 can be modified without changing its interface or behaviour as illustrated in Figure 7. Here Jet now inherits the complete INavigable4Momentum interface implementation from two wrapper classes, one for the navigable and one for the four-momentum branch. These classes themselves implement the methods from the INavigable and I4Momentum interfaces, respectively, without inheriting from these. The real individual implementations for each of these interfaces are provided by the same classes as in the old dessin, i.e. P4PxPyPzE and Navigable\(\langle\)INavigable4MomentumContainer,double\(\rangle\).

The design clearly breaks the double-diamond shaped inheritance structure and avoids any virtual inheritance at all. The actual wrapping can be implemented in two ways. If the wrapped class is complete, meaning it prescribes the behaviour of a complete and instantiable object, the forwarding is easily implemented with the wrapper class encapsulating a private instance of the application class. It is a bit more tricky if the wrapped class is abstract, i.e. it is not complete to instantiate an object. In this case, it is of course not possible to encapsulate a private instance. The rather complex solution to this problem, which actually needs to be addressed in case of the CaloCell, is still under some development, as shown in [29].

In general it seems possible to break the complex multiple virtual inheritance structure of the original navigable four-momentum event data model. This may not necessarily mean reducing the complexity of the static class structure, but it certainly means a more stable design with very convenient client implementations.

## 12 Conclusions

The third annual physics analysis tools workshop was held in May 2006 at the University of Tokyo, Tokyo, Japan. There were a total of sixty-nine participants from the various ATLAS institutions, in particular from the Asia-Pacific region. The workshop consisted of three main parts. A two-day analysis tutorial was organized at the beginning of the workshop. The tutorial covered a wide range of topics in analysis tools, namely, Event Selection, Event Display, Interactive Analysis in ATHENA, Distributed Analysis, Trigger-Aware Analysis, Monte Carlo Truth Tool on the AOD and analysis with the EventView. After the tutorial, there was a user feedback session where there was a discussion on the user requirements for analysis and also reports on the analysis models used in the various, participating Asia-pacific institutes.

The last two days the workshop were devoted to core PAT discussions in the same spirit as the previous PAT workshops. The discussions centered on the evolution of the interactive analysis tools in ATHENA, the status of distributed analysis tools and the associated issues and problems faced by the user, and various salient problems in the analysis Event Data Model (EDM). In many cases, concrete proposals toward common solutions to various problems were made -- as discussed above in the text -- and the discussions are being carried out within PAT and Architectural meetings.

## References

* [1] Physics Analysis Tools, documentation page, [https://twiki.cern.ch/twiki/bin/view/Atlas/PhysicsAnalysisTools](https://twiki.cern.ch/twiki/bin/view/Atlas/PhysicsAnalysisTools).
* [2] Physics Analysis Tools in Japan, May 15-19, 2006, detailed agenda, [http://www.icepp.s.u-tokyo.ac.jp/atlas/pat06/](http://www.icepp.s.u-tokyo.ac.jp/atlas/pat06/).
* [3] K. Assamagan, et. al., "Final Report of the ATLAS AOD/ESD Definition Task Force", ATL-SOFT-2004-006
* [4] K. Assamagan, et. al., Report of Event TAG Review and Recommendation Group, ATLAS-SOFT-PUB-2006-002; [https://twiki.cern.ch/twiki/bin/view/Atlas/TagForEventSelection](https://twiki.cern.ch/twiki/bin/view/Atlas/TagForEventSelection)
* [5] F. Akesson et. al., [https://twiki.cern.ch/twiki/pub/Atlas/PhysicsAnalysisTools/ucl_workshop_summary.pdf](https://twiki.cern.ch/twiki/pub/Atlas/PhysicsAnalysisTools/ucl_workshop_summary.pdf).
* [6] K. A. Assamagan et. al., [https://twiki.cern.ch/twiki/pub/Atlas/PhysicsAnalysisTools/tucson2005_summary.pdf](https://twiki.cern.ch/twiki/pub/Atlas/PhysicsAnalysisTools/tucson2005_summary.pdf).
* [7][http://root.cern.ch/](http://root.cern.ch/)
* [8] T. Maeno, et. al., _[https://uimon.cern.ch/twiki/bin/view/Atlas/AthenaAwareNTuple_](https://uimon.cern.ch/twiki/bin/view/Atlas/AthenaAwareNTuple_)
* [9] C. Fehily, _SQL: Visual Quickstart Guide, 2nd Ed._, Peachpit Press, 2005.
* [10] A. Beaulieu, _Learning SQL_, O'Reilly Media, 2005.
* [11][https://twiki.cern.ch/twiki/bin/view/Atlas/EventSelectionAnalysisTutorial1105](https://twiki.cern.ch/twiki/bin/view/Atlas/EventSelectionAnalysisTutorial1105)
* [12][http://atlantis.web.cern.ch/atlantis/](http://atlantis.web.cern.ch/atlantis/)
* [13][https://twiki.cern.ch/twiki/bin/view/Atlas/JiveXML](https://twiki.cern.ch/twiki/bin/view/Atlas/JiveXML)* [14][https://twiki.cern.ch/twiki/bin/view/Atlas/EventDisplayAnalysisTutorial1105](https://twiki.cern.ch/twiki/bin/view/Atlas/EventDisplayAnalysisTutorial1105)
* [15] HepMC: a C++ Event Record for Monte Carlo Generators, [http://mdobbs.web.cern.ch/mdobbs/HepMC/](http://mdobbs.web.cern.ch/mdobbs/HepMC/)
* [16] M. Wielers, [https://twiki.cern.ch/twiki/bin/view/Atlas/](https://twiki.cern.ch/twiki/bin/view/Atlas/) TriggerSoftwareTutorial-Page#Trigger_aware_analysis
* [17] S. George et al., [https://uimon.cern.ch/twiki/bin/view/Atlas/TriggerEDM](https://uimon.cern.ch/twiki/bin/view/Atlas/TriggerEDM)
* [18] Ganga web-page, [http://ganga.web.cern.ch/ganga/](http://ganga.web.cern.ch/ganga/)
* [19] LJSF Twiki page, [https://twiki.cern.ch/twiki/bin/view/Atlas/LJSF](https://twiki.cern.ch/twiki/bin/view/Atlas/LJSF)
* [20] D. Froidevaux et. al., Discussion within the Users Task Force Meetings
* [21] W. Lampl et. al., Private Communication [http://indico.cern.ch/conferenceDisplay.py?confId=a062235#s2](http://indico.cern.ch/conferenceDisplay.py?confId=a062235#s2)
* [22] D. Duellmann, "The LCG POOL Project General Overview and Project Structure", Proceedings of Computing in High Energy and Nuclear Physics, 24-28 March 2003; _[http://lcgapp.cern.ch/project/persist/_](http://lcgapp.cern.ch/project/persist/_)
* [23] T/P separation wiki page, [https://twiki.cern.ch/twiki/bin/view/Atlas/TransientPersistentSeparation](https://twiki.cern.ch/twiki/bin/view/Atlas/TransientPersistentSeparation)
* [24] S. Binet, Private Commmunication
* [25][https://twiki.cern.ch/twiki/bin/view/Atlas/WriteReadDataViaPool](https://twiki.cern.ch/twiki/bin/view/Atlas/WriteReadDataViaPool)
* [26] K. A. Assamagan, [https://twiki.cern.ch/twiki/bin/view/Atlas/AddingObjectsToAAN](https://twiki.cern.ch/twiki/bin/view/Atlas/AddingObjectsToAAN)
* [27] S. Meyers, _Effective C++_, Second Edition, Addison-Wesley Professional Computing Series (1998)
* [28] P. Loch, presentation at workshop.
* [29] see _[https://twiki.cern.ch/twiki/bin/view/Atlas/INavigable4MomentumEvolution._](https://twiki.cern.ch/twiki/bin/view/Atlas/INavigable4MomentumEvolution._)