[MISSING_PAGE_EMPTY:1]

[MISSING_PAGE_EMPTY:2]

## Table Of Contents

* 1 Foreword.
* 2 Introduction.
* 3 General overview of the TRT DCS.
	* 3.1 System controls - basic assumptions and proposal.
		* 3.1.1 States of TRT DCS 'objects'
		* 3.1.2 Objects in TRT DCS - hierarchical tree
	* 3.2 Hardware
	* 3.3 Software
* conformity with ATLAS DCS * 4.1 General requirements for TRT DCS * 4.1.1 DCS specifications * 4.1.2 General Description * 4.1.3 General requirements
* 5 Temperature Control System.
	* 5.1 Heat sources
	* 5.2 The monitored areas/granularity.
		* 5.2.1 Gas cooling system for the end-cap
		* 5.2.2 Liquid Cooling System
		* 5.2.3 Electronics
		* 5.2.4 Mechanics
		* 5.2.5 Active gas
	* 5.3 Available cabling.
	* 5.4 Temperature measuring method
	* 5.5 Layout of the sensors
		* 5.5.1 Barrel modules
		* 5.5.2 End-cap wheels.
	* 5.6 System monitoring and control
		* 5.6.1 States of the object in temperature monitoring system
		* 5.6.2 Operational requirements for temperature monitoring system
		* 5.6.3 System control functions
		* 5.6.4 Commands implemented by system..
	* 5.7 Correlations with other systems.
	* 5.8 Configuration of the hardware.
* 6 Gas system.
	* 6.1 Gas system overview.
		* 6.1.1 Objects in the TRT gas system.
		* 6.1.2 Gas system operation
	* 6.2 States of the objects in gas system
	* 6.3 List of User Requirements
	* 6.4 Commands implemented by the system

[MISSING_PAGE_EMPTY:4]

## 1 Foreword

The following note presents specifications for the design of the Detector Control System for Transition Radiation Tracker detector of the ATLAS experiment. Some elements of the design itself are also proposed. The full design of the system - at present stage of the detector construction and advancement of the DCS R&D - can be neither accurate nor final.

Since concerned detector has no special demands for the controls - most if not all of the standard solutions taken for central ATLAS DCS will be implemented. Thus inevitable evolution of the design will follow the changes undertaken and implemented by standardisation group. It will also, in certain areas adopt solution which will emerge with evolution of controls technologies. This specially applies to software - industrial SCADA products - being evaluated and hopefully adopted by all CERN LHC experiments in framework of so called JCOP [13-2].

## 2 Introduction

The ATLAS Transition Radiation Tracker (TRT) forms external layer of the Inner Detector, facing the cryostat wall. The detector has to be operated in a high radiation environment inaccessible for servicing during long periods. The mechanical construction optimized for low mass and robustness against radiation has to maintain good performance and mechanical stability over a big volume for a long period. The detector requires a Detector Control System (DCS) that can monitor both fast and very slow variations in important detector parameters, and which can take appropriate corrective action to maintain the required high level of detector parameters stability important for reliable performance.

All the sub-detectors in ATLAS - thus TRT - will have their own DCS, with a minimum of one Local Control Station (LCS) per sub-detector. It is our understanding that most of the high level system, including the high level software and LCS interface boards for connection with a field-bus, will be defined by the central ATLAS DCS group and thus will be easily adaptable to individual detectors. Most boards in the LCS will probably be common for all ATLAS sub-detectors. The DCS of the TRT will fully conform to the requirements set out in the ATLAS DCS User Requirements Document [13-1]. Later in the note we will specify more explicitly how this conformance is achieved

Generally the DCS of the TRT detector controls, monitors and supervises operation of the detector and the related apparatus. Its main aims are;

* parameter monitoring, loading, logging and setting* receiving commands from central system
* mostly operational
* correlating parameters from different parts of the detector
* collaborating with the DAQ system via the 'Run_control' layer
* supervising the safety of the detector in collaboration with Central Safety System
* triggering alarms, emergency procedures etc

The status of all components should - whenever necessary - be available and user intervention should be possible where/when allowed. As the DCS of the TRT is a part of the overall ATLAS DCS, it will accept general ATLAS commands and return the summary status of the TRT subsystem. Additionally, the DCS shall supply a user interface to allow autonomous operation during the construction, testing, commissioning and calibration of the TRT detector. To facilitate the overall integration and operation of central DCS this interface should be identical to the one used by central operator, with variations only specific to the TRT detector.

The lowest level DCS elements (the "front-end" monitors within the TRT volume) must be engineered into the design of the detector with the addition of the minimum amount of extra mass.

Signals from the monitoring of some common parameters, listed below, can be distributed among the DCS's of all the ATLAS sub-detectors:

* in different positions of the apparatus;
* B field (within solenoid);
* in crucial points for the composite material structures;
* status of the inert atmosphere (CO\({}_{2}\) or N\({}_{2}\));
* flammable gas / flammable liquid detection;
* smoke detection;
* low oxygen level detection;

In the following sections we describe the proposed configuration of the TRT DCS, concentrating on the issues which are TRT - specific.

## 3 General overview of the TRT DCS

In this section we review the DCS most important features as defined already in Inner Detector TDR [13-3]. The detector hardware controlled by the DCS is scattered over quite a large surface (and volume). The detector itself is in the experimental cavern. The gas system is going to be placed both on the surface and underground. Different parts of the electronics will be positioned in the experimental cavern, in the service cavern (or two caverns) and in the surface counting room. The L(ocal) DCS will be split according to the 'geographic' positions of the parts of detector to be controlled: we plan to use at least two L(ocal) C(ontrol) S(tations) - one on the surface, the other in the service cavern - USA15. The LCS'es form an intermediate layer between G(lobal)DCS and hardware/software for subsystem control.

### System controls - basic assumptions and proposal

In this chapter we will try to describe the expected system behaviour in terms of 'objects' and the'states' of this objects. Such an approach is very useful since it abstracts from details of the system and underlying hardware allowing for clear definition of logical states of system parts.

Each granule of the system can be seen from outside as an object - entity with certain behaviour and characteristics. An object can be in one of its allowed states which can be set (changed or forced into) either by command or achieved spontaneously. The meaning of the states named the same way can be, obviously different depending of the object definition.

The set of commands used in the system should be hierarchical adopted to the level of access. Some commands should be restricted only to certain cathegor of user (password protected). Each command calls for certain actions, which should at lowest level reach smallest addressed granules (objects) of the system.

Not all commands change the object state e.x. those changing demanded parameters or reading status.

#### States of TRT DCS 'objects'

Below we list all possible states in which any of the objects of which TRT detector is composed can be found during operation. It is clear that some objects will not attain certain states due to theirs internal nature. This will be shown when describing the composition of TRT in nomenclature of the 'objects'. Some of the states are'mandatory' i.e. every object has to be in one of this states; others are '_optional_' (those initalics) i.e. depending on the characteristics and nature the object can be in one of this states. The diagram of the possible states is shown on the Figure 3-1

**MANDATORY:**

* limited control of the object is possible but none of local controlling and monitoring actions is running

**Note**: This is usual state during longer shutdowns

#### 3.2.3 General overview of the TRT DCS

Figure 3-1: Possible states of the object in TRT DCS system

- which is a state where control of the object is possible and object is in some way or another not prepared for data taking (but not in ERROR).

**Note**: This state is a stable operational situation when detector, electronics and other auxiliary equipment are under full control and expecting normal running soon.

* where object is in stable state, control is possible and parameters are within limits.

**Note**: Operational state during running either in 'physics' or 'cosmics'

* any transition between stable states.

**Note**: Intermediate state of the apparatus; effect of the either error occurred in hardware or command resulting in passage between operational states.

* when any of the parameters of object is outside allowed limit value; for a complex object it is 'OR' of statuses of all its components

**Note**: Any misbehaviour of the hardware. Severity of the error shall define kind of the error message send to operator (warning, alarm etc.) or higher level of supervision. Errors might be also structured or subtly qualified and according to defined hierarchy proper corrective procedures invoked.

_OPTIONAL_:
* basically a normal1 state of the object but at the conditions which are in between the READY and NOT_READY states (ex. intermediate HV on the detector during filling up of the LHC).

Footnote 1: ‘normal’ - as opposed to in ERROR

**Note**: Many detector objects (ex. HV system) can attain a state which protects the detector ex. against high background and where return to READY is much faster than from 'not-ready' thus helping to maintain high efficiency of data taking.

* loading of the parameters; reading from database; calibration etc.

**Note**: During 'cold-start' of the detector; when new set of parameters has been defined, after power-cut; any time when necessary, parameters have to be loaded into hardware - this is the state we talk about. Loading of the parameters changes only the'memory locations'. Only following it commands affect the 'outputs' of the hardware.

**ALL BASIC STATES CAN BE ALSO**:

* control is possible either from local system _ONLY,_ or can be performed from both central and local stations.

**Note**: Normal running happens in GLOBAL state where either central operator or detector shifter gets control of the hardware; LOCAL is used for debugging and maintenance purposes where only detector operator has access to detector parameters.

* this state can be combined with any other; signifies that state of the object does not enter into state of the group which the object is a member (excluded from OR).

**Note**: To allow running of the experiment even with improper status of certain objects, parts of the hardware might be'masked'. This status shall obligatorily go into slow-control database as it can affect data quality.

* the state in which local controllers perform the controls of correlated parameters and where access of the DCS is limited to'reading'; passage to MANUAL allows change of parameters.

**Note**: Used by gas system and HV system.

- for some reason, the control/monitoring of the object is impossible, and the real state of the object is unknown (can be any of the itemized above).

**Note**: An error state of DCS system itself; can happen due to abnormal load on the network. The wide use of the field-buses should minimize occurrence of this state. Basically reason for this state is not well defined and the state occurs ex. after some time-out within the communications of the DCS.

* specific instant of the NO_CONTROL
- when reason for 'no-control' is crash of the piece of software.

**Note**: Should be generated by software used in the system.

_RUE: The object can go into error from any state, also from the transition (i.e. CHANGING state). Fixing the error MUST result in LOWER state than the one in which error happened._

#### Objects in TRT DCS - hierarchical tree

The detailed functionality of the TRT subsystems served by DCS was described in the TRT services subchapters of the ID TDR. In present section we will present the functional parts of the TRT DCS in codification of 'object' approach.

Objects can be defined as parts of the detector - each one with pertinent piece of the DCS - or the parts of the different subsystems served by DCS and with the relations to the detector subdivisions. In first approach the highest level object is the TRT detector itself, in the second one the DCS of the TRT is an object of highest hierarchy. We will adopt second approach since the other one is more detector oriented while our note is devoted to problems of the DCS.

**A. Transition Radiation Tracker Detector Control System**

1. Detector temperature measurement system 1. Barrel 1. Endplate A (mechanical space-frame) 2. Endplate C (mechanical space-frame) 3. Modules 1 through 32 4. Electronics cooling - side A 5. Electronics cooling - side C
2. Endcaps 1. Endcap A * Wheels EA1 through EA18 * Electronics cooling * Electronics boards * Detector cooling * Groups 1 through 4 2. Endcap C * Wheels EC1 through EC18
* Electronics cooling
* Electronics boards
* Detector cooling
* Groups 1 through 4
2. Gas system The gas system only at the very low level breaks down into small parts (within the 'gas distribution part) which correspond to detector granules. 1. Mixture delivery 1. gas supply; 2. mixer 3. recuperation system 2. Mixture purification 1. buffer volume; 2. purifier columns 3. gas analysis system * Infrared monitor * Xe-concentration meter * Electronegative impurities Detector * Gas chromatography * CO2 removal system; 1. Membrane filters 2. Pumping & Switching Unit 3. Controller 4. Gas distribution system 1. Pressure controller 2. Safety devices 3. Gas analysis system * Infrared monitor * Xe-concentration meter * Electronegative impurities Detector * Gas chromatography * Main distributor * Cavern distributors * Barrel* Individual lines to detector partitions
* Endcap A
* Individual lines to detector partitions
* Endcap C
* Individual lines to detector partitions
* Pumping station
* Gain stabilisation system
* Data acquisition
* HV power supply

Items a - c are all placed in the gas surface building (SGX).

The distribution system with its pressure control and re-compressor is situated in the underground service area (USA15), whereas the remaining part of the distribution system is situated in the UXA cavern

1. Power supply systems

Power supply system is a typical example of the complex object where its sub-objects (constituents) can follow either the detector functional partitions or rather the hardware parts of system. Lets have as an example LV system presented as in first approach where HV will follow system divisions.

1. LVsystem for the TRT detector

1. Barrel detector
* Side A
* Module 1 through 32
* Side C
* Module 1 through 32
2. Endcap detector
* side A
* Wheel 1 through 18
* Sector 1 through 32
* Side B
* Wheel 1 through 18
* Sector 1 through 32
2. HVsystem for the TRT detector1

Footnote 1: Definition of objects in this system can also follow directly the one adopted in document for LV power supply system.

_group of crates2_
* _group of modules_
* _group of channels_
* individual channel
4. Cooling systems * Gas cooling * Gas supply * Barrel ventilation system * Endcap A * Pumping station * Pressure regulation * Gas distributors A,B,C,D * Endcap C * Pumping station * Pressure regulation * Gas distributors A,B,C,D * Liquid cooling * Iced water system (or local cooler unit) * Filling unit * Cleaning unit * Cooling stations 1 through 5 * Station controller * Pump * Tank * Liquid distributors 1 through N to electronics/modules * Liquid distributors to heat exchangers
5. Electronic racks control system * Surface counting room * Gas control room * Underground control room * Electronics racks * Gas racks * Power supply racks * Experimental cavern 1. Gas racks 2. Power supply racks
6. Interlock system
7. Connection to controls of the system of Uninterruptable Power Supplies
8. Connection to centrally supplied environmental parameters
9. Connection to Central Safety System

In addition, the DCS system has connections to centrally provided services: the Global DCS, the Central Safety System, the Environmental Control System and the LHC machine control system. In principle, these direct connections should be redundant, since sole connection to the G(Global)DCS should suffice. However in our understanding they are necessary to guarantee safe_stand-alone_ operation of the detector.

To perform this function we envisage the DCS hardware structure illustrated in Figure 3-2, Figure 3-3, Figure 3-4 and discussed in the following paragraphs.

### Hardware

#### Local control stations

LDCS has two basic layers - _central_ controls in form of the LCS and _local_ subsystem specific _controllers_ which realize the important policy of the independency of the subsystems and they ability for stand-alone operation.

Two Local Control Stations (the type and nature of the'station' will be defined later. It can be either VME crate or powerful PC or any other computer system) handle the entire system. They contain interfaces to the controllers of the TRT subsystems, input-output registers for less complicated tasks and the CPU which holds the operating system and runs the monitor and control software. The LCSs are connected together via a LAN on which Global DCS communications also take place.

The TRT user (identified through several layers of verification) can communicate with DCS either at

Figure 3: Functional diagram of TRT DCS hardware - surfacethe level of Terminals or via LAN from Local Workstation or remotely from any authorised computer

**TRT detector control system (DCS)**

**UNDERGOUND CONTROL ROOM**

**Local controllers - LC's**

We propose to use in every subsystem (with exception of the temperature measurements) the intelligent controllers - in form of the dedicated PC, CPU in VME or PLC - depending on the complexity of tasks and/or required functionality.

**Interconnections**

The LC's would communicate with the LCS over centrally adopted standard fieldbus - CAN fieldbus at the moment of writing of this note. The choice of this particular fieldbus should not prevent us from exceptional use of other (of three CERN wide fieldbuses) fieldbus which might for specific application prove its higher performance or better matching. If local controller is powerful enough, the use of ethernet might be envisaged as well.

**Sensors**

The sensors used for parameters measurements should be those adopted/proposed by central DCS group to maximize use of standards

Figure 3: Functional diagram of TRT DCS hardware - underground control room

### Software

Figure 3-5 shows the one of the possible software functional configuration at the level of the sub-detector controls which might accomplish the control tasks discussed in the introduction. Presented blocks are rather functional units than practical realisation of the software. Final implementation might be quite different in detailed structure, but we believe that functionalities proposed here represent well properly functioning system.

Individual hardware subsystems (Units on the figure - which represent high level 'objects' as described in section 3.1.2 are accessed through hardware specific interfaces (IF) or drivers connected to the Control Processes (CP) - one process for each of the subsystems. A Control Process is a piece of software which performs all the communication between the controlled hardware and all the other components of the system. The system components are treated as objects which can be in different states as the result of actions or spontaneous changes of state. The software blocks can be specified as follows:

* a software component which performs necessary logic as a reaction to the commands, change of object states, errors and emergencies. This is a port to the upper level of the DCS.

Figure 3-4: Functional diagram of TRT DCS hardware - underground caverns- connects the Local Work Station to the system. This a allows the user to issue commands, display monitored parameter values, set values of the parameters and monitor the behaviour of the controlled system
* connects to the Control Processes and provides the error handling. Logs the errors into log-files, warns the operator if deviations pass pre-set limits, Provides the necessary mechanism for an error priority system. Communicates with EMS of higher level.
* collects the values of the parameters from the Control Processes, updates and writes them to the Data Base. During the detector initialization phase, it reads the parameter values and passes them to the CPs.

The proposed organization of the software will hopefully be realized in standard CERN-wide tools, industrial SCADA system. Very important feature required from any software tool adopted would be ability for creation of objects and setting up of the Finite-State-Machines (FSM) which could implement the necessary logic for operation on the objects states.

This puts a heavy requirement onto the central DCS group. Most of the software should be provided by the ATLAS DCS group. Only hardware specific packages might be written by detector groups. However if the detector uses ATLAS standardized hardware (e.g. field-bus, sensors, power supplies etc.), the relevant software and its maintenance should be centrally supplied.

## 4 Central requirements - conformity with ATLAS DCS

The central requirements define following issues to be followed by the subsystems in theirs design of detector control systems.

1. Product perspective

DCS should be modular and should be composed out of well defined building blocks, both for hardware and software. Care will be taken to have as many building blocks as possible in common with DAQ. Whenever possible industrial standards will be used. The remaining building blocks will be provided by collaborating institutes.

2. User characteristics

Four kinds of users can be identified: the shift operator, the DCS operator, the subcomponents expert, and the subcomponent system developer.

3. Operational environment

Usually the operation will be done at a place remote from the detector. Hence there will be no direct visual feedback of success or failure of control operations. The access to many parts of the detector will be difficult or even impossible as it needs dismantling or for reasons of radiation. Therefore feedback about success or failure of a control operation has to be given via DCS itself, e.g. dedicated sensors.

4. DCS Subsystem States

This section describes the different states which DCS subsystems can occupy and the transitions between them. For TRT DCS the central rules has been somewhat modified and described in Section 3.1.1

5. Detailed user requirements for DCS

**TRT detector control system (DCS)**

**Software organization**

The general user requirements are those listed in document [13-1]. Here we give only brief listing of the requirements referring the reader to original document for more details. Basically general/central user requirements are integral part of this note. They are classified into different categories with the following identifiers:

**SYS**: General system features including operational modes

Specific TRT applications of this rules'see Section 3.1.1)

#### 3.1.4 Central requirements - conformity with ATLAS DCS

Figure 3-5: Schematic diagram of possible solution for TRT Detector Control System software* [MON] Monitoring of parameters The operation of a piece of equipment will be monitored by comparing its parameters, which are either analogue or Boolean values, with predefined values. These may depend on the operational state. The combination of its parameters will result in the overall state this piece of equipment is in (treatment of the equipment as object(s)).
* [OPER] Operations This part of requirements deals with basic operations. It has to be possible to reconstruct the evolution of a piece of equipment in retrospect (operational parameters, commands, fault states, actions, etc.). This is essential for the diagnostics of problems.
* [ASS] Operator assistance This part of requirements deals with advanced controls tools. This includes guidance of the operator based on expert knowledge about the equipment and on previous operations and faults. It also deals with alarm handling and the (automatic) execution of complex commands.
* [UI] User interface This part of requirements deals with computerized tools necessary for easy human access to all information and control tools.
* [STAN] Standardized hardware and software It is the aim to use the same hardware and hence the same application software whenever possible for all subcomponents. This helps saving effort and building up common expertise.
* [INFRA] Experimental infrastructure The experimental environment will be organised for both managerial and geographical reasons into several subcomponents (electricity distribution, gas systems, interface to safety system, cooling systems, etc.)
* [SPEC] Specific requirements It should be possible to use complex control programs in the LCS which are written in a _high level language_.
* [ACC] Access to information Here are defined rules for longevity of information stored in logfiles, logbooks, database etc
* [CON] Constraint requirements Constraints concerning the operations of the DCS and its dependencies on other functional blocks of ATLAS apparatus and LHC machine.

### General requirements for TRT DCS

#### 4.1.1 DCS specifications

TRT detector consists of three basic parts:

* Barrel
* EC-A
* EC-C

Both EC-A and EC-C are identical, from point of view of construction, operation and services. Certain elements can be mirror image of its counter-part from another end-cap.

Each of the three parts should be absolutely independent from operational point of view and be able to operate stand-alone.

In the following sections we will go through all subsystems of TRT DCS formulating the requirements and specifications (if possible and known today) for its control and monitoring.

#### 4.1.2 General Description

Definition of certain terms used within the requirements section:

* lasting for several weeks/months
* lasting for several days
* lasting for few hours

#### 4.1.3 General requirements

**UR AC T0 1**:
* The system shall provide control of access based on login/password. The login/password will be defined for the users' role.
**UR AC T0 2**:
* The system shall automatically log a user off after defined period of inactivity.
**UR TAC 0 1**:
* The system shall be accessible from the experimental area without any restrictions on the available capabilities.
**UR TAC 02**:
* The system shall be accessible from CERN offices, through the Web, without any restriction on the available capabilities.
**UR USB 1**:
* The TRT DCS shall support three categories of users: observers, operators, experts.

[MISSING_PAGE_EMPTY:22]

**UR LOG03**

The TRT DCS system shall log all the alarm status changes.

**UR LOG04**

The TRT DCS system shall log the operator commands which trigger operational state transitions.

**UR OPEG1**

All controllers (PC, computers, embedded CPU's, PLC's etc) in the TRT DCS system will have their clocks synchronised with accuracy to 1 sec.

**UR GRAN0**

The TRT DCS system should follow the granularity of the detector, electronics and its services.Allowed grouping of the commands should be consistent across different subsystems.

## 5 Temperature Control System

In this section we propose a method for temperature control and monitoring together with layout of the sensors in both parts of the detector i.e. barrel modules and end-cap wheels.

The temperature measurements (and sensor distribution) in the external to detector parts of the apparatus i.e.

* cooling liquid plant;
* cooling gas plant;
* active gas system (with all its elements);
* electronics racks;
* read-out electronics;
* power supplies;

is basically internal issue for given system and should be covered in the design note of the pertinent part.

### Heat sources

The entire volume of the TRT detector has to be kept at a constant temperature of about 25\({}^{\circ}\)C. To fulfil this requirement, heat produced inside the detector has to be efficiently removed.

There are two principal heat sources in the TRT detector:

* front-end electronics (including cables from low voltage power supplies): The electronics will be cooled by properly laid-out and structured liquid circuit. The power cables will be situated on cable-trays together with return lines of the electronics cooling circuit, which as proved by calculations and measurements is enough to keep them thermally neutral.
* straws; The ionisation avalanche current in the detector during high luminosity running results in quite important power losses in the straws. The end-cap detectors will be cooled by a two-level system: combined gas/liquid cooling, where a flow of gas first removes heat from the detector and then transports it to a heat exchangers. There, the heat is transferred to cooling liquid, which transports it outside the detector. The barrel modules will be cooled by a liquid system which pipes will be in direct thermal contact with modules shells.

### The monitored areas/granularity.

In this section are shown the areas where the temperature is monitored. This are:

* detector mechanics (for stability of the structures);
* detector detecting elements (for gas gain stabilisation);* electronics (for life time of the components);
* cooling circuitry both gaseous and liquid (for efficiency of cooling and control of the gradients).

The temperature of the liquid will be monitored to \(\pm\,0.5\,^{\circ}\)C, and that of the front-end electronics to \(\pm\,2\,^{\circ}\)C. The gradients along the detecting elements should not be bigger than \(10^{0}\)C to ensure expected quality of performance (more details how to ensure this goal will be given in the chapter dealing with the cooling systems).

#### Gas cooling system for the end-cap

Inside one group of wheels (where "the group" are all wheels of the same type), all individual wheels are connected in series and separated by liquid cooled heat exchangers. Each wheel1 in a group is equipped with a heat exchanger. Within the wheels, the gas is supplied from the inner circumference and is flowing radially out and at the end of group leaves the end-cap volume at the outer radius. Number of cooling circuits (per side) is either 3 - if SCT support is placed between groups of wheels A and B; or 4 if this support sits within the wheels (wheels 3 and 4 in group A).

Footnote 1: wheel is 16-plane unit as in TDR

#### Liquid Cooling System

The liquid cooling system serves:

* electronics; both in end-cap wheels and barrel modules;
* heat exchangers in the endcap cooling system;
* barrel modules;

**Note**: Endcap:

There are 15 inlet and 15 outlet pipes for heat exchangers.

There are 2 input and 2 output pipes for electronics cooling in each wheel i.e. total 36 input and 36 output pipes per end-cap

**Note**: Barrel:

There is 1 input pipe for 6 modules and 1 output pipe for 2 modules - serving cooling of both electronics and detectors; i.e. total 16 + 48 = 64 pipes per side of the detector

#### Electronics

**Note**: Endcap;

The wheels have following number of sets of FE-boards: type A and C - 192, type B - 96 i.e total number of sets is 2688 per side. Several sets of boards are grouped together by roof board. There is 576 roof boards in endcap.

**Note** Barrel;

Each module has on its face-side numerous stamp-board electronics cards, grouped together by 2 roof-boards per module, i.e. total 192 roof-boards per detector side

#### Mechanics

**Note** Endcap;

There are 28 8-plane wheels per side in the endcap. They are mechanically independent detector subunits during construction and assembly. The sealing system - CO2 envelope connects them into 16-plane wheels.

**Note** Barrel;

There is 96 barrel modules.

#### Active gas

**Note** Barrel;

2 pipes per module (per side; where on one side is input, on the other output) i.e. total 192 pipes per side.

**Note** Endcap;

4 input and 4 output lines per wheel i.e. total 144 lines per side.

### Available cabling.

The detector cabling structure provides a set of cables over which the controlling functions can be executed within the detector. The number has been defined in the TDR and at present, we plan to use these for temperature measurements only. The number of lines in the cables is limited to total _2688_, where 192 are for barrel and 1152 for endcap (as given for one side only).

This number corresponds to 2 twisted-pair cables per 1 barrel module (per side) and 1/32 of the endcap wheel.

### Temperature measuring method

**The sensor**

Temperature is the most often measured environmental quantity. Several temperature sensing techniques are currently in widespread usage. The most common of these are RTD's - resistance temperature detectors, thermistors, thermocouples and sensor integrated circuits. The integrated circuits we have to disqualify immediately due to theirs radiation softness.

Platinum RTD's are sensing elements that are made of pure platinum wire coil encapsulated in ceramic or glass (wirewound) or a thin film deposited on a ceramic substrate (thinfilm). Platinum RTD's have a positive temperature coefficient, the resistance increases as temperature rises in a known and repeatable manner. Thin film elements offer performance equal to standard wirewounds, but with improved cost, size and convenience. Compared to thermocouples they have following advantages:

* RTD elements provide a much larger voltage drop signal than do thermocouples.
* Platinum RTD elements follow a more linear curve than thermocouples and most thermistors.
- Platinum RTD utilize standard copper extension leads and require no cold junction compensation.

Another, not negligible argument for platinum RTD's (PT-100, PT-1000) is the fact that they offer excellent immunity to radiation and very low sensitivity to magnetic field - about 0.01% (AT/T) at 2.5 T (2 T in the TRT!) at 300\({}^{0}\)K.

Thermistors - build usually from ceramic materials has been already tested (both by manufacturers and at CERN) and are radiation hard. In certain applications - where high sensitivity is needed and nonlinearity is of no concern - they will be used in TRT [13-4].

Application in TRT

The method used bases on injection into the probe of precisely known current (highly stabilized and temperature compensated) and measurement of the voltage drop which then varies with resistance/temperature of the circuit. We will use any standard temperature sensor adopted by ATLAS, but at the moment we propose resistive sensors (PT-100; PT-1000 or thermistors) with either two-wire or four-wires! read-out. A two-wires read-out introduces quite considerable errors due to very long cables (80 - 100 m) and fact that resistive thermal coefficients of platinum and copper are nearly equal. If it would be possible to keep the cables in the constant (or at least well known) temperature, error introduced could be easily calculated and corrected for. Using PT-100 which have 100 \(\Omega\) resistance at 0\({}^{0}\)C would lead to the errors from cables in order of 5%\({}^{0}\)C (cables resistance is about 53 \(\Omega\)). To avoid this problem two methods could be applied:

1. use sensors with high resistance (PT-1000 or even PT-10000 in thin film execution), where contribution from cable would be negligible;
2. monitor cables resistance.

The second method seems quite attractive since could give higher accuracy (in first one - with PT-1000 i.e. 1000 \(\Omega\), the error would be acceptable but still 0.5%/C) and be easily implemented. The idea is following:

some cables for temperature measurements will not be equipped with sensor, but short-circuited and supplied with exactly the same current as all active circuits. The measured voltage drop would then be used as correcting component for other measuring lines.

### Layout of the sensors

Accordingly to granularity described in section 5.2 we propose following scheme of temperature monitoring.

#### 5.5.1 Barrel modules

To monitor the module temperature position of the sensors will be defined by module designers.The cables should be brought out to the roof-board and there incorporated in the cable harness.

The cooling circuit - both for electronics and module should have one sensor in each supply and return lines. These sensors, in principle should be as close as possible to the boards i.e. close to the cooling plate(s) but due to limited space they might be as well placed on the manifolds outputs/inputs. Probablyanother, more economical solution could be adapted as well, i.e. only one sensor on the supply manifold and separate sensors on each output line to control individual circuits. All this sensors should have very low thermal resistance connection to cooling liquid.

The temperature of the active gas does not need to be measured.

There should be at least 4 sensors placed on the support structure.

There should be at least 4 sensing lines (per side; every 90\({}^{\circ}\)) for cable compensation if PT-100's are used.

Thermal safety of the electronics can be secured by similar thermistor arrangement as for the wheels (see Section 5.5.2). Final solution depends on the detector details and should be proposed by its designers.

Following table summarizes number of sensors for barrel (per side).

\begin{tabular}{l c} \hline
**Sensors position** & **Nb. of lines/sensors** \\ \hline Modules & 96\({}^{\circ}\) \\ Detector and electronics cooling & 64\({}^{\rm b}\) \\ Support structure & 4 \\ Compensating lines & 4 \\ Total nb. of lines & 168 \\ Lines available & 192 \\ \hline \end{tabular}

* modules monitored alternatively from both sides
* only one sensor for input manifold

#### 5.5.2 End-cap wheels.

To monitor the temperature of the mechanical structure of the wheels we should place at least 4 (every 90\({}^{\circ}\)) sensors per one 8 plane wheel. This sensors should be placed at the gas seal layer of the ring 3 i.e external surface of the ring 3. A short 2-lines cable should connect these sensor to the roof connector.

To ensure monitoring of the temperature gradient along the straws we should also place at least 2 sensors on the internal ring in each 8-plane wheel. Connection to the read-out could create some problem, but the cable should run to the outer rings.

To monitor the efficiency of the liquid cooling circuit for electronics we should place one sensor in every supply and return line. To monitor - for fire safety reasons - the temperature of the boards one line per 1/32 detector will be reserved. Final solution - how to ensure efficient way of protection will be defined later. One of the possible solution would be to use thermistors and connect few of them (as many as board-stacks are in detector element) parallel. This idea seems feasible [13-4] but needs further studies and possibly additional tests basicaly to check the dynamic behaviour of the system. Mechanical design of the thermal contact of the board to cooling plate should be made such as to prevent loss of the contact (solid,'screw-like' connection).

To monitor the efficiency of the detector cooling we should place sensor in each gas and liquid supply and return lines for heat exchangers. We should also place at least 2 sensors (at top and bottom) in the cooling gas volume, after every heat exchanger.

We should place at least 4 compensating lines (every 90\({}^{\circ}\)) per group of wheels.

A following table (Sensors position) summarizes the number of lines per endcap.

### System monitoring and control

The presented positioning of the temperature sensors within the detector volume has been optimized from the point of view of detector safety, stability of operation and the control of important regions. Platinum thermal resistor sensors PT100/PT1000 will give a relative precision better than 0.2 \({}^{\circ}\)C and the corresponding absolute precision will be better than 0.5 \({}^{\circ}\)C.

#### 5.6.1 States of the object in temperature monitoring system

The object in the temperature monitoring system can be in one of the following states:

* when system can be controlled but local hardware is not functioning.
* where object is being monitored, control is possible and parameters are within limits.
* loading of the parameters; reading from database
* when any of the parameters of object is outside allowed limit value; for a complex object it is 'or' of statuses of all its components
* this state can be combined with any other; signifies that object error state does not enter into state of the group which the object is a member.
* control is possible either from local system _ONLY_, or can be performed from both central and local stations. Normal running happens in GLOBAL state, LOCAL one is used for debugging and maintenance.
* for some reason, the control/monitoring of the object is impossible, and the real state of the object is unknown (can be any of the itemized above).

#### Operational requirements for temperature monitoring system

**UR OPTEMP1**

System controls shall respect all general requirements of higher levels (central for ATLAS and local for TRT)

**UR OPTEMP2**

System controls can have features which enhance meaning of general requirements

**UR OPTEMP3**

System shall measure temperatures with accuracy of 1/1000 (0.3 K\({}^{0}\))

**UR OPTEMP4**

System shall guarantee medium term stability (1 hour) of the measurements at level of 1/1000

**UR OPTEMP5**

System shall guarantee long term stability (1 day) of the measurements at level of 1/1000

**UR OPTEMP6**

System shall conform to technical specifications described in pertinent notes.(to be defined later)

**UR OPTEMP7**

System shall access every measuring point at least once every 30 seconds.

**UR OPTEMP8**

System shall generate warning signals when any of the measuring points deviates from nominal value by amount defined for a warning.

**UR OPTEMP9**

System shall generate the alarm signal when any of the measuring points deviates from nominal value by amount defined for an alarm.

**UR OPTEMP10**

System shall be coupled to the interlock system to guarantee the safety of the detector even in the case of 'no-control' state.

#### System control functions

This section enumerates requirements for controls functions which should be implemented in the system to allow operations in the demanding environment of the LHC experiment.

**UR CTEMP1**

System shall allow reading the actual value of the temperature.

**UR CTEMP2**

System shall allow to set for every channel the 'normal' state temperature to be compared with actual one.

**UR CTEMP3**

System shall allow to set for every channel the temperature 'window' (dead band) inside which temperature changes does not cause any action of the DCS system.

**UR C NTEMP4**

System shall allow to set for every channel the limits for 'warning' and 'alarm' messages.

**UR C NTEMP5**

System shall allow to set different access frequency for group of channels.

**UR C NTEMP6**

System shall allow to form a group of channels or LMB's.

**UR C NTEMP7**

It should be possible to add channel to group without stopping the system controls

**UR C NTEMP8**

It should be possible to remove channel from group without stopping the system controls

**UR C NTEMP9**

System should allow to mask the channel.

#### 5.6.4 Commands implemented by system.

Basically the temperature measuring system is a 'passive' one i.e. delivers only the information but is not steered. Thus only monitoring commands will have application within system:

* initialize the configuration
* load equipment with default parameters for nominal temperatures expected in setup as well as values for alarms and warnings as defined in the database.
* load parameters from slow-control database (LOAD_DEFAULT) and RUN i.e.: initialize the polling loop.
* as for START but without parameters loading
* forced read-out of all parameters and statuses (action as'refreshing')
* an action necessary for proper handling of alarms
* disables central control of the equipment
* allows central control of equipment

### Correlations with other systems.

Control and monitoring of the temperatures has basically two operational aspects:

1. Safety of the detector and (fire, melt-down, accelerated aging of chemical compounds and electronics)
2. Proper operational conditions for the detector - uniformity of the gaseous gain both within one detector element and within whole detector

None of this functions can be guaranteed by the TM system itself. The first one needs strong coupling to the cooling systems - both liquid and gas, the second one can only be achieved in cooperation with gas system - i.e. its part responsible for gaseous gain stabilisation and high voltage power supply system, and cooling system to watch the gradients along the detecting elements when working at highest luminosity.

The stabilisation of the gain has been described in the pertinent part of the note dealing with gas system. Safety aspect has its discussion contained in the section devoted to cooling systems.

### Configuration of the hardware.

A temperature monitoring system can use standard LMB module developed presently in ATLAS DCS central group. One LMB-ADC board can monitor 64 temperature channels and is being read-out via CAN fieldbus. For the detector itself1 we will need maximum 2688 monitoring channels i.e. 42 LMB-ADC's. One CAN-node can control 6 ADC modules (384 channels) thus we will need 7 sets of CAN-node+ADC's. Probable cable length (\(<\)100 m) limits the bandwidth to 500 kbits/sec. The conversion speed of the ADC - as default - is set to 15 Hz (maximum 202 Hz), making the refreshing time of the analogue value equal to 4.3 secs (0.3 sec min.). The temperatures should be monitored at every point with frequency about 0.3 Hz thus with quoted ADC's default conversion rate one should use only one field bus cable - this is a limit due to ADC itself.

With 7 nodes on one cable available bandwidth is reduced to 70 kbits/sec/node. If each channel produces only 4 bytes of data (2 bytes analogue value/2 bytes channel nb.) the amount of data to be transferred amounts to 11 kilobytes or 90 kbits. It seems that with proposed frequency of data refreshing the available bandwidth should be sufficient. Final number of cables/interfaces will be chosen when more is known about higher level protocol used and overhead introduced by software.

Footnote 1: External equipment of the detector - cooling plants, gas-system etc. will need additional number of sensors, which will be defined in pertinent notes describing the design of the systems

The monitoring hardware2 can be placed close to the power supply crates and the patch panels. The LMB's will be placed within simple racks positioned at the shelves around the detector. (LMB does not require any special crate and can be mounted in industrial type enclosure used for power [220/380 V] distribution). The interfaces driving CAN-fieldbus will be placed in the Underground LCS crate. The CAN-fieldbus cable will connect the local CAN-nodes to interfaces, so the length of the cables will be in order of 70-80 m.

## 6 Gas system

The functionality and role of the gas system have been described in the TDR. The importance of the gas system for the correct performance of the detector necessitates its fully stand-alone operability. The role of the TRT DCS is to serve both, as a supervisor of the stand-alone (and integrated) operation and as a platform where information from different systems is collected and correlations between them are performed. Basically this section will describe only the 'external' aspects of the gas system control. The internal control, steering and regulation of the system is a duty of the system designers. The internal Gas DCS system will monitor and control the following parameters;

* Steering of the execution of functional states: running, purging, filling, stand-by etc;
* setting of the valves, compressors, pumps etc; which basically is part of the execution of functional states
* Quality of the gas mixture i.e Xe/CO\({}_{2}\)/CF4 concentrations, contamination levels of H\({}_{2}\)O and O\({}_{2}\) and other pollutants;
* Pressures and flows in different parts of the system;
* Gas supplies status (pressure), reserve etc.;
* purifying quality, status of purifying media (for maintenance purposes);
* monitoring of the position of Fe-55 peak and delivery to the central TRT DCS the reference voltage from the reference straw(s).

The status of the gas system should be tied to the detector safety interlock, which, in case of unpredictable behaviour, uncontrollable parameter drift or any predefined emergency condition would activate detector shutdown (high voltage, low voltage etc) necessarily via hardwired connections to be operational even in case of lost of controls.

### Gas system overview.

In this section we give short reminder of the TRT gas system functionalities without going into its internal details. The control of the system itself, its regulation and internal automatics is not going to be covered in this document. The gas system being most complicated - from point of view of the automatics object controlled by DCS will be presented in another document being prepared by CERN Gas Working Group in close collaboration with TRT institutes having interest in gas system development. Herein we will treat gas system and its functional parts as objects and will try to describe them in adopted through this document 'object' terms.

#### Objects in the TRT gas system.

The TRT gas system is shown in Figure 6-1. It can be separated into functional modules which from point of view of the controls represent objects (and pertinent sub-objects where important).This object has been already enumerated in chapter Chapter 3.1.2.

#### Gas system operation

All the equipment for the gas distribution and the pressure regulation in the TRT is placed underground in USA15. The gas supplied to the TRT is maintained between 1 and 20 mbar above atmospheric pressure in the cavern. The equipment placed in the surface building operates at a pressure within the range of 1.4 to 1.8 bar (absolute pressure). Gas from the detector in the underground cavern is sent back to the surface building and to a differential flow-meter, where the gas flow is split into two streams: the main gas flow goes directly to a static mixer and the other part, which size depending on the amount of CO2 to be removed - goes first to a CO2-removal system and then to the mixer.

After the mixer, the gas goes to the purification system and to a buffer volume of about 1 m. This buffer volume is used for compensation of the atmospheric pressure variations which can be quite large (up to 10%); an atmospheric pressure change by 5% leads to a drop of the pressure in the buffer volume from 1.6 to 1.45 bar (absolute). If the pressure in the buffer volume drops below 1.4 bar, gas is added from the gas-supply system. In case of an over-pressure (above 1.8 bar), the gas is discarded into the gas-recuperation system.

An analysis system is used for the on-line monitoring of the active gas. It is normally connected to the output line of the gas purification system. Analysis of the gas from different parts of the system is foreseen. The gas-analysis system gives information to the slow control system, which controls the changes of the gas composition.

Figure 6: Block diagram of the TRT active gas system.

The detector will be purged with CO\({}_{2}\) before being filled with the active gas (also during the shutdowns it will be filled with CO\({}_{2}\) only). A special line in USA15 is foreseen for this purpose. As soon as the active gas begins to fill the detector, the gas arriving from the detector is sent through the CO\({}_{2}\)-removal system. The gas flow through the CO\({}_{2}\)-removal system is decreased with time, as soon as the gas mixture becomes close to the nominal one.

Three pipes (IN, OUT, RECUPERATION) go from the surface building to the underground area. The USA15 distribution system is responsible for the TRT pressure stabilisation, the total gas flow, the protection of the TRT from over-pressure, and the compensation of variations of the atmospheric pressure in case of a power cut. Continuous gas-quality monitoring from the TRT outputs is done through a second gas-analysis system, which provides information about the quality of the gas in each TRT module. A gas-gain stabilisation system is also placed in USA15 and connected to the input gas pipe (i.e. one which goes to the cavern)

Three pipes are used for the connection of the USA15 main distribution system to the cavern distributor. One is the gas IN, the other is the gas OUT and the third line (with smaller diameter) is used for the gas analysis from the output of each TRT module.

The final gas distribution is done in the racks placed inside the inaccessible UXA cavern. The incoming gas is distributed over six lines corresponding to six groups of TRT modules/wheels, each containing a certain number of identical TRT modules. Different gas-flow rates are set using remotely controlled needle valves for each super-module, depending on the number of straws. In each line, the gas flow is then split according to the number of TRT modules. An electro-valve in each TRT module line can be used to block the gas flow. The flow within each module is measured with the flow-meters - necessary arrangement should be designed to allow detection of the leaks - by comparison of the input and output flows. The return lines are connected to the multiway-way sampling manifold, to allow gas samples to be taken from each module.

### States of the objects in gas system

The objects in the gas system can be in the states described in this section (some of the states are exclusive, others not and the object can ignore some states as well).

* where control of the object is possible but there is actualy no internal functioning of the object.
* channel, valve, etc.)
- which is a state where control of the object is possible but its outputs are switched off (or set to 'ZERO'), or object is in any other way not prepared for data taking (but not in ERROR).

* For any part of the gas system (apart from channel or valve), basicaly this state is ONLY when there is no active gas flowing and detector is purged
* where object is switched on, its state is stable, control is possible and parameters are within limits.
* this is basic running state of the gas system. Only at this system state the HV can be (automaticaly) applied to the detector
* basicaly a normal' state of the object but at the conditions which are not suitable for physics run.

**Note**: This state can signify ex. detector condition during shutdown when flows are significantly reduced to save the Xenon gas, but otherwise all other parameters are nominal (e.x. gas composition). The HV can be applied to the detector for test purposes.

* loading of the parameters; reading from database
* state between NOT_READY and READY (can be also
- FILLING, PURGING, ADDING-CO\({}_{\rm 2}\) or any other intermediate state)
* when any of the parameters of object is outside allowed limit value; for a group (i.e. complex object) it is 'or' of statuses of all its components
* this state can be combined with any other; signifies that object error state does not enter into state of the group which the object is a member.
* only monitoring i.e. reading of the values of certain parameters is allowed (all control action are performed by local controllers in automatic fashion)/maintenance, manual settings of values and any other interventions are allowed.
* for some reason, the control/monitoring of the object is impossible, and the real state of the object is uknown (can be any of the itemized above).

### List of User Requirements

As it has been already mentioned all internal regulations, settings and automatics assuring proper operations of the gas system are performed by dedicated control system of the gas system itself. Here we are concerned with requirements for controls and monitoring performed by detector DCS over gas system as an object. Since the gas system is most crucial for operations of the detector it is proposed that centraly monitoring is allowed, i.e. reading of the values and statuses. All other control actions like changing the settings, starting, stopping etc. shall be allowed only from the console of the gas system control station.

**UR GASNON01**: The system shall have read acces to all parameters of the gas system hardware.
**UR GASAL01**: The system shall accept and properly handle the warning and alarm messages coming from the GCS (gas control system).
**UR GASAL02**: In the NOT_READY state of the mixer object, the excursion - even substantial - from nominal values for flows and mixture contents are accepted and no warnings nor alarms are generated.
**UR GASCONO1**: In the AUTOMATIC state of any module, the DCS shall not allow any 'combined' parameters to be changed.
**Note**: ex. the combined parameters are the gas components contents in the mixture. This requirement is meant not to upset the regulation function of the local 'hardwired' controller or PLC. Its regulation has to be disabled by going to MANUAL and only then the settings changed.

**UR GASCO N02**

Operator shall be able to force a transition from RUN to STOPPED (or OFF) states for any object in the system. Some of the actions can be blocked if it requires prior stopping of correlated object.

**UR GASCO N03**

Operator shall be able to force a transition from STOPPED (or OFF) to RUN states for any object in the system. Some of the actions can be blocked if it requires prior activation of correlated object

**UR GASOPE01**

In the STOPPED state of mixer and distributor objects in the gas system, there shall be no gas flows.

**UR GASUP501**

When the system is running from UPS i.e. mains status is OFF, the system should be forced to STAND_BY state.

**UR GASOAG01**

The readings from the gain stabilisation system shall be used by gas gain stabilisation algorithm as applied for whole detector.

**UR GASOAG02**

The readings from the temperature measuring system shall be used by algorithm for gas gain stabilisation.

**UR GASOAN01**

The readings from the gas analysis system shall be used by mixer unit to correct the gas contents.

**UR GASOAN02**

The readings from the gas analysis system shall be used to control proper functioning of the purifier unit.

**UR GASAUX01**

The system will control the crates housing gas equipment according to the standard way adopted ATLAS-wide.

### 4 Commands implemented by the system

The central TRT DCS should acces the gas system control only to get the monitored data. All operational commands like starting, stopping, changing any parameter, performing any maintenance should be STRICTLY reserved to be performed from Gas System Control Station.

So basicaly only command which can be executed form TRT LCS is:

* to perform forced readout of all parameters and statuses (action as'refreshing')

## 7 Power supplies system1
Footnote 1: Many ideas presented in this chapter are taken from [13-5]

An attempt is made to specify controls of the power supplies system without differentiating between low and high voltage supplies. It is straightforward to see that low voltage system is a subset of the high voltage one i.e. all features of the low voltage system are contained in characteristics (specifications) of the other one. The specifications pertinent for high voltage system _only_ are marked by special symboF.

### Power supplies system issues

Below the system issues are recapitulated in brief form to serve as a guidance for better understanding of the specific requirements for its control, which appears in following sections:

* Power supply system is contained in the crates.
* Crate contains output modules
* Module contains channels.
* Channels in module and crate can be grouped for control purposes.
* Crates controls can be daisy-chained to have less than one crate controller per crate.
* System controls should:
* allow fast loading of parameters to keep start-up or recovery times as short as possible.
* allow reasonably fast monitoring
* allow polling at low level for minimization of data traffic and operation with dead-band concept (windowing)
* allow for priority scheme of error reporting (according to proposition of central DCS group)
* allow for individual and group commands (which should be based on hardware design and not being emulated by low level software)
* allow for local storage of default (or last) parameters in case of power cut or loss of remote control
* allow for interlock system at different levels. Status of interlocks should be available to control system
* in limited range allow for simple local/hand control in case of emergencies or loss of remote control

### Setting up a system

To initialize the system the group commands should be used. The group command should be available at every level of controls, i.e. groups might incorporate racks, crates, modules and channels. If group commands are not supported, or not used for any reason, then all input parameters must be set one by one. This setups certain requirements for bandwidth of the communication path. The constraints on the communications path between the control computer and the crate are even stronger, due to foreseen daily-chaining of the crates on the same path. The bandwidth of the medium used must also take into account the protocol and overhead.

### Monitoring a system

Old fashioned way of the channels access based on polling and master-slave relationship seems to be obsolete in present day power supply systems. Each crate (or group of crates) will be equipped with local controller allowing for stand-alone, autonomous operation and application of the monitoring based on '_dead-band_' or '_windowing_' concept. Even if for certain systems no windowing for voltages/currents will be used still the errors, i.e. trips, overvoltages or overcurrents will be generated and reported spontaneously in asynchronous way by the crates.

On this model, in addition to the requested voltages and currents, warning and alarm limit parameters would be provided (per channel). The local controller would then compare the requested and actual values of the voltages and currents and if the difference between two readings were not within the defined bands, or a channel state changed, then the crate would send the new value to the control computer without being polled. Given the stability of the power supplies, and assuming the bands were set to a few per cent of the nominal values, this would reduce the network traffic, and the load on the control computer, quite substantially. It will consequently reduce the size of the slow-control database written to permanent media.

In order to be sure that the reason for not receiving messages is that the voltages and currents are all stable, and not that the communications path or the system crate itself has died, then it must be foreseen that a crate which itself carries out the monitoring operations sends regular messages (heart beats, watch-dog signals) to the supervisory system. Failure to receive these within a time limit calculated on the basis of the requested transmission frequency then indicates that something has broken.

The use of this approach assumes that the communications path supports multi-master operation. Many industry-standard field buses are available with this capability, and ATLAS collaboration has accepted the use of a CAN-bus. One could also consider the use of ethernet, as the cost of an ethernet connection is now trivial compared to the cost of a power supply system, however ethernet being not deterministic transport medium can not guarantee observation of the time-out mentioned in preceding paragraph. In any case, it is mandatory that power supplies equipment provides a communications path via standard bus presenting multi-master operation, which would greatly ease the effort required for support and maintenance.

The list of possible scenarios for system operation might include the following:

* Basic system where supervisor polls crates and generates warnings and alarms. Could read single values or make a block read.
* Local controller (crate) reports to supervisor if channel values go outside dead-band.
* i.e. go into error state (disabled during voltage ramping).
* Local controller (crate) sends single message on occurrence of alarm avalanches due to nearly simultaneous error state of many channels.

## 6 Alarm Avalanches

In the case of HV applications it may occur that many channels trip (almost) simultaneously. For crates with many channels this could be a problem for the multi-master operation discussed above because a large number of messages would be sent to the supervisor(s). Thus, consideration should be given to finding a way to group such alarms into a single message, containing a list of the channels with errors. The efficiency gain would depend on the bus and protocol chosen.

Additional (optional if considered to costly) parameters needed for a multi-master system might include those listed further. The terms _Low_ and _High_ refer to values below and above those nominally requested, whilst _Warnings_ and _Alarms_ indicate that a minor or major fluctuation has taken place.

### Controlling a system

Controls of the system are described in terms of operations on objects and its states - the model depicted in more details in the Chapter 3.1.1 and Chapter 3.1.2.

#### States of the object in power supply system

The object in the power supply system can be:

1. a channel,
2. a group of channels,
3. a module,
4. a group of modules,
5. a crate,
6. a group of crates,
7. a HV or LV system for any partition of the detector
8. any other system partition as convenient (and defined) from operational point of view.
9. the system of power supplies

The object in the power supply system can be in one of the following states (some of the states are exclusive, others not and the object can ignore some states as well):

* when system is under control but mains is not ON.
* channel, module, crate)
- which is a state where control of the object is possible but its outputs are switched off (or 'ZERO'), or object is in any other way not prepared for data taking (but not in ERROR).
- where object is switched on, its state is stable, control is possible and parameters are within limits.
* basically a normal1 state of the object but at the conditions which are in between the 'off' and 'on' states (ex. intermediate HV on the detector during filling up of the LHC). Footnote 1: ‘normal’
- as opposed to in ERROR
* loading of the parameters; reading from database
* state between NOT_READY and READY (ex. ramping the voltages up/down) or between READY and ERROR
* when any of the parameters of object is outside allowed limit value or tripped; for a group (i.e. complex object) it is 'or' of statuses of all its components
* this state can be combined with any other; signifies that object error state does not enter into state of the group which the object is a member.
* control is possible either from local system _ONLY_, or can be performed from both central and local stations. Normal running happens in GLOBAL state, LOCAL one is used for debugging and maintenance.
* for some reason, the control/monitoring of the object is impossible, and the real state of the object is unknown (can be any of the itemized above).

#### User requirements for PS controls

**UR OPPS1**:

Users - expert and operator shall be able to access every low level control function separately.
**UR OPPS2**:

For all other categories of user only'monitoring' (i.e. reading of values and statuses) functions can be made available.
**UR OPPS3**:

Any trip of the power supply shall generate a warning. Depending on the size of detector affected by failure, the alarm might follow.
**UR OPPS4**:

If alarm has been generated its source can not be'repaired' without previous acknowledgement of the alarm.
**UR OPPS5**:

The alarm can be acknowledged only when prescribed procedure has been carried out. For certain kind of alarms, the procedure can be automatic i.e. accomplished by FiniteStateMachine based on expert database.
**UR SEPS1**:

Operations involving changing the group contents (i.e. removing or adding group member) shall be performed only when in SETUP state. Correct set of parameters AFTER the operation is performed must be defined for each kind of group and object concerned.

#### Low Voltage System

**UR OPPSLV1**:

Any long term, unidirectional drift of the voltage or current in LV system should generate a warning if value of the max.change is bigger than X%.
**UR OPPSLV2**:

System shall be interlocked to cooling system: any failure or alarm from cooling system shall switch off pertinent part of the LV-PS system.

#### High Voltage System

**UR OPPSHV1**:

Any long term, unidirectional drift of the voltage in HV system should generate a WARNING if value of the max.change is bigger than X%.
**UR OPPSHV2**:

System shall be interlocked to cooling system: any failure or alarm from cooling system shall switch off pertinent part of the HV-PS system.
**UR OPPSHV3**:

System shall be interlocked (hardwired system) to gas system: any failure or alarm from gas system shall switch off pertinent part of the HV-PS system.
**UR OPPSHV4**:

System shall be coupled to the gas system. also via software. Adequate action should be performed following the gas system status.
**UR OPPSHV5**:

The system shall be coupled to gain stabilisation system and to temperature monitoring system to adjust value of the HV accordingly to temperature changes within the detector. The granularity of this correlation should follow the granularity of the temperature sensors.
**UR OPPSHV6**:

The system shall perform certain actions in automatic way. This will be defined during running-in but basically should cover all 'trivial' cases of the trips.
**UR OPPSHV7**:

The gravity of the trip reason shall be determined on the basis of 'history' and expert system database.

#### System control functions

This section enumerates controls functions which should be implemented in the system to allow operations in the demanding environment of the LHC experiment. The contents is structured from bottom to the top i.e. from smallest granule which is the channel to the top i.e. system as the totality.

**The Human-Machine-Interface should allow operator to execute each of the control functions individualy. Operations which will require execution of several control functions we will call COMMANDS and describe them in further sections (the group control functions i.e. many identical actions performed simultaneously are not 'commands').**

#### 7.4.3.1 Channel control (hardware control):

1. System shall allow to set the channel 'normal' state1 (On/Off/Stand-by2) Footnote 1: Other states as tripped, overcurrent or overvoltage shall be called ‘error (or fault) state’
2. System shall allow to set Voltage V\({}_{\rm A}\)
3. System should allow to set stand-by Voltage V\({}_{\rm B}\)
4. System shall allow to set Trip Current I\({}_{\rm A}\)
5. System should allow to set Trip Current I\({}_{\rm B}\)
6. System shall allow to set Trip Current I\({}_{\rm B}\)
7. System shall allow to set Trip Time-Out for V\({}_{\rm A}\)
8. System should allow to set Trip Time-Out for V\({}_{\rm B}\)
9. System shall allow to set Ramp Up Rate
10. System shall allow to set Ramp Down Rate
11. System shall allow to set Action on Trip (Ramp Down or drop to zero)
12. System shall allow to form a group of channels (if necessary extending across the modules boundaries)
13. It should be possible to Add Channel to Group without stopping the system controls
14. It should be possible to Remove Channel from Group without stopping the system controls
15. System shall allow to read Status
16. System shall allow to read Voltage
17. System shall allow to read Current
18. System shall allow to read Demanded Voltage V\({}_{\rm A}\)3 Footnote 3: All requested values should be read in one block-read (items 4 - 12).
19. System should allow to read Demanded Voltage V\({}_{\rm B}\)
20. System shall allow to read Trip Current I\({}_{\rm A}\)
21. System should allow to read Trip Current I\({}_{\rm B}\)
22. System shall allow to read Trip Time-Out for V\({}_{\rm A}\)
23. System should allow to read Trip Time-Out for V\({}_{\rm B}\)
24. System shall allow to read Ramp Up Rate
25. System shall allow to read Ramp Down Rate
26. System shall allow to read Action on Trip (Ramp Down or drop to zero)

#### 7.4.3.2 Additional control Parameters per Channel:

This values are not equivalent to 'trip values' and they are not necessarily loaded (or contained) within channels hardware. This values are control parameters which govern the channel behaviour from point of view of monitoring and error reporting.

1. System shall allow to set voltage dead-band (window)
2. System shall allow to set current dead-band (window)
3. System shall allow to set Low Warning for voltage
4. System shall allow to set High Warning for voltage
5. System shall allow to set Low Alarm for voltage
6. System shall allow to set High Alarm for voltage
7. System shall allow to set Low Warning for current
8. System shall allow to set High Warning for current
9. System shall allow to set Low Alarm for current
10. System shall allow to set High Alarm for current
11. System shall allow to report and then read, Status Additional states: Warning-Up/Down, Alarm-Up/Down

#### 7.4.3.3 Module control

1. It should be possible to set maximum possible voltage allowed for any channel in the module.
2. It should be possible to read back the module type.
3. It should be possible to read maximum possible voltage allowed for any channel in the module (even if this value is being set by hardware only).
4. System shall allow to form a group of modules.
5. It should be possible to Add Module to Group without stopping the system controls
6. It should be possible to Remove Module from Group without stopping the system controls

#### 7.4.3.4 Crate control

1. System shall allow to set state Off/On for the crate (only when proper status of the channels/modules)
2. System shall reset Communications. This may actually need to be a hardware reset via a push button (or external hw signal), or may be possible via the network or field bus employed. However, it must be possible to restart communications to the crate without affecting the channel parameters in any way (neither values nor statuses).
3. System shall allow to disable/enable interlocks
4. Status of interlocks control shall be available even if power to the system is off.

5. System shall read global crate status with following parameters: - Security key On/Off; - Inhibit hardware signal enabled1 - Interlock status: enabled/disabled; - Crate controls in Local/Global
6. System shall read list of modules currently installed in crate.
7. System should allow to form a Group of Crates
8. It should be possible to Remove Crate from Group without stopping the system controls

**Crate control in Multi-Master mode of operation:**

1. System shall allow to set "Heart beat"2 period in seconds. This period might be different for each master in the system.
3. System shall allow to set multi-master operations mode. Note that for all multi-master operation modes a change of status (e.g. On to Off or an Error Condition) will be reported. Mode 1: Report _only_ changes voltage or current if they vary outside the dead-band; Mode 2: Report _only_ warning or alarm conditions; Mode 3: Combination of both Mode 1 and Mode 2.
4. System shall allow to set the time period (or mode of operations) during which a change of status, as well as warnings and alarms, should be grouped to avoid avalanches of individual messages.
5. System shall allow to handle properly the warning or alarm states. Procedures for warnings and alarms handling should be established, depending on gravity of error/warning/alarm. Only after proper execution of the prescribed procedure the error/warning/alarm can be cleared.
6. System shall allow to read "Heart beat" period in seconds
7. System shall allow to read multi-master operations mode
8. System shall make _warning_ or _alarm_ state clearly displayed and brought to attention of the operator.
9. System shall allow to read time period during which warnings & alarms should grouped to avoid avalanches.
10. System shall allow to read list of channels which produced an error within the defined "avalanche" time period. (Could be implemented as a general block read depending on bus and protocol.) If feasible the channels should be ordered by time-stamp of error occurrence, which might be practical for analysis of the incident.

#### 7.4.3.5 Group control

1. The group control functions should allow to perform the same control functions for a group (i.e. all members of group performing the same actions) as are possible for individual members of the group.
2. System shall provide a _Block Write_ to the group which would allow a single network transfer to send out information (values or control functions) to the group.
3. System shall provide a _Block Read_ from a group which would allow a single network transfer to obtain information (status or values) from a group.
4. System shall allow'masking' of any object (channel).

#### 7.4.4 Commands implemented by the system

Each command, depending of the level it is implemented at, can be broken down either into elementary functions acting directly on hardware or into commands performing action on lower level objects.The commands used in the system, at the lowest level make use of the system control functions described in detail in previous sections.

Here we try to enumerate proposed commands and show theirs action:

* initialize communication and configuration (the parameters stay unchanged)
* load equipment with default parameters as defined in database.
* load (in one or block 'write') parameters from slow-control database (LOAD_DEFAULTUITS) and RUN i.e.: * for a channel (single piece of hardware)
- switch ON * for a group
- issue ON for every member of group (or START if group is not of channels)
* as for START but without parameters loading
* switch outputs to OFF (in orderly way i.e. ramping if available or in certain sequence if necessary or both), either for single channel or for a group
* switch outputs OFF immediately; emergency action
* forced read-out of all parameters and statuses (action as'refreshing')
* set all equipment to stand-by value of parameters; can be performed either from state NOT_READY or from READY
* reset channels in ERROR state (channels stay OFF)
* reset channels in ERROR state (should first reset to OFF and then issue RUN command for a channel)
* reset communications
* an action necessary for proper handling of alarms
* disables central control of the equipment
* allows central control of equipment

### Proposed configuration of the hardware

Low voltage power supplies are going to be placed in the experimental cavern - in the racks placed around the detector. Since all the TRT services exit from the ATLAS detector at z=0, then racks placements should be chosen in such a way as to minimize the magnetic field and keep cables as short as possible.

High voltage system has racks reserved in underground control room and should be kept as compact as possible.

#### The Barrel TRT

* 1152 HV lines
* 960 LV channels

#### The Endcap TRT

**Both endcaps:**

* 5376 HV lines
* 3456 LV channels

#### TRT summary

**HV sources:**

* 5376 (endcaps) + 1152 (barrel) = **6528**

**LV sources:**

* 3456 (endcaps) + 960(barrel) = **4416**

#### Proposed system granularity

Both High Voltage and Low Voltage power supply systems should have minimal granularity of 3 following basic divisions of TRT detector. By this we are meaning number of controlling station of high level. Such a control station should be able to perform all necessary control, monitoring and safety functionalities over others, dumb (or less intelligent) subsystems containing totality of power supplies modules. Density of the channels within modules should be as high as possible.

This would mean that we will have 6 local controllers for power supply system. We assume that HV crate houses 256 channels and LV crate contains 64 sets of voltages (set = 3 voltages).

Thus one endcap will be served by (2688 / 256 = 11 crates / 4 =) 3 racks containing its HV channels and 'local controller'. For whole barrel we will implement (1152 / 256 = 5 crates / 4 =) 2 racks. Thus HV system will need 8 racks in USA-15 underground control room.

In the experimental area - on the superstructure surrounding the detector we will place LV system - but only the executive elements, without control stations. For one endcap we will need (1728/64 = 27 crates/ 4 = ) 7 racks, and for barrel (960 / 64 = 15 crates /4 = ) 4 racks. Thus all LV system for TRT we need 18 racks on the'shelves' and another one in USA-15 for control station. The local controllers will communicate with host computer(s) dedicated to serve power supply systems over the medium which will be defined later (standard field-bus or LAN).

## 8 Cooling systems

The entire volume of the TRT detector has to be kept at a constant temperature of about 25\({}^{\circ}\)C. The heat sources are the straws and electronics.

The cooling system uses two cooling media; gaseous CO\({}_{2}\) for cooling and ventilation of the TRT endcap detectors (straws - ID TDR Section 12.9.5), and fluorinert liquid which removes heat from the CO\({}_{2}\) gas and cools both front-end electronics and barrel detectors (ID TDR Section 12.9.6). In following sections we will give brief reminder of the systems, with pertinent details necessary to understand the controls required.

### Liquid Cooling System

The simplest possible liquid monophase cooling system working above atmospheric pressure was chosen for this task.

The heat load for the entire TRT detector (including some safety factors) totals about 62 kW with the liquid temperature at the inlet to detector required to be at 15\({}^{\circ}\)C (the dew point for air in the experimental area) and increase of temperature of cooling medium within the circuit should be around \(\mathcal{P}\)C.

In following section we will sketch proposed layout of the system at the level of the source of chilled liquid with pertinent specifications for the control.

#### Basic system functionality

The liquid cooling system contains three basic functional subsystems:

1. Liquid distribution and piping inside the detector (patch panel PP3 down to detector)
2. Liquid supply manifolds
3. Cooling stations

The granularity of piping and manifolds is governed by available space for piping, considerations of its hydrodynamic properties etc. and will be defined by specialized note on a cooling system. Number of cooling stations depends on detailed studies of the system and also will be defined later. At present we can safely accept - for control and monitoring purposes - numbers from TDR, for the manifolds and assume that we have at least 3 cooling stations

1. 1for each endcap to cool electronics and heat exchangers
2. 1 for barrel - electronics and detector

The detailed construction of the cooling station is not defined yet however the specifications for designers has been determined [13-7]. Since there is an available central supply of the iced water (at temperature around \(\theta^{0}\)C) - the station is basically a pump and heat exchanger between detector liquid and iced water. If iced water parameters are inadequate, the cooling station will additionally contain typical cooler based on two-phase principle. Controls of the cooling station will be fully independent - based most likely on the PLC and its role will be to keep the temperature of the outgoing liquid at defined level - in conditions of changing flow and temperature of returning fluid.

The output from the cooling station will supply the manifold where liquid flow is split into piping granularity needed for the detector. No controls of these outputs is foreseen, the cooling units being set for maximal heat load.

#### States of cooling system objects

The objects in cooling system has been enumerated in Chapter 3.1.2. Below we list states in which those objects can be found.

* as defined in Chapter 3.1.1; all local controllers in MANUAL mode
* when started from STOPPED and waiting to get into NOT_READY state; all local controllers in AUTOMATIC mode
* state where limits for alarms and warnings are disabled due to unstable heat load conditions
- ex. LV power supplies switched off
* as defined in Chapter 3.1.1
* as defined in Chapter 3.1.1

#### Detailed requirements

**UR OPE01**

The temperature of the cooling liquid will be regulated to \(\pm\) 0.5 \({}^{\circ}\)C.

**UR OPE02**

The temperature of the liquid delivered to detector cooling module will be 15\({}^{\mathrm{0}}\)C

**UR OPE03**

The temperature of the cooling liquid in the return lines from heat exchangers should be reported to gas cooling system.

**UR OPE04**

Controls of the system will keep defined conditions of the liquid coolant with flow variation of +/-40% of the nominal

**UR MON01**

Both cooling systems - liquid and gas are controlled by common computer.

**UR MON02**

The cooling system local controllers (PLC's) shall deliver upon request from LCS values of all local parameters (like flows, temperatures, statuses etc).

**UR MON03**

Variations of the conditions in the cooling station will be controlled and corrected by local controllers without any intervention of the LCS or cooling system computer.

**UR MON04**

Local controllers - if not interrogated - will deliver on regular basis to higher level of controls only the status. The period of reporting will be externally programmable.

#### 8.1.4 Commands implemented by the system

.Here we try to enumerate the proposed commands and show theirs action:

* initialize communication and configuration; leaves system in MANUAL
* allow for programming of local controllers and parameters change; can be issued only in LOCAL state
* load equipment with default parameters as defined in database.
* issue a 'go' command to local controllers; disable change of parameters
* start all control and regulation mechanisms, but do not allow the regulation to be performed; must be interlocked with state of power supplies

The command which allows monitored parameters to vary outside allowed limits - basically to stay BELOW nominal values; to be used when the system should be turning but ex. power supplies are OFF

* stop the system
* to move from NOT_READY into READY state; start error reporting mechanism and observation of set limits.
* forced read-out of all parameters and statuses (action as'refreshing')
* reset communications

This command MUST not in any way change values of the parameters remembered by hardware controllers.

* an action necessary for proper handling of alarms
* disables central control of the equipment
* allows central control of equipment

### 8.2 Gas Cooling

As described in TDR, ionisation avalanche current in the detector during high luminosity running results in quite important power losses in the straws. The end-cap detectors will be cooled by a two-level system:combined gas/liquid cooling, where a flow of gas first removes heat from the detector and then transports it to a heat exchangers. There, the heat is transferred to cooling liquid, which transports it outside the detector. The cooling of a liquid has been described earlier, here we give brief outline of the gas cooling and it controls.

#### Basic system functionality

The circulation pump supplies three groups of wheels per end-cap. The pump itself is fully integrated with PID controller, which ensures the proper pressures at input and output.

The flow and pressure regulation is done individually for each set of wheels. The flow rates are adjusted by flow controllers at the inlet of each parallel line. The pressure control is achieved by a regulation valve at the outlet and is driven by the pressure measured at the last wheel of that set. In order to protect the wheel against over/under pressure, a manometer is connected to one of the pressure-sensing pipes. This device is equipped with two limits that can be adjusted around the operational pressure. Excursions outside of these limits would switch off the main pump, thus protecting the detector hardware.

An increase in atmospheric pressure would require the supply of additional gas to the system for compensation. A pressure regulator is planned for this purpose. If the differential pressure falls below defined value, gas would be added to the main circuit. This system would also compensate for gas losses due to leaks. On the other hand, a decrease in atmospheric pressure will activate second pressure regulator which controls the venting. Actions described here will be fully automatic and independent of the operator control. They will be performed by local controllers based on PLC's. Only the flow adjustment will be performed by cooling system computer after careful measurements of the response of regulated circuit.

The heat exchangers are supplied in cooling liquid by liquid cooling system. The flows of medium in heat exchangers might be controlled (if found useful) basing on data from gas cooling system.

#### States of cooling system objects

The objects in the gas cooling system has been defined in Chapter 3.1.2. Below we list states in which those objects can be found.

* as defined in Chapter 3.1.1;
* when started from STOPPED and waiting to get into NOT_READY state
* state where limits for alarms and warnings are disabled due to unstable conditions
* as defined in Chapter 3.1.1
* as defined in Chapter 3.1.1

#### Detailed requirements

**UR OPE01**:

The temperature of the cooling gas will be controlled with accuracy \(\pm\) 1.0 \({}^{\circ}\)C.
**UR OPE02**:

The temperature of the gas delivered to detector will be 20\({}^{\circ}\)
**UR MON0 1**:

The cooling system local controllers (PLC's, PID) and sensors shall deliver upon request from LCS values of all local parameters (like flows, temperatures, statuses etc).
**UR MON02**:

The logic of cooling control in the gas cooling system should be implemented on the common computer responsible for both cooling systems.
**UR INT0 1**:

The signal that cooling system fails shall be taken from pump and flowmeter
**UR INT02**:

Hardwired interlocks will automatically turn off the HV power supplies, in the event that the cooling system should fail
**UR ALM0 1**:

Alarms and warnings from cooling system shall have highest priority.
**UR CON1**:

The system will accept STOP command only when HV power supplies are in NOT_READY state.
**UR CON2**:

The system will not accept any change of parameters when in GLOBAL state

#### 8.2.4 Commands implemented by the system

.The commands to the gas cooling system shall be those already implemented for liquid cooling system.

## 9 Interlock system

The interlock system plays a very important role in safety of detector operations. Many different failure modes require urgent intervention. Normal control paths over links, networks, operating systems and supervising software could be much to slow and moreover not reliable enough. Hardwired protections have to be implemented. We plan to use the following interlocks in the TRT DCS.

1. general safety system (GSS) interlock to LV_PS, HV_PS, gas system
2. HV power supply system to Gas System and Detector Cooling systems
3. LV power supply system to Electronics Cooling system
4. LV power supply system to Temperature Measuring system (might be executed in software only)
5. HV power supply system to Temperature measuring system (might be executed in software only)

The status of implemented interlocks has to be monitored carefully. Access to this data has to be ensured even in the case of power failure. _Thus we believe that the interlock system has to be put onto UPS._

At the moment no more can be specified for the interlocks. Its implementation will depend strongly of technical solutions adopted in TRT subsystems.

## 10 Readout crates / electronics racks

The controls of this system should be standard for whole of ATLAS. At the moment TRT has no special requirements for control of this equipment.

The ATLAS DCS group plans to equip VME readout crates/fan units with field-bus interfaces. It will be possible using standard ATLAS DCS protocols to monitor all crate voltage rails, current consumption and temperatures and to remotely turn crate supplies on and off. Protocols for crate control are under study in the ATLAS DCS community. There is a general DCS requirements for this controls: "**INFRA3** - DCS shall monitor the status of each crate and shall have the possibility to control it".

Our crates will be distributed in several geographical places: surface control room, surface gas building, underground control room (USA-15), underground experimental cavern (UX-15) and possibly underground auxiliary control room (US-15).

The surface LCS will control crates lodged in control room (nn field-bus connections) leaving the gas crates to the Gas Control Station (one control path). Underground LCS will control all crates positioned in underground areas (mmm controlling field-bus lines).

### States of the system crates:

* when crates are powered from mains but LV is not ON
* crates and LV are ON
* any trip of the LV in crates, overheating, fan failure etc.
* the 220/380 volts is not ON at the crates
* when for some reason the control path is down

### User requirements:

**UR CRAOPE1**

The system shall be able to switch the crates in two steps:

* connect to mains;
* switch the LV on.
**UR CRAOPE2**

The system shall generate warnings and alarms when status of the crates parameters falls into defined range.
**UR CRAOPE3**

The system should have hardware interlock to General Safety System

### 11 Uninterruptable power supplies

Certain parts of the equipment should be supplied from UPS. The TRT DCS should have access to the status of the UPS in use. Following a power failure - the remaining power autonomy on UPS should be known. Very important pieces of hardware should be also put on 'generator' back-up power. One example is the gas control system, where failure to deliver fresh gas can result in unstable periods of many hours before the gas composition returns to nominal specifications.

### 11.1 5. TRT system requirements

**UR UPSSOPE01**

The UPS system shall guarantee flawless operation of the supplied subsystems until the'safe-power' from diesel generators is not established.
**UR UPSMO 1**

The UPS system shall supply DCS system the information on how long it will still operate delivering power.
**UR UPSMO 2**

The DCS system shall have implemented emergency procedures for subsystems shut-down in case the'safe-power' will not be started.
**UR UPSMO 103**

At any moment the DCS system shall be able to check status of the UPS system.
**UR UPSDET01**

The DCS system of TRT detector shall be supplied from Uninterruptable Power Supplies.
**UR UPSDET02**

The gas system of TRT detector shall be supplied from Uninterruptable Power Supplies.

**Note**. The critical subsystem for the TRT detector is the gas system due to the stringent specifications for the gas composition and rather big volume of the detector. Prolonged stoppage of the system could lead to gas contamination and then a need for long flushing to re-establish proper running conditions.

**UR UPSDET03**

The interlock system of TRT detector shall be supplied from Uninterruptable Power Supplies.

**UR UPSPOW01**

The power delivered to DCS system by UPS system shall be 4 kW distributed evenly to two Local Control Stations - surface and underground control room.

**UR UPSPOW02**

The power delivered to gas system by UPS shall be 2 kW distributed between three locations - surface, underground control room and experimental cavern

_The detailed specification of the UPS system will follow those established by central ATLAS DCS group._

## 12 Environmental parameters

Environmental parameters including humidity and atmospheric pressure in the cavern and at surface, the purity of the air in cavern (dust levels etc.) should be monitored by GDCS and made available to the detectors. The TRT might need the humidity measurements inside the detector due to widespread use of composite materials which can exhibit high dimensional sensitivity to moisture. The method of measurement is presently not clear and the number of available lines/pipes etc to the volume of detector is very limited.

The LDCS should also have access to the control and status of infrastructure including AC mains, air conditioning etc. However this area again should be standard across the ATLAS experiment.[13-6]

## References

* 13-1 ATLAS Detector Control System User Requirement Document, PSS05-ATLAS-DCS-URD, Nov. 1995
* Joint Controls Project; [http://itcowww.cern.ch/jcop/](http://itcowww.cern.ch/jcop/)
* ATLAS TDR 5, CERN/LHCC/97-17, ISBN 92-9083-103-0; 30 April 1997
* Temperature monitoring of the FE-boards
- September 1999
* 'Dave Myers
- Control Requirements for HV Supplies
* Technical Design Report LHCC 99-01, ATLAS TDR 13, January 1999; Chapter 22
- Controls summary pages 393-398
* ATLAS document ATL-IT-ES-0010
_Transition Radiation Tracker Engineering Noe ATL-ITES-0009 Detector Control Sys tem_