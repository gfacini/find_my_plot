# Towards a Computing Grid for the LHC Experiments:

_ATLAS Data Challenge 1_

The ATLAS DC1 Task Force

The ATLAS Collaboration is preparing for data taking and analysis at the CERN Large Hadron Collider, which will start operating in 2007. As part of this preparation, a series of computing Data Challenges was started in 2002 with the goal of the validation of the Computing Model, of the complete software suite, of the data model, and to ensure the correctness of the technical choices to be made for the final offline computing environment. A major feature of the first Data Challenge (DC1) was the preparation and the deployment of the software required for the production of large event samples as a worldwide distributed activity.

It should be noted that it was not an option to "run the complete production at CERN" even if we had wanted to; the resources were not available at CERN to carry out the production on a reasonable time-scale. The great challenge of organising and then carrying out large-scale data production at a significant number of sites around the world had therefore to be faced. The benefits have been manifold: valuable experience has been gained in organizing and arranging access to large-scale, distributed computing resources, and the exercise has created worldwide momentum for ATLAS computing as a whole.

This paper describes in detail the main activities carried out in DC1, and what has been learned from them, as a step towards a computing Grid for the LHC experiments.

_(Submitted to Nucl. Instr. Meth.)_

[MISSING_PAGE_FAIL:2]

S. Albrand, J. Collot, J. Fulachier, F. Ledroit-Guillon, F. Ohlsson-Malek, S. Viret, M. Wielers1

_LPSC, CNRS-IN2P3, Universite Joseph Fourier, Grenoble, FRANCE_

Footnote 1: Now at Rutherford Appleton Laboratory

Footnote 2: Deceased

Footnote 3: Now at Max Planck Institute for Extraterrestrial Physics, Munich

K. Bernardet, S. Correard, A. Rozanov, J-B. de Vivie de Regie

_CPPM, CNRS-IN2P3, Universite de la Mediterranee, Marseille, FRANCE_

C. Arnault, C. Bourdarios, J. Hrivnac, M.Lechowski, G. Parrour, A. Perus, D. Rousseau, A. Schaffer, G. Unal

_LAL-Orsay, CNRS-IN2P3, Universite Paris XI, Orsay, FRANCE_

F. Derue

_LPNHEP, CNRS-IN2P3, Universite Paris 6/7, Jussieu, Paris, FRANCE_

L. Chevalier, S. Hassani, J-F. Laporte, R. Nikolaidou, D. Pomarede, M. Virchaux1

_CEA/DAPNIA, Saclay, FRANCE_

Footnote 2: Now at University Roma Tre

N. Nesvadba2

Footnote 3: Now at University of Freiburg

_Rheinische Friedrich-Wilhelms Universitat, Bonn, GERMANY_

Footnote 4: Now at Max Planck Institute for Extraterrestrial Physics, Munich

S. Baranov

_Universitat Freiburg, GERMANY_

A. Putzer

_Kirchhoff -Institut fur Physik, Universitat Heidelberg, GERMANY_

A. Khomich

_Universitat Mannheim, GERMANY_

G. Duckeck, P. Schieferdecker

_LMU Munchen, GERMANY_

A. Kiryunin, J. Schieck

_MPI fur Physik, Munchen, GERMANY_

Th. Lagouri

_Nuclear Physics Laboratory, Aristotle University of Thessaloniki, GREECE_

E. Duchovni, L. Levinson, D. Schrager,

_Weizmann Institute of Science, ISRAEL_

G. Negri

_CNAF, Bologna, ITALY_

H. Bilokon, L. Spogli4

Footnote 4: Now at University of Freiburg

_LNF, Frascati, ITALY_

D. Barberis, F. Parodi

_Universita di Genova e INFN, ITALY_

G. Cataldi, E. Gorini, M. Primavera, S. Spagnolo

_Universita di Lecce e INFN, ITALY_

D. Cavalli, M. Heldmann1, T. Lari, L. Perini, D. Rebatto, S. Resconi, F. Tartarelli, L. Vaccarossa

Footnote 1: Now at University of Freiburg

_Universita di Milano e INFN, ITALY_M. Biglietti, G. Carlino, F. Conventi, A. Doria, L. Merola,

_Universita di Napoli "Federico II" e INFN, ITALY_

G. Polesello, V. Vercesi

_Sezione INFN di Pavia, ITALY_

A. De Salvo, A. Di Mattia, L. Luminari, A. Nisati, M. Reale, M. Testa

_Universita di Roma "La Sapienza" e INFN_, ITALY_

A. Farilla, M. Verducci\({}^{\rm{m}}\)

_Universita di Roma Tre e INFN, ITALY_

M. Cobal, L.Santi

_Universita di Udine e INFN_, ITALY_

Y. Hasegawa

_Shinshu University_, JAPAN_

M. Ishino, T. Mashimo, H. Matsumoto, H. Sakamoto, J. Tanaka, I. Ueda

_International Centre for Elementary Particle Physics (ICEPP), the University of Tokyo_, JAPAN_

S.Bentvelsen, A.Fornaini, G.Gorfine, D.Groep, J.Templon

_NIKHEF, NETHERLANDS_

J. Koster\({}^{\rm{n}}\)

_Parallab / UNIFOB, University of Bergen, NORWAY_

A. Konstantinov\({}^{\rm{o}}\), T. Myklebust\({}^{\rm{g}}\), F. Ould-Saada

_University of Oslo, Department of Physics, NORWAY_

T. Bold\({}^{\rm{n}}\), A. Kaczmarska, P. Malecki, T. Szymocha, M. Turala

_The Henryk Niewodniczanski Institute of Nuclear Physics, Krakow,POLAND_

Y. Kulchitsky\({}^{\rm{g}}\), G. Khoriauli, N. Gromova, V. Tsulaia\({}^{\rm{a}}\)

_Joint Institute for Nuclear Research, Dubna, RUSSIA_

A. Minaenko\({}^{\rm{i}}\), R. Rudenko, E. Slabospitskay\({}^{\rm{j}}\), A. Solodkov

_Institute of High Energy Physics, Protvino, RUSSIA_

I. Gavrilenko

_P.N. Lebedev Institute of Physics (FIAN), Moscow, RUSSIA_

N. Nikitine, S. Sivoklokov, K. Toms

_Skobeltsyn Institute of Nuclear Physics, Moscow State University_, RUSSIA

A. Zalite, I. Zalite

_St.Petersburg Nuclear Physics Institute, RUSSIA_

B.Kersevan

_University of Ljubljana and Iozef Stefan Institut, Ljubljana, SLOVENIA_

[MISSING_PAGE_FAIL:5]

S. George, A. Lowe

_Royal Holloway College, University of London, Egham, UK_

C. Buttar1, I. Dawson, A. Moraes2, D. Tovey

_University of Sheffield, UK_

J. Gieraltowski, T. LeCompte, D. Malon, E. May, A. Vaniachine

_Argome National Laboratory, USA_

D.L. Adams, K. Assamagan, R. Baker3, W. Deng, V. Fine, Y. Fisyak, B. Gibbard, H. Ma, P. Nevski, F. Paige, S.Rajagopalan, J. Smyth, A. Undrus, T. Wenaus, D. Yu

_Brookhaven National Laboratory, USA_

P. Calafiura, S. Canon1, D. Costanzo2, I. Hinchliffe, W. Lavrijsen., C. Leggett, M. Marino, D.R. Quarrie, I. Sakrejda, G. Stavropoulos, C. Tull

_Lawrence Berkeley National Laboratory, USA_

P. Loch

_University of Arizona, Tucson, USA_

J. Shank, S. Youssef

_Boston University, USA_

D. Engh3, E. Frank, A. Gupta, R. Gardner, F. Merritt, Y. Smirnov

_University of Chicago, USA_

J. Huth

_Harvard University, USA_

L. Grundhoefer, F. Luehring

_Indiana University, USA_

S. Goldfarb

_University of Michigan, USA_

H. Severini, P. Skubic

_Oklahoma University, USA_

Y. Gao, T. Ryan

_Southern Methodist University, USA_

K. De, M. Sosebee, P. McGuigan, N. Ozturk

_University of Texas, Arlington, USA_

S. Gonzalez

_University of Wisconsin, Madison, USA_Introduction

In 2007 the Large Hadron Collider (LHC) is due to come into service at the European Particle Physics Laboratory (CERN) in Geneva. At the LHC two proton beams, each of energy 7 TeV, are steered to collide head-on at the centre of large complex detectors. These collisions are expected to reveal fundamental new processes in particle physics.

The ATLAS1 (A Toroidal LHC Apparatus) detector is one of several that are being constructed to exploit the LHC. The ATLAS collaboration consists of about 1800 physicists and engineers from more than 150 universities and laboratories in 34 countries. The ATLAS detector weighs \(\sim\)7000 tons and comprises several particle-detection technologies that surround the colliding beams almost completely. The ATLAS detector will measure the directions and energies of the particles produced in LHC collisions. At design intensity ("luminosity") these collisions will occur at a rate of \(\sim\)1GHz. This very high rate is required in order to accumulate statistically meaningful samples of the extremely rare processes that are expected to be of the highest physics interest. Since each collision (or "event") results in \(\sim\)100-200 MB of off-line data, very fast real-time selection and on-line data reduction and compression by a factor of several million are required to produce manageable off-line data volumes. Even so, ATLAS will produce annually several petabytes of off-line data to be analysed by institutes around the world. To ensure that the requisite computing resources are available, each experiment will need a worldwide distributed databank and computer system, on a scale that is one to two orders of magnitude larger than previous experiments in particle physics.

Footnote 1: [http://cern.ch/Atlas/](http://cern.ch/Atlas/)

To prepare for this task, the LHC Computing Review2 in 2001 recommended that the LHC experiments should carry out Data Challenges (DC) of increasing size and complexity. A Data Challenge comprises, in essence, the simulation, done as realistically as possible, of data (events) from the detector, followed by the processing of that data using the software and computing infrastructure that will, with further development, be used for the real data when the LHC starts operating. The goals of the ATLAS Data Challenges are the validation of the ATLAS Computing Model, of the complete software suite, of the data model, and to ensure the correctness of the technical computing choices to be made.

Footnote 2: [http://cern.ch/lhc-computing-review-public/Public/Report_final.PDF](http://cern.ch/lhc-computing-review-public/Public/Report_final.PDF)

From the point of view of LHC computing, the computational Grid3 is a very timely development. The Grid concept is that data processing is carried out at the most suitable site(s) on an international multi-site network, with the so-called Grid middleware organizing access and assigning computing capacity in a way that is entirely seamless from the perspective of the user. Grid technologies offer several obvious advantages for a multi-national and geographically distributed project: they allow for a uniform computing infrastructure of the project; they simplify the management and coordination of the resources while potentially decentralizing such tasks as software development and analysis; and last, but not least, they provide an affordable way to increase the effective computing power.

Footnote 3: Grid computing has grown rapidly since the late 1990â€™s, particularly after the publication of the influential _The Grid: Blueprint for a New Computing Infrastructure_ (1999), edited by Foster and Kesselman The term

*Computational Grid' (or Grid for short) has been coined by analogy with the electrical power grid. The world body is the Global Grid Forum, [http://www.gridforum.org](http://www.gridforum.org)

The ATLAS collaboration intends to perform its Data Challenges using as much as possible Grid tools provided by the LHC Computing Grid (LCG) project4, the NorduGrid5 and the USGrid6. DC1 saw the first usage of these technologies in ATLAS in limited-scale tests.

Footnote 4: [http://cern.ch/lcg/](http://cern.ch/lcg/)

Footnote 5: [http://www.nordugrid.org](http://www.nordugrid.org)

Footnote 6: [http://www.usatlas.bnl.gov/computing/grid/](http://www.usatlas.bnl.gov/computing/grid/)

In this report the ATLAS Data Challenge 1 (DC1) is described, with emphasis on the computing aspects. As background and introduction, Section 2 describes the scope and scale of DC1, and summarises the resources used and the amount of data processed. The main part of the paper is sections 3 and 4. Section 3 covers the various elements that comprise the computing infrastructure required for any large-scale exercise of this type: Software distribution, Data Book-keeping, Data Access and replication, and Workflow control. Section 4 reports on our use of some of the emerging Grid technologies for part of the DC1 exercise. Section 5 draws together the major lessons learned from the DC1, and the overall conclusions are given in Section 6.

## 2 DC1 Scope and Scale

The design and construction of ATLAS requires a large amount of simulated data in order to optimise the design of the detectors, to estimate the physics performance that will be achieved, and to test the software and computing infrastructure. Samples of simulated data consist of a large number (\(\sim\)10\({}^{7}\)) of simulated "events". Each "event" is the result of a collision between two protons, and the full simulation requires the following steps:

* "Event Generation": Particles produced in the proton-proton collision are generated using a program based on physics theories and phenomenology. Several such programs are available, and are referred to as "event generators", or simply "generators";
* "Detector Simulation": The produced particles are transported through the detector elements according to the known physics governing the passage of particles through matter. The resulting interactions with the sensitive elements of the detector are converted into information similar to, and in the same format as, the digital output from the real detector (the "digitisation" step);

Because of the very high intensity at which the LHC will operate, many proton-proton collisions will usually occur during the sensitive time of the particle detectors. Hence the digitised output from the ATLAS detector will contain the "piled-up" signals from the particles produced in several collisions. This leads to the next step:

* "Pile-up": the digitised signals from several simulated collisions are combined to produce events with realistic signal pile-up;

These events can now be used to test the software suite that will be used on real LHC data:

* "Reconstruction": the events are reconstructed to recover particle trajectories and energies from the detector data;The simulated data fed to the Reconstruction step is, as far as possible, indistinguishable from real data, with one important exception: since the original information on the particles' trajectories and energies is known from the Event Generation step, this information ("truth") is also recorded so that the output from the Reconstruction step can be compared with it.

For all Data Challenges it is essential to have realistic physics content in order to engage the physicist community with the exercise, leading to a more thorough validation of the software. For DC1, in 2002-2003, a major goal was to provide simulated data needed for the preparation of the ATLAS High Level Trigger (HLT) TDR7.

Footnote 7: ATLAS TDR 016; CERN/LHCC/2003-022; ISBN 92-9083-205-3; [http://cern.ch/atlas-proi-hltdaqdcs-tdr/](http://cern.ch/atlas-proi-hltdaqdcs-tdr/)

Operationally, the DC1 production was split into three phases: Phase 1: Event Generation and Detector Simulation, Phase 2: Pile-up, and Phase 3: Reconstruction, which are described in more detail below.

Figure 1: **Number of normalised processors8 per country available to DC1 (Countries are displayed clockwise, starting with Australia (48)). The numbers of processors per site varied between 9 and 900, resulting in a total of ~5000 CPUs at peak time corresponding to a processing power of ~2.2 MSI2k.**

The following 56 institutes in 21 countries9 participated in DC1:

Footnote 9: The list of the NorduGrid sites used by the three Nordic countries (Denmark, Norway, Sweden) can be found on [http://www.nordugrid.org/hardware.html](http://www.nordugrid.org/hardware.html)

**1. Australia (Melbourne)**

**2. Austria (Innsbruck)**

**3. Canada (Alberta, Montreal, Simon Fraser, Toronto, Victoria)**

**4. CERN**

**5. China (Beijing, Nanjing)**

**6. Czech Republic (Prague)**

**7. France (Grenoble, Marseille; using CCIN2P3 in Lyon)**

**8. Germany (Heidelberg, Munchen; using GridKA in Karlsruhe)**

**9. Greece (Thessaloniki)**

**10. Israel (Weizmann)**

**11. Italy (CNAF Bologna, Frascati, Milano, Napoli, Roma)**

**12. Japan (Tokyo)**

**13. NorduGrid (NBI, Odense, Bergen, Oslo, Linkoping, Lund, Stockholm, Umea, Uppsala)**

**14. Poland (Cracow)**

**15. Russia (Dubna, ITEP Moscow, MSU Moscow, Protvino)**

**16. Spain (Valencia)**

**17. Taiwan (Taipei)**

**18. UK (Birmingham, Cambridge, Glasgow, Lancaster, Liverpool, RAL, Sheffield)**

**19. USA (ANL, Arlington, BNL, Boston, Dallas, Indiana, LBNL, New Mexico, Oklahoma)**

**Fig. 2 Map of the sites (indicated by red dots) taking part in the ATLAS DC1 activities. The countries shaded in yellow have at least one institute in an LHC experiment.**

### 2.1 Phase 1: Event Generation and Detector Simulation

Phase 1 was carried out during summer 2002. The Event Generation step, which is relatively light in terms of computing resources, was carried out at CERN, and the generated events were then distributed widely for the more compute-intensive Detector Simulation step. At peak time ~3200 processors located in 40 institutes in 19 countries were used in Phase 1. This corresponds to ~1400kSI2k10 or ~50% of the CPU power estimated to be required for one Regional Facility ("Tier 1") at the time of LHC start-up (2007). The hardware investment, in cash terms, made by those institutes in the twelve months prior to the DC1 exercise corresponds to ~50 % of the annual hardware investment needed from 2006 onwards for the non-CERN part of the ATLAS Offline Computing.

Footnote 10: SI2k stands for SPEC INT 2000, the classification of CPU integer performance defined by the Standard Performance Evaluation Corporation. See [http://www.spec.org/cpu2000/](http://www.spec.org/cpu2000/)

#### Event Generation

The event generation used the event generator Pythia 6.2031 running in the ATLAS Athena/Gaudi12 framework. The ATLAS physics and HLT communities required many differing samples of events, with each sample characterised by certain requirements on the type, energies, and directions of subsets of the final-state particles. Because of the physics governing the production of final-state particles, it is not possible to impose such requirements with 100% efficiency through the input parameters to Pythia: it can only be done approximately. In practice ~50 million events were generated by Pythia, of which only ~11 million were selected by a fast filter for full Detector Simulation. In addition, ~40 million'single-particle' events (muons, photons, electrons, pions) were generated for full Detector Simulation in order to scan the detector response in a systematic way. These events can be generated very rapidly as there is only one particle per event, and it was convenient to use the older Fortran-based Atlsim13 framework for this task.

Footnote 11: [http://www.thep.lu.se/~torbiom/Pythia.html](http://www.thep.lu.se/~torbiom/Pythia.html)

Footnote 12: [http://cern.ch/atlas-proj-computing-tdr/Hml/Computing-TDR-21.htm](http://cern.ch/atlas-proj-computing-tdr/Hml/Computing-TDR-21.htm)

Footnote 13: [http://cern.ch/atlas/GROUPS/SOFTWARE/DOCUMENTS/ATLSIM/atlsim.html](http://cern.ch/atlas/GROUPS/SOFTWARE/DOCUMENTS/ATLSIM/atlsim.html)

The total CPU time necessary to generate all the events was about 200kSI2k-days and the resulting data volume was 8 TBytes.

#### Detector Simulation

The ATLAS detector simulation code used for DC1 is Fortran-based. It uses GEANT 3.2114 to track the particles through the detector and runs in the Atlsim framework.15 As mentioned above, the events generated by Pythia were filtered at the start of the detector-simulation job to select those (~11 million) that met certain requirements on final-state particles. These selected events, and all the ~40 million single-particle events, were then processed through the full detector simulation, and the results were written out in the form of ZEBRA16 banks.

Footnote 14: CERN Program Library W5013: [http://cern.ch/wwwasdoc/geant](http://cern.ch/wwwasdoc/geant) html3/geantall.html

Footnote 15: ATLAS has now completed its testing and verification of the C++ GEANT4 package ([http://cern.ch/wwwasd/geant4/geant4.html](http://cern.ch/wwwasd/geant4/geant4.html) ), and the detector simulation starting with DC2 uses GEANT4 running in the Athena/Gaudi framework.

Footnote 16: CERN Program Library Q100/Q101 ([http://cern.ch/wwwasdoc/zebra](http://cern.ch/wwwasdoc/zebra) html3/zehramain.html )

As already noted, the detector simulation requires substantial computing resources, and this step was carried out at 40 institutes in 19 countries. The compute power required was 1.4 M SI2k-days, and generated an output data volume of 24 TBytes.

### 2.2 Phase 2: Pile-up production

The cross-section for inelastic non-diffractive pp interactions at the LHC is expected to be around 67 mb. At the design intensity (luminosity of \(10^{34}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\)) of the LHC, this implies an average of 23 "minimum-bias" events per bunch crossing, varying according to a Poisson distribution. Any collision recorded in the ATLAS detector therefore contains a superposition of particles coming from several events. In general the particles from a single "interesting physics" event will have triggered the readout, and additional particles will come from other "uninteresting" pp collisions.

The number of particles "seen" by an ATLAS sub-detector depends not only on the LHC intensity but also on the signal-collection time of that sub-detector. The collisions within a given bunch crossing occur essentially instantaneously (time-spread \(<1\) ns), and all ATLAS sub-detectors are sensitive to all these collisions. Some of the sub-detectors have a sensitive time much greater than the 25 ns interval between bunch crossings, and hence are sensitive to collisions in bunch crossings occurring before and after the bunch crossing containing the "interesting" collision that triggered the readout.

Furthermore, neutrons fly around the ATLAS cavern (in which the detector is installed) for a few seconds until they are thermalised, thus producing a kind of permanent neutron-photon "bath" resulting in a steady rate of Compton electrons and spallation protons, which are observed in the outermost parts of the detector. This component, i.e. additional hits created by long-lived particles, is called "cavern background".

It is, therefore, quite a complex task to add the "pile-up", ensuring that the signals from the "uninteresting" collisions are properly distributed in time and overlay correctly, sub-detector by sub-detector, the signals from the "interesting" collision. Pile-up was added to a sub-set of data samples: 3.9M events were produced with low-luminosity (\(2.10^{33}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\)) pile-up and 2.65M events with pile-up corresponding to design luminosity (\(10^{34}\,\mathrm{cm}^{-2}\mathrm{s}^{-1}\)).

New countries, China and Greece, and new institutes from Canada, Italy, NorduGrid, UK and USA joined the effort in the course of the Phase 2 so that 56 institutes in 21 countries participated in the pile-up production, giving a maximum capability of more than 2 MSI2k. This phase of DC1 took about 3.2 MSI2k-days and produced a total data volume of about 34 TBytes.

### 2.3 Phase 3: Reconstruction

The ATLAS reconstruction software has been developed from the earlier Fortran version, largely re-written in C\(++\), and runs in the Athena/Gaudi framework. In essence, the Athena framework embodies a separation between data-specific members and methods on the one hand and algorithm-related members and methods on the other, e.g. for reconstruction. Data objects are handled through the Transient Event Store for event information and a Transient Detector Store for condition information. Data (specified by object type and string key) are read by Algorithms, or persistified by Converters. Algorithms are driven through a flexible event loop. Common utilities are provided through services. ASCII files, called Job Options,allow the specification of algorithms and services parameters and sequencing. The Athena executable itself is very small, as all the significant software is dynamically loaded at run time, with typically one library per package.

DC1 was carried out before the new C++ data-persistency system (POOL17), being developed in the LCG context, was available, and hence the output of reconstruction was stored in HBOOK ntuples. This was done using a special algorithm, named CBNT for ComBined NTuple, capable of writing the content into an ntuple through the Gaudi ntuple converter. The algorithm is fully configurable, so that only the needed information is written out, which is especially important for the large "truth" information. The main drawback was that the downstream analysis could only be done in PAW (or ROOT after conversion), as the C++ design of the original objects was not preserved.

Footnote 17: POOL: [http://lcgapp.cern.ch/project/persist/](http://lcgapp.cern.ch/project/persist/)

In a separate step, algorithms running in the HLT Selection Software environment reconstructed objects and extracted the key features from event data that are used to derive the trigger decision.

To facilitate the access to the large distributed datasets, since not all production sites were accessible via Grid tools, the data for the reconstruction step were replicated to just 8 sites, and the reconstruction of the data was (mostly) carried out at those sites. About 6.4M events were processed during the reconstruction phase. This part of DC1 took about 4.4 M SI2k-days and produced an output data (ntuple) volume of about 200 GBytes.

### 2.4 Software and Site Validation

In any computing exercise of this type it is of course essential to test very thoroughly that the software is functioning correctly before embarking on large-scale use of computing resources. As significant parts of DC1 were carried out at geographically distributed sites, it was also essential to validate each site, i.e. to ensure that the installed software was functioning as expected.

#### Monitoring Generated Events

As described in sections 2.1 above, the event-generation step was carried out at CERN. The generated events were monitored by histogramming various characteristic properties of the events, and 'gross' errors (e.g. a mistyped input parameter) were easily detected in such histograms. A characteristic feature of the generated events is that they should contain one or more "jets" of final-state particles. ("Jets" are tightly collimated groups of final-state particles that arise from the "fragmentation" of the more fundamental quarks or gluons.) Identifying the jets in an event is one of the key tasks of the reconstruction and analysis software. In order to check the jet properties of the generated events without having to go through the full chain of detector simulation, reconstruction, and analysis, the package Atlfast was used. As its name implies, Atlfast is an extremely fast package that approximates well the full detector simulation and particle reconstruction, using carefully tuned parameterisations. Jet finding was performed by running Atlfast, and then using Atlfast utilities to perform the jet finding at the particle level. The output ntuple was then used in a secondary job, running in the Physics Analysis Workstation (PAW18) environment, to produce histograms of the number of reconstructed jets, their transverse momentum (\(\mathrm{p_{T}}\)) spectra and pseudo-rapidity distributions. These are normalised in various ways as appropriate: to the number of events; to the number of jets; and to the total cross section. Finally, the two histogram samples (event characteristics and jet properties) were merged and a postscript summary of all of the histograms produced was made and checked for consistency with the physics expectations for the given sample.

#### Quality Assurance and Validation

The aim of the ATLAS DC quality assurance and validation procedure19 was threefold:

- to ensure the compatibility and reproducibility of samples produced at different sites (site validation);

- to monitor the changes and improvements to the ATLAS detector geometry;

- to check the physics content of the generated samples (physics validation).

As the new C++ reconstruction software to be used in DC1 (section 2.3) was undergoing continual development, it was decided to base the quality assurance and validation on the older, but stable, Fortran-based reconstruction framework, Atrecon20. The validation test suite consists of a modular analysis structure based on PAW, which runs off a general-purpose ntuple from Atrecon and which contains information on Monte Carlo event generation and the reconstruction for all ATLAS sub-detectors.

Footnote 19: [http://cern.ch/atlas/GROUPS/SOFTWARE/DC/Validation/www](http://cern.ch/atlas/GROUPS/SOFTWARE/DC/Validation/www)

Footnote 20: [http://cern.ch/atlas/GROUPS/SOFTWARE/DOCUMENTS/reconstruction.html](http://cern.ch/atlas/GROUPS/SOFTWARE/DOCUMENTS/reconstruction.html)

The analysis procedure consists of comparing two output ntuples in two steps. First, an open-ended list of sub-detector-specific macros is run from a master process to produce the two sets of validation histograms. Second, a histogram-by-histogram comparison is performed between two sets of validation histograms, providing a bin-by-bin significance plot and a \(\chi^{2}\) test. At the end a summary \(\chi^{2}\) bar chart for all compared histograms is made.

The site validation was done by comparing the outputs from identical input samples run at different sites and by comparisons of larger, statistically independent, samples of the same physics process. The validation provided an important check of the simulation infrastructure at the contributing DC sites. For example, it made it possible to spot slight but significant differences of the run-time libraries. During the initial phase this was a quite complex and intensive, but absolutely necessary, activity.

The physics validation of the data was carried out in parallel with the other checks. A comparison of the number of jets, b-jets, c-jets, electrons and photons in each event, with a similar sample produced in a previous large-scale production, was performed. In addition, new fully simulated samples were inspected and were used for detailed detector calibration and similar purposes. New samples were also used extensively to study b-tagging. The b-physics group validated the DC1 simulation-reconstruction software chain for several detector layouts 21.

Footnote 21: DC1-b-physics validation: ATL-COM-PHYS-2003-003Computing Elements of DC1

In section 2 we have outlined briefly the scope of a large-scale simulation study for the ATLAS experiment, i.e. _what_ DC1 was trying to achieve at each stage. We turn now, and for the rest of this paper, to _how_ DC1 was carried out, in terms of the software infrastructure used. This infrastructure is applicable to any large-scale computing operation of this type, carried out in a worldwide context. We identify the following elements:

1. Software distribution: the ability to distribute and install the software executables and libraries is an essential pre-requisite for the use of worldwide computing resources, and the distribution process used for DC1 is described in section 3.1.
2. Data bookkeeping: DC1 comprises in essence a series of "transformations" on large quantities of data and the details ("metadata") of each transformation must be recorded. The database used for this purpose for DC1 is described in section 3.2.
3. Data access and replication: Distributed processing of data requires knowledge of where data resides, so that it can be accessed or copied ("replicated") to appropriate storage. The infrastructure used for this is described in section 3.3.
4. Workflow control: This covers the preparation of job parameters, the submission of jobs, monitoring their progress, and the action taken if a job fails. This is described in section 3.4.

It is worth emphasising that these elements are present, logically, in almost any computing task that involves the processing of data external to the computer programme. As the scale of such data processing has grown, software has been evolved to automate the overall management. From this perspective the Grid is just the latest step in the development of software to automate these elements as much as possible. The use of three of the emerging Grids, NorduGrid, US Grid, and EDG, for part of DC1 is described in section 4.

### Software Distribution

The ATLAS software is split into more than 500 packages residing in a single CVS repository at CERN. The Configuration Management Tool, CMT22, manages package dependencies, libraries and building the executables. New releases are built at CERN approximately every three weeks, following an agreed schedule for the introduction of new features into the software.

Footnote 22: [http://www.cmtsite.org/](http://www.cmtsite.org/)

The compilation process is done on Linux machines. Users with a good network connection and access to AFS could use executables and data files directly from CERN. This approach is of course not suitable for remote sites with a limited network bandwidth or without access to AFS. Therefore, the relevant ATLAS software releases have been packaged into 'kits' in RPM (RedHat Package Manager) format. For DC1 the kits, along with the installation scripts, were available for download via secure web connection or, otherwise, from the various Grid sites, EDG, USGrid or NorduGrid.

The general criteria followed during the package-architecture development phase have been to build a self-consistent distribution procedure, not dependent on the Linux version. The installation has been designed to keep the same directory structure as in the CERN repository. To be consistent with the reference software produced at CERN, all the executables and libraries shipped with the kit are copied from the official ATLAS AFS software repository.

The packages are organized in a set of base tools that are required for all installations, and several additional components. The minimal installation provides the following items:

-the set-up and management scripts;

-the official ATLAS compilers;

-the required libraries not part of the ATLAS software (external packages);

-the ATLAS executable, libraries and data needed at runtime.

Other packages are provided to enable code development.

The RPM suites have proven to be robust and efficient. Most of the countries and sites have installed the software using the official set of RPMs, but some sites for the DC1 production have also adopted other types of installations. In particular a procedure based on full mirroring of the distributions directly from the CERN AFS repository, and the production of an alternate set of RPMs, have been used on the NorduGrid testbed.

The main drawback found in the use of RPMs was the lack of flexibility: bug fixes in the new reconstruction software required entire new releases to be built and distributed.

### 3.2 Data Bookkeeping

The associated bookkeeping and meta-data services are an essential component of DC1. The DC1 data comprises a large number of "datasets" and "partitions". Each "dataset" is a collection of events, and has a logical dataset name, unique within ATLAS, assigned according to a nomenclature established by the production team. Datasets are further divided into partitions because of file-size limitations. A partition is a file that contains a part of a dataset, and is assigned a unique Logical File Names (LFN) that contains, by convention, at least the key part of the logical dataset name, and a partition number.

#### Ami

The Atlas Metadata Interface (AMI) is a framework consisting of relational databases that contain their own description, a software layer to exploit this auto-description, and a set of interfaces for insertion and searching. AMI was used in the context of DC1 to manage the bookkeeping database that stores descriptive information (meta-attributes) about the data (binary files).

The attributes of the binary data file that are stored by the bookkeeping database are "logical". These are application specific properties that could not be guessed by any outside system, such as a grid application. An example of a logical attribute is the type of event contained in the data file. A logical attribute never changes when data files are replicated.

The aim of the bookkeeping catalogue is twofold:

* to make it possible to understand the contents of a file of binary physics data without actually having to open it,
* to search for data given a set of logical attributes.

The set of attributes defined for DC1 datasets was established in close collaboration with the production management team. It does include a few parameters that are not strictly logical, for example the data file sizes. These were included to ease the production of statistics.

AMI provides several interfaces for use by physicists, and an API for developers.

#### AMI Architecture

AMI is written in Java. In consequence, it is independent of platform, operating system and database technology. The only prerequisite is that Java is installed on the client system.

The AMI software is built in three levels above the Java Data Base Connection (JDBC) layer, as shown in Figure 3. The lower level packages wrap the remote connections to database, the transmission of SQL commands, and the recuperation of query results. While mySQL was used for DC1, any database that understands SQL, and for which a JDBC driver is available, may in principle be used.

Each AMI compliant database namespace must contain a certain number of tables that describe it. The middle layer of AMI provides generic classes for making use of these internal descriptions. For example, a table called "db_model" exists in each namespace, which describes the relations between tables. This generic mechanism hides details of database implementation from clients. A client who queries an AMI database is not expected to have knowledge of the name of the database, or the name of any database tables, or the relation between the database tables. The architecture allows clients to express queries in terms of the application semantics. Thus a user of the DC1 AMI production bookkeeping database should be able to work with a schema of datasets, partitions or files, whereas a client of another AMI based application would work with a different semantic. The lower and middle layers of software are common to the two sets of users. The generic web-search interface is part of the middle layer of AMI.

The top layers of the software are application specific. Packages in this top layer have knowledge of non-generic features of the database tables. In the case of DC1 bookkeeping, the top layer contains in particular classes to manage dataset provenance, dataset definition, dataset nomenclature protocol and production statistics. Some specific commands and web interfaces have been developed that make use of this layer.

The JAVA API of AMI provides access to both the application specific and the generic functions.

The architecture is designed to allow geographic distribution of databases; all connections pass through a central router, which redirects requests to the correct site. This central router should be mirrored. For DC1 however, all the databases are physically at the LPSC Grenoble, and are situated on the same server.

### The Command Line Interface

A large proportion of the updating of AMI was done by using the production tools AtCom and GRAT described below. AMI also provides a command line interface so that physicists running jobs without using the production tools were able to insert to and update the bookkeeping database.

For DC1 the client software was available either as a stand-alone JAR file, or as part of the Atlas software release. The client software makes use of a configuration file containing the server and port details of the database, and other parameters common to all the commands. The first client session must configure the software using a database password distributed by a separate mechanism.

A large number of commands are available, including commands to query the database, and to obtain information about the attributes.

For jobs that did not use AtCom or Grat, the information for updating AMI was contained in batch job log files. Scripts were provided to parse the log files and to generate a set of AMI commands for insertion or update of AMI records.

### The Web Interfaces

AMI contains a generic read-only web interface for searching. It is generated from the auto-descriptions of the core databases, which means that the database schema can be changed without touching the web interface code. The interface has a "quick" search, where certain fields for searching are pre-selected, and an "advanced" search, which gives access to all the

Figure 3: The 3-tier architecture of AMIdatabase fields. In both cases, the SQL constructed is visible to the user, and can be directly edited if the user desires. Users can navigate from the search results to a graph showing the provenance of a dataset, and also to the MAGDA database that contains information on the physical location of the files.

Other special Atlas Production interfaces have been provided:

* Users can request a new dataset by specifying the desired characteristics. The request is then sent to the production manager.
* The Production manager can examine the requested dataset, and either approve the dataset, with the possibility of editing some fields, or refuse it.

A third interface is available to give an overview of the state of production. Some simple statistics are available, and it is possible to obtain a pie chart of jobs done per production site.

### 3.3 Data Access and Replication

To complement the information stored by AMI, a well-defined strategy for how to replicate and access the large worldwide-distributed datasets is needed to ensure that:

* the provenance of each partition is uniquely defined and documented (including all processing and selection steps i.e. the metadata information);
* identical results are obtained independent of the actual location of each replica.

#### Manager for Grid-based Data (MAGDA)

MAGDA23, developed at Brookhaven National Laboratory (BNL), provides automated file registration and replication tools. Thus it is a manager well suited to Grid-based data, but can be used in a non-Grid environment. It is built upon a database that describes _where_ the data reside; thus it complements AMI, which describes _what_ the data are.

Footnote 23: [http://arxiv.org/abs/physics/0306105](http://arxiv.org/abs/physics/0306105)

The ATLAS experiment data are distributed across globally dispersed storage facilities. In MAGDA the concept of a'site' is used to abstract the storage facility, as shown in Figure 4. The term 'location' is used to denote data locations within a site - typically directory trees. Locations carry attributes characterizing the data stored there, such as whether the data are a master or replica copy. The concept of a 'host' is used to represent a set of computers that have access to a defined set of sites. Thus a MAGDA service, having knowledge of the host it is running on, is automatically aware of all the data locally accessible to it.

MAGDA makes use of MySQL, Perl, Java, and C++ to provide file cataloguing, retrieval and replication services. For data movement, gridFTP, bbftp and scp can be chosen depending on available protocols at the given site. Third party transfers are also implemented, if allowed.

The principal catalogues maintained by MAGDA, as MySQL tables, are: a file catalogue with logical and physical file information and metadata; site, location and host catalogues; a catalogue of logical file collections; and a task catalogue. File metadata describe the file attributes and content. The file catalogue supports the notion of master and replica instances. Automatic file replication operations are organized into reusable tasks.

MAGDA provides the following major services:

1. Setting up and managing distributed sites with associated locations, and locations within those sites, and the hosts on which data-gathering servers and user applications run.
2. Gathering the file information from the various sorts of data sites. A file spider cron job can be set up to automatically scan several storage sites and reports to the catalogue database.
3. Interfacing to users via web interfaces for presenting and querying catalogue information and for modifying the system.
4. Replicating and serving files to production and end-user applications.

All input and output files in DC1 were registered in the MAGDA catalogue. Automated tools like GRAT (see section 4.2) searched for and transferred input and output files from grid sites using the MAGDA client tools. Physicists used similar tools to find data files for analysis. MAGDA spiders were used to catalogue files that were generated on batch machines using tools other than GRAT. Over 1.5 million files at hundreds of sites were catalogued and made accessible for DC1 through MAGDA.

### 3.4 Workflow Control

A number of tools were developed and used to facilitate the production and the monitoring of the DC1 data-processing. The principal ones are:

AtCom24 (short for ATLAS Commander), developed in Europe, an automated job definition, submission and monitoring tool, working directly with AMI;

Footnote 24: [http://atlas-project-atcom.web.cern.ch](http://atlas-project-atcom.web.cern.ch)

GRAT25 (short for the GRid Application Toolkit), developed in the US, an automated Grid-based workflow management tool.

Figure 4: Logical relations among site, location and host in MAGDA

The purpose of both of these tools is to automate as much as possible the task of a production manager: defining and submitting jobs in large quantities, following up their execution, scanning log files for known errors, updating the various ATLAS bookkeeping databases in case of success, and cleaning-up and resubmitting in case of failure. AtCom is described in the next section; GRAT, which was designed for and used in Grid applications, is summarised in section 4.2.

#### ATLAS Commander (AtCom)

The AtCom design26 is modular, separating the generic, basic job-management functionality from the interactions with the various databases on the one hand, and the computing systems on the other hand. The interactions with the various computing systems on which the jobs run are defined by means of separate plug-ins, which are loaded dynamically at run time. Since different flavours of computing systems ('legacy' and various Grid flavours) will be deployed concurrently at the various ATLAS sites, AtCom allows several of them to be used transparently at the same time.

Footnote 26: [http://arxiv.org/abs/hep-ex/0305089](http://arxiv.org/abs/hep-ex/0305089)

The design of the tool assumes that jobs can be defined in a computing-system neutral way. The DC1 implementation features a virtual-data-inspired approach that equates job definitions with a reference to a "transformation" definition, combined with actual values for the transformation's formal parameters. The definition of a transformation includes a reference to a script/executable, its needed execution environment in the form of 'used' packages, and a signature enumerating the formal parameters and their types.

Figure 5 shows the top-level architecture of AtCom. In the middle is the AtCom core application that implements the logic of defining, submitting and monitoring jobs. On the left are the two modules that interface AtCom to the ATLAS bookkeeping databases, AMI and MAGDA. On the right is the set of plug-ins that interface AtCom to the various flavours of computing systems.

The underlying production model is based on the concepts of datasets, partitions, transformations and jobs. A dataset is a chunk of data that logically forms a single unit. As explained in section 3.2, datasets are for practical reasons split into a number of partitions, each corresponding to a separate logical file. At the dataset level, abstract transformations create datasets based on a number of parameters and possibly using one or more other datasets

Figure 5: The AtCom architecture

as input. This transformation process is implemented using a number of concrete transformations, each being a single job operating on the partition level.

The computing system plug-ins implement an abstract interface that defines methods and signatures for the usual operations: submitting a job, getting the status of a job, killing a job and getting the current output (stdout and stderr) of a job.

AtCom supports three classes of operations: job definition, job submission and job monitoring:

From the definition panel, the user can select, by means of an SQL query composer, a dataset to be defined into partition. The user defines the fields of the dataset he/she wants to see and the selection criteria. Pull-down menus allow the composition of the most common queries, but the query text can be edited when needed. The search is executed and the result is displayed. The user can then select a single dataset and choose a particular version of the associated transformation. Based on this concrete transformation's signature, AtCom will compose a form that will allow the definition of the values for all required parameters for all the wanted partitions.

The second AtCom panel allows the user to submit any defined partition to any configured computing system. The procedure starts again with an SQL composer allowing the retrieval of a set of partitions. Given a set of retrieved partitions the user can select an arbitrary subset and select a target computing system for job submission. The jobs are submitted and automatically transferred to the next panel for monitoring.

The monitoring panel allows the user to check the status of all monitored jobs on demand, or to poll automatically at regular intervals. Additionally, the user can select a number of jobs and right click on them to invoke one of a large set of operations: kill, submit, refresh, revalidate, etc.

When a job moves from 'running' to 'done', post-processing is automatically initiated. If the job has terminated successfully, the output files are registered with the replica catalogue (MAGDA). If the job failed, the output as defined in the partition's output mapping are deleted and the status is set to 'failed'. If the job is 'undecided', the status is changed accordingly, pending a decision by the user.

## 4 DC1 and the Grid

A recent and highly significant advance in computing is the emergence of Grid technologies. Powered by various middleware, Grid computing infrastructures are becoming a reality, and as such are particularly important for large distributed projects such as High Energy Physics experiments like ATLAS. By harnessing distributed and scarce resources into a powerful system, the Grid is expected to play a major role in the near future. Apart from optimising the usage of distributed resources, the Grid will naturally offer to all members of the ATLAS collaboration a uniform way of carrying out computational tasks. This is essential for large production tasks, which need plenty of worldwide distributed resources, both hardware and human.

A significant fraction of DC1 was performed in the Grid environment, involving about 21 sites and several flavours of Grid middleware. Members of the ATLAS DC Team also participated in a task force to test EDG middleware on a dedicated test-bed, and provided valuable feedback to EDG developers. The concept of Virtual Data, put forward by the U.S. GriPhyN Project27, was also used in a prototypical way in a part of the DC production, and this approach is described in section 4.1. The workflow manager GRAT, developed specifically for the Grid, is described in section 4.2, and the use of the US Grid testbed, NorduGrid, and EDG is described in the subsequent sections.

Footnote 27: [http://www.grighyn.org](http://www.grighyn.org)

### 4.1 Prototyping Virtual Data Approach

In HEP computing, preparation of the recipes for data production requires significant effort and encapsulates a considerable amount of knowledge. The experience in ATLAS so far has demonstrated that development of the production recipes typically involves several feedback cycles in order to assure the correctness of the generated data. The necessity to verify and reproduce these results makes the development of the production recipes a laborious iterative process.

The GriPhyN project emphasises this perspective on recipes as virtual data:

- recipes are as valuable as the data;

- production recipes are the virtual data.

It is useful to distinguish (both conceptually and in design) the data required before the invocation of a transformation from the history information collected during and after the data transformation28. In that regard the Virtual Data Cookbook (VDC) catalogue encapsulates the specific data transformation knowledge and the validated parameters settings that must exist before the invocation of a transformation. For each data-transformation step in the DC1 processing pipeline, the essential content of the verified data production recipes was captured and preserved in a Virtual Data Cookbook database. The collection of production recipes -VDC - complements ATLAS Grid tools deployed in ATLAS Data Challenge production as shown in Figure 6.

Footnote 28: A. Vaniachine et al., Prototyping Virtual Data Technologies in ATLAS Data Challenge 1 Production. [http://arxiv.org/abs/cs.DC/0306102](http://arxiv.org/abs/cs.DC/0306102)

Fig. 6: Architectural view of the relationships between the three catalogues present in the Virtual Data System29 and the corresponding ATLAS Grid tools that were deployed and used in ATLAS DC1 as the components of data management architecture supporting the processing workflow.

Because Virtual Data technologies were in the prototyping stage at the start of DC1, the data volume allocated for the production test of the system was limited to about one fifth of all the DC1 data. A production system, utilizing the VDC prototype, implemented the scatter-gather data-processing architecture to enable high-throughput computing. The parameters for simulations were catalogued in the VDC database, with attribute collections normalized according to their non-overlapping domains: data reproducibility, application complexity, and grid location.

To provide the local-remote transparency during DC1 production, the VDC database server delivered in a controlled way both the validated production parameters and the templated production recipes for thousands of event-generation and detector-simulation jobs around the world, simplifying the production management. Given that the production system relied on the VDC server running at one central location (CERN), the reported failure rate was remarkably low (less than 1 per thousand) over the whole DC1 production period. Further improvement in the VDC services robustness will be achieved by deploying catalogue replicas at different geographic locations.

The major benefit of VDC database technology was demonstrated by simplifying the management of the parameter collections that were different for each of the more than two hundred datasets produced in DC1. Significant reduction in the parameter-management overhead enabled successful processing of about half of all the DC1 datasets, representing 20% of the total data volume, using the VDC services.

### 4.2 Grid Application Toolkit (GRAT)

GRAT was developed to facilitate automated ATLAS Monte Carlo production in a Grid environment. It consists of bash and python shell scripts, and was frequently modified and updated over the course of DC1, both to add new features and to adapt to the evolving Grid middleware. GRAT provided a flexible prototyping environment to test the emerging grid middleware in a real production environment during DC1.

The scripts in GRAT provide software utilities to handle all phases of Monte Carlo production, including job definition, job submission, verification of results, data storage management, as well as production-site management. A MySQL database (called production database or prodDB for short) was used to store persistent information about all jobs processed by GRAT. Towards the end of DC1, this database was merged with the AMI database. Job definition tasks include adding new datasets to the production database and incorporating new steps into the production chain as required. During actual job submission, GRAT can launch either single or multiple jobs at a remote site, or a given dataset can be subdivided for submission to several sites in parallel. Post-execution, results are verified by performing data quality checks (usually by analysis of log files), and errors are automatically corrected where possible by restarting jobs and moving output files to the final storage location. Failed jobs that cannot be recovered are cleaned up and the jobs database modified accordingly. Management of storage resources includes the movement and/or deletion of verified input files, cleaning up temporary storage areas once jobs are completed, and disposing of replica file copies from intermediate processing steps. In the area of production-site management, GRAT monitors job manager queues and running jobs, using both dynamic lookups and database queries. Information about disk storage resources at a site is provided, and the availability of required production software is verified. In the case of "pile-up" production, pre-staging and management of the pile-up input files is performed.

Data-management tools are provided to facilitate interactions with the various databases. New information can be added, for example by creating or updating entries in the production database or AMI. Database queries allow one to obtain information regarding single entries in the production database, to view summary information, to make a decision whether a job is in a "hung" state, and to view the characteristics of jobs waiting to process. Other utilities enable consistency checks such as: (1) scanning for and correcting bad records; (2) ensuring the accuracy of replica copies; (3) verifying the existence of generated files in MAGDA; and (4) ensuring the integrity of common data distributed among multiple databases.

### 4.3 DC1 Production on the U.S. Testbed

DC1 production in the U.S. was carried out using both batch and grid facilities. Batch processing was done at BNL, and is not described further here. Grid processing took place in the U.S. ATLAS grid testbed, a widely distributed computational grid comprising eleven institutions. The grid testbed became available in summer 2002. A special tarball of the GEANT executables was made for the grid, containing binaries only for RedHat Linux. The executables were initially installed on Globus gatekeeper machines at three U.S. testbed sites. Production was eventually expanded to seven testbed sites. All simulation and pile-up production in the U.S. testbed was carried out using the GRAT system described above.

GRAT was used to submit approximately 50k jobs during DC1 and had an 80% success rate - most failures happened due to hardware and software failures or scheduled outages. The submission process was completely automatic and required very little supervision or intervention. In most cases of a site being unavailable, the scheduler continued production with the other available sites without problem.

Each production job on the grid has many stages. First the Globus gatekeeper of the site selected by the scheduler is queried for software location information. Next, a suitable available partition is chosen for production. The proposed logical filename (LFN) is registered in MAGDA along with various production related information. All executables are staged into a temporary location. A script with the location of the executables and environment variables is sent to the queue on the selected site. The job is started asynchronously by the batch queue system. The GRAT scheduler checks every 5 minutes if the production job has finished. Once it finishes (on average after 14 hours for simulation jobs), the files are moved to the BNL HPSS tape storage system by MAGDA. All LFNs are registered in the MAGDA catalogue. A disk-based replica is also made by MAGDA at one of the available grid sites.

An independent semi-automatic quality of service (QOS) process is run periodically. This job checks the MAGDA production database for the job status of every partition (the production job updates this database periodically during staging and execution). It checks the job status on the submitted queue through Globus. It verifies through MAGDA that all files are correctly stored in the HPSS and replica locations. It checks if the temporary staging location has been cleaned up after production. This process can correct for many failures and updates the production database if it can recover files. For example, the BNL HPSS was unavailable for a couple of days - production continued without any changes. When HPSS was available again, the QOS process automatically copied and catalogued all primary files from the replicas using MAGDA.

Most of the problems during the two weeks of production were typical of distributed systems spread out over four locations thousands of miles apart (New York, California, Texas and Oklahoma). Various machines were not available at critical times. Even when empty queues were available, however, we could not run production faster than about 70-80 jobs per day at any one site. After some tuning of middleware parameters, this limitation was eliminated for Phase 2 and all U.S. pile-up production was done on the Grid. Seven sites were used for pile-up production, which had much more complex requirements since hundreds of inputs files were merged together randomly. The output files also had to be split because of maximum file size limitations.

More than two million events were fully simulated, piled-up and reconstructed in the U.S. The majority of the jobs during the simulation phase was done with the batch system at BNL, while we learned how to use the grid systems and developed the GRAT software. All the complex jobs in the pile-up phase were done on the grid testbed. A majority of jobs during the reconstruction phase was done with the batch system. A new grid system based on Chimera was used also for reconstruction towards the end of DC1. During the various phases, over 30 Terabyte of data was stored on disk and HPSS systems in the U.S., the equivalent of about 40 years of cycles on a single CPU cycles were used, and over 100k files were generated and catalogued using MAGDA. Approximately half the jobs were done using the GRAT software, demonstrating the usefulness of the grid in exploiting ten sites distributed in the U.S.

### 4.4 DC1 Production on NorduGrid

The aims of the NorduGrid project have been, from the start, to build and operate a production Grid in Scandinavia and Finland. The project was started in May 2001 and has been running a testbed since May 2002. Taking advantage of the existence of the NorduGrid testbed and tools, physicists from Scandinavia and Finland were able to participate in the overall DC1 exercise using solely the NorduGrid environment30. During the whole of DC1, more than 2 TB of input data were processed and more than 2.5 TB of output data were produced by more than 4750 Grid jobs31.

Footnote 30: [http://www.nordugrid.org/documents/atlasdc1.html](http://www.nordugrid.org/documents/atlasdc1.html)

The NorduGrid resources range from the original small test-clusters at the different physics institutions to some of the biggest supercomputer clusters in the region. It is one of the largest operational Grids in the world with approximately 1000 CPU's available 24 hours a day, 7 days a week. It is, however, not exclusively dedicated to ATLAS, but used by all sciences.

In Phase 1 (Section 2.1) of DC1, all the input files were pre-staged (replicated) at all the sites and output files were stored at a designated Storage Element.

During Phase 2 (Section 2.2), those output files, together with files containing minimum bias events, were the input for the pile-up production. Therefore, pre-staging, as in Phase 1, was not feasible, and so the Grid Manager had to download input files for each job. However, to optimise the task, "minimum bias" files were pre-staged at several sites, sometimes only partially (i.e. not the entire set). Thus, whenever an input file (containing either signal or minimum bias events) was missing for a specific job, the Grid Manager would proceed to download it and cache it for potential use by another job. This caching was particularly convenient for "minimum bias" files, as they were often re-used by several jobs.

Phase 3 (Section 2.3) followed the same scheme as Phase 2, except that there were no "minimum bias" files to be pre-staged. Input data for this phase were stored in many storage facilities, including servers at BNL, and were located by the NorduGrid services with the help of the Replica Catalogue (MAGDA).

It is worth mentioning that part of the NorduGrid success was due to the RPM installation of the ATLAS software releases, different from the "build-in-place" structure current at the time. This "Linux-style" approach to the distribution of group binaries, libraries, etc. was adopted by CMT via the "install area", and is now widely used as the production installation procedure (see also section 3.1). NorduGrid RPMs were used by the USGrid via the package manager, Pacman32

Footnote 32: [http://physics.bu.edu/](http://physics.bu.edu/)\(\sim\)youssef/pacman/htmls/

NorduGrid has contributed substantially in all three phases of DC1. Important lessons about the NorduGrid middleware have been learned during the production periods, which have been used to extend the stability, flexibility and functionality of the software and NorduGrid itself.

### DC1 Production using the EDG testbed

The European DATAGRID (EDG)33 project aims to develop a complete Grid solution, which includes the Globus-based middleware, as well as tools for fabric and mass-storage management, and network monitoring.

Footnote 33: [http://cern.ch/eu-datagrid/](http://cern.ch/eu-datagrid/)

In July 2002, at the start of DC1 Phase 1, it was decided to start a focused effort by forming a Task Force involving ATLAS and EDG personnel, with substantial support from the EDG management and especially by members of the HEP Applications Work Package. To evaluate the EDG middleware, a small subset (\(\sim\)1% of the complete data) of the DC1 was chosen. The first tests (July-Sep 2002) showed several major problems in various areas of EDG middleware. This triggered developments during September 2002 in the areas of Information Systems, Data Management and Job Submission. More details on the ATLAS EDG tests in DC1 Phase 1 are available on the web34.

Footnote 34: [https://edms.cern.ch/file/375586/1.2/wp8-D8_3-0119-3-1.pdf](https://edms.cern.ch/file/375586/1.2/wp8-D8_3-0119-3-1.pdf)

During the tests, EDG releases 1.2.0 and 1.2.2 were used first in Phase1; the final Phase 1 tests were performed with release 1.4.3. These releases were installed at the 5 core EDG Testbed sites35 and one external site36 involved in the CrossGrid Project37, constituting the Applications Testbed infrastructure. All the sites were listed in the Information Index of the dedicated Applications Testbed Resource Broker. Only these Resource Brokers were used in the tests.

Footnote 35: CERN (Geneva, Switzerland), NIKHEF (Amsterdam, The Netherlands), CC-IN2P3 (Lyon, France), RAL (Oxford, United Kingdom), and CNAF (Bologna, Italy).

Footnote 36: FZK (Karlsruhe, Germany)

Footnote 37: The CrossGrid project, [http://www.crossgrid.org/](http://www.crossgrid.org/)All the resources and users had certificates issued by one of the EDG Certificate Authorities38. Registration in the ATLAS Virtual Organization was performed for all ATLAS users participating in the EDG tests. The final Phase 1 tests showed a global efficiency of only about 50%, mainly due to local problems (site configuration, lack of disk space on the Storage Element, etc.).

Footnote 38: EDG Certification Authorities, [http://marianne.in2p3.fr/datagrid/ca/](http://marianne.in2p3.fr/datagrid/ca/)

### Small-scale test:

On the basis of these results, it was decided in May 2003 to use the EDG testbed for the production of a small part of the ATLAS reconstruction (DC1 Phase3). Thus 250 reconstruction jobs processing about 50 000 events have been run in spring 2003. The sites involved were Cambridge, Lyon, Milan, Roma and Bologna, with Resource Brokers set up at Bologna and Lyon.

The ATLAS reconstruction software required RH 7.3, which was not yet officially supported by this version of EDG middleware. Additional work had to be done to create new LCFG39 profiles to install both the operating system RH 7.3 and the EDG software on the Worker Nodes, while keeping the gatekeepers running RH 6.2. This required certain merging with EDG release 1.5.0, which was forthcoming at the time.

Footnote 39: A Large Scale UNIX Configuration System, [http://www.lcfg.org](http://www.lcfg.org)

The input data were copied on the Storage Elements of the participating sites. The job submission was made as transparent as possible, specifying only the required input file (which was always local) and the job type. Output files were manipulated manually, by storing them at a local SE and registering them into the ATLAS RC. The task of the matchmaking of the resources has been assigned to the EDG Resource Broker (RB), which performed it successfully.

The Resource Broker and the whole EDG middleware have shown good stability over a period of about 2 weeks, requiring only slight interventions of the site managers. It should be noted that this mini-production did not constitute a stress test: the ATLAS job rate was modest and few activities from other users were going on in parallel.

Approximately 15% of the jobs failed for various reasons (not mainly due to EDG middleware) but ran successfully after re-submission. This mini-production demonstrated that the EDG middleware was actually very capable of handling ATLAS production jobs. Although it was not of a scale to verify the stability and the scalability of the middleware in case of a full-scale production, it has provided evidence that the EDG performance was greatly improved during the first quarter of 2003 and that many of the problems, previously identified by ATLAS, had been solved.

## 5 Lessons Learned

The principal lessons learned, or rather reinforced, from the DC1 exercise are the expected, general ones:

* there is no substitute for testing, and testing "at production scale" or as near to it as possible;time contingency will always be used up;
* the'system' will fail in ways that could not have been'reasonably' anticipated;
* too many problems fall on an already over-stretched small team of people.

(At times the exercise brought to mind a well-known military dictum.40)

Footnote 40: â€œKein Plan Ã¼berlebt die erste FeindberÃ¼hrung.â€ Helmuth Graf von Moltke

More specifically, particular lessons from DC1 are:

* Software installation at remote (from CERN) sites must be as automated as possible: the standard ATLAS installation kit now runs a series of test jobs to check the installation;
* Meta-data stored in databases must be correct ('integrity') and complete ('coherence'), otherwise users are forced to complement the meta-data by ad hoc, manual intervention, which is wasteful of effort and error-prone. A flexible database system has, by definition, many ways of being used, and arriving at the appropriate tools to load it and retrieve data from it needs care and time;
* There is not going to be just one Grid system: the LHC experiments will have to operate with several, somewhat heterogeneous Grid systems, but this heterogeneity must be hidden from the experiment-level application.

## 6 Conclusions

ATLAS Data Challenge 1 ran from spring 2002 to spring 2003. For several reasons it was divided into phases.

* Phase 1 was used to put in place the worldwide production infrastructures and to produce the bulk of simulated data needed by our colleagues of the High Level Trigger for their Technical Design Report. Over a period of 40 calendar days the equivalent of 13.5 million of SI2k-days were used to produce 10 million physics events and 40 million single particle events for a total volume of 30 TBytes. The success of a worldwide exercise of this scale certainly exceeded our most optimistic expectations. 40 institutes in 19 countries actively participated in the effort.
* The pile-up production (Phase 2) ran smoothly. 3.2 MSI2k-days were needed to produce about 34 TBytes of data.
* A large fraction of the data has been reconstructed (Phase 3) in offline and/or trigger reconstruction mode. 4.4 MSI2k-days were needed to produce about 200 GBytes of data.

The numbers for all three phases together are approximately:

* 21 MSI2k-days
* 70 TBytes produced
* 100k partitions

Data Challenges are the perfect opportunity to evaluate the current status of the Grid middleware and assess what has to be done by the collaboration in order to make a smooth transition to Grid tools. Therefore ATLAS has been extremely active in Grid matters since mid-2002. During DC1 we have seen the emergence of the production on the Grid. Grid tools were used intensively on NorduGrid and U.S. testbeds, and EDG was developed and tested. We are confident that their use will continue to grow.

In summary, ATLAS DC1 has proved to be a very fruitful and useful enterprise, with much valuable experience gained, providing feedback and triggering interactions between various groups, for example groups involved in ATLAS computing (e.g., HLT, offline-software developers, Physics Group), Grid middleware developers, and CERN IT. Much has been learned from DC1, and much more will doubtless be learned in the future, when more Grid tools will be used in DC2. However, we can already be rather confident that ATLAS will be able to marshal world-wide resources in an effective way; let us hope that the Grid will make it all rather easy.

Finally, perhaps the most important benefits of DC1 have been to establish a very good collaborative spirit between all members of the DC team and to increase the momentum of ATLAS Computing as a whole.

## Acknowledgments

We would like to thank all members of the ATLAS collaboration who participated to the effort. It would not have been possible to run our Data Challenge successfully without the involvement of numerous people, from many institutes and computing centres, who helped us to put in place the production chain and to run the production. We thank all of them most sincerely.

It is with the deepest regret that we must note the untimely deaths of our colleagues Steve O'Neale (Birmingham) and Marc Virchaux (Saclay). Both made huge contributions to ATLAS computing over many years, and will be sorely missed.