[MISSING_PAGE_EMPTY:1]

## 1 Introduction

Collisions at the Large Hadron Collider (LHC) [1] produce high-energy quarks and gluons at high rates. The strong interactions then create a shower of collimated particles called a jet. Jets initiated by quarks and gluons tend to be very similar. Still, there are some differences, e.g. the gluon jets tend to be wider, with more constituents that are softer on average. Thus, a tagger of quark- and gluon-initiated jets (\(q/g\) tagger) can be trained to identify the jets' origin.

Distinguishing between quark- and gluon-initiated jets can be used in a variety of applications, such as the search for new physics and precise measurements of the Standard Model (SM). Many interesting electroweak processes produce two quark-initiated jets associated to the vector boson or Higgs production [2, 3]. Many new physics models predict heavy particles decaying to two quark-initiated jets [4, 5]. Gluinos in supersymmetric (SUSY) models could decay to quark-initiated jets and neutralinos [6]. In RPV SUSY models, the neutralinos could further decay into additional quark-initiated jets [7]. New physics searches could also profit from gluon-initiated jets tagging, e.g. searches for resonances decaying to the \(b\)-quark and the gluon [8] or searches for specific stealth SUSY models [9]. Also, the jet calibration is slightly different for quark- and gluon-initiated jets and a \(q/g\) tagger could increase the accuracy of the jet calibration [10].

The task of separating quark- and gluon-initiated jets is more difficult than typical tagging problems such as those for top and \(W\)-initiated jets, due to the large similarity of quark- and gluon-initiated jets. Monte Carlo (MC) simulations are used to model the SM multijet production, providing the truth information about the jet origin, which is used to train the tagger. In the past, variables describing the jet as a whole (high-level jet variables), such as the number of charged tracks detected in the jet, were used [11, 12]. Quark- and gluon-initiated jets have different distributions of energy between their constituents, and variables sensitive to the energy and angular correlations between the constituents were developed [13]. Machine Learning techniques allow the utilization of several variables and take into account correlations among these variables. The most adopted for \(q/g\) tagging in ATLAS is the Boosted Decision Tree (BDT) algorithm [14], which is the latest developed ATLAS \(q/g\) tagger [15], using jet transverse momentum, \(p_{\mathrm{T}}\), jet pseudorapidity, \(\eta\)1, jet width, the count of constituents within a jet, and the two-point correlation function [13, 16] as inputs. One study at ATLAS used a more granular description of the jet utilizing the full radiation pattern inside a jet processed as an image in a convolutional neural network \(q/g\) tagger [17]. The CMS collaboration used a likelihood discriminator method [18, 19], with 3 high-level jet variables as inputs.

Footnote 1: ATLAS uses a right-handed coordinate system with its origin at the nominal interaction point (IP) in the centre of the detector and the \(z\)-axis along the beam pipe. The \(x\)-axis points from the IP to the centre of the LHC ring, and the \(y\)-axis points upwards. Cylindrical coordinates \((r,\phi)\) are used in the transverse plane, \(\phi\) being the azimuthal angle around the \(z\)-axis. The pseudorapidity is defined in terms of the polar angle \(\theta\) as \(\eta=-\ln\tan(\theta/2)\). Angular distance is measured in units of \(\Delta R\equiv\sqrt{(\Delta\eta)^{2}+(\Delta\phi)^{2}}\).

The present study approaches the problem of \(q/g\) tagging using modern Deep Learning (DL) techniques based directly on jet constituents. The jet constituents considered are Particle Flow Objects (PFOs) reconstructed with Particle Flow algorithm [10, 20]. The Energy Flow Network (EFN) and Particle Flow Network (PFN) [21] are architectures developed for jet tagging that use the jet constituents as inputs. Ref. [22] paper introduces a novel architecture, called a Transformer. Recently, it was explored by the particle-physics community as a constituent-based jet tagger [23]. The present study extends the work and applies it specifically to the problem of \(q/g\) tagging. For comparison to previous techniques, an architecture based on expert jet features is also considered such as the Fully Connected (FC) Network [24].

This document is organized as follows. Section 2 describes the generation of the MC samples used in this study. Section 3 describes the reconstruction of jets. The tagger architectures and their training are described in Section 5. The performance of the taggers is evaluated in Section 6. Finally, Section 7 concludes the study.

## 2 Simulated Data

MC simulations are used to model Standard Model multijet production at a center of mass energy of 13 TeV. Pythia 8.235 [25] is used as the nominal MC generator for this analysis. Samples of \(2\to 2\) dijet events were simulated using the A14 tune [26], the Lund string hadronisation model and the NNPDF2.3lo[27] leading-order (LO) parton distribution function (PDF) set. The Pythia parton shower (PS) algorithm uses a dipole-style \(p_{\mathrm{T}}\)-ordered evolution, and its renormalisation and factorisation scales were set to the geometric mean of the squared transverse masses of the outgoing particles. EvtGen 1.6.0 [28] was used to model decays of heavy-flavour hadrons. This Pythia 8 (abbreviated as 'Py') sample is referred to as the nominal sample and the other samples described below as alternative samples. The nominal sample is used both for the training of \(q/g\) taggers and the evaluation of their performance, while the alternative samples are used to check how the performance varies between the different MC generators.

Two sets of multijet events were generated using the Sherpa 2.2.11 [29] generator. The matrix element calculation was included for the \(2\to 2\) process at LO. For one sample (referred to as 'Sherpa Cluster' or 'Sh Cl.') the default Sherpa parton shower [30] based on Catani-Seymour dipole factorisation was used for the showering with \(p_{\mathrm{T}}\) ordering, using the CT14nnlo PDF set [31]. For the other sample (referred to as 'Sherpa Dire' or 'Sh Dire') the Dire [32] PS model was used. Both samples used the dedicated Sherpa AHADIC model for hadronisation [33], based on cluster fragmentation ideas.

One set of Sherpa 2.2.5 [29] multijet events (referred to as 'Sherpa String' or 'Sh St.') was generated, using a similar settings to the Sherpa Cluster sample. However, it used the Sherpa interface to the Lund string fragmentation model of Pythia 6[34] and its decay tables.

Two sets of Herwig 7.1.6 [35; 36; 37] multijet events were generated with the NNPDF2.3lo PDF set, default cluster hadronisation model and either the default angle-ordered PS (referred to as 'Herwig Angular' or 'Hw Ang.') or alternative dipole PS (referred to as 'Herwig Dipole' or 'Hw Dip.'). These samples model \(2\to 2\) matrix elements with LO accuracy.

Two additional samples of dijet events with NLO matrix element accuracy were produced with Powheg Box v2 [38; 39; 40; 41], matched to either the Pythia 8 or angle-ordered Herwig 7 parton showers configured as for the corresponding samples described above. For the Pythia PS, the default Lund string hadronisation model was used with the NNPDF2.3lo PDF set and A14 tune. For the Herwig sample, the NNPDF3.0nlo PDF set was used along with the default Herwig cluster-based hadronisation model. These samples are referred to as the 'Powheg+Pythia' (abbreviated as 'PhPy') and 'Powheg+Herwig' (abbreviated as 'PhHw') samples.

Simulated events were passed through the ATLAS detector simulation [42] based on Geant4 [43]. The effect of multiple \(pp\) interactions in the same and neighboring bunch crossings (pileup) was modeled by overlaying the hard-scatter event with minimum-bias \(pp\) collisions generated by Pythia 8 with the A3 tune [44] and NNPDF2.3lo PDF set. Additional details of the MC samples used in this measurement may be found in Ref. [45].

## 3 Jet Reconstruction and Selection

Jets are reconstructed with the anti-\(k_{t}\) algorithm [46] with \(R=0.4\) as implemented in FastJet [47] using PFOs as constituents [20]. PFOs are a collection of topo-clusters [48] formed from energy deposits in calorimeter cells and an algorithmic combination of charged-particle tracks with those topo-clusters. An overall jet energy calibration [10] accounts for residual detector effects as well as contributions from multiple proton-proton collisions in the same bunch crossing (pileup) to make the reconstructed jet energy correspond to an unbiased measurement of the particle-level jet energy.

In each event, a second set of "truth-jets" is built directly from the generated events without detector simulation. Truth jets are composed of stable final-state particles, excluding muons, neutrinos, and particles from pile-up interactions. The anti-\(k_{t}\)\(R=0.4\) algorithm is used. The truth information about the parton that initiated the jet is retrieved [11] from the hardest final-state parton ghost-associated [49] to the truth jet. Truth jets are geometrically matched to the reconstructed ones using the angular distance \(\Delta R\) with the requirement \(\Delta R<0.3\). The complex phenomena of radiative showering and the inherently non-perturbative hadronization that characterize the development of jets from partons make it unphysical to establish a definitive distinction between "quark" and "gluon" jets at the hadron level [50; 51]. Despite this ambiguity, studies would benefit from this definition of quark- and gluon-initiated jets (see Section 1). An important observation was made in Ref. [52] - that the quark- and gluon-initiated jets, given a certain definition of the origin label, have similar properties in different processes of their production.

Reconstructed events are further processed beforehand to make the training of the neural networks more efficient. Firstly, pileup jets are removed. All reconstructed jets are required to have \(p_{\mathrm{T}}\) smaller than 2.5 \(\mathrm{\,Te\kern-1.0ptV}\) and larger than 200 \(\mathrm{\,Ge\kern-1.0ptV}\). Jets are also required to have \(|\eta|<2.1\), ensuring that they are completely within the acceptance of the inner detector.

It is preferred that the tagger is not reliant on the \(p_{\mathrm{T}}\) of the jet for the decision, allowing physical analyses using either high pT or low pT jets to benefit from the tagger. To achieve this, the \(p_{\mathrm{T}}\) spectrum of quark- and gluon-initiated jets are separately reweighted to be flat in the range of interest. This is only done with the training dataset. The sum of weights for quark- and gluon-initiated jets is equal to the number of jets in the dataset to preserve the size of the loss function during training. After this preprocessing, the training dataset contains 10M jets. Jet spectra of the training dataset without weights and with flattening weights applied are shown in Fig. 1 and Fig. 1, respectively.

Parallel to the training dataset, a validation dataset with the same preprocessing steps, but only 1M jets, is created. The validation dataset is used to monitor the training process and to prevent overfitting.

For the test dataset, the original MC predicted \(p_{\mathrm{T}}\) spectrum is used, as shown in Fig. 1. The test dataset is used to evaluate the performance of the taggers after the training. The number of jets in the test dataset is 1M.

The fraction of quark jets within the datasets primarily comprises those originating from light quarks (\(u\), \(d\), \(s\)), while the representation of \(c\) and \(b\) quark jets remains minimal, constituting less than 5% in all datasets.

## 4 Input Variables Definition

The input variables are created from the variables contained in the reconstructed events. There are four types of variables: constituent variables, constituent interaction variables, high-level jet variables, and linear constituent variables. They are described in the following sections.

### Constituent Variables

Jet constituents are PFOs, characterized by their four-momenta and reconstructed by the Particle Flow algorithm [20]. The complete list of constituent variables is shown in Table 1[23].

QCD radiation is expected to be approximately rotationally invariant, so pseudorapidity (\(\eta\)), azimuthal angle (\(\phi\)), and distance parameter (\(R\)) are only used with respect to the jet axis. Other variables include the \(p_{\mathrm{T}}\) and energy, \(E\), of the constituent. To improve the performance of the network, the logarithms of

Figure 1: Distribution of jet \(p_{\mathrm{T}}\) (a) in the training dataset before weighting, (b) in the training dataset after using flattening weights, and (c) in the test dataset after using physical weights.

\(p_{\mathrm{T}}\) and \(E\) are used to make the inputs all within the same magnitude. To emphasize the importance of a given constituent in a jet, the logarithm of the ratios of \(p_{\mathrm{T}}\) (\(E\)) of a constituent to a jet \(p_{\mathrm{T}}^{\mathrm{jet}}\) (\(E^{\mathrm{jet}}\)) are used. Additionally to the variables used in Ref. [23], the mass \(m\) of the constituent is used. Charged PFOs are assigned the charged pion mass during reconstruction; similarly, topo-clusters unmodified by the Particle Flow algorithm are assigned zero mass. The mass of topo-clusters modified by the algorithm can take other values. The mass variable considerably increases the performance of constituent-based networks not using the Constituent Interaction Variables described in Section 4.2.

### Constituent Interaction Variables

The complete list of constituent interaction variables is shown in Table 2[23]. They are calculated for each pair of constituents in a given jet, forming a matrix of size \(N_{\mathrm{const}}\times N_{\mathrm{const}}\), where \(N_{\mathrm{const}}\) is the number of constituents in a jet. The matrix is symmetric, and the diagonal is filled with zeros.

The variable \(\Delta\) stands for the angular distance between two jet constituents. The invariant mass of the two constituents \(m^{2}\) is calculated from their four-momenta. The \(k_{\mathrm{T}}\) variable is the transverse momentum of the softer constituent with respect to the harder constituent. The last variable, \(z\), is the fraction of \(p_{\mathrm{T}}\) carried by the softer constituent.

The average value of the interaction variables computed over the training dataset (with flat \(p_{\mathrm{T}}\) spectrum) for a matrix of dimensions \(N_{\mathrm{const}}\times N_{\mathrm{const}}\), focusing on the 50 hardest constituents within a jet, are shown in Figs. 2 and 3. Constituents are arranged in descending order of \(p_{\mathrm{T}}\), where the hardest constituent is

\begin{table}
\begin{tabular}{l} \hline \hline Constituent Variables \\ \hline \(\Delta\eta=\eta-\eta^{\mathrm{jet}}\) \\ \(\Delta\phi=\phi-\phi^{\mathrm{jet}}\) \\ \(\Delta R=\sqrt{\Delta\eta^{2}+\Delta\phi^{2}}\) \\ \(\log p_{\mathrm{T}}\) \\ \(\log E\) \\ \(\log\frac{p_{\mathrm{T}}}{p_{\mathrm{T}}^{\mathrm{jet}}}\) \\ \(\log\frac{E}{E^{\mathrm{jet}}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Constituent variables used as an input to PFN, ParT, and DeParT.

\begin{table}
\begin{tabular}{l} \hline \hline Constituent Interaction Variables \\ \hline \(\log\Delta^{ab}=\log\sqrt{(\eta^{a}-\eta^{b})^{2}+(\phi^{a}-\phi^{b})^{2}}\) \\ \(\log k_{\mathrm{T}}^{ab}=\log\left(\min\left(p_{\mathrm{T}}^{a},p_{\mathrm{T}}^ {b}\right)\Delta^{ab}\right)\) \\ \(z^{ab}=\min\left(p_{\mathrm{T}}^{a},p_{\mathrm{T}}^{b}\right)/(p_{\mathrm{T}}^ {a}+p_{\mathrm{T}}^{b})\) \\ \(\log m^{2,ab}=\log\left(p^{\mu,a}+p^{\mu,b}\right)^{2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: List of interaction variables between constituents used as an input to ParT, and DeParT. Indices \(a\) and \(b\) indicate the two constituents for which the interaction variables are calculated. The index \(\mu\) denotes the four-momentum component.

assigned the index 0. If a jet has less than 50 constituents, the remaining entries in the matrix are not filled.

There are measurable differences between quark- and gluon-initiated jets, encouraging the usage of the jet constituents in the \(q/g\) tagging. Differences between average values of the interaction variables between quark- and gluon-initiated jets are displayed in Figs. 2, 2, 3 and 3. The variables \(\log\Delta\) and \(\log k_{\mathrm{T}}\) are bigger for gluon-initiated jets than for quark-initiated jets between constituents with similar \(p_{\mathrm{T}}\), i.e. along the diagonal as can be seen in Figs. 2 and 2, respectively. This is not unexpected, as gluon-initiated jets are known to have higher jet width [15]. The variable \(z\) is bigger for gluon-initiated jets than for quark-initiated jets for all constituent pairs except for the few highest-\(p_{\mathrm{T}}\) ones. This hints at a more even splitting of the gluon-initiated jet \(p_{\mathrm{T}}\) between its lower-\(p_{\mathrm{T}}\) constituents. Concerning the mass variable, \(\log m^{2}\), the primary discrepancy is noticeable in the region between the highest-\(p_{\mathrm{T}}\) constituents, i.e. the bottom left corner in Fig. 3. In the region of pairs where at least one constituent has lower \(p_{\mathrm{T}}\), the pair masses are higher in quark-initiated jets.

Figure 2: A heatmap representation of the interaction variable matrix for the average value of \(\log\Delta\) for (a) quark-initiated jets and (b) gluon-initiated jets. The average value of \(\log k_{\mathrm{T}}\) for (d) quark-initiated jets and (e) gluon-initiated jets. Differences between average values of the interaction variables (c) \(\log\Delta\) and (f) \(\log k_{\mathrm{T}}\) between quark- and gluon-initiated jets. Before applying the logarithmic transformation to the variable \(k_{\mathrm{T}}\), it is originally expressed in units of \(\mathrm{MeV}\). Constituents are ordered based on their \(p_{\mathrm{T}}\), with the constituent possessing the highest \(p_{\mathrm{T}}\) assigned the index 0.

### Linear Constituent Variables

IRC safety is a property of a jet algorithm that ensures that the jet clustering is stable under small perturbations of the input pattern. A subset of the constituent variables is formed that may be used in IRC safe taggers. These variables are referred to as linear constituent variables and their list is shown in Table 3. Linear constituent variables are linear in the fraction of constituents' \(p_{\mathrm{T}}\) with respect to the jet \(p_{\mathrm{T}}\), do not contain any information about the energy of the constituent, and angular variables are defined with respect to the jet axis. The linear constituent variables shown in Table 3 can be used as an input to a IRC safe

\begin{table}
\begin{tabular}{l} \hline \hline Linear Constituent Variables \\ \hline \(\Delta\eta=\eta-\eta^{\mathrm{jet}}\) \\ \(\Delta\phi=\phi-\phi^{\mathrm{jet}}\) \\ \(\Delta R=\sqrt{\Delta\eta^{2}+\Delta\phi^{2}}\) \\ \(\frac{p_{\mathrm{T}}}{p_{\mathrm{T}}^{\mathrm{jet}}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: The linear constituent variables used as an input to IRC safe EFN.

Figure 3: A heatmap representation of the interaction variable matrix for the average value of \(z\) for (a) quark-initiated jets and (b) gluon-initiated jets. The average value of \(\log m^{2}\) for (d) quark-initiated jets and (e) gluon-initiated jets. Differences between average values of the interaction variables (c) \(z\) and (f) \(\log m^{2}\) between quark- and gluon-initiated jets. Before applying the logarithmic transformation to the variable \(m^{2}\), it is originally expressed in units of \(\mathrm{MeV}^{2}\). Constituents are ordered based on their \(p_{\mathrm{T}}\), with the constituent possessing the highest \(p_{\mathrm{T}}\) assigned the index 0.

network.

### High-level Jet Variables

High-level jet variables are calculated from the jet constituents and describe the jet as a whole. They are used as input to the FC and Highway networks. There are 10 of these variables, forming a 10-dimensional vector as the input to the neural network. They are:

* Calibrated jet four-momentum.
* Number of PFOs.
* Numbers of charged PFOs passing two different pT thresholds (500 and 1000 MeV).
* Jet width [15] calculated using PFOs with \(p_{\mathrm{T}}>1000\) MeV.
* Fraction of the jet energy deposited in the EM calorimeter. Fraction of the jet energy carried by charged particles.

In addition to these variables, a second set of 5 high-level jet variables is employed. These variables correspond to the ones utilized in the prior ATLAS \(q/g\) tagger, as detailed in the reference [15]. Concretely, these encompass jet \(p_{\mathrm{T}}\) and \(\eta\) and the count of PFOs within a jet. The last two variables are the jet width, and the two-point correlation function [13; 16], both calculated using the PFOs.

## 5 Deep Learning Models

Five different models are trained and evaluated in the present study. The general training configuration is the same for all models. Each model is trained for 10 epochs with a batch size of 512. The LAMB optimizer [53] is employed with an initial learning rate of 0.001, cosine-decay to zero over the 10 epochs. A linear warmup is applied for the first 100 steps of the training. The standard binary cross-entropy loss function [54] is utilized without label smoothing. All models have approximately 2.6M parameters, to uniformly fix the total learning capacity of the models. The number of parameters is based on the number of training data samples available to prevent overfitting and related work reporting improvements of transformer models with more parameters [55; 56]. For all models during training, we regularly assessed accuracy by classifying jets accurately using a 0.5 threshold on the model's output. These evaluations were conducted on the validation dataset after each epoch to prevent overfitting and training divergence. The number of epochs equal to 10 was observed to be sufficient. Higher numbers did not result in a significantly higher performance but they made the networks more fragile to overfitting.

A maximum of 100 constituents are considered by all constituent-based taggers. Jets with more than 100 constituents are truncated to 100 constituents with the highest \(p_{\mathrm{T}}\). This limitation is due to the GPU memory constraints and high memory consumption of the Transformers, whose memory usage is proportional to the square of the number of constituents. The constraint of 100 constituents has an impact on a very small fraction of jets, about 0.08% of jets in the training dataset.

The models are implemented and trained in TensorFlow framework [57] and Keras API [58]. There was no extensive hyperparameter tuning, they were chosen based on related works [23; 55; 56; 59], and matching of the total number of parameters. Some conclusions of the results presented later may depend on the chosen hyperparameters.

### Fully Connected Network

The FC Network [24] is the most straightforward neural network, where all the neurons in one layer are connected to all the neurons in the previous and next layers. The input vector of fixed length contains high-level jet variables (see Section 4.4) and the output vector is just a single number.

Two versions of the FC Network are trained in the present study. One version uses the set of 10 high-level jet variables (see Section 4.4) as input and it is denoted as FC. The other version uses the set of 5 high-level jet variables (see Section 4.4) as input and it is denoted as FC reduced or FC red. Both versions have 11 hidden layers with 512 hidden units each.

### Particle Flow and Energy Flow Network

The PFN and EFN [21] are two networks that are designed to utilize the information about the constituents. They use the theory of Deep Sets [60] to preserve the permutation invariance of the input constituents and construct a jet-level representation from the constituent-level representations. The general idea of these networks is to map each constituent to a feature vector, sum the feature vectors of all constituents, and then map the sum to the output. The difference between EFN and PFN is that EFN is specifically designed to be IRC safe. The mathematical structure of the EFN limits it to consider information which is linear in constituent \(p_{\mathrm{T}}\), ensuring IRC safety [61]. The unmapped constituents \(p_{\mathrm{T}}\) directly multiply the mapped values of the angular variables in the EFN. The advantage of PFN and EFN over the high-level feature networks is they can fully employ the lower-level features of the constituents as an input.

PFN uses the constituent variables as input (see Section 4.1), whereas EFN uses the linear constituent variables as input (see Section 4.3). PFN has 5 layers of sizes 512, while EFN has 4 layers of sizes 512 and one of 136 to embed the individual constituents. Both models use 6 layers of sizes 512 for the mapping of the sum of the constituents.

### ParticleNet

ParticleNet [62] (abbreviated as P.Net) is a graph neural network that represents a jet as a graph composed of nodes and edges. Each constituent in a jet is associated with a node, where all 7 pre-processed constituent-level features defined in Section 4.1 are taken as features of the node. Each node is connected by an edge to its \(k\) nearest neighbors in the \(\Delta\eta-\Delta\phi\) plane of the constituents, where \(k\) is a network hyper-parameter that is set to 16 in the present study. ParticleNet further applies an EdgeConv operation [63] to this graph. The EdgeConv operation is similar to the two dimensional convolution used in convolution neural networks, but it is defined on graphs instead of images. It can be stacked to enable ParticleNet to extract both local and global features hierarchically. Similarly to the EFN and PFN, ParticleNet naturally handles the variable lengths of jet constituents and enforces permutation invariance. However, the EdgeConv operation acts on the feature vectors of pairs of constituents that are spatially close to each other, rather than each constituent separately. This allows ParticleNet to exploit the local relationships between constituents.

ParticleNet uses the constituent variables (see Section 4.1) as input. They are passed through 5 EdgeConv layers, the first 2 of them with 128 filters, the next 2 with 256 filters, and the last one with 512 filters. Afterward, the output is passed through 2 fully connected layers with 768 hidden nodes each.

### Particle Transformer

Particle Transformer (ParT) [23] is a Transformer [22] variant with particle physics in mind. It is based on the CaiT [56] architecture with the additional inclusion of the interaction variables. Interaction variables are given for each pair of input vectors (for each pair of constituents in our case) and used as a second input to each Self-Attention Block, as shown in [23]. The Transformer architecture provides a way for constituents to communicate and exchange valuable information, creating a complete representation of the jet (input data). Despite its complexity, the Transformer provides a potential advantage over the ParticleNet, PFN, and EFN, which is the ability to learn long-range dependencies in the input data. Unlike the ParticleNet, the Transformer does not rely on the spatial proximity of the constituents, but rather on the importance of the constituents.

Utilizing the interaction variables steers the model in creating representations of the input data more suitable for jet tagging. The extension of the ParT with the interaction variables compared to one without them is reported to be significant in Ref. [23].

Thus, ParT with both the constituent variables (see Section 4.1) and the constituent interaction variables (see Section 4.2) as inputs is trained in the present study. ParT embeds the constituent variables into 128 dimensions, which are then passed through the 13 attention blocks with an expansion factor of 4, the last two of them being the Class Attention Blocks. The integration constituent variables are embedded into 8 dimensions, matching the number of heads in the multihead attention layers.

### Dynamically Enhanced Particle Transformer

The Dynamically Enhanced Particle Transformer (DeParT) presented in this study is a novel expansion upon the ParT architecture, utilizing the Talking Multihead Attention [55] mechanism. Other alterations include the use of Stochastic Depth [64], Layer Scale [56], and Gated Feed Forward Network [65], all visualized together in Fig. 4. The overall structure of the model is the same as in Ref. [23], but the Particle Attention Blocks are replaced with Talking Particle Attention Blocks and Class Attention Blocks with Talking Class Attention Blocks.

The Talking Multihead Self-Attention, visualized in Fig. 4, utilizes the interaction variables by allowing the heads to exchange information about a given feature each head possesses. The interaction variables force each head to focus on a different aspect of pair information of two jet constituents. Adding the Talking Heads mechanism allows more efficient communication between the heads sharing multiple aspects of pair-wise jet constituent features. In addition, the Layer Scale and Stochastic Depth allow for more efficient training of larger models without overtraining and diverging during training.

The Talking Multihead Class Attention represents an extension of the Multihead Class Attention presented in Ref. [23], facilitating inter-communication amongst the various attention heads with the same mechanism as in the Talking Multihead Self-Attention. While not as important as in the Talking Multihead Self-Attention, this augmentation remains an enhancement to the model, underscoring the uniform integration of the talking heads mechanism across all multihead attentions. Additionally, the Gated Feed Forward Network,Layer Scale, and Stochastic Depth are applied to the Class Attention Blocks at the same positions as in the Particle Attention Blocks, as shown in Fig. 4.

DeParT uses the same inputs as ParT: the constituent variables (see Section 4.1) and the constituent interaction variables (see Section 4.2). The general hyperparameters of DeParT are the same as in ParT. The constituent variables are embedded into 128 dimensions, which are then passed through the 13 attention blocks with an expansion factor of 4, the last two of them being the Talking Class Attention Blocks. The integration constituent variables are embedded into 8 dimensions, matching the number of heads in the multihead attention layers.

## 6 Tagger Evaluation

The taggers are evaluated on the test dataset with the physical \(p_{\mathrm{T}}\) spectrum. Several performance metrics of the model are shown in Table 4. AUC is the area under the Receiver Operating Characteristic curve. The efficiency of the quark-initiated (gluon-initiated) jet tagging, denoted by \(\varepsilon_{q}\) (\(\varepsilon_{g}\)), is the fraction of correctly classified quark-initiated (gluon-initiated) jets with respect to the total number of quark-initiated (gluon-initiated) jets in the used sample. The symbol \(\varepsilon_{g}^{-1}\) stands for the rejection of gluon-initiated jets, inverse of \(\varepsilon_{g}\). It is evaluated at a working point which yields a predefined fixed \(\varepsilon_{q}\) of 0.5 for the testing

Figure 4: The architecture of the Talking Particle Attention Block of DeParT. The U represents the interaction variables.

sample, disregarding any dependencies of \(\varepsilon_{q}\) on \(p_{\mathrm{T}}\) or other variables. The number of trainable parameters, inference time of one batch of 512 jets and the maximum GPU memory usage during inference are also shown in Table 4. Note that the number of parameters of each model is the same (2.6M) to compare the actual architectures at a fixed capacity. To check robustness of the results, the Transformer-based architectures were retrained several times, using different random initialisation of models' parameters. The AUC metric variations were of the order of 0.0001, not changing the order shown in Table 4.

Figure 5 displays \(\varepsilon_{g}^{-1}\) as a function of \(\varepsilon_{q}\) for each trained model.

The gluon-initiated jet rejection as a function of the jet \(p_{\mathrm{T}}\) is shown in Fig. 6 for studied taggers. Two working points were chosen, with \(\varepsilon_{q}\) of 0.5 and 0.8, respectively. Here, the \(\varepsilon_{q}\) is required to be the same

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Model & AUC & \(\varepsilon_{g}^{-1}\) & \(\varepsilon_{q}=0.5\) & \# Params [10\({}^{6}\)] & Inference Time [ms] & GPU Memory [MB] \\ \hline DeParT & 0.8489 & 15.4242 & 2.62 & 266.51 & 1684 \\ ParT & 0.8479 & 15.2457 & 2.62 & 233.84 & 1730 \\ ParticleNet & 0.8476 & 15.4402 & 2.59 & 768.74 & 5410 \\ PFN & 0.8406 & 14.2387 & 2.64 & 136.93 & 393 \\ FC & 0.8280 & 13.5199 & 2.63 & 65.53 & 76 \\ FC reduced & 0.8038 & 10.3639 & 2.63 & 84.84 & 47 \\ EFN & 0.7761 & 7.7222 & 2.60 & 101.53 & 337 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results and technical information of the different taggers. The total number of parameters is given in millions. The time and memory are measured on two NVIDIA Tesla V100 GPU 32GB. The inference time corresponds to one batch of size 512. Memory is the maximum GPU memory usage during inference of batch with size 512.

for each jet \(p_{\mathrm{T}}\) bin.

DeParT or ParT provide the highest \(\varepsilon_{g}^{-1}\) in most jet \(p_{\mathrm{T}}\) bins. Their order changes between the different jet \(p_{\mathrm{T}}\) bins. ParticleNet has the third highest \(\varepsilon_{g}^{-1}\) in almost all jet \(p_{\mathrm{T}}\) bins. The performance of PFN is between that of ParticleNet and FC in all jet \(p_{\mathrm{T}}\) bins. Both variants of the FC network provide lower \(\varepsilon_{g}^{-1}\) in all jet \(p_{\mathrm{T}}\) bins than the constituent-based taggers, excluding EFN. The reduced FC provides lower \(\varepsilon_{g}^{-1}\) than the FC as a result of the reduced number of input variables. EFN provides \(\varepsilon_{g}^{-1}\) that is lower than that of the other models.

The low performance of EFN is caused by its IRC safety requirements, which limit the information available to the classifier. The fraction of constituents' \(p_{\mathrm{T}}\) dominates over the angular variables in the sum over constituents making them less effective. Furthermore, the \(p_{\mathrm{T}}\) fraction acts as a weight for the angular variables in the sum, emphasizing only the most energetic constituents due to its long-tailed distribution. Taking the logarithm of the \(p_{\mathrm{T}}\) fraction would remove the problem with its long-tailed distribution, but break the IRC safety requirements.

It is interesting to compare \(\varepsilon_{g}^{-1}\) at \(\varepsilon_{q}=0.5\) of the previous \(q/g\) tagger by ATLAS [15], BDT-based, with that of the presented models. In the \(500-600\) GeV jet \(p_{\mathrm{T}}\) bin, it is about 13 for the previous \(q/g\) tagger. The reduced FC tagger provides the same value of \(\varepsilon_{g}^{-1}\) in a similar jet \(p_{\mathrm{T}}\) bin, \(400-600\) GeV. Except for EFN, all presented models have \(\varepsilon_{g}^{-1}\) higher in the \(400-600\) GeV \(p_{\mathrm{T}}\) bin. They vary between 16 and 20. For comparison with other networks, the reduced FC tagger could be used as a proxy for the previous ATLAS \(q/g\) tagger. Across the whole jet \(p_{\mathrm{T}}\) range, all presented taggers show higher \(\varepsilon_{g}^{-1}\) than the reduced FC tagger. The only exception is the EFN whose \(\varepsilon_{g}^{-1}\) is lower.

The variances in performance between the taggers in this analysis exhibit a lesser magnitude in comparison to analogous studies as referenced in [66; 67]. This can be attributed to the different physical structures of the processes examined, alongside the constrained total parameter count within the taggers in this study. Notably, the difference between the performance of ParT and ParticleNet in this investigation is less pronounced than in Ref. [23], wherein the total parameter count was not constrained, thus further substantiating this assertion.

Figure 6: The rejection of gluon-initiated jets, \(\varepsilon_{g}^{-1}\), as a function of the jet \(p_{\mathrm{T}}\) of studied taggers for (a) \(\varepsilon_{q}=0.5\) and (b) \(\varepsilon_{q}=0.8\) working points. The rectangle around each point displays the statistical uncertainty.

[MISSING_PAGE_EMPTY:15]

Figure 8: Comparison of \(\varepsilon_{g}^{-1}\) in different MC samples of selected taggers: (a) DeParT, (b) ParticleNet, (c) EFN, (d) FC. The 80% \(\varepsilon_{q}\) working point is used.

The EFN tagger is the least sensitive to the MC generator variations. The reason could be its IRC-safe construction, which reduces sensitivity to non-perturbative effects. However, it provides lower \(\varepsilon_{g}^{-1}\) than the other taggers. The DeParT has higher \(\varepsilon_{g}^{-1}\) than the other taggers in the majority of the \(p_{\mathrm{T}}\) bins across the whole spectrum of alternative MC samples. The Powheg+Pythia sample is the most similar to the nominal sample, the taggers even outperform the nominal sample in some \(p_{\mathrm{T}}\) bins.

Figure 9 displays the maximum distance of unity and the ratio of \(\varepsilon_{g}^{-1}\) evaluated on alternative MC samples and on the nominal MC sample. It is presented in bins of jet \(p_{\mathrm{T}}\), for the two \(\varepsilon_{q}\) working points and for each of the studied taggers. This quantity gives some indication of each tagger's overall model dependence.

## 7 Conclusion

The performance of constituent-based jet taggers and high-level jet taggers for quark- and gluon-initiated jets in realistic simulated collisions is presented. MC simulations of proton-proton collisions at center of mass energy of 13 TeV are used, taking into account the ATLAS response to the individual particles. Transformer-based taggers trained in this study (ParT, DeParT) provide slightly higher \(\varepsilon_{g}^{-1}\), at a fixed \(\varepsilon_{q}\), than the taggers using high-level jet variables (FC networks) and than simpler constituent-based taggers (EFN, PFN). The Transformer-based taggers benefit from the use of the attention mechanism exploiting the correlations between the jet constituents. ParticleNet closely trails Transformer-based taggers due to its capacity to learn correlations between constituents, but only among the 16 nearest ones, ignoring the long-range dependencies. EFN provides \(\varepsilon_{g}^{-1}\) that is lower than that of other models in all jet \(p_{\mathrm{T}}\) bins. The dependence of tagger performance on the different MC samples was presented. The differences in \(\varepsilon_{g}^{-1}\) between the presented \(q/g\) taggers, excluding EFN, are less pronounced than the \(\varepsilon_{g}^{-1}\) variations of a given tagger, evaluated using alternative MC samples. The MC generator dependence of each tagger performance is found to increase with the \(\varepsilon_{g}^{-1}\) provided by the classifier. EFN is the least sensitive to the MC generator variations. Because the hyperparameters were not optimized in this study, certain findings in the results may rely on the specific hyperparameter choices made.

Figure 9: Maximum distance of unity and the ratio of \(\varepsilon_{g}^{-1}\) evaluated on alternative MC samples and on the nominal MC sample. It is presented in bins of jet \(p_{\mathrm{T}}\), for the two \(\varepsilon_{q}\) working points, (a) \(\varepsilon_{q}=0.5\) and (b) \(\varepsilon_{q}=0.8\), and for each of the studied taggers.

## References

* [1] L. Evans and P. Bryant, _LHC Machine_, JINST **3** (2008) S08001 (cit. on p. 2).
* [2] A. M. Sirunyan et al., _Electroweak production of two jets in association with a Z boson in proton-proton collisions at \(\sqrt{s}=\) 13 TeV_, Eur. Phys. J. C **78** (2018) 589, arXiv: 1712.09814 [hep-ex] (cit. on p. 2).
* [3] ATLAS Collaboration, _Search for the Standard Model Higgs boson produced by vector-boson fusion and decaying to bottom quarks in \(\sqrt{s}=8\,\text{TeV}\,pp\) collisions with the ATLAS detector_, JHEP **11** (2016) 112, arXiv: 1606.02181 [hep-ex] (cit. on p. 2).
* [4] ATLAS Collaboration, _Search for dijet resonances in events with an isolated charged lepton using \(\sqrt{s}=13\,\text{TeV}\) proton-proton collision data collected by the ATLAS detector_, JHEP **06** (2020) 151, arXiv: 2002.11325 [hep-ex] (cit. on p. 2).
* [5] CMS Collaboration, _Search for high mass dijet resonances with a new background prediction method in proton-proton collisions at \(\sqrt{s}=13\,\text{TeV}\)_, JHEP **05** (2020) 033, arXiv: 1911.03947 [hep-ex] (cit. on p. 2).
* [6] CMS Collaboration, _Search for new physics in multijet events with at least one photon and large missing transverse momentum in proton-proton collisions at \(13\,\text{TeV}\)_, (2023), arXiv: 2307.16216 [hep-ex] (cit. on p. 2).
* [7] ATLAS Collaboration, _Search for R-parity-violating supersymmetric particles in multi-jet final states produced in \(pp\) collisions at \(\sqrt{s}=13\,\text{TeV}\) using the ATLAS detector at the LHC_, Phys. Lett. B **785** (2018) 136, arXiv: 1804.03568 [hep-ex] (cit. on p. 2).
* [8] ATLAS Collaboration, _Search for new resonances in mass distributions of jet pairs using \(139\,\text{fb}^{-1}\) of \(pp\) collisions at \(\sqrt{s}=13\,\text{TeV}\) with the ATLAS detector_, JHEP **03** (2020) 145, arXiv: 1910.08447 [hep-ex] (cit. on p. 2).
* [9] CMS Collaboration, _Search for top squarks in final states with two top quarks and several light-flavor jets in proton-proton collisions at \(\sqrt{s}=13\,\text{TeV}\)_, Phys. Rev. D **104** (2021) 032006, arXiv: 2102.06976 [hep-ex] (cit. on p. 2).
* [10] ATLAS Collaboration, _Jet energy scale and resolution measured in proton-proton collisions at \(\sqrt{s}=13\,\text{TeV}\) with the ATLAS detector_, Eur. Phys. J. C **81** (2020) 689, arXiv: 2007.02645 [hep-ex] (cit. on pp. 2, 4).
* [11] ATLAS Collaboration, _Light-quark and gluon jet discrimination in \(pp\) collisions at \(\sqrt{s}=7\,\text{TeV}\) with the ATLAS detector_, Eur. Phys. J. C **74** (2014) 3023, arXiv: 1405.6583 [hep-ex] (cit. on pp. 2, 4).
* [12] ATLAS Collaboration, _Quark versus Gluon Jet Tagging Using Charged-Particle Constituent Multiplicity with the ATLAS Detector_, ATL-PHYS-PUB-2017-009, 2017, url: [https://cds.cern.ch/record/2263679](https://cds.cern.ch/record/2263679) (cit. on p. 2).
* [13] A. J. Larkoski, G. P. Salam and J. Thaler, _Energy Correlation Functions for Jet Substructure_, JHEP **06** (2013) 108, arXiv: 1305.0007 [hep-ph] (cit. on pp. 2, 9).
* [14] Y. Coadou, _Boosted decision trees_, (2022), arXiv: 2206.09645 [physics.data-an] (cit. on p. 2).
* [15] ATLAS Collaboration, _Performance and calibration of quark/gluon-jet taggers using 140 fb\({}^{-1}\) of \(pp\) collisions at \(\sqrt{s}=13\,\text{TeV}\) with the ATLAS detector_, (2023), arXiv: 2308.00716 [hep-ex] (cit. on pp. 2, 7, 9, 14).