June 19th 1998

DAQ Note 109

**Overview of the**

**Sequential Single Farm**

**option for the**

**ATLAS High Level Trigger**

**(LVL2 Architecture C)**

Atlas Trigger-DAQ

The Demo C group

_Abstract_

This document recalls the basic principles of the sequential trigger strategy and describes a T/DAQ option based on this concept. Major results of the small full slice demonstrator are given as well as modelling and emulation studies for the extrapolation to a full system.

## Table of Contents

TABLE OF CONTENTS

1. OVERVIEW OF THIS DOCUMENT

2

1.1. SUMMARY

1.2. PARTICIPANTS

3

**2. THE ATLAS T/DAQ SYSTEM**

4

2.1. THE THREE LEVELS ARCHITECTURE

4

2.2. THE ATLAS TECHICAL PROPOSAL T/DAQ MODEL

5

**3. THE SQUENTIAL EVENT SELECTION CONCEPT.**

6

3.1. MOTIVATIONS

6

3.1.1. Number and multiplicity of RoI per events

6

3.1.2. Execution time of algorithms

7

3.1.3 Data transfer bandwidth

7

3.2 SEQUENTIAL PROCESSING

8

3.2.1. Basic principle

8

3.2.2. Sequential menus

9

3.2.3. Advantages

10

**4. SINGLE FARM ARCHITECTURE**

11

4.1. DATA FLOW

4.2. SOME PARTICULARITIES

12

4.3. HYBRID APPROACH

13

**5. DEMONSTRATOR "C" RESULTS**

14

5.1. DESCRIPTION OF THE DEMONSTRATOR

14

5.1.2. Hardware configuration

14

5.1.2. Structure of the Demonstrator Software

14

5.2. OPERATION AND PERFORMANCES

15

5.2.1. ATM Networking Technology Studies

15

5.2.2. Supervisor................

16

5.2.3. Source Module

16

5.2.4. Destination Processor

17

5.2.5. Monitor................

19

5.3. SYSTEM PERFORMANCE ANALYSIS

19

5.4. SUMMARY OF RESULTS

21

**6. PAPER MODEL OF MODEL OF MODELLING THE C ARCHITECTURE**

**7. EMULATION**

**7.1. MACRAME EMULATION**

**7.1.1 Input assumptions**

**7.1.2. Measurement and results**

**7.1.3. Conclusion**

**7.2. ATM EMULATION**

**8. CONCLUSIONS AND PLANS FOR THE FUTURE**

**8.1. CONCLUSIONS FROM THE DEMONSTRATOR C PROGRAM**

**8.2 PILOT PROJECT TESTBED**

**8.2.1. Network technologies issues**

**8.2.2. Components integration**

**8.3. SYSTEM SCALABILITY ISSUES AND COST ESTIMATE**

**8.4 TOWARD A " SINGLE NETWORK T/DAQ SCHEME"

**9. REFERENCES**

## 1 Overview of this document

### 1.1 Summary

The high rate of interaction in future LHC experiment places stringent demand on the trigger and data acquisition systems. The first level trigger for the ATLAS experiment based on special purpose processors designed to reduce the the trigger rate from the 40 MHz beam crossing rate to below 100 KHz. Higher levels trigger are required to reduce the data flow to aroud 100 MBytes/s (100 Hz with full data) for permanent storage. These higher level trigger will be implemented if possible, using general purpose processors and commercial switching networks. Several trigger/DAQ architectures are currently under study in an ATLAS LVL2 demonstrator program. The sequential processing scheme discussed in this document executes trigger algorithms in a number of steps designed to reject background events as soon as possible.

A Trigger/DAQ architecture based on a single processing farm and a single switching network has been developed as a natural implementation of the the sequential processing scheme. A supervisor assigns a single processor to each event accepted by the first level trigger and sends a list of Region of Interest (RoI) identified by the first level trigger. This processor request event data from the read out buffers as needed for the execution of the sequential sequence of selection algorithms. The data request messages and the requested data fragments are transported over the same switching network. When the decision is made to accept or reject an event, the processor inform the supervisor, which broadcast the decision to all the read out buffers.

System modelling, using sample trigger menus satisfying the ATLAS physics requirements for low and high luminosity operation are used to evaluate the resources necessary, estimate the system performance and begin the optimisation of the system design. Simulation with an ATM network show encouraging results. Nonetheless, preliminary studies indicate that system capability could be improved by the addition of FPGA coprocessors for the tracking systems.

Results are given of a small full slice demonstrator that validate the " prof of principle " of such concept. Data sources, processors destination and supervisor nodes are based on Pentium and PowerPC platforms running WindowNT and LynxOS operating systems. Two types of interconnection networks where envisaged ATM switching fabrics with 155 Mbit/sec ports and a switching fabric based on C104 packets router with 100 Mbit/s DS links. The latency and throughput of a large DS link testbed, with up to 1024 nodes are measured for the expected traffic pattern. The results obtained with present-day hardware and software will be extrapolated to estimate the performance of future systems.

This document intend to give a full overview of this option, specially in its concept in order to present the possible consequences for the overall T/DAQ architecture. More details for each specific topics are developped in dedicated reference documents.

[MISSING_PAGE_EMPTY:4]

## 2 The ATLAS T/DAQ system

This section outline a functional overview of the three levels event selection and DAQ system as described in the the ATLAS Technical Proposal [1].

### The three levels architecture

The Level 1 Trigger (LVL1) operates at the LHC beam-crossing rate of 40MHz. The front end read-out electronics is designed for a maximum of 100KHz. Present studies [2, 16] estimate this output to about 40-50KHz. For event accepted by the LVL1, data from all detector front end electronics are transferred to some 2000 Read Out Buffer (ROB). The LVL1 trigger defines "Region of Interests " (RoIs) to guide subsequent event selection.

The Level 2 (LVL2) has access to the full granularity and full precision data from all the subdetectors. In order to minimize the data transfer requirements, only data belonging to the "Regions of Interest " are transmitted to the LVL2 processors (about 1% of the front end raw data bandwidth). The RoIs are processed to extract "features" such as calorimeter cluster energy or track parameters. Individual particle identification requires combining features from different subdetectors. The LVL2 decision is issued after a topological analysis of the event. The output event rate resulting from the LVL2 trigger is estimated to be 1-1.5 KHz [2, 16].

The Event Filter (EF) uses full event data to perform and event analysis similar to that of the off-line reconstruction. EF provides a further reduction of factor 10. Events selected by the EF are recorded on permanent storage for subsequent off-line studies (Figure 1).

Figure 1: Event selection data flow

### 2.2 The ATLAS Technical Proposal T/DAQ model

The ATLAS Technical Proposal [1] describes the "Data Driven" and "Local Global" options for the implementation of the LVL2 trigger. Suggested "Push" data flow protocols implies that data are of all RoIs from all subdetectors are systematically sent to the feature extraction processors via dedicated local networks. Even though the RoI concept reduces the requirements on the data transmission and processing power, these requirements still remain high (see table1). The final LVL2 decision is issued by a processor from the "global farm", which communicates with the feature extractors by a "global network" A dedicated control network used to transfer the LVL2 decision to the Read Out Buffers and the EF system.

We have studied ways to avoid the complexity of such model, which is due to :

\(\bullet\) the parallelism in the processing of the RoIs

\(\bullet\) the separation of the level 2 and EF event selection

\(\bullet\) the presence of several different networks

\(\bullet\) the "Push" data flow protocol

We propose a different approach based on a sequential event selection strategy [5, 24].

## 3 The Sequential event selection concept.

### Motivations

#### 3.1.1 Number and multiplicity of RoI per events

Each ROI identified by the level 1 trigger is characterised by its type (muon, em, jet...), its spatial coordinates and information on energy and and isolation thresholds. Two categories of ROIs can be distinguished. The " Trigger RoI " are those contributed to the level 1 decision with LVL1 threshold cuts. The " non-trigger RoIs " have low thresholds give additional information on the global topology of the event. Figure 2 shows the distributions of the total number of RoIs and number of RoIs for dijets events (70% of the events accepted by the level 1 trigger.

If the total number of RoI is around five, there is only one "Trigger RoI" in 80% of the case [3]. A significant event rejection can be achieved by processing only these "Trigger RoI" as most of them are not confirmed with the higher granularity and improved precision of the LVL2 algorithms. The calorimeter and/or muon detector data alone can be used to sharpen the energy and momentum cuts and to refine the region of interest. In addtion, confirmed mu and em cluster RoIs can be matched to track in the inner dedetector to lower the event rate even further. The figure 2 shows the expected background rejection factors due to confirmation of of the EM RoI at low (10\({}^{33}\)cm\({}^{-2}\)s-1) and high (10\({}^{34}\)cm\({}^{-2}\)s-1) luminosity. Each rejection factor corresponds to the rate reduction obtained for 90% efficiency for isolated electrons at the nominal threshold energy [ID TDR].

\begin{table}
\begin{tabular}{c c c} \hline Luminosity & Low & High \\ \hline Nominal threshold (Gev/c) & 20 & 40 \\ Calorimeter algorithm alone & 3 & 10 \\ Calorimeter and Inner tracker algorithms & 25 & 60 \\ \hline \end{tabular}
\end{table}
Table 1: Background rejection factor at 90% efficency for EM RoI trigger

Figure 2: Number of RoI per events

#### Execution time of algorithms

As shown in the table 2, the execution time of tracking algorithms are considerably longer, about 1 msec per RoI, than those of the LVL2 calorimeter and muon algorithms [ ]. The majority of events can be rejected using the calorimeter and muon feature only Therefore, performing track finding algorithms only for confirmed ROIs allows a reduction of the data transfer bandwidth and the procssing power requirements.

#### Data transfer bandwidth

The expected average data size for all subdetectors is given in table 3. The table shows the aggregate bandwidth requirement for data transmission to the LVL2 and EF systems. This corresponds to LVL2 and EF inputs rates of 75 KHz and 1KHz respectively.

\begin{table}
\begin{tabular}{c c c} \hline \hline Trigger & System & Processing \\ \hline Muon trigger & MUON & 100 \(\mu\)sec \\ EM trigger & CALO & 100 \(\mu\)sec \\ Jet trigger & CALO & 100 \(\mu\)sec \\ Track & TRT & 600 \(\mu\)sec \\ Track & SCT-PIX & 800 \(\mu\)sec \\ TRT scan L=10**33 & TRT & 50 msec \\ TRT scan L=10**34 & TRT & 200 msec \\ Missing Et & CALO & 100 \(\mu\)sec \\ B jet tag & SCT-PIX & 10 msec \\ \hline \hline \end{tabular}
\end{table}
Table 2: Normalized feature extraction execution time

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline Partition & Channels & FE & FE data & FE & av.RoI & LVL2 \& Gbit/s & EF \\  & count & occ.\% & bit & Gbit/s & size(KB) & & Gbit/s \\ \hline Pixel & 1.40. 10\({}^{7}\) & 0.01 & 48 & 50 & 0.3 -1.0 & 0.12-1.2 & 3.6 \\ SCT & 9 314 304 & 1.0 & 32 & 224 & & & \\ \hline TRT & 424 576 & 20 & 32 & 204 & 0.3 - 1.0 & 0.24-0.6 (13)* & 2.7 \\ \hline EM calo & 173 952 & 100 & 32 & 418 & 2.0 & 0.8-5.8 & 6.4 \\ HAD calo & 25 714 & 100 & 32 & 62 & 0.9 & & \\ \hline MU TR & 789 704 & 0.12 & 32 & 2 & 0.1 & 0.16-0.35 & 1.4 \\ MU CH & 431 392 & 10 & 32 & 104 & 1.9 & & \\ \hline TR & 8000 & 100 & 16 & 10 & & - & 1.3 \\ \hline TOTAL & & & & 1063 & & 2-11 (24)* & 15.4 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Estimated rate and bandwidth for LVL2 and EFThe conclusion is that some further work has to be done in order to refine the LVL2 bandwidth, particularly the way the RoI mapping and extraction is done at each ROB level is critical. The lower limit corresponds to the theoretical LVL2 RoIs instead of the upper limit come from the paper model [15] taking in account the real mapping of RoIs in ROBs.. The LVL2 data bandwidth required for the "full TRT scan" is also presented (*), showing that specific algorithms double the global LVL2 bandwidth.

### 3.2 Sequential processing

Based on these conderations, a the sequential event selection strategy for the ATLAS Trigger has been proposed [4].

#### Basic principle

Event selection proceeds with a number of successive steps. A decision can be issued at each step so that background events can be rejected as soon as possible. The data necessary for the next steps are requested only if the analysis is still consistent with at least one set of trigger conditions. Figure 4 also shows estimated input rates for various processing steps..

Figure 4: Sequential event selection strategy

* In the first step called " Rol confirmation", the Level 1 "trigger RoIs" (average 1.2) are analyzed using data from calorimeter and muons subdetectors only. The event rate is reduced significantly by sharpening the the energy and momentum cuts. In a second step the confirmed muon and electromagnetic region of interest are matched to track found in the inner detector.
* The next step in the event selection sequence proceeds with the confirmation of "non-trigger RoIs ". As for the "trigger RoIs" data from calorimeter and muon systems is processed before the data from the the Transition Radiation Tracker (TRT) and Semi-Conductor Trackers (SCT) subdetectors.
* The next event selection step is based on topological analysis of the event. More complex algorithms, such as mass calculation, missing Et or search for secondary vertices, can be used at this stage. The trigger menus consulted and the decision is made to reject or accept the event for "off-line" type analysis in the Filter processors. For accepted events, a data acquisition system collects the full event data. Partial event collection can be envisaged, if this will be motivated by the trigger process. The event are classified into calibration and various physics streams. Full or partial event data data is recorded on permanent storage for subsequent off-line studies.

#### Sequential menus

Since this original principle, more refined strategies and menus were investigated including specific signature like B Physics and B jet tags [3, 19]. A model of the sequential processing scheme is presented in figure 5.

In this example, studied for low luminosity [19], the processing and decision making can be broken down into six processing steps for high Pt physics channels plus an additional four processing steps for B-Physics candidates

Figure 5: Model for sequential LVL2 processing at low luminosity

Each processing step corresponds to a limited number of trigger algorithms and requires data from a limited number of sub-detectors. All data required for all RoIs concerned by that processing step can be requested at the same time. In this case, the maximum number of data request is equal to the maximum number of processing steps (10). Each processing step has an associated input menu to choose the events that needs information from that processing step.While waiting for data for the next processing step, the processor can execute algorithms or request data from read-out buffers.

#### Advantages

The advantages of such concept are :

* use same architecture when the LHC luminosity increases from \(10^{33}\) to \(10^{34}\), allowing to mix guided (high Pt) and non-guided RoI physics process (B Physics).
* allow to make selection and reject events as soon as possible, decreasing the volume of data to be transfered and then reducing the network bandwidth.
* allow flexible boundaries between the various selection steps, in particular between the high frequency part (LVL2) and low frequency part (Event Filter).
* make possible the redefinition of ROI's size and position after each processing step.

There are typical physics process or signatures that need in any case a sequential processing. These are incompatible with a full parallel feature extraction architecture since each step should be guided by the previous one.

For B Physics, the sequence is :

* confirm LVL1 muon RoI decreasing the LVL1 rate a factor 2.
* extract tracks from the TRT full scan defining "new RoIs"
* processing these RoIs

For B vertices in jet events for high Pt trigger (B-jet tag)

* find impact parameters and tag displaced vertices.
* find tracks in jet RoIs
* process new RoIs

Increase the muon acceptance for "non-trigger" RoIs
* The acceptance of the muon LVL1 trigger is limited to eta \(<\) 2.2. Finding new " non-trigger " RoIs above this limit can be done using the forward precison chambers.

## 4 Single farm architecture

The sequential selection strategy suggests that a single processor executes all triggering algorithms and makes the final decision. We propose [5] to implement this strategy using a single processor farm linked to the ROBs by a single switching network as shown in figure 6. The main features of such architecture are :

* Integration of data and protocol networks
* "Pull" data flow control strategy
* A single processor per event
* Several events processed in parallel per processor

### Data flow

Each source groups several ROBs from the same subdetector by a bus in order to reduce the the number of network links. We think that 1 to 8 ROBs buffer (ROBin) per source [12] will match the the network link bandwidth to the capacity of the bus. Similarly, a destination serves several processors. The network ports are bi-directional, so that control information can flow from the the processors towards the ROBs.

In what follows we describes the operationof the system under the so-called "PULL" protocol (figure 7) which allows each destination processor to request data as needed.

For each event accepted by the level 1 trigger the supervisor gets a list of RoIs and sends this information to a destination. A processor within the destination is allocated to perform selection algorithms. It proceeds with the sequential steps described in section 3.2.1. At each step, the processor sends request messages to the sources that contains the data needed to complete the current selection step. Each source gets the event data fragment from the relevant ROBs memory, preprocesses it (if necessary) and sends it to the destination. The destination node passes the data to the processor, which executes the algorithm.

Figure 6: Single Farm architectureAfter each step, the processor decides to continue with the event selection or to stop. If the processing is to continue, new data is requested from the sources. While waiting for the requested data the processor can execute selection algorithm for another event. If the processing is to stop, the trigger decision is sent to the supervisor that broadcasts it to all sources. The sources forward the decision to the ROBs. Rejected events are discarded. Accepted events are kepts in the ROBs until their transfer to the DAQ system for event building and on-line analysis.

In a full integrated option where the "high level trigger selection and DAQ data flow are merged", it is possible that the same processor will perform also the on-line analysis algorithms. Another option is to allocate a different processor for this task. from a farm dedicated to the "on-line" analysis.

### 4.2 Some particularities

This concept of single farm architecture follows the rule of trying to minimize the number of components as well as the network connection. The particularities and advantages are listed below:

* Needs full LVL1 ROI's info at ROI Builder level, in particular to flag the "Trigger RoIs" that pass the LVL1 trigger thresholds.
* Decrease ROB complexity by avoiding to broadcasts control info at LVL1_Accept rate to the Read-Out Buffers.
* Simplify the LVL2 supervisor that can use only one control network.
* The "Pull protocol" should simplify the error diagnostic and recovery
* The Multitask Processors farm allow multiple event processing.

Finally, this architecture can integrate both LVL2 & DAQ/EF selection process in the same collection and control network, that will simplify considerably the overall architecture. Such possibility is strongly connected to the availability of powerful

Figure 7: Pull architecture

commercial network technologies.

### 4.3 Hybrid approach

This approach is driven by the fact that for the low luminosity trigger, the critical item is the processing of the complete data volume of the TRT (Table 3). This leads to an enormous demand in term of computing power and data bandwidth. This problem could be solved by using a larger network and processor farm. However, a hybrid solution that combines fast processing technology like FPGAs as co-processor can be more cost effective. Such combination could also increase the trigger flexibility and expands the physics potential.

* The optimized track finding algorithm embedded in FPGAs was developped in order to show the feasability of such approach. The principle of the integration of FPGA as co-processor was investigated [9] with a small set-up showing that such scheme could be done. Some further technical investigation is necessary in order to understand the best efficient way to couple the various elements (see section 8.3.2).

Figure 8: Principle of an Hybrid Architecture - FPGAs for Track Finding

## 5 Demonstrator "C" results

The demonstrator "C" program [25] is designed to test the concepts of the single farm architecture, measure the performance of a real full slice system, understand certain technology issues, and provide feedback to the modeling activities.

### 5.1 Description of the demonstrator

#### Hardware configuration

A small-scale demonstrator has been built around an ATM switch (FORE ASX 1000) connecting 4 sources, 4 destination processors, a supervisor and a monitor (Figure 9). The switch is equipped with 16 ports at 155 Mbit/s. Data sources are emulated by PowerPC single board 100 MHz computers running LynxOS. PentiumPro 200 MHz PCs running Windows NT act as destination processors.

Portable code is used to allow flexibility in the configuration of the demonstrator. Any host platform can be a source, destination, monitor, or even a supervisor emulator. For performance reasons and because of mechanical constraints, LynxOS VME platforms are preferred for the source modules and the supervisor. PCs provide a cost effective solution for the destination processors and the monitor.

The demonstrator set-up and operations are controlled via Ethernet by a workstation. The system configuration and a set of input parameters for the run are stored in a parameter file that is accessed by all nodes at start time. At the end of the run, each node dumps a log file on the screen or on disk for off-line analysis.

#### Structure of the Demonstrator Software

The demonstrator software is a distributed application that runs on a heterogeneous environment. It has been organized in a way that ensures portability across host platforms and independence with respect to the networking technology. The software has a layered and modular structure. It consists of more than 20,000 lines of C code split into three major blocks:

Figure 9: Configuration of the single-farm demonstrator* _The core of the application_ contains those parts that are common to all types of nodes. This block does not depend on the particular operating systems. New types of nodes can be created quickly because they all use the same core part of the software.
* _The host-platform and operating-system specific modules_ implement thread creation and manipulation routines, a mechanism for synchronization and mutual exclusion, and timer routines. At present, this specific part is supported on Digital UNIX for DEC workstations, on LynxOS for PowerPC VME SBCs and on WindowsNT for Pcs..
* _The networking-technology specific module_s implement a certain number of functions to set-up and close connections and to send and receive messages across the network. Currently demonstrator "C" can use either ATM or UDP/IP. The same functions could be implemented for other networking technologies. The ATM layer is based on the optimized library described in [2]. By supporting UDP/IP, the demonstrator software can run on any cluster of machines interconnected via Ethernet, or even on a single workstation (to test soft real-time functionality). To further facilitate software development, the so-called "File IO network emulation" has been implemented. It allows any demonstrator node to run in a stand-alone mode in which outgoing network messages are written to one file and incoming network messages are read from another file. Network byte order can be chosen by the user. The demonstrator "C" software runs with either big or little median byte ordering. Functions have been defined to convert internal representations of structures to or from their network representations

### 5.2 Operation and Performances

#### ATM Networking Technology Studies

The demonstrator "C" program has largely profited from the experience of the RD31 project in ATM networking [7]. ATM technology seems to be adequate to handle the data traffic for large event building applications. Simulation studies of the single farm architecture confirm that specific features of ATM allow the construction of high performance networks capable of transporting the various types of traffic characteristic of this application simultaneously [2]. Congestion avoidance mechanisms implemented with industrial ATM components minimize the influence of network contention on system performance. Furthermore, ATM supports multicast services efficiently. These features make ATM an attractive candidate for a network that could transfer both level 2 and event filter data, as well as data collection protocol and monitoring traffic. Within the demonstrator "C" program an optimized ATM library has been developed for network interface cards that use the IDT NicStar segmentation and reassembly chip [6]. The library runs on LynxOS and WindowsNT. It avoids the context switches between user and kernel modes used in traditional network drivers, thus reducing the communication latency and increasing the throughput. It also implements a true zero-copy protocol, placing a minimal load on the host CPU for data movement. This library exploits the full bandwidth of the 155 Mbit/s link for messages larger than 100 bytes (2-3 ATM cells) due to its low data transfer and receive overheads of \(\sim\)10 us and \(\sim\)20 us respectively. This can be compared to the performance of commercial ATM products from DEC with \(\sim\)20 us transmission and \(\sim\)50 us receive overheads, that allow full bandwidth utilization only for messages larger than 1 kbyte [4]. Nonetheless, the trend in high speed networking technologies indicates that future host platforms equipped with commercial network adapters will be able to make efficient use of the bandwidth offered by these technologies.

ATM seems to be gaining acceptance in the telecommunication market, and 100 Gbit/s switches are now available. Several vendors provide a large choice of LAN backbone switches with 10 Gbit/s aggregate bandwidth at a moderate cost of \(\sim\)1000 US$ per 155 Mbit/s port. Although the acceptance of ATM on the LAN market is relatively slow because of competing technologies such as Fast and Giga Ethernet, an ATM network in the range of 40-80 Gbit/s should be affordable, and adequate for ATLAS needs.

#### Supervisor

A detailed description of the supervisor can be found in [10]. Only a minimum configuration (1 RoI CPU) has been deployed to demonstrate the supervisor operation with the rest of the system. Events accepted by the level 1 trigger are routed to the RoI CPU by an arbiter/router. The RoI CPU assigns the events to destination processors and collects trigger decisions, packs them, and multicasts them to the sources. This is done via the ATM network. A credit-based flow-control mechanism avoids overloading of the destination processors. A time-out mechanism is used to detect and isolate faulty processors.

The supervisor task is implemented in a single thread. Polling is used to check for input from the arbiter/router and the ATM network. The supervisor with 1 RoI CPU (100 MHz PowerPC) is able to handle events at \(\sim\)8 kHz. The performance of the supervisor is expected to scale linearly with the number of RoI CPUs, so a supervisor with multiple RoI CPUs should be able to achieve the target rate of 100 kHz.

#### Source Module

A source comprises a variable number of Read-Out Buffers (ROBs) [6] connected via a shared media (e.g. PCI bus) to a ROB-to-Switch Interface (RSI). The ROBs receive the event data from the detector front-ends at the level 1 trigger rate and buffer it during the event selection process. The RSI is in charge of servicing requests transmitted via the ATM network. When a request for event data is processed by the RSI, it posts a request to each of the ROBs concerned. When all ROBs have replied, the RSI formats the data and sends it to the processor that requested it; a time-out mechanism ensures that a malfunctioning ROB does not prevent the RSI from responding with data from the other ROBs. The RSI also distributes the trigger decisions received from the supervisor to the ROBs.

At present, the design of the ROB is not completely final, and the ROBs are not integrated in the sources. Instead, the ROB functionality is emulated in the RSI processor (single thread application). This approach is sufficient to test the demonstrator system but it does not give an accurate view of the ROB/RSI interaction.

Figure 10 presents the maximum rate of data requests, F\({}_{\text{src}}\), that can be serviced by a source as a function of the size of the response message.

Without pre-processing (Figure 2a), the limitation due to saturation of the output link is reached for packets larger than 1 kbyte. On a 100 MHz PowerPC, the source code executes in :

\[\text{T}_{\text{src}}=30\ \text{\mu s}+2.5\ \text{\mu s}\ \text{* nb}\_{ \text{of}\_{\text{ROB}}}\]

When pre-processing is performed by the source, the maximum sustainable data request rate is reduced (Figure 2b).

Because there are no physical ROBs connected to the sources, more conservative numbers have to be considered for realistic sources. Investigations of the ROB/RSI interaction are in progress.

For a source module, an important parameter is how frequently it participates in an event. This depends on the sub- detector, the number of ROBs per source, their granularity, etc. It is estimated that some sources will participate in 20% of the events, so they will have to handle request rates of up to 20 kHz. Even though the results obtained with the ROB emulation are compatible with this requirement, improvements will probably be needed for operation with real ROBs.

#### Destination Processor

The destination processor is in charge of accepting or rejecting the events assigned to it by the supervisor. At each step of the selection process, the processor requests data from the relevant sources for the execution of the next selection algorithm. While waiting for the arrival of the requested data, it can continue processing selection algorithms and/or data requests for other events.

The processor task is implemented in a multi-thread application. On the network side, the Rx thread is in charge of handling incoming packets (interrupt driven) and the Tx thread is in charge of transmitting messages. A certain number of processing threads are in charge of running the selection algorithms (one event per thread). When a new event is received, it is passed to one of the processing threads. This thread posts a request to the Tx thread and becomes idle until data is delivered. When a small number of sources are concerned, an individual message is sent to each source. When a large group of sources is concerned (e.g. the whole calorimeter for missing energy calculation, or all sources for full event building), the Tx thread sends a single message on a multi-cast channel to the group of sources. When the requested data has been completely gathered by the Rx thread, the block of data is posted to the processing thread that

Figure 10: Performance of the data sources.

executes the next step of the selection algorithm. This sequence is repeated until the decision to accept or reject the event can be made. To make the system more robust, a time-out thread periodically scans the list of pending requests posted to the sources. If any of the sources failed to reply in the allotted time, the processing thread is informed that its data request has failed. At present, incomplete events are discarded. In the demonstrator, the selection algorithms are emulated by dummy processing of the desired duration.

The destination software can execute on single processor platforms or on Symmetric Multi-Processor machines. It could easily be adapted to run on clusters of processors because the processing threads are independent of the Tx, Rx and time-out threads.

Figure 11a shows the time needed to handle one event on a single-processor platform, T\({}_{\text{dst}}\), versus the size of the event fragment sent by each of several sources, for a single-step algorithm with zero processing time.

On a 200 MHz PentiumPro, the code executes in:

\[\text{T}_{\text{dst}}=215\text{ $\mu$s}+45\text{ $\mu$s}\text{ * nb\_of\_request}\]

For blocks larger than \(\sim\)2 kbyte the data transfer time is dominant; for smaller data blocks, processor overheads are dominant. The maximum event rate that can be sustained by a single destination is F\({}_{\text{dst}}\)= 1 / T\({}_{\text{dst}}\). For example, a destination can accept short events with data spread across 4 sources at rates up to \(\sim\)2.5 kHz.

When processing is introduced, the amount of time to handle an event is increased (Figure 11b). The destination processor re-formats the received data, then executes the selection algorithm. Measurements show that the data received is copied at a speed of 30 Mbyte/s. When the processing time is dominant, the maximum event rate is given by: F\({}_{\text{dst}}\)= 1 / (T\({}_{\text{dst}}\) + T\({}_{\text{algo}}\)). When the data transfer time is dominant, the speed of the link determines the maximum event rate: F\({}_{\text{dst}}\)= 1 / T\({}_{\text{transfer}}\).

A correct match of the CPU power of the destination and the speed of the link should be made to avoid a waste of network bandwidth or processing resources.

Assuming a typical feature-extraction algorithm requiring \(\sim\)100 \(\mu\)s to process \(\sim\)4 kbyte of data spread over 4 sources and an overhead derived from equation (2) of 400 \(\mu\)s, the corresponding data transfer rate is 80 Mbit/s, well adapted to a 155 Mbit/s link.

Figure 11: Performance of the destination processor

The number of destination processors can be adjusted to cope with the rate of incoming events. The first step of event selection in ATLAS is the confirmation of primary Regions of Interest (~1 such RoI per event) using a feature-extraction algorithm such as the one mentioned above. The model described here predicts that today's processors should be able to process such events at ~1.5 kHz. The computing power needed for this first step of event selection would be ~66 processors for a 100 kHz level 1 trigger rate. A farm of a few hundred processors should be adequate for the ATLAS level 2 trigger.

#### Monitor

The monitor is in charge of regularly checking that all nodes are working properly and taking the appropriate action when a node does not respond. It is also in charge of periodically gathering relevant statistics and making these results available to a visualization program. The monitor itself does not include a graphical user interface and is therefore able to run on any platform. A separate visualization program communicating with the monitor via shared memory could be used to provide a more user-friendly interface. When used without graphical output, the monitor prints results on the screen at regular intervals and dumps the final statistics at the end of the run. Monitoring and gathering of statistics are done via ATM. The real time constraints placed on the monitor are relatively loose (human reaction time) and the performance of this node is not a critical issue.

### 5.3 System Performance Analysis

This section presents the demonstrator performance when all nodes are placed together as shown in Figure 12. The event decision latency T\({}_{\text{dec}}\) is defined as the time elapsed between the event allocation by the supervisor and the return of a decision by the destination processor. The latency distribution shown in Figure 12a is based on the following scenario: For each event, a processor fetches 1 kbyte of data from each of the 4 sources. The selection is made in a single 100 us step. The event rate is 3.6 kHz, corresponding to ~50% utilization of the destination processors and ~20% utilization of the link bandwidth.

Figure 12: Performance of the system.

The average trigger latency in this scenario is 1.2 ms, and 99% of the decisions arrive within 1.4 ms. The latency of the communication between the supervisor and the destination processor is ~200 \(\upmu\)s. The latency introduced by the destination is ~500 \(\upmu\)s (4 requests plus the 100 \(\upmu\)s algorithm). The transmission of the requests to the sources takes ~100 \(\upmu\)s. The source adds ~60 \(\upmu\)s and the transfer of 4 kbyte of data over a 155 Mbit/s link takes ~250 \(\upmu\)s. The total amounts to 1.1 ms, which is the minimum observed. Additional delays are due to queuing in various places, operating system scheduling, etc.

The average trigger decision latency as a function of event rate is plotted in Figure 12b. The demonstrator operates safely at a rate of 1.5 kHz per destination (6 kHz for the 4 destinations). The saturation point is around 1.9 kHz per destination. The limitation for this set of input parameters comes from the destination processors. The rate limit is compatible with the model described in Section 5.2.4: \(\mathrm{F_{dst}}\)\(=\) 1 / (215 + 4 * 45 + 100) \(\upmu\)s \(=\) 2 kHz.

Tests have been made using sequential selection algorithms with several steps having different execution times and requiring different numbers of sources, as shown in Figure 13a.

The histogram of the latency due to the successive processing steps is plotted in Figure 13b. The event rate is 3.35 kHz (50% load on the system). The first two steps execute at a priority higher than that of the last step so that new events are not delayed by the small fraction of events with longer processing times. This test demonstrates the capability of handling several events concurrently in destination processors. It also demonstrates the capability of using a single network to carry different types of traffic: requests, event data, monitoring and statistics. Unspecified Bit Rate channels have been used to transfer data for the first and second step of the algorithm and Constant Bit Rate (CBR) channels for the last step (full event building). The monitoring uses low priority, low bandwidth CBR channels. This bandwidth allocation scheme is sufficient to avoid network congestion and cell loss.

Figure 13: Sequential processing.

The latency measurements described above do not include the time needed to transfer the decision from the supervisor to the sources, and then to the ROBs. The event decision latency at the ROB level is a parameter that determines the depth of the event buffer in each ROB. Assuming that 10 MB of memory are placed on the ROB and that this buffer is filled at 100 MB/s, the storage capacity would be 100 ms. Paper model studies using full level 2 trigger menus (including b-jet tags and missing-Et calculations) indicate that the average level 2 latency could be about 30 ms, compatible with the 100 ms limit for 10 MB buffers [11].

### 5.4 Summary of Results

The operation of a demonstrator for the sequential event selection option of the ATLAS level 2 trigger has been presented in this section. The Request / Response data collection protocol was implemented. The possibility of making sequential data transfers and sequential selection using partial event data or full event data has been shown. The capability of handling several events simultaneously in a single processor has been demonstrated. The merging of different types of traffic on the same ATM network has been shown. On-line statistics gathering and monitoring via ATM have been implemented. Several mechanisms for error detection and recovery were used to ensure the correct operation of the demonstrator. A certain number of parameters and simple formulas have been derived to characterize the system.

## 6 Paper model of Modelling the C architecture

A spreadsheet model of Architecture C has been implemented [15], It assumes:

* for each event, the feature extraction and global algorithms are processed in sequence in a single processor.
* these processor request RoI data and execute event selection algorithms sequentially.
* the ROB and farm network interfaces act as concentrators/dispatchers for data transfers.

The DAQ-NO-54 low luminosity trigger menu was used for this study. The parameters from DAQ-Note 70 [12] have been updated to account the improvements in the process model. It includes b-jet tag and a missing Et triggers. Inclusive RoI rates were calculated for each processing step. The B-Jet tag algorithm is assumed to take 10 ms in 500 MIPS processors, according to the recent pixel version of the track finding part. Inclusive rates were used at each processing and selection step to find the RoI rates in each of the subdetectors. The TRT full scan was performed for every confirmed muon.. After the TRT full scan, an average of 5 track RoIs are selected for further study by the SCT. The number of ROB's per RSI is set to 1 for the hadron. Overhead times are set to 10 usec for I/O processing and 40 usec for context switching. The context switching overhead is applied to all destination nodes except the ROBs and RSI. Data merging CPU time was considered only in the SFIs, not in the RSIs, since DMA data transfer is foreseen between ROBs and RSI. The transfer through the same network for all event data to the Event Filter was included as an option. The RoI rates shown in table 4 were obtained for a 38.7 Khz LVL1 trigger rate.

Figure 14: Switching network configurationOccupation levels were calculated for each of the system components. Encouraging results were found with a model based on a 1024 nodes network and 15 Mbytes/ links. No difficulties were found for operation at the nominal 38.7 KHz LVL1 rate. Even when the trigger rate was scaled to the maximum possible LVL1 rate of 100 KHz, all the systemcomponents had occupancies below 100%, as shown in table 5. Very recently, new paper studies have been carried out to evaluate the effects of recent technological evolution in our system performance. With 10 usec context switching time, 2000 MIPS LVL2 processors, 60 MBytes/s networks links, 80 MBytes/s RoI fragment merging speed, and 2 ROBs per RSI for the ECAL, occupation levels falls below 60% even including the Event Filter traffic for all system components but the TRT RSIs (Full scan). The TRT scan would probably better handled by dedicated FPGA co-processors.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline RoI rate (KHz) & MU sys. & EM cal & HAD cal & SCT & TRT & TRT FS \\ \hline Muon RoI & 8.4 & 4.4 & 4.4 & 6.3 & 6.3 & \\ EM RoI & & 21.3 & 21.3 & 3.8 & 3.8 & \\ Jet RoI & & 24.0 & 24.0 & 3.2 & & \\ Special RoI * & & 0.83 & 0.83 & 20.0 & & 4.0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Low luminosity RoI rates at nominal 38.7 KHz LVL1 rate.

For LVL2 operation, the highest occupation level is the TRT RSI (116%). If the TRT full scan is performed on a separate FPGA processor, the highest occupations are the ECAL RSI (100%) and the HCAL ROB (98%). Note that the major effect due to the addition of the LVL3 traffic is to increase the occupation of the ECAL network links from 45% to 80%. Parameters will be reviewed soon to take into account recent technological evolution as well as a more refined view of the sub-detector electronics.

## 7. Emulation

The demonstrator program has shown systems of a modest size with the largest one consisting of only 10 nodes interconnected by a network. This is at least two orders of magnitude under the size of the expected ATLAS T/DAQ. It is therefore needed to investigate larger systems to show scalability. Building larger demonstrators is on the way, but these systems become rather expensive as the number of nodes is increased. Also, because computer hardware becomes obsolete very quickly, it is unlikely that any component of the demonstrators will be used in the final system. Therefore, the investments to make large demonstrators have to be made carefully. For this reason, emulation on existing networks is useful and provides a cost-effective approach, complementary to demonstrator systems. Although modeling and simulation are the only appropriate tools for investigating the performance of the complete system before it is actually built, emulation on existing networks and demonstrators provide the necessary input and means to validate the models used in the simulation.

\begin{table}
\begin{tabular}{c c c} \hline \hline EF traffic included & No & Yes \\ \hline Total data rate (MBytes/sec) & 3043 & 3995 \\ Number of SFIs available on a 1024 ports switch & 438 & 438 \\ Data rate per SFI (MBytes/sec) & 6.95 & 9.12 \\ Network occupancy per SFI port (\%) & 46.3 & 60.8 \\ Average SFI occupancy (\%) & 84.2 & 91.8 \\ Minimum number of LVL2 processor required & 777 & - \\ Number of LVL2 processor per SFI & 2 & - \\ LVL2 processor occupancy (\%) & 88.7 & - \\ \hline \hline \end{tabular}
\end{table}
Table 5: Processor and switch port occupancies scaled to 100KHz LVL1 rate.

## 7.1 MACRAME emulation

The 1024-node Macrame switch based on DS-link technology is being used for emulation and traffic studies [16]. This system gives the possibility to test very large systems. The disadvantage of this system is that most of the nodes attached to the network are not "intelligent" and are just there to generate some background traffic. Nonetheless, important results have been obtained.

### 7.1.1 Input assumptions

The emulationof the architectureC option for the LVL2 has been performed using the parameters from DAQ NOTE 55. The algorithm sequences are produced using the LVL2 trigger menu in DAQ note 54 [ ] (input rate 34330 Hz, with 75 different sequences possibles). Sixteen nodes are dedicated to the supervisor, 240 to processors and 192 to RSIs ( 32 muon, 64 calorimeter, 64 TRT, 32 SCT). The events are generated with a Poisson distributionin time, and the messages from different events are mixed. The messages are distributed on each source node and ordered by time stamp.

The emulation of the full event buildingrequired for DAQ/EF has been performed using the barrel-shifted transfer mode. For each event, each of the 192 sources send 4Kbytes of data to the same processor,the transmission from successive sources is delayed by 500 usec. (The full event data is transmitted in about 100 ms). For this test, 112 nodes were dedicated to the EF processors.

### 7.1.2 Measurement and results

No congestion was observed for LVL2 or EF/DAQ traffic up to 60% port occupation. Data was lost when the occupation exceeded 60% on one of the input nodes (for the LVL2 traffic) or on one of the output nodes ( for EF/DAQ traffic). More studies will be necessary to explore any systematic limitations.

With the LVL2 model used in this study, the system was able to operate for input rates up to 75Khz. For thez EF/DAQ traffic with thebarrel-shifted transfer mode and Poisson distributed timing the system was able too operate up to 850 Hz (LVL2 output rate).

Figure 15: Network traffic in function of event rate.

#### Conclusion

These studies have shown that the traffic patterns genrated and those generated by full event building for EF/DAQ can be handled successfully by a switching network in a CLOS configuration.

Extrapolating from these results, we would expect that an ATM network with 155 Mbit/sec nodes (15 Mbytes/sec for application data) could operate successfully for LVL2 input rates above 100Khz and EF/DAQ input rate above 1Khz. On the other hand, a larger switching network might be required for stable operation at these high rates (The design studied was based on a switching network with 1024 nodes). Further studies will be necessary to understand the influence of the particular MACRAME hardware, such as the very limited size of the input buffers.

### 7.2 ATM emulation

Another possibility for emulation is the use of an existing ATM based computer network like the one in operation at RCNP [8]. The laboratory network is based on fast multi-processor servers and several tens of fast workstations connected by state-of-the-art ATM links. The implementation of the sequential option (Architecture "C") on the computer network at RCNP is under investigation.

One of the goals of this program is to show the operation of a system larger than the current demonstrator. It is expected that the operation of a system twice as large as what has been shown so far (i.e. a system with \(\sim\)20 nodes connected via ATM) can be demonstrated. The portability of the Architecture "C" software to Digital Unix, the operation of a system with 32-bit machines and 64-bit machines from different vendors and the operation of multi-processor machines have to be shown. Investigations on the performance of commercial off-the-shelf ATM hardware and software from DEC are on the way. The performance of the largest emulation testbed that can be constructed at RCNP will be reported and discussed along with the issues that have been identified.

Figure 16: Processor to ROB latency distribution

## 8 Conclusions and Plans for the future

We believe that the results presented in this paper are an important step toward proving the validity and the feasibility of the sequential event selection scheme for ATLAS. This work will continue in the context of the new LVL2 "Pilot Project" workplan [22, 23]. This chapter outlines and justify the future program.

### 8.1 Conclusions from the demonstrator C program

The operation of a 10 nodes ATM technology full slice demonstrator for the sequential event selection option of the ATLAS trigger was used to show :

* The Sequential selection principle and its advantages.
* The Request/Response "pull" protocol
* The possibility to make a sequential event data transfer and selection using partial event data or full event data.
* The capability of handling several events simultaneously in one processor.
* Several mechanisms for error detection and recovery that ensure the correct operation of the demonstrator.
* Merging different types of traffic in the same ATM network.
* On-line statistics gathering and monitoring via ATM.
* Operation of platform/network technology independent software.
* Measurement of a certain number of critical parameters and simple formulas that characterize the system were derived.

However, several important points have not been addressed yet by this demonstrator and will be the main goals of future challenges:.

* The integration and implementation of a real algorithms and physics process in the destination processors.
* The Multi ROBIN/RSI interaction and its performance.
* The 75-100 kHz operation of the supervisor unit with multiple RoI CPU-s.
* Performance measurement of larger systems (32 ports and up).
* Integration of co-processor for the tracking algorithms (TRT).

### 8.2 Pilot project testbed

The full slice application testbeds will embed the Level-2 Trigger Process as developed in the reference software on relatively larger and better optimized systems (see figure 17) to address the following issues:

* the study of full algorithms, steering sequences and global menus (menus based on trigger RoIs only, full menus with secondary RoIs, B-physics menus) [17, 19]
* the evaluation of protocols and measurements of the hardware performance required for a complete system (latency, link occupancy and software overheads);
* the integrated tests of optimized components (ROB prototypes, data pre-processing units...) and better commercial switching networks once they become availableFor the first two issues the hardware platforms which have been selected use: ATM and Fast-Ethernet for the network technology; Pentium processors for the processing farms and for ROB and Supervisor emulation; optionally PowerPC based cards for ROB emulation and Supervisor prototypes. The software should allow the use of WNT and Linux in PCs and LynxOS in the VME cards.

#### Network technologies issues

We believe that the principles of the architecture proposed are realistic and that ATM technology is suited to this application. However, the emergence of faster Ethernet 100Mbit/s -1Gbit/s devices (interfaces and switches) make this technology very attractive and possibly very cost effective and should be investigated in the context of the "single farm" architecture.

* _ATM_ The proof of principle to use Asynchronous Transfer Mode (ATM) technology for the central network was demonstrated with a limited number of 155 Mbit/sec nodes (10) and a simple message exchange scheme. The ATM testbed will be used to study the options for the trigger process as detailed in 6.2 above, but it will also allow further evaluation of the technical details and performance of this technology with a larger number of nodes.

Figure 17: Application testbed configuration

Figure 18. Example of multiswitch and multipath networks

The idea is to assemble several (3 to 4) standard commercial 16 nodes subsystems in order to reconfigure the components into a larger one (48 to 64 max). This will allow us to gain experience with multiswitch and multipath network by building in a first step a two stage 16 port interconnect and then a Clos multipath network. The final 48-64 port full slice system would also be used to evaluate the performance of the trigger process itself on a moderately large system. An important element of the programme will be measurements of the various parameters (overheads, latencies and occupancies) throughout the system as input to the modelling.

* _Fast-Ethernet_

Ethernet is a popular widely used networking technology. However, a demonstration is needed in a large full slice system that the performance and technical features are adequate to solve our specific problem of managing high rate small packets. In addition to studying the basic technology (interface cards and custom drivers) within the "technology activity", it is planned to use a large system of 32 to 64 nodes to study the options for the trigger process and to evaluate and measure the various critical parameters using the same criteria and trigger models as in the ATM testbed [23].

#### Components integration

This architecture rely not only on the switching network, but also on some specific components interfaced to it in an optimised way (hardware and software). In order to understand the functional organisation of the data flow, it is foreseen to measure performance and limitation of each component integrated in the overall system.

* _Multisupervisor_

The Supervisor has 3 main functions (Figure 18):

- Receive at the LVL1 accept rate RoI information fragments and build the RoI record message.

- Assign the LVL2 processor as needed and route the RoI record to it.

- Receive and broadcast LVL2 decisions to each ROB Complex.

processors. In addition to the parallelism, such concept allows also the partitionning of the network system - important functionality in case of the single network for LVL2 and DAQ/EF. An additional functionality is to integrate in this element with the monitoring of the overall event flow process.

Validation of such object with its various functionalities is an important step to understand the requirements of the 'event flow control" of the system.

* _"ROB Complex"_

The "ROB Complex" concept (Figure 19) was developped [11] in order to adapt the network input bandwidth to the data fragment volume for each part of the detector. This "Complex" is divided into 5 functional blocks.

- The ROBIN that store the "raw data" from the Read-Out Drivers (ROD) during the LVL2 latency.

- ROBin can be grouped together by a "collector bus or network". The number of ROBIN connected (1, 4 or more ) depends on the detector.

- The network Interface (NIC) that receive the control message and send the requested information.

- The ROB Controller for managing and monitoring the overall system.

- An optional Co-Processor can be added if necessary in order to process locally the data collected from several ROBin.

The present functional prototype [14] separates each one of these last 3 functionalities (NIC, Controller and Co-Processor) but there is no reason that in a final design, they are not integrated in a single object (Intelligent NIC). Critical issues are the measurement of software overheads and the optimisation of the various parameters when the number of ROBin increase.

Figure 19: Multisupervisor functional blocks

* _Co-Processor integration_ As it was presented in the chapter 4.3 (Hybrid approach), the TRT full-scan algorithm is a very demanding technical issues in term of computing and networking. Using FPGA as "co-processor" can be a cost effective solution that have to be studied.
* To integrate such device, two options are possible :
* The initial idea (Figure 20 A) needs a separate network/router to collect the TRT data from the ROB. However, such architecture superpose the architecture A (Data Driven) and C (Single farm) complexity and might be not the best optimisation of the concept.
* Another idea (Figure 20 B) use the fact that the FPGA could act very efficienly as a co-processor of the processor farm. In this architecture, the farm processor request the TRT Full-scan algorithm to the "co-Processor" (COP), the COP request data from ROB sources, passes it to the FPGA processor (Enable++) and return the results to the farm processor. This option simplifies the hardware but places more demand on the network and software. The integration of such device in the single farm concept is an important element to use efficiently the processing power adapted for very specific algorithms.

Figure 20: ROB Complex functional organisation

## 8.3 System scalability issues and cost estimate

An important issue is the scalability and how a larger system will perform. The operation of the supervisor does not depend on the size of the system. The performance of the supervisor is expected to scale linearly with the number of RoI CPU-s.

For a source module, the important parameter is how frequently it participates in an event. This depends on the sub-detector, the number of RoBs per source, their granularity... It is estimated that some sources will participate in 10% of the events, so they will have to handle a 10 kHz request rate. Although this number is compatible with the measurements we have made, some improvements on the source performance will probably be needed.

The number of destination processors can be adjusted to cope with the rate of incoming events. The first step of event selection in ATLAS is the confirmation of primary Regions of Interest (~1 such RoI per event). The amount of data to be collected is ~4 kB and is spread across 4 to 8 sources. Our model predicts that today's processor running a 100 \(\upmu\)s algorithm should be able to process events at ~1.5 kHz. The computing power that would be needed for the first step of event selection at a 100 kHz trigger rate is ~66 processors. A farm of a few hundred processors for the ATLAS trigger seems realistic. On our demonstrator with four source modules we could study the scalability by adding up to four destination processors to a small scale event selection system. Figure 12 shows that sustained event rate increases linearly with the number of destinations. However, influence of network contention on the system scalability can not be observed on such a small setup.

The table 7 shows an evaluation of the cost of a the various components of a system based on the model described in the Paper model (chapter 6). It should be noticed that in this model, the switch is also used for the final event building.

Figure 21: Possible schemes for TRT full-scan using FPGA co-processorFor the switching network, several vendors offer 10 Gbit/s ATM switches at a moderate cost (\(\sim\)1 kUS$ per 155 Mbit/s port). Larger switches (up to a few 100 Gbit/s) are available for the telecommunication market. At present, we estimate that a switch in the 40-80 Gbit/s range will be adequate for ATLAS and affordable in 2002. Simulation and measurements show that the bandwidth allocation scheme used in the demonstrator seems adequate to avoid network congestion in a large system [25].

\begin{table}
\begin{tabular}{l c c} _Total_ & 17.5 & 11.6 \\ \hline \end{tabular}
\end{table}
Table 7. Cost estimate from the LVL2 paper model 

## 8.4 Toward a " Single Network T/DAQ scheme "

As the fast networking technology is becoming a key element of the modern multimedia world and data communication systems [20], and since the concept of single farm for the high frequency part looks promising, this paragraph introduces a more challenging concept based on a single network as a backbone for the integration of the overall T/DAQ data flow system. Even, if the validation of such architecture is far to be demonstrated, the potential simplification in term of number of different components, interfaces, systems aspects and partitionning, distributed controls and cost has to be adressed now in order to balance its advantages and possibles issues.

The figure 21 shows the functional dataflow scheme of such architecture. Like in the multimedia world, the key element is the central multipurpose switching network able to manage various type of traffic : from very low to high frequency short control messages to variable size of data fragments. This network connect together the various components of the system: the data sources, the supervisor and event control system, the processing farms (High Level trigger, Event Filter), the mass storage and data recording medium and finally the end user PC stations. The various interfaces ports are adapted and optimised for each component from low speed commodity path to high speed intelligent interconnect. The modularity allows a smooth integration of each element from a minimal "local autonomous system " to the global final system.

A complete description of the principle of such architecture is under documentation [21].

Figure 22: Single network T/DAQ system principle

## 9. References

[1] ATLAS Technical Proposal (Dec 94)

[2] DAQ-NO 28 J. Bystricky et al (jan 95), Performance E/photon Trigger at LVL2

[3] DAQ NO- 54 - Trigger menus at low luminosity. R. Hubbard et al.

[4] DAQ-NO-55 J. Bystricky, et al., "A Model for Sequential Processing in the ATLAS LVL2/LVL3 Trigger", June 1996.

[5] J. Bystricky et al. "A Sequential Processing Strategy for the ATLAS Event Selection", (NSS 96), IEEE Transactions on Nuclear Science, vol. 44, 3 June 1997.

[6] D. Calvet et al.,"Performance Analysis of ATM Network Interfaces for Data Acquisition Applications", in Proc. Second International Data Acquisition Workshop on Networked Data Acquisition Systems, Osaka, Japan, 13- 15 November 1996, World Scientific Publishing 1997, pp. 73-80.

[7] M. Costa et al., "Results from an ATM-based Event Builder Demonstrator", IEEE Trans. on Nuclear Science, vol. 43, No 4, June 1996, pp. 1814-1820.

[8] DAQ-NO-069. I. Mandjavidze, "Evaluation of the RCNP ATM Network and Computing Facility for Emulation Purposes of Network Based Trigger/DAQ Systems", ATLAS Internal Note (May 1997).

[9] DAQ-NO-78. A.Kugel et al. A Hybrid approach for the ATLAS LVL2 trigger (1997).

[10] R.E. Blair et al., "The ATLAS Level 2 Trigger Supervisor", in Proc. Second Workshop on Electronics for LHC Experiments, Balatonfured, Hungary, 23-27 September, 1996, pp. 191-193.

[11] O.Gachelin et al., "ROBIN: A Functional Demonstrator for the ATLAS Trigger/DAQ Read-Out Buffer", in Proc. Second Workshop on Electronics for LHC Experiments, Balatonfured, Hungary, 23-27 September, 1996, pp. 204- 207.

[12] DAQ NO 70, S.George et al. "Input parametres for Modelling the ATLAS Second Level Trigger" (1997).

[13] DAQ-NO- 104 - An ATM demonstrator System for the Sequential Option of the ATLAS Trigger. D.Calvet et al.

[14] DAQ-NO- 105 - A Read-Out Buffer for ATLAS. M. Huet et al.

[15] DAQ-NO- 106 - Architecture C performance from Paper models. A.Amadon et al.

[16] DAQ-NO- 107 - Emulation of architecture C om MACRAME. B.Thooris et al.

[17] DAQ-NO- 108 -ATLAS trigger rates from fast simulations. J. Bystricky,R. Hubbard

[18] DAQ-NO- 109 - Overview of the Sequential Single farm option for the ATLAS High Level Trigger. The Demo C group.

[19] DAQ-NO- 110 - ATLAS Trigger Menuat Luminosity 10\({}^{33}\) cm\({}^{-2}\) s\({}^{-1}\). R.Hubbard et al.

[20] S.C. Use of commercial data switches in the SDC Event Builder - Loken and W.H. Greiman SDC Note LBL - 3/2/92

[21] P. Le Du et al. Toward an single network based T/DAQ system. DAQ Note in