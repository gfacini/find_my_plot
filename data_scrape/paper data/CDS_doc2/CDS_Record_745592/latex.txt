**Using \(Z^{0}\to e^{+}e^{-}\) for Electromagnetic Calorimeter Calibration**

**F. Djama**

**CERN**

_CH 1211, Geneva, Switzerland._

and

**CPPM**

_163, Avenue de Luminy,_

_Case 902, 13288 Marseille Cedex 9, France._

Abstract

A new fitting method, using \(p\)\(p\to X+Z^{0}\) events, with \(Z^{0}\to e^{+}e^{-}\), has been developped to calibrate the ATLAS electromagnetic calorimeter. Validity tests were carried on Pythia events, and the method was tested on the DC1 simulation data. These results can be extrapolated to the ATLAS configuration. The forseen 0.3 % precision on the calibration coefficients of 448 \(\Delta\eta\times\Delta\phi=0.2\times 0.4\) regions could be achieved within two to three days, and monitored on this basis, at low luminosity.

[MISSING_PAGE_EMPTY:2]

Introduction

Calibration of the electromagnetic calorimeter with real events will be an important issue in ATLAS. Extrapolation from test beams and hardware \(in\)-\(situ\) calibration will not be able to give us the ultimate precision, since these methods are not sensitive to the material in front of the calorimeter, nor to local liquid argon temperature and purity variations, nor to high voltage fluctuations du to pile-up induced space charge.

What we learnt from test beams and the hardware calibration should enable us to know the calibration coefficients with a 0.5-0.6 % precision inside \(\Delta\eta\times\Delta\phi=0.2\times 0.4\) windows. This is the so called local constant term, with contributions from mechanics, electronics (including hardware calibration), and finite size clusters.

There are 448 such regions inside \(|\eta|<2.5\).1 The ATLAS strategy to achieve the desired total constant term of 0.7 % is to intercalibrate these regions to a level of 0.3 % using physics data. Several methods have been discussed in the EM calorimetry and physics TDR's [1][2].

Footnote 1: The standard region dimension corresponds to 2 mother boards in azimuth, equivalent to 2 HV sectors in the barrel. In each end-cap, there are 4 mother boards and 7 HV sectors, along pseudorapidity, for the considered acceptance. Here, we chose the HV sectors definition, which leads to higher number of regions. There are 2 regions in \(\eta\) which have \(\Delta\eta=0.1\).

The precise knowledge of material in front of the calorimeter is crucial. The study of position and shape of the E/p distribution peak for isolated electrons, and the analyses of position and rate of photon conversions may improve the knowledge of the material quantity. Electrons can then be used to intercalibrate the 448 regions, using the E/p peak, provided a powerfull electron track fit, taking into account bremsstrahlung and multiple scattering.

The most popular intercalibration method uses the \(Z^{0}\to e^{+}e^{-}\) decay. This channel provides us with two electromagnetic objects, which are kinematically correlated. This correlation enables an autocalibration of the EM calorimeter, if the material quantity is known. In addition this channel has high production rate (between 0.5 and 1 Hz after selection), low background (few %) and rather uniform distributions in pseudorapidity and azimuth.

It is clear that, at least in the beginning, there will be iterations between all these methods (and others) before reaching a good knowledge of the material quantity, a good electron track fit, and the desired constant term. Such a scenario is still to be defined, and great care should be taken while applying it, in order not to correct for residual effects coming from the unavoidably limited precision on some corrections affecting the local constant term, in the beginning of data taking period.

The goal of a global constant term of 0.3% can be achieved in two or three days of data taking at low luminosity. It may be improved by accumulating statistics, but this depends on the stability of the whole system. This issue is still very speculative, and the base line remains the use of physics data to monitor the calibration each two or three days at the 0.3% level.

This note describes a method to intercalibrate the 448 regions using the \(Z^{0}\to e^{+}e^{-}\) decay. The method is presented in section 2 and section 3 shows correlation studies. Section 4 is dedicated to extensive tests with \(10^{6}\) Pythia events. Analysis of DC1 data is shown in section 5.

Method

To determine the relative calibration constants of the 448 regions, several studies have been achieved so far [3][1][2]. All of them make a global fit of the biased \(e^{+}e^{-}\) invariant mass distribution to the reference one (which is the same distribution before bias), and extract the values of the injected biases. Depending on the fit method, correlations and instabilities may occur while fitting such a big number of unknown parameters.

An alternative method has been developped, using very simple one parameter fits and a simple analytical least squares minimization.

On simulated events, a bias \(\alpha_{i}\) is generated for each region \(i\). All the electrons falling in this region have their energy \(E_{i}^{true}\) biased according to:

\[E_{i}^{new}=E_{i}^{true}\cdot(1+\alpha_{i}) \tag{1}\]

\(\alpha\)'s are generated on a gaussian distribution, centered at 0, with a variance \(\sigma_{bias}^{\,2}\). In the following, values between 1.5 to 5 % have been used for \(\sigma_{bias}\).

The invariant mass \(M_{ij}\) of 2 leptons found in 2 regions \(i\) and \(j\) is given by:

\[M_{ij}^{true}=\sqrt{2E_{i}^{true}E_{j}^{true}(1-\cos\theta)} \tag{2}\]

where \(\theta\) is the angle between the two leptons.

Neglecting second order terms, the biased invariant mass is given by:

\[M_{ij}^{new}=M_{ij}^{true}\cdot(1+\frac{\alpha_{i}+\alpha_{j}}{2})=M_{ij}^{ true}\cdot(1+\frac{\beta_{ij}}{2}) \tag{3}\]

where

\[\beta_{ij}=\alpha_{i}+\alpha_{j} \tag{4}\]

The \(e^{+}e^{-}\) invariant mass for a given couple of regions \((i,j)\) depends linearly on a single parameter \(\beta_{ij}\). By fitting it to the reference distribution, which is the total invariant mass distribution before bias \(M^{true}\), \(\beta_{ij}\) and its error \(\sigma_{ij}\) are determined.

Such simple fits can be carried on even analytically, if \(M^{true}\) is parametrized by a simple function. Here, \(M^{true}\) has been considered as a probability density function (pdf), and, scanning the \(\beta_{ij}\) domain, a likelihood maximization is done and \(\beta_{ij}\) and \(\sigma_{ij}\) are easily extracted. To avoid unwanted local fluctuations in the pdf, the pdf histogram has been smoothed using multi-quadratic technique.

This method will rely, at least in the commissioning period, on a reference distribution coming from simulation, without any \(a\)-\(priori\) on its analytical form. No complicated convolution of the Breit-Wigner with a Gaussian or whatever detector effect function is necessary.

For a given couple of regions \((i,j)\), a likelihood is computed for each \(\beta_{ij}\) value:

\[-\ln L=\sum_{k=1}^{N_{ij}}-\ln pdf[\frac{M_{k}}{1+\frac{\beta_{ij}}{2}}] \tag{5}\]Where \(k\) runs on events, \(N_{ij}\) is the number of events in couple \((i,j)\) and \(M_{k}\) is the biased invariant mass of event \(k\).

Depending on statistics and number of regions, we have a large number of filled \((i,j)\) couples (several thousands for \(100k\) events and \(400\) regions). To recover the \(\alpha_{i}\) biases, and given the simple relation \(\beta_{ij}=\alpha_{i}+\alpha_{j}\), least squares method is applied:

\[A=U^{-1}\otimes B \tag{6}\]

Where

\[A(i)=\alpha_{i}, \tag{7}\]

\[B(i)=\sum_{j,\ \mathrm{if(i,j)}\ni}\frac{\beta_{ij}}{\sigma_{ij}^{2}}, \tag{8}\]

and

\[U(i,j) =0\ \ \mathrm{if\ (i,j)\ is\ empty} \tag{9}\] \[=\frac{1}{\sigma_{ij}^{2}}\ \ \mathrm{if\ i\neq j}\] (10) \[=\sum_{k,\ \mathrm{if(i,k)}\ni}\frac{1}{\sigma_{ik}^{2}}\ \ \mathrm{if\ i=j} \tag{11}\]

The errors on \(\alpha_{i}\)'s are easily computed:

\[\sigma_{i}=\sqrt{U^{-1}(i,i)} \tag{12}\]

To apply this simplest version of the least squares method, the \(\beta_{ij}\) should be uncorrelated. One may suspect a strong correlation between \(\beta_{ij}\) and \(\beta_{ik}\) since they have region \(i\) in common. On the other hand, these two parameters will be determined by two different and independent sets of events. Region \(i\) is then "partially measured" twice, by two independent measurements which will improve our knowledge of region \(i\), without introducing correlation.

This is shown in the next section.

In practice, each region is associated to much more than 2 other regions, giving us redundancy. One may be temptated to reject couples with low statistics. Detailled studies have shown that low statistics couples associate regions which are not back-to-back in azimuth. Rejecting them would result in an under-determined system and would lead to a singular matrix \(U\).

A second iteration has been tried, in order to improve the method beyond the linear approximation of equation (3). The true multiplicative factor \(\sqrt{(1+\alpha_{i})(1+\alpha_{j})}\) has been developped in Taylor series around the solution of the first iteration, keeping it linear, and a second least squares determination has been made. We found that the forseen statistics were insensitive to such an improvement, and decided to limit the method to the linear approximation.

The method is based on the assumption that, independently from the considered region, the \(Z^{0}\) invariant mass follows the same distribution. However, while dealing with events reconstructed in a detector, minimum transverse energy cuts are applied on particles, and events are selected inside a window around the \(Z^{0}\) mass. For some couple of regions, \(E_{t}\) cuts resulted in an implicit cut on the \(Z^{0}\) mass \(M_{min}\), which is higher than the lowest end of the used window. Such couples (one region at high positive and the other at high negative \(\eta\)) could be rejected without degrading the method performance. The redundancy is such that even if we keep them, no major effect on the \(\alpha\) determination has been observed.

As explained in the introduction, this method will be applied at a level where we are confident in the knowledge of material in front of the calorimeter. The reference distribution from simulation shall reproduce the photon radiation rate and kinematics, thus minimizing the impact while fitting real data to it. The absolute electromagnetic scale at \(M_{Z}\) can be derived, but in addition to external bremsstrahlung, we have also to study not only the effects of the internal one, but also effects of underlying event, pile-up,...which is beyond the scope of this note.

## 3 Correlation studies

To study correlations between the \(\beta\)'s, \(10^{6}\)\(p\)\(p\to Z^{0}\to e^{+}e^{-}\) Pythia events have been generated. The electron energies have been biased randomly with a \(10\%/\sqrt{E}\) width to simulate the detector effect.

These events were divided into 100 sets of \(10^{4}\) events each. Events were selected if both electrons had \(|\eta|<2.4\) and \(P_{t}>18GeV\), and if \(80<M_{e^{+}e^{-}}<100\ GeV\), leaving about 8000 events in each set.

The considered acceptance was divided into 24 regions (\(\Delta\eta\times\Delta\phi=0.8\times 1.57\)). The 24 bias constants were generated and the same were used for all the sets. The method was then applied to the 100 sets.

Figure 1 shows the \(\beta\)'s and \(\alpha\)'s pull distribution for one set. The pull variance is compatible with 1, which is a necessary condition, but not a sufficient one, for the \(\beta\)'s and the \(\alpha\)'s to be uncorrelated.

Coefficient of correlation between \(\beta_{ij}\) and \(\beta_{kl}\) are given by:

\[\rho_{(i,j)-(k,l)}=\frac{1}{N-1}\sum_{n=1}^{N}P^{n}_{(i,j)}P^{n}_{(k,l)}=\frac {1}{N-1}\sum_{n=1}^{N}\frac{(\beta_{ij}^{n}-<\beta_{ij}>)(\beta_{kl}^{n}-< \beta_{kl}>)}{\sigma_{ij}^{n}\sigma_{kl}^{n}} \tag{13}\]

Where \(n\) runs on data sets, \(N=100\) is the number of data sets, \(<\beta_{ij}>\) is the mean value of the \(N\) fitted \(\beta_{ij}^{n}\), \(\sigma_{ij}^{n}\) being their errors.

Figure 2 shows the \(P_{(i,j)}P_{(k,l)}\) distribution for 3 examples of combinations of couples. The autocorrelation is obviously 1, while the correlation obtained for the two others, even with a region in common is very low and compatible with 0.

Autocorrelation and correlations between combinations sharing or not one region, for all the possible combinations of couples are displayed in figure 3. Correlation factors are peacked on 0. Examples of highest observed correlations are shown in figure 4. Even those are compatible with 0. The \(\beta\)'s are uncorrelated and the simple least squares method is allowed.

## 4 Extensive tests

The same \(10^{6}\) Pythia events have been used, in a single set, to perform extensive tests with high statistics. Events have been selected with the same cuts as in the previous section. Inside the same acceptance, we have 384 \(\Delta\eta\times\Delta\phi=0.2\times 0.4\) regions. The number of non-empty couples of regions was 12368, going to 11660 when rejecting couples where \(M_{min}>80GeV\). An example of a single distribution before and after biasing, and after recovery with this method is shown in figure 5, together with the total reference distribution.

Figure 6 shows two examples of likelihood maximization, for a low and a high statistics couple. To avoid binning effects, errors have been extracted after parabolic smoothing.

The \(\beta\) error dependence on couple statistics is shown in figure 7, and we see that it nicely scales as \(1/\sqrt{N}\). Depending if the regions are back-to-back in azimuth or not, the couple will fall in the high or low statistics population. The \(\alpha\) error dependence on region statistics is shown on the same figure.

Several robustness tests were performed and no abnormal behavior was observed (\(\sigma_{bias}\) variations, dead zones simulation, higher \(P_{t}\) cut...). Figure 8 shows that the \(\beta\) pull is independent from the likelihood value, statistics in couple and injected \(\beta\).

Pulls for both \(\beta\)'s and \(\alpha\)'s are plotted in figure 9, while the \(\alpha\) residuals, and their correlation with the injected \(\alpha\)'s are displayed in figure 10. The precision with \(800k\) selected events is 0.14%.

If we use statistics corresponding to two days of data taking at low luminosity, (about 120k events after selection), figure 11 shows that the goal of a 0.3% global constant term is fulfilled, althought these are smeared Pythia events. Full simulation data results will be shown in next section.

## 5 Application to DC1 events

About \(184k\)\(p\)\(p\to Z^{0}\to e^{+}e^{-}\) events have been generated with Pythia and passed through the complete ATLAS detector simulation. Electrons were identified and \(Z^{0}\to e^{+}e^{-}\) events were selected using the same cuts as in [2]. Both leptons were required to have \(P_{t}>20GeV\) and their invariant mass was asked to be inside a 80-100 GeV window. This window has been enlarged by 10 GeV on both ends, to avoid event leak, while scanning the \(\beta\) domain. Such leaks result in sudden variations of the likelihood, and prevent a good maximization.

Simulation problems, in particular in the end-cap regions, prevented us to apply this method in the full acceptance of the ATLAS electromagnetic calorimeter.

This can be seen in figure 12: 200 regions have been defined for \(|\eta|<2.5\). Each region corresponds to an azimuthal ring of one single pseudorapidity cell in the second longitudinal sampling (\(\Delta\eta=0.025\)). The method was applied without injecting biases (\(\alpha_{i}=0\) for all \(i\)). Despite the use of a calibration derived from single electron production [4], we clearly see effects of lead thickness change in the barrel at \(\eta=0.8\), and unexpected effects in the end-caps and in the cracks.

The method has been run only on events having both leptons inside the first thickness of the lead in the barrel, \(|\eta|<0.8\). This corresponds to 128 \(\Delta\eta\times\Delta\phi=0.2\times 0.4\) regions. \(\sigma_{bias}\) was 3 %. \(28k\) events survived the cuts. Figure 13 shows that the bias could be recovered with a precision of 0.4%. Such statistics is equivalent to \(100k\) events with 448 regions, and thus, this result could be extrapolated to 0.3% precision with \(150k\) events. Doing such an extrapolation, one may not forget that events in \(|\eta|<0.8\) are better reconstructed than in all the other regions, and thus, certainly few more events will be needed to achieve 0.3%.

Nevertheless, the use of DC1 data showed that this method will satisfy the ATLAS requirements on the global constant term within two to three days of data taking.

## 6 Conclusions

A method has been developped to achieve a global constant term of 0.3%, using the \(p\)\(p\to Z^{0}\to e^{+}e^{-}\) events. This method is based on several thousands of simple single parameter fits, followed by least squares matrix inversion. It has been tested on pure Pythia and fully ATLAS simulated events.

Next step would be to test this method in a more realistic situation, by adding noise, pile-up and irreducible background (mainly \(\gamma\to e^{+}e^{-}\)) to the simulated events. Realistic scenario of bias coefficients (temperature gradiant, mechanical deformation, impurities in liquid argon...) have to be simulated and tested.

This method could be used also to investigate material quantity, by giving 2 weights for each region, one for the first sampling and the second for the sum of the two others. Their ratio should be sensitive to the material quantity. The method would became complicated, du to the obvious correlation between the samplings.

Acknowledgements: I wish to thank M. Boonekamp and J.B. Hansen for their help and support.

**References**

## References

* [1] ATLAS Calorimeter Performance TDR, CERN/LHCC/96-40.
* [2] ATLAS Detector and Physics Performance TDR, CERN/LHCC/99-14.
* [3] A. Amorim, L. Poggioli, A. Maio, ATL-PHYS-93-015, ATL-GE-PN-15.
* [4] L. Carminati, EM Calibration, Talk given at the ATLAS Physics Workshop, Athens 2003.