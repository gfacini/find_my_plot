Eigenvector recomposition: a new method to correlate flavor-tagging systematic uncertainties across analyses

The ATLAS Collaboration

###### Abstract

In order to simplify the treatment of the flavor-tagging scale factor uncertainties in physics analyses, their large number is currently significantly reduced using an _eigenvector decomposition_ approach that preserves both the total size of the uncertainty and the underlying correlations. This method provides an effective way to reduce the number of uncertainties, while keeping the correlation information for further use in the analyses. However, when combining physics analyses with different flavour tagging setups - i.e different taggers, working points, or jet collections - the flavour tagging eigenvectors are in general not the same, so the uncertainties can not be directly correlated. This note introduces the _eigenvector recomposition_ method to overcome this problem and enable the correlation of flavor-tagging systematic uncertainties across different setups. The tool is designed to transform the eigenvectors into the original set of flavor-tagging scale factor uncertainties, which can be safely correlated across different analyses. This note describes the method and gives practical examples about its usage in physics analyses, focusing on the \(VH,H\to b\overline{b}\) analysis case.

## 1 Introduction

The identification of jets containing \(b\)-hadrons against \(c\)- and _light_-flavored jets is a core element of many physics analyses in ATLAS, such as the study of Higgs boson decays into a pair of \(b\)-quarks produced in association with a vector boson (also referred to as \(VH,H\to b\overline{b}\) in the text) [1]. The sequence of algorithms used to identify these jets is known as \(b\)-tagging. The procedure is based on multivariate algorithms relying on specific kinematic properties of the \(b\)-hadron decays (i.e. long lifetime, high mass, high decay multiplicity); more details can be found in Ref. [2]. The \(b\)-tagging algorithms are applied independently on each jet in the event. In particular, the performance of a \(b\)-tagging algorithm is quantified by its probability to tag a real \(b\)-jet, also referred to as \(b\)-tagging efficiency \(\epsilon_{b}\), against the probability to wrongly tag (mis-identify) \(c\)- and _light_-flavored jets as \(b\)-jets, defined as \(\epsilon_{\mathrm{c}}\) and \(\epsilon_{\mathrm{light}}\) respectively.

The flavor-tagging-efficiency \(\epsilon_{f}\) for a given jet flavor \(f\) is defined as the probability for such jets to be \(b\)-tagged. It is estimated using

\[\epsilon_{f}=\frac{N_{f}^{b-\mathrm{tag}}}{N_{f}}, \tag{1}\]

where \(N_{f}^{b-\mathrm{tag}}\) is the number \(b\)-tagged jets of flavor \(f\), and \(N_{f}\) is the total number of jets of flavor \(f\). The rejection is defined as \(1/\epsilon_{f}\)[3]. The efficiency is usually parameterized as a function of the transverse momentum \(p_{\mathrm{T}}\) of the tagged jet. For simplicity, this is the only kinematic variable used in this note but the method can be easily extended to multiple kinematic dependencies (e.g. \(\eta\)). In case of multiple kinematic dependencies, the N-dimensions can be mapped into a 1-dimensional index identifying a specific bin in the N-dimensional space. The efficiencies are measured in data in well defined control samples (\(\epsilon_{\mathrm{data}}(p_{\mathrm{T}})\)), and compared to the corresponding efficiencies predicted by the Monte Carlo (MC) simulation (\(\epsilon_{\mathrm{MC}}(p_{\mathrm{T}})\))1. The MC efficiency is calibrated to match the efficiency in data using dedicated calibration analyses described for example in Ref. [3]. Scale factors (SF) are derived as ratios of efficiencies in data and simulation. In particular, the scale factors are measured separately for each flavor and in bins of jet transverse momentum (and jet pseudorapidity for _light_-jets) for each combination of algorithm and working point. The working point is defined as a particular threshold of the \(b\)-tagging output discriminant above which the jet is considered as \(b\)-tagged. It can be tuned to yield a tighter or looser selection based on the \(b\)-jet tagging efficiency. Currently working points are defined corresponding to \(b\)-jet tagging efficiencies of roughly 85%, 77%, 70% and 60%, as determined inclusively on a Monte Carlo sample of \(t\overline{t}\) events before any data-based calibration is applied. Different methods are used to calibrate the tagging efficiencies of \(b\)-, \(c\)- and _light_-flavored jets [4]. The resulting scale factors are applied, jet-by-jet, as multiplicative event weights to any simulated physics sample where \(b\)-tagging is applied during event reconstruction and selection. The scale factors are defined as:

Footnote 1: The MC efficiency estimates for each of the flavors is provided by the full simulation of the ATLAS detector based on Geant4.

\[\mathrm{SF}(p_{\mathrm{T}})=\frac{\epsilon_{\mathrm{data}}}{\epsilon_{\mathrm{ MC}}}(p_{\mathrm{T}}) \tag{2}\]

In case of discrete \(p_{\mathrm{T}}\) bins, the SF becomes:

\[\mathrm{SF}_{i}=\frac{\epsilon_{\mathrm{data},i}}{\epsilon_{\mathrm{MC},i}} \tag{3}\]

where \(i\) is the index of the \(p_{\mathrm{T}}\) bins, \(i\in\{1,...,N\}\), and \(N\) is the total number of \(p_{\mathrm{T}}\) bins. For every \(b\)-tagged jet passing the \(b\)-tagging requirement, the SF is applied directly as an event weight. In order to conservethe overall normalization - so \(p\)(tagged jet) + \(p\)(non-tagged jet) = 1 - for every \(b\)-tagged jet which does not pass \(b\)-tagging requirement (non-tagged jet), an inefficiency SF is applied:

\[\mathrm{SF}_{\mathrm{ineff}}=\frac{1-\mathrm{SF}(p_{\mathrm{T}})\cdot\epsilon_{ \mathrm{MC}}(p_{\mathrm{T}})}{1-\epsilon_{\mathrm{MC}}(p_{\mathrm{T}})}. \tag{4}\]

In many physics analyses, the distribution of the \(b\)-tagging output discriminant is different for the signal and the different backgrounds and can therefore be used to enhance the analysis sensitivity. In order to correctly use this binned distribution, rather than just the tagged/non-tagged binary information, the distribution needs to be calibrated in each bin. The so-called "pseudo-continuous" calibration simultaneously calibrates a binned version of the continuous output values of the \(b\)-tagging discriminant, expressed in intervals of the \(b\)-tagging efficiency2. It thus considers the dependence of the scale factors both on \(p_{\mathrm{T}}\) and on the \(b\)-tagging score. This calibration gives more freedom to the analyses when developing their \(b\)-jet selection. For example, jets in the same event can be selected with \(b\)-tagging requirements corresponding to different efficiency working points to increase the signal over background ratio of the analysis. This is, however, a more complex procedure from the calibration point of view, as the scale factors are derived as a function of both jet kinematics and simultaneously for all \(b\)-tagging bins. In the pseudo-continuous case no inefficiency SFs are applied as the SFs are already extracted as a function of the binned \(b\)-tagging score, while simultaneously applying a constraint that the total number of jets of a given flavor in each jet \(p_{\mathrm{T}}\) bin must be preserved.

Footnote 2: Concisely, each bin of the \(b\)-tagging output score distribution will be referred to as \(b\)_-tagging score bin_ in the text.

The total uncertainty on the scale factors is calculated considering both statistical and systematic uncertainties on the measured and simulated efficiencies entering in the calibration analyses. For example, about \(\sim\)100 separate uncertainty sources are considered for the \(b\)-jet SFs. In this note, these uncertainties will be referred to as original \(b\)-tagging uncertainty sources.

As described in section 2.1, an eigenvector decomposition method is used to reduce the number of flavor-tagging uncertainty components in the analyses while correctly preserving correlations across jet kinematic bins, and - for the pseudo-continuous WP calibration - also across \(b\)-tagging score bins. In brief, the set of original \(b\)-tagging uncertainty sources (\(\sim\)100 for \(b\)-jets) is decomposed and replaced by a smaller set of eigenvector uncertainties [3], which are then used in physics analyses. The eigenvectors are strictly related to the diagonalization of a specific covariance matrix, so when combining different analyses, the eigenvectors computed starting from different flavor-tagging setups (i.e. \(b\)-tagging algorithm, working point, jet collection) can not be directly correlated, since they correspond to different combinations of the same sources of uncertainties. A similar procedure is also used in ATLAS to reduce the number of jet calibration uncertainties, as described in Ref. [5].

This note introduces a new method to correlate the \(b\)-tagging calibration uncertainties across analyses using different flavor-tagging setups. The method is encoded into a tool that takes the likelihood function of a given analysis as input, and replaces all the flavor-tagging related nuisance parameters corresponding to the eigenvector uncertainty components with new nuisance parameters corresponding to the complete set of _original_\(b\)-tagging uncertainty sources. In this way, the likelihood functions of multiple analyses can be easily re-expressed in terms of the same original \(b\)-tagging uncertainty sources, making their correlation possible. This is very useful for example in combination analyses, such as the combination of all Higgs boson measurements. An additional feature naturally emerging from this approach is the possibility to correlate the recomposed original uncertainty sources with the analogous uncertainties affecting the physics analyses. For example, the recomposed jet energy calibration uncertainty components from the flavor-tagging calibration could be correlated across different calibration setups but also with the same jet calibration components of the physics analyses being combined. This additional feature will not be further discussed in the text as it strongly depends on the meaning of the systematic uncertainties in the input analyses being combined. As an additional simplification, only \(b\)-jet uncertainties will be considered in the following sections of the note, but the method is applicable to other flavors as well.

In a similar context, a different technique has been developed in ATLAS in the jet energy scale (JES) calibration domain to allow for the correlation of the leading JES uncertainties across different experiments, jet types and data taking years [6], and a concrete application of this technique can be found in the context of the \(H\to b\overline{b}\) analysis, as reported in Ref. [7].

### Correlation of uncertainties between different \(b\)-tagging setups

In order to better understand the impact of the recomposition tool when combining multiple physics analyses, one important step consists in quantifying the level of correlation between the uncertainties coming from different \(b\)-tagging setups. The larger the correlations, the more the uncertainties need to be carefully treated in combination to avoid biases in the final result. As an example, this section focuses on quantifying the correlation of the total \(b\)-jet uncertainties between two analyses using the same tagger and working point, but different jet collections. This particular example uses Variable Radius (VR) track jets [8] and Particle Flow (PF) jets [9]. The correlation between uncertainties coming from different setups can be assessed through the correlation matrix:

\[\rho_{ij}=\frac{\sum_{k=1}^{M}u_{ik}^{\text{PF}}\cdot u_{jk}^{\text{VR}}}{\sqrt {V_{i}^{\text{PF}}\cdot V_{j}^{\text{VR}}}} \tag{5}\]

where the indices \(i\), \(j\) are the kinematic bin indices of the two flavor-tagging setups, \(k\) is the index running over the original \(b\)-tagging uncertainty sources and \(u_{ik}^{\text{PF}}\) is the effect of the uncertainty on the SF in the kinematic bin \(i\) for the PF setup. The total number of original uncertainty sources common to both flavor-tagging setups is denoted by \(M\), while \(V_{i}^{\text{PF}}\) in the denominator is the total SF variance in the kinematic bin \(i\):

\[V_{i}^{\text{PF}}=\sum_{k=1}^{M}(u_{ik}^{\text{PF}})^{2} \tag{6}\]

The analogous terms for the VR setup are \(u_{ik}^{\text{VR}}\) and \(V_{i}^{\text{VR}}\). Figure 1 shows the correlation of the total \(b\)-tagging SF uncertainties between two different jet collections: Variable Radius Track Jets and Particle Flow Jets, using the DL1r tagger at the 70% working point. The correlations can be as high as 50%, so a proper correlation of the \(b\)-tagging uncertainties between analyses using these two different jet collections is important. The recomposition method introduced in this note enables physics analyses to account for such correlations, e.g. in the context of modeling or tracking uncertainty sources.

### Brief introduction to the \(VH,H\to b\overline{b}\) analysis

Most of the cross-checks presented in the note have been performed on the \(VH,H\to b\overline{b}\) analysis. This section will thus introduce the analysis and summarise the key points needed to understand the following sections.

The analysis aims at studying the Standard Model Higgs boson decay to a pair of bottom quarks, also referred to as H\(\to b\overline{b}\), which is expected to happen with a branching ratio of about 58%. The search is carried out selecting the Higgs boson produced in association with a leptonically decaying vector boson (\(VH\)) to ensure high background rejection thanks to the additional leptonic signatures in the final state. The analysis version used in this note refers to the full Run-2 analysis, carried out on 13 TeV data collected with the ATLAS detector between 2015 and 2018 [1].

The \(VH,H\to b\overline{b}\) signatures are categorized by the decays of the vector bosons (\(Z\) or \(W\)) into: \(ZH\to\nu\nu bb\), \(WH\to\ell\nu bb\) and \(ZH\to\ell\ell bb\). Depending on the number of charged leptons in the final state, the events are thus referred to as 0-, 1- and 2-lepton channels. The main backgrounds are \(t\overline{t}\), \(Z\)+jets, \(W\)+jets (also referred to as \(V\)+jets in the text) and single top-quark production. The Higgs boson candidate is reconstructed from exactly two \(b\)-tagged small-radius calorimeter jets, using the MV2c10 \(b\)-tagging algorithm at the 70% working point.

A multivariate analysis (MVA) making use of Boosted Decision Trees (BDTs) is implemented to separate the Higgs boson signal from the backgrounds. Different multivariate discriminants are reconstructed, trained and evaluated in each lepton channel and signal region separately. The input variables used for the BDTs are chosen to maximise the separation power. One of the leading discriminating variables is the

Figure 1: Correlation between the total \(b\)-tagging SF uncertainty for VR Track Jets and PFlow Jets, both tagged using the DL1r algorithm at the 70% Working Point.

invariant mass of the di-jet system, also abbreviated to \(m_{bb}\). This variable will appear later in the note, as it has been used to validate the recomposition tool3.

Footnote 3: An \(m_{bb}\)-based analysis is used to validate the nominal MVA analysis, since \(m_{bb}\) provides a more straightforward physical interpretation of the result.

The events are categorised in Signal Regions (SRs) and Control Regions (CRs). The CRs are designed to control the normalisation of the main backgrounds (top and \(V\)+jets) in the fit. The results are obtained from a simultaneous likelihood fit that combines together all channels and analysis regions. The BDT classifier score is used as fitting variable. All the cross-checks shown in section 4 and 5 have been performed taking the BDT distribution of the 1-Lepton SR as an example.

## 2 The eigenvector recomposition method

This section will first show a brief overview of the eigenvector (EV) _decomposition_ method, currently used to reduce the number of flavor-tagging related uncertainty components in physics analyses. Then, the second part will describe the eigenvector _recomposition_ method.

### Brief overview of the eigenvector _decomposition_ method

As summarised in the introduction, the scale factors measurement is affected by \(M\) sources of statistical and systematic uncertainty sources, typically around \(\mathcal{O}(100)\) for each jet-flavor calibration analysis. Each source of uncertainty \(k\in\{1,...,\,M\}\) is assumed to have a correlated effect on all \(p_{\mathrm{T}}\) bins and, generalising to the pseudo-continuous case, on different \(b\)-tagging score bins. Partially correlated or uncorrelated uncertainty sources such as the statistical uncertainties in the calibration analysis 4 are treated by decomposing them into multiple correlated uncertainty components. For the \(b\)-jet and \(c\)-jet calibrations, the main sources of systematic uncertainty are related to the MC modeling, the jet energy scales and the statistical uncertainties (the latter for both data and simulation). From here the covariance matrix \(\mathrm{V}_{ij}\) of the scale factors between two different \(p_{\mathrm{T}}\) bins (\(i\),\(j\in\{1,...,\,N\}\)) is given by:

Footnote 4: Most calibration analyses do a simultaneous fit of the scale factors across kinematic bins, and events do typically contain more than one jet, thus statistical uncertainties from such calibration analyses turn out to be partially correlated and partially uncorrelated across bins.

\[V_{ij}=\sum_{k=1}^{M}u_{ik}\,u_{jk}, \tag{7}\]

where \(i\) and \(j\) are \(p_{\mathrm{T}}\)-bin indices, \(k\) denotes the uncertainty component and \(u\) the bin-dependent SF uncertainty amplitude. The mathematical description of how to derive this covariance matrix can be found in Appendix A.

Note that, as shown in Eq. 6, the diagonal element \(V_{ii}\) of the covariance matrix is equal to the total uncertainty squared. The covariance matrix is in general not diagonal, which means that the scale factor uncertainty sources are correlated across different kinematic bins [5]. Being symmetric and positive-definite, the total covariance matrix can be diagonalized, so as to provide orthogonal uncorrelated uncertainty components through its eigenvectors. After the diagonalization, the covariance matrix contains the eigenvalues \(\lambda_{i}^{2}\):\[\text{V}\xrightarrow{\text{Diagonalize}}\begin{pmatrix}\lambda_{1}^{2}&&&&\\ &\lambda_{2}^{2}&&\\ &&\ddots&\\ &&&\lambda_{N}^{2}\end{pmatrix} \tag{8}\]

There are in total \(N\) eigenvectors which could be used to construct a matrix with element \(v_{ik}\), one for each dimension of the covariance matrix. The eigenvector uncertainty can be calculated by multiplying the eigenvector with the corresponding eigenvalue. To be specific, the \(i^{th}\)\(p_{\text{T}}\) bin of \(k^{th}\) eigenvector uncertainty is:

\[\tilde{u}_{ik}=\lambda_{k}v_{ik}. \tag{9}\]

The variance \(V_{i}\) in the \(i^{th}\) kinematic bin should be exactly the same in the two bases, and can thus be written as:

\[V_{i}=\sum_{k=1}^{N}(\tilde{u}_{ik})^{2}=\sum_{k=1}^{M}(u_{ik})^{2} \tag{10}\]

After discarding the negligible eigenvector uncertainties (smaller than \(10^{-20}\) for all kinematic bins) arising from the smoothing procedure described in Ref. [4], one ends up with a set of \(L\) eigenvectors, with \(L\leq N\). Since normally \(L\ll M\) this leads to a great reduction in the number of uncertainty components. These \(L\) eigenvectors, derived separately for each tagging setup (combination of algorithm and working point), are used as uncorrelated systematic uncertainty components in the physics analyses. When applying the eigenvector decomposition method to the pseudo-continuous calibration, no change is required in the formalism, apart from the index \(i\) which is generalized to all combinations of jet kinematic bins and \(b\)-tagging score bins considered in the calibration.

### Mathematical description of the eigenvector _recomposition_

Using the results of the EV decomposition, described in the previous section 2.1, the original \(b\)-tagging calibration uncertainty sources \(\lambda_{i}\) are expressed in the basis defined by the eigenvectors. The total SF variation in each \(p_{\text{T}}\) bin \(i\), referred to as \(T_{i}\), can be expressed as:

\[T_{i}=\sum_{k=1}^{L}\theta_{k}\lambda_{k}v_{ik}=\sum_{k=1}^{L}\theta_{k}\tilde {u}_{ik} \tag{11}\]

where \(k\in\{1,...,L\}\), is the eigenvector index and \(\theta_{k}\) are the nuisance parameters used in the analysis.

At the same time, \(T_{i}\) can also be expressed as a function of the _original_ sources of systematic uncertainties:

\[T_{i}=\sum_{j=1}^{M}\eta_{j}u_{ij} \tag{12}\]

where \(\eta_{j}\) is one of the \(M\) nuisance parameters representing the original sources of systematic uncertainties.

Combining (11) and (12) and rewriting it in matrix form:

\[\begin{pmatrix}\eta_{1}&...&\eta_{M}\end{pmatrix}\begin{pmatrix}u_{1,1}&...&u_{N,1}\\ \vdots&\ddots&\vdots\\ u_{1,M}&...&u_{N,M}\end{pmatrix}=\begin{pmatrix}\theta_{1}&...&\theta_{L}\end{pmatrix} \begin{pmatrix}\lambda_{1}v_{1,1}&...&\lambda_{1}v_{N,1}\\ \vdots&\ddots&\vdots\\ \lambda_{L}v_{1,L}&...&\lambda_{L}v_{N,L}\end{pmatrix} \tag{13}\]

Note that the left hand side is not exactly equal to the right hand side since some negligible eigenvector uncertainty components might be discarded. However the discarded uncertainties are too small (\(<10^{-20}\) in all kinematic bins) to be visible in any of the tests performed in this note, the two sides of equation above can be treated as equal. The previous equality can be written shortly as:

\[\vec{\eta}\cdot U=\vec{\theta}\cdot\tilde{U} \tag{14}\]

One can see that the non-square matrix \(\tilde{U}\) in Eq. 13 has dimension \(L\times N\). Each row of matrix \(\tilde{U}\) is an eigenvector times eigenvalue. Therefore, \(\tilde{U}\) is a matrix with rank \(L\). In this case, it is known that the right inverse \(\tilde{U}^{-1}_{\text{right}}=\tilde{U}^{T}(\tilde{U}\tilde{U}^{T})^{-1}\) is a pseudo-inverse (Moore-Penrose inverse) of \(\tilde{U}\) and therefore must be unique, thus multiplying the expression above by \(\tilde{U}^{-1}_{\text{right}}\) leads to an expression for \(\vec{\theta}\) in terms of \(\vec{\eta}\):

\[\vec{\theta}=\vec{\eta}\cdot U\cdot\tilde{U}^{-1}_{\text{right}} \tag{15}\]

One can hence define a dimension \(M\times L\) matrix:

\[\Re\equiv U\,\tilde{U}^{-1}_{\text{right}} \tag{16}\]

that translates the uncertainties between the original set and the eigenvector decomposed uncertainty set.

Most physics analyses using \(b\)-tagging in ATLAS rely on a maximum-likelihood fit to extract their parameters of interest, with the diagonalized \(b\)-tagging uncertainties encoded in the nuisance parameters \(\theta\). The corresponding likelihood function is:

\[\mathcal{L}=L(\vec{\mu},\theta_{1},\theta_{2},...,\theta_{L})P(\theta_{1})P( \theta_{2})...P(\theta_{L}), \tag{17}\]

where \(\vec{\mu}\) is the vector of the parameters of interest, for example the Higgs boson production signal strengths, and the \(P(\theta_{k})\) represent the Gaussian priors centered at zero and with variance set to unity which constrain the values of the corresponding components of the flavor-tagging systematic uncertainty encoded as eigenvectors.

The aim of the eigenvector recomposition approach is to transform the likelihood function in Eq. 17, back into the corresponding likelihood function where the \(b\)-tagging uncertainty components are encoded into the nuisance parameters \(\eta\) of the original sources of systematic uncertainties:

\[\mathcal{L}=L(\vec{\mu},\eta_{1},\eta_{2},...,\eta_{M})P(\eta_{1})P(\eta_{2})...P(\eta_{M}), \tag{18}\]

where the \(P(\eta_{k})\) again represent Gaussian priors.

The nuisance parameters \(\theta\) (after diagonalization) and the nuisance parameters \(\eta\) (before diagonalization) represent the exact same uncertainties in the space of the \(b\)-tagging scale factors, i.e. the same central valuesand same covariance matrix. The results of the two maximum-likelihood fits for Eq. 17 and Eq. 18 will thus be identical (within numerical accuracy) in terms of parameters of interest (POIs) and any nuisance parameters (NPs) unrelated to \(b\)-tagging. However, since the space of the \(\theta\) parameters is lower-dimensional than the space of the \(\eta\) parameters, a transformation from the \(\theta\) to the \(\eta\) space of parameters requires a replacement of the (lower-dimensional) prior constraints in \(\theta\) with the complete set of prior constraints in the \(\eta\). The latter will include not only the directions in the \(\eta\) space already spanned by variations of the \(\theta\) parameters, but also additional directions that keep the \(b\)-tagging scale factors invariant and to which the analysis likelihood is thus insensitive. The identity between the two likelihood functions and the procedure to transform one into the other is demonstrated and justified in more detail in Appendix B.

In order to modify Eq. 17 to make it identical to Eq. 18 the eigenvector recomposition tool proceeds through the following steps:

1. The priors \(P(\theta_{1})P(\theta_{2})...P(\theta_{L})\) are removed;
2. The variables \(\vec{\theta}\) are expressed as a function of \(\vec{\eta}\), by using Eq. 15, thus effectively replacing the \(\vec{\theta}\) with the \(\vec{\eta}\);
3. The original priors \(P(\eta_{1})P(\eta_{2})...P(\eta_{M})\) are added back.

With these substitutions, the analysis likelihood function becomes equivalent to Eq. 18, in the sense that they will both converge towards the same postfit SFs and the same postfit impact of the \(b\)-tagging systematic uncertainties. Its \(b\)-tagging nuisance parameters will now be encoded again in the \(\vec{\eta}\), representing the original systematic variations. It should be noted that Eq. 18 is the likelihood function one would have obtained without applying the eigenvector decomposition (diagonalization) method in first place. One notable feature of this procedure is that it is agnostic with respect to the observable used in the analysis and the detailed structure of the likelihood function, which means the method can be applied without specific and detailed knowledge of a particular analysis.

#### 2.2.1 Treatment of the statistical uncertainties in the scale factors

The recomposition tool is designed to recompose both the systematic and the statistical components of the total calibration uncertainty. However after they have both been recomposed, at the moment, only the systematic components can be correctly correlated across different calibrations. The correlation between the statistical NPs across different setups is not straightforward to evaluate, since it requires taking the event overlap between the different calibrations into account, e.g. through direct evaluation or through bootstrap techniques, see Ref. [10] and references therein for more details. Thus, missing a dedicated study, it is only possible to correlate the _original_ uncertainty sources that have a systematic origin. One exception to this is the case of the \(b\)-jet calibrations differing from each other only by their working points, with the events used for the tighter WP calibration being a subset of the looser one. Since these are currently all derived from the more generic pseudo-continuous calibration, common nuisance parameters among different working points are already used, thus describing in a coherent way the impact of statistical uncertainties on common/orthogonal events in the calibration analysis. In a similar way, the treatment of the statistical uncertainties could be improved in the future also for setups with different \(b\)-tagging algorithms or jet collections.

The impact of this approximation is expected to be small as in the current calibrations the systematic component is largely dominant over the statistical component, especially for the region of jet kinematics relevant for most physics analyses in ATLAS. In Figure 2 the systematic component is compared to the total uncertainty for the particle flow \(b\)-jet efficiency calibration at the 70% working point. The statistical component starts to be significant only for jet \(p_{\mathrm{T}}\) above 200 GeV, while in the bulk of the kinematic range the systematic component accounts for more than 90% of the total uncertainty, with a peak of 98% around 80-90 GeV.

## 3 Validation of the eigenvector recomposition

The method derived above and its implementation in the ATLAS software have been verified through multiple crosschecks and validation tests. This section summarizes the most relevant ones, performed based on the \(VH,H\to b\overline{b}\) analysis.

### Recomposed uncertainties

The procedure to modify the likelihood function introduced in section 2.2, and based on the relation between nuisance parameters expressed in Eq. 15, is in principle all what is needed to apply the recomposition method to the analysis likelihood function. However, in addition, one can also derive an equation that

Figure 2: The upper panel shows the total uncertainty compared to its systematic component as a function of \(p_{\mathrm{T}}\), for the particle flow \(b\)-jet calibration at the 70% working point. The bottom panel shows the ratio between the systematic component and the total uncertainty.

relates systematic variations for a generic analysis _observable_ in the space of eigenvector variations, to the same variations expressed in the space of the original sources of uncertainties. Here and throughout this note the impact of \(b\)-tagging uncertainties on the expected event yield (in each of the bins of a physical observable such as \(m_{bb}\)) is assumed to be linear, following the likelihood formalism used across most ATLAS analyses. To yield the best possible approximation of the full non-linear dependence, the variations of the yields (binned in such observables) are linearized with respect to variations of the underlying nuisance parameters around their \(\pm 1\sigma\) variations (see Section 4.1 in Ref [11]). As a consequence, the recomposition method can be applied to any analysis observable in the same way as to the underlying \(b\)-tagging scale factor variations.

Considering that a given kinematic observable has \(R\) different bins, the flavor-tagging-related variations of the event yield in each bin \(r\) can be expressed both in the eigenvector space (i.e. the SFs _diagonal_ space), or in the space of the original sources of SF uncertainties (i.e. the _original_ space):

\[b_{r}=\sum_{j=1}^{M}\eta_{j}\Delta y_{rj}=\sum_{k=1}^{L}\theta_{k}\Delta d_{rk} \tag{19}\]

where \(r\) is the index of the kinematic bin considered, with \(r\in\{1,...,R\}\), \(b_{r}\) is the total systematic variation for the given observable in bin \(i\), \(\Delta y_{rj}\) is the 1-sigma systematic variation corresponding to the \(j^{th}\) source of original uncertainty for the bin \(r\) which affects the yields in that bin, and \(\Delta d_{rk}\) represents the impact of the \(k^{th}\) eigenvector uncertainty component on bin \(r\).

Rewriting (19) in matrix form, one obtains:

\[\vec{\eta}\cdot Y=\vec{\theta}\cdot D \tag{20}\]

or more explicitly:

\[\begin{pmatrix}\eta_{1}&...&\eta_{M}\end{pmatrix}\begin{pmatrix}\Delta y_{1,1}&...&\Delta y_{R,1}\\ \vdots&\ddots&\vdots\\ \Delta y_{1,M}&...&\Delta y_{R,M}\end{pmatrix}=\begin{pmatrix}\theta_{1}&...& \theta_{L}\end{pmatrix}\begin{pmatrix}\Delta d_{1,1}&...&\Delta d_{R,1}\\ \vdots&\ddots&\vdots\\ \Delta d_{1,L}&...&\Delta d_{R,L}\end{pmatrix} \tag{21}\]

The goal here is to determine the matrix \(Y\). To obtain the matrix \(Y\), Eq. (15) can be plugged into Eq. (21) to get:

\[\vec{\eta}\cdot Y=\vec{\eta}\cdot U\cdot\tilde{U}^{-1}_{\rm right}\cdot D \tag{22}\]

This identity remains valid for any arbitrarily chosen set of values for the components of the \(\vec{\eta}\) vector. Therefore \(\vec{\eta}\) can be removed from both sides to get the recomposed original sources of uncertainty5:

Footnote 5: On the right hand side of Eq. 23, matrix \(U\) and \(\tilde{U}^{-1}_{\rm right}\) are derived from the standard ATLAS \(b\)-tagging calibration analyses. Matrix \(D\) refers to the \(b\)-tagging eigenvector variations relative to the analysis observable.

\[Y=U\cdot\tilde{U}^{-1}_{\rm right}\cdot D \tag{23}\]

### Comparison between _single_ recomposed uncertainties and original sources

This section aims at verifying if a _single_ uncertainty source recomposed using the recomposition method matches the _original_ uncertainty source before the decomposition and recomposition steps. As a concrete example, the impact of each original uncertainty source has been quantified on the \(VH,H\to b\overline{b}\) dijet-mass (\(m_{bb}\)) observable. In order to establish the reference for this check, the analysis has been re-run disabling the decomposition step. In this way the \(b\)-tagging systematic variations remain equal to the original sources, rather than being decomposed into flavor-tagging eigenvectors.

At the same time, each original uncertainty source can also be derived from the flavor-tagging eigenvectors using the recomposition method, by applying the \(Y\) matrix as in Eq. (23). A comparison of the derived uncertainty source and the original one (also referred to as "reference" in the figures) for the leading original uncertainty source of the \(b\)-jet calibration analysis - namely the Parton Shower (PS) modeling uncertainty6 - is shown in Figure 3 along with their ratio. The recomposed uncertainty source matches with the original reference uncertainty within 0.5% in all the considered bins7. These small non-closures have to be attributed not to the recomposition method, but to the assumption of linearity, i.e. they are due to the small differences observed between linearizing the relation between the \(b\)-tagging scale factor variations and the expected analysis yields (in bins of the \(m_{bb}\) observable) around the \(\pm 1\sigma\) variations of the eigenvector uncertainties, versus applying the same linearization around the \(\pm 1\sigma\) variations of the original uncertainty sources. It should be noted that a non-closure of this size on single uncertainties will have no appreciable impact on the physics analysis results.

Footnote 6: This source of uncertainty refers to the \(t\overline{t}\) Parton Shower modeling. It has been calculated comparing the nominal generator produced using PowhegBox v2 interfaced to Pythia 8.230 with an alternative sample generated using PowhegBox v2 interfaced to Herwig 7. More details are discussed in Ref. [4].

Footnote 7: Detailed studies discussed in section 5, show that this matching works only if no significant _pruning_ is applied to the eigenvector uncertainty components at analysis level. Pruning the uncertainty components is a widely spread practice in physics analyses and it consists in removing uncertainties, in this case the single eigenvectors, that have sub-leading effects, region by region. More details about the pruning procedure used in \(VH,H\to b\overline{b}\) can be found in Ref. [12]

### Comparison of _total_ eigenvector and recomposed uncertainties

Since the expressions of the uncertainties in the original space and in the SF diagonal space are equivalent, as related by Eq. (23), the total flavor tagging uncertainty should be preserved when computing the sum in quadrature of the set of uncertainties in the eigenvector space and in the space of the recomposed uncertainties. In a given bin \(r\) of the \(m_{bb}\) distribution, the sum in quadrature of the original uncertainty amplitudes (\(\Delta y_{rj}\)) and the sum in quadrature of the eigenvectors (\(\Delta d_{rl}\)), defined in the \(VH,H\to b\overline{b}\) analysis phase space, can be defined as follows:

\[\sqrt{\sum_{j=1}^{M}(\Delta y_{rj})^{2}}=\sqrt{\sum_{l=1}^{L}(\Delta d_{rl})^{ 2}} \tag{24}\]

Eq. (24) has been experimentally verified using the \(m_{bb}\) distribution, to see if the sum in quadrature of the recomposed uncertainty sources is equal to the sum in quadrature of the uncertainty components in the SF diagonal space.

The result of this validation check is shown in Figure 4, where for simplicity the two terms in Eq. (24) have been rewritten as:

\[\sqrt{\sum\Delta Y^{2}}=\sqrt{\sum\Delta D^{2}} \tag{25}\]

The two total uncertainties are exactly the same within the numerical precision available (\(10^{-6}\)). In this case the linearization always happens at the \(\pm 1\sigma\) points of the eigenvector uncertainty components, and no re-linearization is applied when moving to the space of recomposed uncertainty sources. So, since both are coming from the same run of the \(VH,H\to b\overline{b}\) analysis, almost perfect closure is observed. This differs from the previous section, where the linearization was applied first in the space of the original uncertainty sources and then in the space of the eigenvector uncertainty components.

## 4 Implementation in statistical analyses

This section will focus on the application of the recomposition tool to real physics analyses. The tool is designed to be applied at the latest stages of the analyses, directly on the analysis workspaces (WS). This ensures flexibility and applicability even a-posteriori on already published ATLAS analyses.

The workspace mentioned in this section is based on the RooWorkspace class [13], built with HistFactory [14]. The tool has been tested only on workspaces built with HistFactory, however its applicability is

Figure 3: Comparison between reference (black) and recomposed (red) systematic variation of the \(m_{bb}\) observable corresponding to the leading single source of uncertainty of the \(b\)-jet efficiency calibration, i.e. the parton shower uncertainty on the top anti-top process. The lower panel shows the ratio of the derived uncertainty source over original (reference) uncertainty. Both recomposed uncertainty source and original uncertainty source are computed on \(VH,H\to b\overline{b}\) signal events in the one lepton channel (targeting the \(WH\) production mode). The pseudo-continuous calibration is used. The residual difference is typically less than \(10^{-2}\).

expected to be independent from the chosen statistical analysis framework. As described in section 1.2, all the fits shown in this section are based on the full Run 2 \(VH,H\to b\overline{b}\) one-lepton workspace, which uses an MVA-based classifier, named BDT(\(VH\)) hereafter, as fitting variable in the Signal Regions (SR).

### Workspace modification: technical details

All the tests described in this section are performed modifying the \(VH,H\to b\overline{b}\) analysis workspace. The WS is modified using the relation derived in Eq. (15):

\[\vec{\theta}=\vec{\eta}\cdot U\cdot\tilde{U}_{\rm right}^{-1} \tag{26}\]

After this modification, the eigenvector nuisance parameters (NPs) in the likelihood are replaced by a linear combination of nuisance parameters representing the _original_ calibration uncertainty sources. The matrices \(U\) and \(\tilde{U}_{\rm right}^{-1}\) have been computed from the original set of uncertainty sources for the given analysis \(b\)-tag setup, and can be applied to any other analysis using the same setup. The newly introduced NPs encoding the original uncertainty sources are constrained using Gaussian priors with mean equal to zero, and standard deviation equal to one. Only the \(b\)-jet related uncertainties have been modified, but the method can be applied identically to all jet flavors. In this particular workspace the total number of original uncertainty sources is \(M=127\), while the number of eigenvector uncertainty components is \(L=N=45\), as no smoothing is applied to the \(b\)-tagging SFs. After pruning \(L\) is reduced to 4 eigenvectors (see section 5 for more details).

Figure 4: Comparison between the total systematic variations \(\sqrt{\sum\Delta Y^{2}}\) and \(\sqrt{\sum\Delta D^{2}}\) relatively to the nominal yields for the \(m_{bb}\) distribution. The bottom panel shows their ratio, which is within the available numerical precision (\(10^{-6}\)).

### Inspection of the uncertainties

Section 3.2 shows the validation of the method at histogram level (prior to the workspace creation) within \(\sim 1\%\) accuracy. This study aims at cross-checking that the recomposed uncertainty in the modified workspace is equal to the original source uncertainty histogram given by the analysis before creating the WS8. The results are shown in Figure 5. The \(\sim 0.2\%\) residual non-closure has been discussed in section 3.2.

Footnote 8: Note that during the generation of the \(VH,H\to b\overline{b}\) workspace, the BDT(VH) histogram is rebinned by a specific algorithm explained in Ref. [1]. Therefore, before making the comparison, the standard histogram has been rebinned by the same algorithm.

### Effects on the recomposed uncertainties

From the equivalence of the expression of the uncertainties in the original and in the recomposed spaces, it is expected that the post-fit \(b\)-tagging SFs nuisance parameter value of the modified workspace match the fitted parameter values of the eigenvectors in the standard workspace. To check this expectation, since the original NPs and eigenvector NPs are related by Eq. 15, the pull values of the \(b\)-tagging NPs fitted

Figure 5: Comparison between the _original_ uncertainty source and the _recomposed_ uncertainty source. The upper panel shows the systematic variation, while the lower panel shows \(\frac{\text{recomposed}}{\text{standard}}-1\). The test is performed on the \(t\overline{t}\) MC-modeling related uncertainty source \({}^{*}b\)-jet efficiency: ttbar PS uncertainty\({}^{*}\) on the MVA classifier output used in the \(VH,H\to b\overline{b}\) analysis.

from the modified workspace are used to calculate the pull values of the corresponding eigenvector NPs by Eq. 27:

\[\vec{\theta}_{\text{calculated}}=\vec{\eta}\cdot U\cdot\tilde{U}_{\text{right}}^{-1} \tag{27}\]

Then \(\vec{\theta}_{\text{calculated}}\) can be compared with the eigenvector NPs pulls \(\vec{\theta}_{\text{standard}}\) of the standard workspace. This calculation was performed for the 4 leading \(b\)-tagging eigenvector NPs, where the agreement was found to be better than \(10^{-4}\).

## 5 Pruning effects

The pseudo-continuous \(b\)-tagging calibration leads to 45 eigenvectors after the decomposition. In order to ease the convergence of the fit and reduce the computation time, a pruning procedure is usually applied at physics analysis level, excluding the smallest eigenvector uncertainty components from the fit (sometimes small uncertainty components are combined according to the method presented in Ref. [15]). The pruning criteria have a negligible impact on the final result. However, a pruned sub-set of the flavor-tagging eigenvectors by construction loses a certain amount of information on the total uncertainty. This might lead to accuracy losses when applying the recomposition. This section aims at studying these effects on the accuracy of the recomposed uncertainty sources.

### Eigenvector pruning effects and the accuracy of the recomposed uncertainties

In order to quantify how pruning affects the accuracy of the recomposed uncertainty sources, the flavor-tagging uncertainty components of the \(VH,H\to b\overline{b}\) workspace have been recomposed into the original uncertainties, using three different sub-sets of eigenvectors, corresponding to three different pruning thresholds:

* The first case corresponds to the full set of 45 eigenvectors with no pruning applied.
* The second case corresponds to 29 EV, which is the number of uncertainty components after the dedicated pruning step of the \(VH,H\to b\overline{b}\) analysis. The analysis keeps all the eigenvectors for which the variation is above 1% in at least one \(p_{\text{T}}\) bin in at least one of the three considered \(b\)-tagging score bins 100-70%, 70-60% and 60-0%.
* The third case corresponds to 4 EV, which is the number of \(b\)-tag uncertainty components after applying the pruning criteria of the \(VH,H\to b\overline{b}\) 2019 fit. These are applied on top of the pruning criteria described before. An uncertainty is pruned if its normalisation or shape effect is smaller than 0.5%. In addition, the uncertainties on the very small background processes (their yield is less than 2% of the signal) are pruned. In non-sensitive regions (where the signal is less than 2% of the background in each bin) the uncertainty components are pruned if the impact is smaller than 0.5% of the total background.

These numbers correspond to the number of uncertainty components in the specific region tested here (150 < \(p_{\mathrm{T}}^{V}\) < 250 GeV, SR, 2 jets, 1 lepton).

The comparison of one of the leading uncertainty sources: "\(b\)-jet efficiency: ttbar PS" recomposed starting from 45, 29 and 4 eigenvectors is shown in Figure 6. In all the three cases the "standard" uncertainty source (in black) represents the original reference histogram before the creation of the workspace. The corresponding accuracy loss is respectively 0.1%, 1% and 20% for 45, 29 and 4 eigenvectors.

For completeness, the test has been enlarged also to the top 10 recomposed uncertainty sources impacting the BDT(VH) histogram. The sources of uncertainty have been ordered by their impact on the \(t\bar{t}\) background yields.

The relative deviation between each recomposed and original uncertainty source is calculated as the sum of the absolute yield difference between the two histograms in each bin and it is shown in the last three columns of Table 1. The accuracy loss on the recomposed uncertainty sources spans between 1% to \(\sim\)40% with 29 eigenvectors. With 4 eigenvectors the accuracy loss increases, with losses between 10% and up to \(\sim\)130%. In order to get accurate recomposed uncertainties and correctly correlate the \(b\)-tagging uncertainty components for example in downstream combinations, the various input analyses must have similar pruning settings. The most conservative suggestion would be to remove the pruning on the eigenvector uncertainty components before applying the recomposition tool.

A complementary test consists in checking whether the pruning affects the closure between the total uncertainty calculated before and after the recomposition step. Figure 7 shows the total relative variation

Figure 6: Comparison between the \(b\)-jet efficiency ttbar PS uncentrainty recomposed starting from 45 (full set, “recomp. from 45 EV” label in green), 29 (”recomp. from 29 EV” label in blue) and 4 (”recomp. from 4 EV” in red) eigenvector uncertainty components. The original uncertainty source (black) is labeled as “original”.

calculated using 4, 29 and 45 eigenvectors and the corresponding original uncertainty sources recomposed from those eigenvector sets. The difference remains within \(2\times 10^{-3}\), which implies that the total uncertainty is conserved after recomposition, independently from the chosen pruning threshold.

## 6 Conclusion

This note summarizes the methodology and the validation of the eigenvector recomposition method. This method uses the information from the different flavor-tagging eigenvector uncertainty components and "recomposes" them back into the original sources of uncertainty of the flavor-tagging calibration analysis. This allows analyses having different flavor-tagging setups to properly correlate their flavor-tagging uncertainties by a simple modification of the final likelihoods used in the signal extraction fits. This is useful, for example, to take the correlated impact of the flavor-tagging uncertainties in combination analyses properly into account, e.g. in the combination of all Higgs boson measurements [16], where the flavor-tagging uncertainties can play a non negligible role. In addition, by reverting to the original sources of uncertainty in the flavor tagging calibration, the method also enables the correlation of physics analysis NPs with flavor-tagging NPs related to the same source of uncertainty.

A careful validation has been performed both at histogram level, as well as at workspace level. The checks performed at histogram level show that the recomposed uncertainties match the original source uncertainties within numerical accuracy. At workspace level, no significant difference is observed in terms of fit results and pulls of NPs between the original and the recomposed workspace. The software tool implementing the new method is thus ready to be deployed more widely in ATLAS physics analyses.

\begin{table}
\begin{tabular}{l c c c c} \multirow{2}{*}{Uncertainty Name} & \multirow{2}{*}{Impact on \(t\bar{t}\) yield (\%)} & \multicolumn{3}{c}{Rel. deviation from original unc. (\%)} \\  & & 45 Eigen & 29 Eigens & 4 Eigens \\ \hline ttbar PS & 0.84 & 0.09 & 1 & 22 \\ PDF4LHC NP. 1 & 0.63 & 0.09 & 6 & 48 \\ diboson modeling & 0.56 & 0.07 & 15 & 43 \\ pile-up reweighting & 0.51 & 0.14 & 24 & 23 \\ single top DR vs DS & 0.49 & 0.06 & 5 & 57 \\ PDF stat. NP. 1 & 0.45 & 0.18 & 1 & 8 \\ Jet Energy Resolution & 0.41 & 0.48 & 14 & 25 \\ MC stat. & 0.39 & 0.17 & 39 & 136 \\ fake correction & 0.39 & 0.20 & 5 & 56 \\ Z+jets modeling & 0.31 & 0.03 & 4 & 25 \\ \end{tabular}
\end{table}
Table 1: Top 10 original uncertainty sources impacting the \(t\bar{t}\) yield in the 1L 2jet \(150<p_{\mathrm{T}}^{V}<250\) SR. The second column shows their impact on the \(t\bar{t}\) yield. The three last columns show the relative loss of accuracy when the recomposition is performed starting from 45, 29 and 4 eigenvector (EV) uncertainty components respectively. This relative deviation is calculated by using the sum of the absolute difference in each bin between the recomposed histogram and the original histogram.

## References

* [1] ATLAS Collaboration, _Measurements of \(WH\) and \(ZH\) production in the \(H\to b\bar{b}\) decay channel in \(pp\) collisions at 13 TeV with the ATLAS detector_, Eur. Phys. J. C **81** (2021) 178, arXiv: 2007.02873 [hep-ex] (cit. on pp. 2, 5, 15).
* [2] ATLAS Collaboration, _Performance of \(b\)-jet identification in the ATLAS experiment_, JINST **11** (2016) P04008, arXiv: 1512.01094 [hep-ex] (cit. on p. 2).
* [3] ATLAS Collaboration, _Measurements of \(b\)-jet tagging efficiency with the ATLAS detector using \(t\bar{t}\) events at \(\sqrt{s}=13\) TeV_, JHEP **08** (2018) 089, arXiv: 1805.01845 [hep-ex] (cit. on pp. 2, 3).
* [4] ATLAS Collaboration, _ATLAS \(b\)-jet identification performance and efficiency measurement with \(t\bar{t}\) events in \(pp\) collisions at \(\sqrt{s}=13\) TeV_, Eur. Phys. J. C **79** (2019) 970, arXiv: 1907.05120 [hep-ex] (cit. on pp. 2, 7, 12).
* [5] ATLAS Collaboration, _Jet energy measurement and its systematic uncertainty in proton-proton collisions at \(\sqrt{s}=7\) TeV with the ATLAS detector_, Eur. Phys. J. C **75** (2015) 17, arXiv: 1406.0076 [hep-ex] (cit. on pp. 3, 6).
* [6] ATLAS Collaboration, _Determination of jet calibration and energy resolution in proton-proton collisions at \(\sqrt{s}=8\) TeV using the ATLAS detector_, Eur. Phys. J. C **80** (2020) 1104, arXiv: 1910.04482 [hep-ex] (cit. on p. 4).

Figure 7: Sum in quadrature of the relative uncertainties calculated from 4, 29 and 45 eigenvectors (solid lines) and sum in quadrature of all recomposed uncertainties based on 4, 29, 45 eigenvectors (dashed lines).The bottom panel shows the ratio between the sum in quadrature of the total recomposed uncertainty sources and the corresponding sum in quadrature of the eigenvector uncertainties.