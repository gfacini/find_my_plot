**ATLAS Internal Note**

**SOFT-NO-006**

**4 November 1993**

**COMPUTING IN THE ATLAS**

**EXPERIMENT AT LHC**

KORS BOS

_National Institute for Nuclear Physics and High Energy Physics, NIKHEF_

_PO Box 41882, 1009 DB Amsterdam, The Netherlands_

and

STEPHEN W. O'NEALE

_School of Physics and Space Research, University of Birmingham_

_Birmingham B15 2TT, United Kingdom_

Presented at

THIRD INTERNATIONAL WORKSHOP ON

SOFTWARE ENGINEERING, ARTIFICIAL INTELLIGENCE

AND EXBERT SYSTEMS FOR HIGH ENERGY AND NUCLEAR PHYSICS

October 4 - 8, 1993

OBERAMMERGAU, Oberbayern, Germany

Under the Sponsorship of the

GERMAN PHYSICAL SOCIETY and

EUROPEAN PHYSICAL SOCIETY

[MISSING_PAGE_EMPTY:2]

[MISSING_PAGE_EMPTY:3]

[MISSING_PAGE_EMPTY:4]

**COMPUTING IN THE ATLAS EXPERIMENT AT LHC**

KORS BOS

_National Institute for Nuclear Physics and High Energy Physics, NIKHEF PO Box 41882, 1009 DB Amsterdam, The Netherlands_

and

STEPHEN W. O'NEALE

_School of Physics and Space Research, University of Birmingham Birmingham B15 2TT, United Kingdom_

ABSTRACT

We present the computing needs of the ATLAS experiment at the LHC. The current simulation and reconstruction software used for the optimisation of the detector design is described. The activities of the future computing group are reviewed and some areas are shown where computer assisted software engineering products are (yet) inadequate. The replacement of the CERN central mainframe system with workstation group services for ATLAS is discussed.

**1. Introduction**

In this paper we give a brief description of the data handling needs of the general purpose ATLAS detector [1] proposed for operation at the Large Hadron Collider (LHC) at CERN. The software effort now is primarily concerned with the simulation and reconstruction of events in ATLAS to be used for the selection and refinement of the detector design options - to be presented in a detailed Technical Proposal in May 1994. This work, the _current_ software system, relies on the mature HEP products of the LEP era and is reviewed in the third section. Then we give an overview of the software activities of the _future computing_ group. Here modern software engineering techniques, in particular object oriented analysis, design and implementation problems, are being studied. This work has not reached the stage in which results would normally be presented to a conference but the flavour of the problems arising may be useful input for further work in, and arising from, this workshop. Lastly, we address the computing infrastructure needs and the working environment for ATLAS which is migrating to workstation services from a central mainframe facility.

**2. The ATLAS Experiment**

The ATLAS collaboration, comprising a thousand physicists from a hundred universities and research laboratories, proposes a general purpose pp experiment

[MISSING_PAGE_EMPTY:6]

for operation at the LHC. The main focus of interest is the discovery of the Higgs particle which is hypothesised as the origin of mass on the electroweak scale. ATLAS is designed to be sensitive to many processes in order to cover the full mass range above the LEP discovery limit. The detector is also able to cope with studies of top quark decays, supersymmetry searches and has some potential to deal with any unexpected new physics.

The LHC is planned for installation in the LEP ring so a major investment in the civil engineering has already been made. The current design has 8 TeV colliding beams in the proton proton mode. The luminosity is 1.7 \(10^{34}\) cm\({}^{-2}\) s\({}^{-1}\) and the bunch crossing time is 25 ns. For an inelastic cross-section of 80 mb this leads to an average pileup of about 30 overlapping events per beam crossing. The cross sections for Higgs production and other interesting processes such at \(t\overline{t}\) pair production may be ten or more orders of magnitude below the total cross section at LHC energies so a large data rejection power will be needed.

The ATLAS detector has four major subsystems built around an interaction point of the LHC beam line. The inner detector records points on the trajectory of electrically charged particles moving in a 2 Tesla toroidal magnetic field. An analysis of these points (reconstruction) leads to a measurement of the momentum of the particles and an identification of the decay topology of hadrons containing b c and s quarks and of tau leptons. The electromagnetic calorimeter provides a large signal for incident electrons and positrons which measures their energy. Incident photons yield a similar energy signal as they also convert into a shower of electrons, positrons and photons. The granularity of the calorimeter permits some resolution of the two photons produced by the decay of a neutral pion in the intersection region. Iron absorbers are used for the next layer, a calorimeter to record the passage of particles emerging from electromagnetic calorimeter and to measure the energy of the hadrons. The iron may also form part of the return yoke of the magnetic field. Charged particles which penetrate the iron are identified as muons and their momentum and charge may be measured with a stand-alone muon system in the toriodal field outside the calorimeters.

The data flow from the detector will be dominated by the inner tracking system which may yield 1 MByte every 25 ns. The ATLAS trigger and data acquisition (DAQ) model has three levels. Level-1 may trigger at \(10^{4}\) - \(10^{5}\) Hz and deal with \(10^{10}\) - \(10^{11}\) Bytes/s. Level-2 decisions may require the transfer of \(10^{8}\) - \(10^{9}\) Bytes/s from level-1 and run at \(10^{2}\) - \(10^{3}\) Hz. At level-3 we wish to run event reconstruction to make further data reductions yielding a data recording rate of 10-100 MByte/s - possibly data suitable for physics analysis. The software running in Level-3 may be the traditional offline reconstruction. We estimate we will need \(10^{6}\) Mips for the level-3 filter and reconstruction and \(10^{7}\) Mips for simulation. The computer assisted software engineering (CASE) tools and techniques used to develop and document the DAQ system in the context of the RD13 project are presented at this workshop and the parallel sessions have talks on formal methods and simulation for the level-2 trigger and DAQ system.

An extrapolation of todays technologies for cpu power, data storage and data 

[MISSING_PAGE_EMPTY:8]

access to the LHC era [2] suggests we may obtain \(10^{7}\) Mips in \(10^{4}\) workstations in a few hundred boxes for 10-20 M$. Data recording and robotic manipulation do not present major problems, 10 PByte systems seem quite feasible. However data access for analysis is less clear, a few percent of recorded physics data may be placed on 10 TByte of random access (disk) devices for 5M$. At this stage we must defer the complexity of detailed design and note the cost estimates in the budgets for the construction and operation of our experiment.

We anticipate a need to go from 1 Mbit/s wide area networks of today to 1 Gbit/s for the preparation and analysis of data at the LHC. Some new applications include audio-visual mail and tele-conferencing sessions and the use of ASF mounted disks to avoid tedious and repetitive file transfers. Many physicists have just realised that there is a crisis looming in HEP networking as EBONE has just gone down and alternative routes have much reduced bandwidths. Links to DESY have been reduced and the transatlantic T1 link is not funded for operation beyond January 1994. The situation in Europe is particularly bad when compared with de-regulated commercial systems and data highways proposed in the USA. It is difficult to avoid the conclusion that European monopoly holders enjoy inflated profits and/or shameful inefficiencies.

We expect to use our _current_ software for the Technical Proposal up to May 1994 and hope for formal approval of the LHC project and the ATLAS experiment by the end of 1994. Design refinements and test-beam studies are likely to use this software through 1995-96. _Future_ computing studies should come to fruition during 1995-96 with decisions on methodologies, tools, data models, milestones and quality control techniques. This leaves more than six years for an incremental implementation of the ATLAS computing model ready for beam in 2003.

## 3 Detailed studies for the Technical Proposal

The ATLAS collaboration is engaged in a detailed study of detector options which are to be presented in the form of a Technical Proposal in May 1994. This proposal needs software to simulate interactions, the response of the ATLAS detector and some reconstruction of events to optimise the detector design. One disappointment is that the CASE vendors appear to have little to offer such as industrial strength tools which might help this effort, on this time scale, today.

The current ATLAS software is written in Fortran 77, a draft style guide and some verification with packages like FLOPPY [3] are close to formal approval within the collaboration. The code management is performed with CMZ [4]. The ZEBRA [5] package is used as a memory manager and for the organisation of the data structures. It was first proposed to write code using "bare" ZEBRA but several authors have felt the need to use access routines to avoid coding the navigation of the banks themselves.

The strongest support of ATLAS software is on the VM/CMS and HP-UX platforms. This is simply because the entire collaboration has network access to the 

[MISSING_PAGE_EMPTY:10]

high quality service available on the CERNVM mainframe machine and because the CERN simulation facility (CSF) and the ATLAS cluster of workstations at CERN are HP 9000/700 series workstations. There is no enthusiasm to support ATLAS code on VAX/VMS or its more recent open variety within the central group. Investment in the infrastructure on CERNVM is kept to a minimum and it is expected that reference systems on several Unix platforms will be available in 1994.

The simulation code is based on GEANT version 3.16 [6]. The geometry description and the response of the detectors to particles are handled by the DICE [7] package. DICE provides the CONS, GEOM, HITS and DIGI user functions in the GEANT framework and will also facilitate the use of other packages in reconstruction and other codes.

The SLUG [8] package is an interface between GEANT and DICE which controls input/output and also acts as an interface to physics generators like PYTHIA [10]. For a project of 15 to 20 years duration the "expert down the corridor" approach will clearly fail and already the principle author of SLUG, and a generator package GENZ [9], has left the ATLAS collaboration to enhance British Industry. Fortunately he has written excellent documentation and implemented the features needed for current ATLAS work.

Work has started on a reconstruction package ATRECON [11] and an event viewing system is operational but not released for public usage yet.

There are 100 institutes in the ATLAS collaboration of which 45 have designated a contact person who contributes to the overall computing effort or simply represents the needs and interests of his home laboratory. The ATLAS automated code update system has only 10 subscribers off the CERN site. Although the code is available to all, this raises the possibility that a complete software development environment will not be needed in each and every laboratory. Given the trend towards commercial tools for software development and the reluctance of many physicists managers to fund a proper working environment, this may help to raise the lowest common level which constrains offline software development.

The quality and detail of the current software was demonstrated with a few pictures from the ATLAS collection produced through the GEANT graphics interface which can not be reproduced in this publication. They may be obtained from the ATLAS Secretariat at CERN (email atlassec@cernvm.cern.ch). A display from the current software applied to the evaluation of the ATLAS potential for B physics at low luminosity is presented in figure 1.

## 4 The Future Software Working Group

ATLAS has a future computing working group which is concerned with all aspects of computing between now and the start of ATLAS running. The membership is the same as the object oriented working group. A software effort of the scale of ATLAS needs effective planning with a strategy, tools and a budget for project management. We expect to find some specialisation of people working as softwareengineers, electronics engineers and physicists and may go as far as allowing physicists to specify their requirements to another group who build the software based upon the analysis of these requirements.

Formal software engineering methodologies have not been particularly successful in particle physics. The SA/SD methodology was in vogue in the planning stages of the LEP experiments and there is a fairly broad concensus that the dataflow diagrams were a very useful aid to discussion. However the transition from analysis to design was clumsy and the modifications introduced in the design and implementation stages were rarely fed back into the documentation. Extension for real time systems came rather late. The tools available for SA/SD, and indeed any other variant examined by LEP experiments, were rather poor. Implementations running on the PCs of the day were very slow once a project of a moderate size had to be handled and in some cases there was no access to the "internal" tables. The efforts of the ALEPH collaboration in the use of SA/SD and the entity relationship model are well publicised [12]. We might now be helped by better tools but it is difficult to find a consultant in software engineering who would recommend the use of these techniques today [13].

Object Oriented methodologies are the current fashion and ATLAS is currently studying the Object Modeling Technique (OMT) [14]. The methodology covers analysis, design and coding with no awkward transitions in notation. The Objectiveering CASE tool is being used for pilot projects. Objectering covers a subset of the OMT methodology and generates code in C++ [15]. Code management (of the input to objecteering) is migrating to CVS [16].

The first OO prototype model was a naive detector which gave the group some experience of OO analysis and design. Effort was transferred to a more realistic project - prototype II - an OO approach to the Atlas Inner Tracker reconstruction and electron identification. A C program has been written to read (construct objects) from Zebra banks for the GEANT GEOM, HITS and KINE banks. The pattern recognition for the tracker hits uses an external point from a calorimeter cluster to start the search for tracks in the barrel region. For a project of this type a lot of time is spent installing tools, learning how to use them and reporting errors, learning the methodology and developing suitable class libraries. Nevertheless it was decided to deal with the algorithmic complexity of tracking in a magnetic field before presenting the results to the collaboration. There are now several projects, notably GISMO [17], which demonstrate the feasability and advantages of the OO paradigm. They do not (yet) cover the same functionality and performance of existing software running with procedural languages like Fortran. Prototype II should be able to provide a direct comparison with an ATLAS Fortran 77 implementation with the same functionality.

Writing objects (data and methods) to external files for later use in analysis or for reprocessing on other machines with new algorithms is still considered to be a problem area (associated with object persistence and/or object control). A short term solution has been found via Lonnblad's scheme in the CLHEP C++ class libraries. For Eiffel I/O a few changes have been made to the format to make it fully self describing (removing the markers at the beginning and end of subobjects and adding a marker at the beginning of every complete object). The data is written in ascii format for convenience in program development. Reading data / objects is less satisfactory and the schemes we have devised so far require a dictionary - hardly a satisfactory technique for eventual use in a DAQ system passing data at a few 100 Mbytes per second. We would particularly like to raise this topic to the attention of the experts and commercial software vendors attending this workshop.

Several members of the oo-atlas group feel that Eiffel is a cleaner language than C++ and a better choice for physicists moving from Fortran to a high level language supporting object oriented work. The prototype model has been translated to Eiffel and an ATLAS Eiffel style guide has just been completed.

Moving away from Fortran 77 will be a major problem for many physicists. Fortran 90 has many innovations which are particularly suitable for high performance scientific computing but the language is very different from its Fortran 77 subset. Classic works on numerical methods are now available in C versions but the language seems most popular with people wanting to exploit system features in an online environment. If one uses an object oriented methodology then it seems sensible to use a language which supports object oriented techniques. The C++ language is readily available in varied implementations on many platforms. The commercial success of the language leads us to expect there will be many software tools which first appear in C++ implementations. However C++ (C with Objects) has been under development for many years and development continues after the ANSI base document has been produced. There are many other object oriented languages now receiving serious consideration [19]: C+@, Eiffel, Sather, Beta, Liana, Paradox and lots more. Although we now place more emphasis on analysis and design and we hope to find tools and working environments with reliable code generation it will still be neccessary to provide code for the methods and algorithms. It will be difficult to persuade experienced physicists-programmers to migrate to a language with a lifetime of a few months.

Studies of Object Oriented Data Base Management Systems (OODBMS) have begun with GEMSTONE and ITASCA. OnX [20] and TkTcl [21] are candidates for programming a production environment on unix platforms.

Documentation, agenda for meetings and so on are made available through the World Wide Web [22] (WWW). This has proved to be very popular, far more so than early conferencing systems, but the need to bring new items to the browser's attention has often been expressed. There is a large amount of HEP documentation written in LateX and a converter to the HTML language of WWW is a useful tool. A desktop publishing package, for example Framemaker, may also be extended and then used to collect information from many sources and give output for publication on paper and in HTML.

## 5 Computing Infrastructure and the working environment

The ATLAS computing model follows the approach of OPAL in the exploitation of workstation compute power and data management. The raw data will be processed in a reconstruction farm which is effectively part of the data acquisition system. The processed data will be accessed, for analysis, in a central repository. Other needs will be satisfied by workstation (or PC) servers selected and tuned for each application. ATLAS already performs most of its computing on the CERN simulation facility (csf) and similar facilities are following this model in the home laboratories. A reconstruction facility based on HP workstations is operational and this may evolve into a testbed for the level-3 system. Information services such as mail, news, file access now based on CERNVM will move to workstation systems.

Wide area networks are needed to provide access to data, information and for communication between people when working in a distributed environment. A notable feature of particle physics research is the contribution of physicists based in universities. Most of these physicists have teaching and administration duties. It should not be neccessary to commute to the experimental site at CERN in order to contribute to ATLAS. The social upheaval is an unwelcome strain and such frequent travel detracts from the cohesion of the university teams.

Atlas collaboration now has a 1000 members and this is expected to grow to about 1500. Of these 300 are expected to need significant computing resources for their work and 100 of them will be based at CERN. All members of the collaboration will need good services for information exchange. We expect 500 people will have their prime computing facilities at CERN.

The computing infrastructure is needed to support the production of the Technical Proposal and for forthcoming test beam preparation, data taking and analysis. People working at levels 1,2 and 3 (daq) and levels 3,4 (offline) will need a good working environment and communication with offsite users.

It is ATLAS policy to discourage the development of the central "mainframe" services provided at CERN, notably CERNVM. The use of VXCERN and VMS in general has little support. CERN plans to remove much of the batch capacity of CERNVM at the end of 1994 which will be achieved by extentions of the OPAL/CN SHIFT project for the four LEP experiments.

The provision or migration of interactive services to a workstation like platform for 500 to 1500 users is considered to be a much more difficult task than supporting their batch work, particularly by the physicists spending some of their time to support clusters of 10 to 100 workstations today. CN division is planning to provide workgroup services now to prevent the growth and reliance on CERNVM by the LHC collaborations. They are particularly encouraged by the success of the Novell Integration, Coordination and Evolution (NICE) [23] working group in providing a standard environment for Personal Computers on the CERN site. Most desk-top PC users are running shrink-wrapped applications under MS-DOS and Windows 3.0 and very few have made any modifications to the distributed system.

The Application Software Installation Server (ASIS) [24] intended primarily to provide a local software repository for both Cernlib and public domain software via anonymous ftp, has also been a success.

A workgroup service project (WGSWG) with the CHORUS experiment, a 

[MISSING_PAGE_EMPTY:18]

small group with 8 DecStations some X-terminals and little data, has also proved popular with the physicists users. The experiment has about ten serious users and the services are provided on 2 RS6000 AIX machines.

The CN workgroup services working group (WGSWG) aim to provide the infrastructure for the workstation or X-terminal to plug into the network and to need nothing else.

The major components for the desktop service are home directory file services, binary and source program services, electronic mail and multi-media, information and print services. Central responsibility is assumed for licensing, registration, accounting and security as well as for a reliable, managed and performant network with access to and from homelabs via Internet. Of course there will be some restrictions on the number of recommended sets of applications, hardware configurations and reference environments. The WGSWG aims to exploit the features of AFS/DFS to provide a safe, backed-up Home Directory file space. Profiles are expected to emerge from the workings of the HEPiX group. Some emerging technologies may be exploited such as COSE (for the standard user interface) and OSF Motif, DCE and DME. Batch computing facilities and data handling will be provided on shift-like resources.

CN division have recently adopted a policy to develop structured wiring for the onsite networking (twisted pairs to hubs rather than ethernet cables with spikes or tee joints in many offices).

Electronic mail, extended to handle audio-visual information, and repositories browsed via WWW are a useful aid to communication in a large, dispersed collaboration. Personal interaction between physicists is also needed and ATLAS is engaged in pilot projects to develop teleconferencing systems for remote meetings. This offers the advantage of saving travel time and money and may retain a record of conversations and pictures. The basic hardware exists now: a workstation and a shared whiteboard, a projector and camera, and a microphone and speaker. There is strong interest in the workstation based packet mode running over the Internet at NIKHEF and IN2P3.

## 6 References

* [1] ATLAS Collaboration _Letter of Intent for a General-Purpose pp Experiment at the Large Hadron Collider at CERN_ CERN/LHCC/92-4.
* [2] R. Mount and L. Robertson, in CERN Academic Training Program (1993).
* [3] J. Bunn _The FLOPPY and FLOW User's Guide_ CERN CN/SW/02
* [4] CodeME S.A.R.L. _A Source Code Management System_
* [5] O. Schaile, J. Shiers and J. Zoll _The ZEBRA System_ CERN Program Library Q100 and Q101.
* [6] F. Carminati et al., _GEANT Detector Description and Simulation Tool_ CERN Program Library Q123 June 1993 Edition.

7] K. Bos _A Subroutine Package for the Atlas Detector Simulation_ Atlas Note (under WWW, unpublished).
* [8] R. DeWolf _Simulation for LHC Using Geant_ Atlas Note (under WWW, unpublished).
* [9] R. DeWolf _Generated Event Handling Using Zebra: User's Guide version 2.0_ Atlas Note (under WWW, unpublished).
* Hadron Hadron high-Pt Scattering_ Reprinted in The LUND Monte Carlo Programs long writeup.
* [11] M. Dodgson _The ATLAS Atrecon Manual_ (under WWW, unpublished)
* [12] J. Knobloch _Reality of Software Engineering in High Energy Physics_ in Computing in High Energy Physics '91 (Universal Academic Press, Inc., Tokyo 1991).
* [13] E. Yourdon _Decline and Fall of the American Programmer_ (Yourdon Press, Englewood Cliffs 1992).
* [14] J. Rumbaugh et al _Object-Oriented Modeling and Design._ (Prentice Hall, Englewood Cliffs 1991).
* [15] M. Ellis and B. Stroustrup _The Annotated C++ Reference Manual_ (Addison-Wesley, Reading 1991).
* Concurrent Versions System_ Unix man pages.
* [17] W. Atwood and T. Burnett _Gismo: C++ Classes for HEP_ in Computing in High Energy Physics '92,CHEP92 (CERN 92-07, Geneva 1992).
* [18] B. Meyer _Eiffel: the Language_ (Prentice Hall 1992)
* [19] Dr Dobb's Journal _Annual Object Oriented Edition_ October 1993
* [20] G. Barrand _The OnX Manual_ LAL Orsay (unpublished)
* [21] J. Ousterhout _An Introduction to Tel and Tk_ (Addison Wesley, Reading 1994)
* [22] T. Berners-Lee _World-Wide Web_ Proceedings CHEP92, CERN 92-07
* [23] C. Andrews et al _A Guide to Personal Computer Networks at CERN_ The NICE Working Group
* [24] P. Defert and I. Reguero _ASIS: The Installer's Guide_ CERN CN/CO/152

[MISSING_PAGE_EMPTY:22]

Figure 1: _Display of one half of ATLAS barrel tracker in the transverse plane for a signal event from \(\mathrm{B_{d}^{0}\to J/\psi K_{S}^{0}}\) decay at a luminosity \(L=\) 10\({}^{33}\) cm\({}^{-2}s^{-1}\) (left), where two minimum bias events have been added on top of the signal event, and at a luminosity \(L=5\times 10^{33}\) cm\({}^{-2}s^{-1}\) (right), where ten minimum bias events have been added on top of the signal event. The straw tracker hits are shown as large dots and the precision hits as dashes. The reconstructed tracks with \(p_{\mathrm{T}}~{}>\) 0.5 GeV/c and 0 \(~{}<~{}\eta~{}<~{}0.8\) are shown as dotted lines between the detector planes and the interesting ones corresponding to the electrons from the \(\mathrm{J/\psi}\) decay, the charged pions from the \(\mathrm{K_{S}^{0}}\) decay and the muon tag as full lines._

[MISSING_PAGE_EMPTY:24]