## References

* [1] A. A. Belov, _The Quantum Theory of Quantum Mechanics_, (Cambridge University Press, Cambridge, 1998).

[MISSING_PAGE_POST]

Since the available logic in the FPGAs is growing so fast, it is lucid to implement complete functional blocks into the FPGA. This can be done by the user with a design or a core tool, or it can already be hardwired inside the FPGA by the vendor. The latter usually allows a faster execution which needs less resources, however it decreases the flexibility of the FPGA if it takes too much chip area. Components which are good candidates for a hardwired implementation are 'on chip memory', SDRAM controller, PCI interface or network interface, e.g. ATM or Ethernet. Of these components on chip memory is the most general-purpose, because memory in the form of FIFO, SRAM or DPRAM (Dual-Ported RAM) is needed in almost any design. Therefore modern FPGAs like the Xilinx Virtex XCV 1000 E already provide 384 kbit of block RAM in addition to the 384 kbit of distributed LUT RAM. The block RAM is even true DPRAM, which gives the possibility to create FIFOs with two separate clocks. Internal block RAM is much more compact than building memory out of small LUTs and allows a higher design speed. It is an important step towards complete Systems designed on a Chip (SoC). When this is achieved, FPGA system design can change substantially.

Currently an important parameter of the FPGA is the number of I/O pins. FPGAs with 672 pins, of which 470 pins can be used as I/O pins, are available, and FPGAs with 804 I/O pins (XVC 3200 E) are already advertised. However, when SoC becomes reality for many applications, the number of I/O pins is not that much of an issue anymore, see section 2.2.

Other important parameters are the internal FPGA speed, and, for the system, the external system speed. The internal FPGA speed for identical designs will continue to grow, however when the designs grow in size and high-level design entry is used, more than 80 MHz design speed will continue to be a challenge. To achieve higher speeds, multiple present-size designs should be used. The development of high-level languages which produce efficient implementations is an active area of work which is on-going, see section 2.3.

The prices of FPGAs currently range from 1200 CHF for small, complete single-FPGA co-processors, to \(\sim\) 4000 CHF for a single XCV 1000 E FPGA. Clearly, the prices will drop soon. Taking into account the development of the FPGA prices in the past, a projected price for an FPGA co-processor of the year 2001 with a Xilinx Virtex XCV 3200 E and 5 MByte SRAM might be \(\sim\) 4000 CHF.

### FPGA System Prospects

Figure 1 shows possible couplings between FPGAs and CPUs (The names of buses are examples for computers with x86 architecture):

First, reconfigurable hardware can be used solely to provide reconfigurable functional units within a host processor. The opposite is also possible: to use an FPGA and include a processor core in the FPGA.

Second, an FPGA can be connected to the e.g. AGP-bus, and is then able to perform computations without the constant supervision of the host processor. Generally it is possible that the FPGA and the CPU execute simultaneously.

Third, an FPGA can be coupled via e.g. Frontside-bus. Then it behaves like an additional processor in a multi-processor system. The host processor's data cache is not visible to the attached FPGA, therefore there is a higher delay in communication between the host processor and the FPGA.

Finally, a reconfigurable computing unit can be connected via the I/O interface, e.g. PCI-bus. This allows for a great parallelism in program execution due to the possibility to connect a powerful reconfigurable computing unit with a large SRAM word length and a concurrent execution of CPU- and FPGA-processing.

Here, only the FPGA units coupled to the PCI-bus (or successors of PCI) are discussed, because only this allows a system-independent coupling. All other couplings are highly system-dependent and can change frequently. Once such systems are available commercially as standard-components they are attractive for applications in ATLAS. However, for the time being, only a stable and system-independent coupling (currently PCI) allows the development of general-purpose FP GA processing units.

#### 2.2.1 Future FPGA Co-processor

A schematic of an FPGA co-processor, coupled via the PCI-bus, is shown in Figure 2. It contains a large FPGA, a PCI interface and SRAM with large word length. The successor of the PCI-bus might be PCI-X [5], Future I/O or Next Generation I/O [6].

The SRAM can be organized in 17 bit address and 32 bit word length, which gives 5 MByte of SRAM. For ATLAS, the full scan TRT application needs only 1.25 MByte of SRAM with the new algorithm as described in section 3.2. The remaining memory could be used to store a finer grain LUT e.g. for a Hough transform in the SCT. However, this first has to be investigated. If other applications require more SRAM, larger chips with more address bit can be used (already purchasable, e.g. used for the 1999/2000 FPGA co-processor).

Figure 1: **Coupling in reconfigurable system.**_There are different levels of coupling in a reconfigurable system. Reconfigurable logic is shaded. The names of buses are examples for computers with x86 architecture. Figure from [1]._

Figure 2: **FPGA co-processor.**_Schematic view of the FPGA co-processor. Values given are for the hardware of 1999/2000, and values in brackets are for the year 2001._

The SRAM might in the future be complemented by SDRAM, which is cheaper and larger, and also allows a large bandwidth for block transfers, however needs an SDRAM controller. This SDRAM controller can be implemented in the FPGA. A part of the required fast SRAM will in future be replaced by internal block RAM, which allows the ultimate RAM bandwidth and parallelism (address and data parallelism). SRAM and SDRAM will soon be available with frequencies of up to 200 MHz. The advantage of the SRAM, especially zero bus turnaround SRAM, is that there is no idle cycle in intermixed reads and writes. They allow 100 % bus utilization and have a common data-bus which is turnaround in 1 bus-cycle. SRAM usually needs only 1 or 2 clock-cycles to start a burst data transfer.

### FPGA Programming Prospects

Nowadays, FPGA designs can be programmed with very different programming languages/environments. The simplest one, which is still used from time to time is the design entry with a schematic entry tool. It is based on a graphical interface and allows the developer to generate a structural description of the FPGA design functionality. Besides the schematic entry, there are several module generators like the Pam-Blox [7] or CHDL [2] that use a textual design entry in contrast to the graphical design entry. These module generators also allow to describe the FPGA design in a structural way based on the available elements. Some of these languages (e.g. CHDL) make it possible to describe parts of the design like final state machines in a behavioural way. These module generators are quite popular nowadays due to the direct netlist output and the ability to perform 'in system simulation'. The main disadvantage is based on the structural description which can lead to very long descriptions for complex designs.

Design entry that work on behavioural descriptions are the standardized languages like VHDL and Verilog. These enable the designer to create a more abstract description of the FPGA functionality. It is still possible to write structural descriptions or even mixed structural and behavioural design descriptions. Nowadays, these languages are most commonly used to write designs for FPGAs. The VHDL/Verilog design itself is architecture independent and a special VHDL/Verilog syntheses tool, called a VHDL compiler, is necessary to map the described design to the target FPGA architecture. Due to the market competition of several VHDL compiler vendors, these tools improve all the time. The outcome of these improvements are better syntheses results and also a steady increase of the highest-possible VHDL abstraction level.

Hardware design on an abstraction level like the C language is also possible with tools like HandelC [3] or Transmogrifier C [4]. The HandelC language for example allows a sequential and parallel computation and it also has a complete channel concept to synchronize running processes. Porting existing C code can be easily done with only a few changes, but for exploiting the whole possibility of parallelism, the code has to be rewritten in the HandelC language. These tools are available either free like the Transmogrifier C or as a commercial product like the HandelC, but only rarely used through lack of the operating frequency of the output results. The difference between a HandelC design and optimized VHDL design concerning the design operating frequency can be up to a factor of 10.

Today, design performance depends highly on three characteristics: the resource utilization, the number of clock cycles needed for processing one data word, and the design operating frequency.

Due to the rapidly growing resources on the FPGA devices the resource utilization will not be that important in the future. So the main focus concerning the overall performance of the design is the product of the needed clock cycles with the operating frequency. Optimizations on the clock cycles result in only small performance gain, because the average needed clock cycles is only 1 or 2. That means the operating frequency will become the parameter with the main influence on the overall performance. This operating frequency will also increase in the future, because of new chip manufacturer processes.

The difference in the operating frequency which are existing nowadays between the module generators or the VHDL/Verilog tools and the high level languages will decrease due to two reasons. One reason is the almost perfect performance of the VHDL tools, whereas the high level language tools have only started to optimize the output code. The other reason is based on the FPGA architecture, that allows to pack more complex logic with lots of inputs in only one or two cascaded cells to keep the propagation delay small. This will enable the high level languages in the future to generate FPGA designs that are comparable to hand optimized VHDL designs nowadays, but with less development time.

### FPGA Applications

The areas where reconfigurable computing architectures have demonstrated very high performance are usually applications with a high degree of parallelism, such as image processing, pattern recognition, feature extraction, object tracking and sequence matching [8, 9, 10, 11].

More and more companies distribute commercial products based on FPGAs [12].

The FPGA group at the Chair of Computer Science V from the University of Mannheim is involved in building reconfigurable computers, based on FPGAs, and employing them for edge detection [13], traffic sign identification, volume rendering, and last but not least pattern recognition and feature extraction in high energy physics (Fopi detector at GSI in Darmstadt and ATLAS detector at CERN).

## 3 Prospects of Reconfigurable Computing for the ATLAS LVL2 Trigger

### The ROB Complex Application

In view of a ROB design it is well possible today to implement a ROB-in in an FP GA. FPGAs combined with processors seem to be a very powerful combination for the entire ROB complex [14, 15].

Such a combination allows to split the relatively simple high speed jobs from the slower intelligent ones (like bookkeeping the ROB buffer memory) [16]. It may be an option to include a processor core as a piece of Intellectual Property (IP) in the FPGA so a separate processor is not necessary. Although IP is a fast growing market is is not clear if processor cores with properties such as necessary (routing large amounts of data high speed) will be available in the future. FPGA devices today already have enough usable gates and on chip memory to implement the high-speed input part of a ROB-in. However now and in the future it will not be possible to implement the rather large ROB buffer memory on chip. Separate memory has to be used. The high-speed input part of the ROB-in design can be implementedconsidering a system speed of 40 MHz. The speed of interfacing to the outer world (I/O-pins) will not increase as much as on chip speed. Therefore system speed for future ROB-ins will probably increase less than the on chip speed would suggest. Because gate count is growing quadratically with the shrinking chip structure size, it may be an option to concentrate more high-speed ROB-ins on one chip.

### The Feature Extraction Application

Modelling [17] shows that the required size of the LVL 2 trigger farm is determined primarily by the need of the B-physics trigger to execute a feature extraction (FEX) in the full inner detector volume. This can be achieved by a full scan of either the pixel detector or the transition radiation tracker (TRT), to produce track candidates, which are used as track seeds for a Kalman filter (or Hough transform) algorithm in the semiconductor tracker (SCT).

Here, as an example the full scan of the TRT is discussed, though a full scan of the pixels is also attractive. Profiling of a C++ implementation of the algorithm, which is based on a Hough transform using Look-up tables (LUTs) shows that most of the computing time is spent with access of LUTs, incrementing of 8-bit numbers, and a non-linear 2-D filter operation (local maximum finding) [20]. All these algorithms are very good candidates for an FPGA implementation. The track fit requires only a small fraction of the overall computation time (see Table 2) and is based on floating-point computations, which cannot be implemented efficiently in FPGAs. Therefore the strategy is to execute the time-consuming pattern recognition on the FPGA FEX co-processor and the fit on the CPU. Measurements done with ATLANTIS [18] with a different algorithm implementation are shown in [19].

#### 3.2.1 Memory Requirements for the TRT FEX

The full scan TRT algorithm is detailed in [20]. The FPGA part of the algorithm consists of the following steps: initial track finding (Hough transform), threshold application and local maximum finding and track splitting.

The RAM requirements for a search range of 10 24 \(\times\) 80 pre-defined \(\phi\)\(\times\)\(p_{T}\) (end-caps: 960 \(\phi\)\(\times\) 80 \(p_{L}\)) for the different steps of the algorithm are as shown in Table 1.

Since the initial track finding step is completely independent of the track splitting step, the "straw-ordered" and "bin-ordered" LUTs are never needed at the same time and can therefore be implemented in the same physical RAM at different addresses. They are always read-only.

The "histogram" array and the "active straws" hash-table are used consecutively. Both need read-write access and are initialized to zero and filled once per event. This allows to use the identical RAM for both.

The "histogram" array stores the histogram counters. A histogram counter corresponds to a pre-defined road with given \(\phi\) and \(p_{T}\). The histogram counters are divided into 80 \(p_{T}\) blocks with 10 24 \(\phi\) each (end-caps: 960 \(\phi\)\(\times\) 80 \(p_{L}\)). The maximum number of hits in a pre-defined road can occur in the end-caps with 224 hits, therefore 8 bit data (word length) are sufficient, which is easily available in the FPGA.

The"straw-ordered" LUT stores for all TRT straws (barrel and end-caps, using detector symmetry) the corresponding histogram counters which have to be incremented. The 1-2 \(\phi\) values out of 10 24 possible values, which have to be in cremented, are stored in 11 bit (10+1), because the (possibly) 2 values are always directly neighboured.

The "activestraws" hash-table is filled once per barrel-half and three times per end-cap with the pattern of 0's and 1's corresponding to straws without and with hits. It provides the possibility to tell quickly whether a certain straw has a hit or not. It can also store threshold (high/low) and drift-time (5 bit) information. Without storing drift-time information, only 2 bit word length are required (hit/no hit and high/low threshold).

The "bin-ordered" LUT stores for each pre-defined (\(\phi\), \(p_{T}\)) road (using detector symmetry; for end-caps 960 \(\phi\) bins defined to increase the symmetry) all corresponding straws. The data is organized in 75 \(\times\) "layer/plane" and 10 bit "straw in layer/straw in plane". For the end-caps three passes with different addresses are needed for the 224 planes, but this is no problem, since the symmetry in the end-caps is very high. For the barrel the 32-fold symmetry allows to store only 32 \(\phi\)\(\times\) 80 \(p_{T}\) = 2560 bins. The end-caps require with the 192-fold symmetry 5 \(\phi\)\(\times\) 80 \(p_{L}\) = 400 bins. Each bin in the end-caps requires three passes, thus 120 addresses are needed for the end-caps. A 12 bit address at the external SRAM (with 750 bit word length) is sufficient to store the "bin-ordered" LUT.

The total amount of required SRAM for both LUTs is 13 bit address (12 bit each) and 880 bit data, which is less than 1 MByte1. The organisation of the SRAM can also be 14 bit address and 440 bit data or 15 bit address and 220 bit data or 16 bit address and 110 bit data. This is even nowadays available on ATLANTIS

\begin{table}
\begin{tabular}{c c} \hline \hline
**Application** & **Size** \\ \hline \hline ”straw-ordered” LUT & 12 bit address, 880 bit data ( = word length) \\ for track finding & (80 \(p_{T}\) blocks \(\times\) 11 bit each) \\ \hline ”bin-ordered” LUT & 12 bit address, 750 bit data \\ for track splitting & (75 layers/planes \(\times\) 10 bit each) \\ \hline ”histogram” array & 80 \(\times\) (10 bit address, 8 bit data) \\ for track finding & read-write access \\ \hline ”active straws” hash-table & 75 \(\times\) (10 bit address, 7 bit data) \\ for track splitting & read-write access \\ \hline \hline  & **Totally** \\ \hline SRAM for LUT & 13 bit address, 880 bit data \\ \hline internal block RAM & 80 \(\times\) (10 bit address, 8 bit data) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Required RAM for the different steps of the algorithm.**_For barrel and end-caps, using symmetry. For 1024 \(\times\) 80 pre-defined \(\phi\)\(\times\)\(p_{T}\) roads (end-caps: 960 \(\phi\)\(\times\) 80 \(p_{L}\)). Track splitting in the end-caps in executed sequentially in 3 z-ranges, to shrink the ”bin-ordered” LUT size. The SRAM can be partitioned and read in several passes, e.g. 14 bit address, 440 bit data and two passes instead of 13 bit address, 880 bit data and one pass._and the FPGA co-processor 1999/2000. With 880 bit word length only one pass is required, with each division of the word length by two the number of required passes doubles. Todate systems are restricted by the availability of RAM blocks, as mentioned below, therefore currently having e.g. 110 bit or 220 bit data per FPGA is sufficient.

The RAM blocks, which can be used consecutively for both read-write arrays, require a size of 80 blocks with 10 bit address and 8 bit data. This is only 80 kByte of RAM, but it should be DPRAM (Dual Ported RAM) for an efficient implementation and it should be addressed in parallel with 80 different addresses. The optimal solution is to use internal block RAM for the 80 RAM blocks. This allows a fast, simple, nice and efficient implementation. Having all histogram values at once "in the memory" allows the local maximum finding to eliminate multiple track candidates in the entire (\(\phi\), \(p_{T}\)) range without any 'borders'. It will be available in the year 2001 in one FPGA, e.g. in the Xilinx Virtex XCV 3200 E. This chip comprises 104 kByte of block RAM, which is more than the required 80 kByte of block RAM. However, as long as 80 internal RAM blocks of the required size are not yet available, both the initial track finding (histogramming) and the track splitting can be performed in several separate passes. This leads to a small degradation of the capability of the maximum finding step to eliminate multiply reconstructed tracks and complicates the implementation. Acceptable solutions for present reconfigurable computing units are:

* The problem can be distributed to 2 computing boards with 4 FPGAs each. Then each FPGA needs to store 10 RAM blocks. 4 of them can be stored internally and 6 RAM blocks can be stored in the external SRAM. For the LUTs 110 bit per FPGA are sufficient.
* An alternative is to use one computing board with the memory distribution as before. Then two passes are needed for the initial track finding. The track splitting can be executed in the host CPU or executed sequentially in the FPGA.
* **FPGA co-processor 1999/2000 [21]*
* The initial track finding can be done in 10 passes with 8 internal RAM blocks. 10 passes are needed due to the restricted internal RAM resources and the SRAM word length of 144 bit. The track splitting can be executed in the host CPU or executed sequentially in the FPGA.

#### 3.2.2 TRT FEX Implementation

An FPGA co-processor of the year 2001 with 5 MByte SRAM and an XCV 3200 E FPGA is assumed as underlying hardware architecture for the implementation. The FPGA, which is already advertised by Xilinx, comprises 876000 logic gates and 104 kbyte of block RAM. 5 MByte of SRAM on an FPGA co-processor are nowadays already possible, and only 1.25 MByte is required for the LUTs, using the high detector symmetry.

The existing VHDL implementation for an XCV 300 (the 1999/2000 co-processor) is mapped to the XCV 3200 E (the 2001 co-processor). This unit should exist in each LVL2 farm computing node once. This architecture allows that each computing node can compute any assigned event completely without the need to communicate with other computing nodes.

Usually an implementation in FPGAs makes use of massive parallelism to gain a considerable speed-up. Both steps of the algorithm can exploit an 80-fold parallelism and make use of the fast internal block RAM. The main difference between an FPGA implementation and a CPU implementation of the Hough transformation is, that the inner loop, which is executed once per active straw and increments \(\sim\)130 histogram counters, is executed sequentially in the CPU and in parallel (two FPGA clock cycles) in the FPGA. A schematic view of the algorithm is shown in Figure 3. The presented algorithm implementation takes advantage of both the external SRAM and the entire available internal RAM blocks.

The initial track finding works as follows: the symmetry of the detector in \(\phi\) is used to store the LUT in a very compact format. The symmetry for the barrel is 32-fold. For the end-caps the symmetry is 192-fold, this is the biggest common denominator of 960 ( = 5 \(\times\) 192) pre-defined roads in \(\phi\), 768 ( = 4 \(\times\) 192) straws in the "short wheels" and 576 ( = 3 \(\times\) 192) straws in the "long wheels". The symmetry in z is used as well. This requires an address space of 12 bit for the straws in the "straw-ordered" LUT. The SRAM itself has 17 bit addresses, of which 12 are used to identify the current straw. The LUT stores for each straw address 880 bit (80 \(\times\) 11 bit). The word length of the SRAM is only 320 bit, therefore 3 passes with the same straw address (12 bit) and changing pass addresses (2 bit) are needed. This explains the use of 14 address bit of the SRAM. The other address bits are not needed for the "straw-ordered" LUT.

The histogramming for each straw is executed in three passes with one external

Figure 3: **FPGA track finding and local maximum finding.**_The straw address is divided into the module number and the ”straw in module” number. Using the high detector symmetry, only 1.25 MByte of SRAM is needed for the full scan of the TRT (initial track finding, local maximum finding and track splitting, both barrel and end-caps)._

clock cycle each, whereas a CPU implementation requires to loop sequentially over 130 histogram bins.

The FPGA implementation transmits 80 \(\times\) 11 bit words (in three passes). One 11 bit word always contains 1 or 2 histogram counters which have to be incremented. The construction of the LUT with overlapping roads in \(\phi\) and \(p_{T}\) guarantees that each active straw contributes to exactly one or two pre-defined \(\phi\) values of a certain pre-defined \(p_{T}\). For the barrel, there are 80 '\(p_{T}\)-blocks' with 1024 possible \(\phi\) each, thus 81920 patterns are pre-defined. Two of such '\(p_{T}\)-blocks' are shown in Figure 4. The SRAM data codes the histogram counters which have to be incremented. The histogram is stored distributed in the RAM blocks in the FPGA. The implementation profits from a true DPRAM to allow a fast read-modify-write cycle during one external SRAM cycle.

After the histogramming step the maximum finding step first applies a threshold filter and the a local maximum finding within each block. The maximum finding has to be applied in two dimensions (\(\phi\) and \(p_{T}\)). This is done in two steps. First, histogram counters (data stored in RAM blocks) above a certain threshold which are a maximum with respect to the two neighbouring values are output. This corresponds to the maximum finding in \(\phi\). Then, the output of the different RAM blocks are compared to the corresponding values of the directly neighboured \(p_{T}\)-blocks and only if they are also the local maximum in \(p_{T}\) they are output. Resulting are the histogram counters with values above the threshold and which are local maximum in both \(\phi\) and \(p_{T}\). The output information contains the pre-defined \(\phi\) and \(p_{T}\) values of the track (pre-defined from the LUT) and the number of active straws corresponding to the track.

When the track splitting step is also done in the FPGA (efficiently possible at latest with the XCV 3200 E, which makes the RAM blocks of the required size possible), the results of the local maximum finding step are stored internally in the FPGA for the following track splitting.

The track splitting step uses the RAM blocks (each block for one layer/plane) for a hash-table, which stores for each straw if it is active or not ("active straws" hash-table), and the threshold information (and optionally the drift-time). The SRAM is used to address the bin-ordered LUT with histogram bin numbers above the threshold which were a local maximum. The LUT outputs all straws which belong to the pattern definition and all active straws of them (stored in "active straws" hash-table). The result is a list of track candidates with all straws which contributed to the track. This result is passed to the host CPU for the track fit.

Figure 4: \(p_{T}\) **block.**_Each RAM block in the FPGA corresponds to the histogram of a \(p_{T}\) block in the detector (barrel). Shown is a \(p_{T}\) block for low-\(p_{T}\) tracks (left) and one for high-\(p_{T}\) tracks (right). For the end-caps, each \(p_{L}\) block consists of straight lines with one common intersection point._

#### 3.2.3 TRT FEX Benchmarking

CPU and FP GA can execute in parallel, thus the execution frequency is determined by the sum of FPGA processing time and PCI transmission time, which is 2.77 ms + 0.4 ms = 3.17 ms. The latency for each event is determined by the sum of the FPGA processing, PCI transmission and CPU processing to 5.67 ms. The speed-up in execution frequency between a 600 MHz Athlon and a 600 MHz Athlon with co-processor of the year 2001 is 152.7 ms / 3.17 ms = 48. However, until then CPUs will be faster by probably a factor 2-3 for this application due to 64 bit architectures and higher clock frequencies, which results in a remaining speed-up of factor \(\sim\) 20.

#### 3.2.4 FEX Application Conclusions

It has been shown that in the year 2001 an FPGA co-processor with one large FPGA (e.g. XCV 3 200 E) and SRAM with large word length (e.g. 320 bit) in combination with an commodity PC are capable of executing the full scan of the TRT in 3.17 ms. Putting this time in the paper model [22] results in the numbers of processors shown in Table 3:

This table shows that for B-physics the entire FEX of the inner detector should be done in the FPGA co-processor. Therefore, apart from the TRT FEX the SCT/pixel FEX should be studied as well. It is likely that a Hough transform in the SCT/pixels can be implemented in the FPGA co-processor, and the LUTs can be stored in the remaining free SRAM of the co-processor. The situation for a Kalman filter still has to be analysed.

\begin{table}
\begin{tabular}{c c c} \hline
**Steps of the** & **Execution Time** & **Execution Time** \\
**Algorithm** & 600 MHz Athlon & FPGA co-processor \\  & (measured) & (simulated, 2001) \\ \hline \hline Initial & & \\ Track Finding & & \\ \hline Threshold and & & \\ Local Maximum Finding & & \\ \hline Track Splitting and & & \\ Extended Maximum Finding & 36.8 ms & 1.5 ms \\ \hline Fit and & & \\ Final Selection & 2.5 ms & - \\ \hline \hline
**Full Algorithm** & 152.7 ms & 2.77 ms \\ \hline \end{tabular}
\end{table}
Table 2: **Execution time comparison of the C++ implementation and the combined FPGA/CPU implementation.**_TRT full scan for \(b\overline{b}\rightarrow\mu X\) events including pile-up at low luminosity. FPGA co-processor with XCV3200 E, 80 MHz internal and 40 MHz external clock frequency and with 320 bit word length (simulated). Track finding and local maximum finding implemented in VHDL. Event latency: 2.77 ms FPGA processing + 2.5 ms CPU processing (fl) + 0.4 ms PCI transmission = 5.67 ms. Event frequency determined by 2.77 ms FPGA processing + 0.4 ms PCI transmission = 3.17 ms; CPU and FPGA compute in parallel (different events)._

## 4 Conclusions

It has been shown that the evolution of FPGAs and FPGA co-processor systems in the next 1-2 years close the currently existing gap between the required system complexity for both an FPGA ROB complex and an FPGA FEX-co-processor and the available system complexity. This allows a compact, reliable, fast and cheap implementation of an FPGA ROB complex and an FPGA FEX-co-processor, used for the B-physics FEX at low luminosity. Both applications could be based on an identical FP GA co-processor, which is adapted to the different requirements by dedicated sub-modules satisfying the different memory- and I/O-requirements.

## References

* [1]_Configurable Computing: A Survey of Systems and Software_, K. Compton and S. Hauck, Technical Report, 1999.
* [2] Simulating FPGA-Coprocessors using FPGA Development System CHDL 2. K. Kornmesser et al., International Conference on Parallel Architectures and Compilation Techniques (PACT), Paris, France, 1998.

Footnote 2: Further information: [http://mp-pc53.informatik.uni-mannheim.de/fpga/chdl/index.html](http://mp-pc53.informatik.uni-mannheim.de/fpga/chdl/index.html)
* [3]_HandelC_3
Footnote 3: [http://www.embedded-solutions.ltd.uk/corp_page.htm](http://www.embedded-solutions.ltd.uk/corp_page.htm)
* [4]_Transmogrifier C Hardware Description Language and Compiler for FPGAs_4, D. Galloway, IEEE Symposium on FPGAs for Custom Computing Machines (F CCM), Napa Valley, April 1995.

Footnote 4: [http://www.eecg.toronto.edu/EECG/RESEARCH/tmcc/tmcc](http://www.eecg.toronto.edu/EECG/RESEARCH/tmcc/tmcc)
* Special Interest Group 5

Footnote 5: [http://www.pcisig.com](http://www.pcisig.com)
* [6] The Next Generation of I/O Servers 6

\begin{table}
\begin{tabular}{c c c} \hline \hline \multicolumn{3}{c}{**Number of LVL2 computing nodes**} \\ \hline B-physics in CPU & 
\begin{tabular}{c} TRT FEX in FPGA \\ SCT/pixels FEX in CPU \\ \end{tabular} & B-physics in FP GA \\ \hline
631 & 357 & 194 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Number of LVL2 computing nodes.**_Low luminosity, 40 kHz LVL1 rate, with B-physics (TRT FEX and SCT/pixels FEX). Each computing node consists of one commodity CPU and one FPGA co-processor of 2001. TRT FEX time simulated (3.17 ms), Same speed-up as for the TRT assumed for the SCT/pixels Hough transform (2.54 ms). Studies for the SCT/pixels Kalman filter still have to be done. Numbers calculated with the paper model [22]._* [7]_PAM-Blox High Performance FPGA Design_7, O. Mencer, M. Morf, M.J. Flynn, Adaptive Computing IEEE Symposium on FPGAs for Custom Computing Machines (FCCM), Napa Valley, 1998.
* [8]_Building and Using a Highly Programmable Logic Array_, M. Gokhale et al., IEEE Computer, 24(1):81-89, January 1991.
* [9]_An FPGA-based Hardware Accelerator for Image Processing_, D. Ross et al., Proceedings of the 1993 International Workshop on Field-Programmable Logic and Applications, 1993.
* [10]_Finding Lines and Building Pyramids with Splash 2_, A. L. Abbot et al., Proceedings of FCCM, April 1994.
* [11]_Flexible Image Acquisition Using Reconfigurable Hardware_, M. Shand, FCCM 1995.
* [12]_Programmable Logic Research Groups_8
Footnote 8: [http://www.pitamagic.com/research.html](http://www.pitamagic.com/research.html)
* [13]_Schnelle Berechnung von 2D-FIR-Filteroperationen mittel s FPGA-Koprozessor micro Enable_, S. Hezel and R. Manner, 21. DAGM Symposium Mustererkennung, Bonn, 15-17 September 1999.
* [14]_The active ROB complex_, R. K. Bock et al., Future DAQ note, 29 October 1999.
* [15]_An Overview of the Mannheim ROB Complex Hardware_9, M. Muller, November 1999.
* [16]_FPGAs Now (October '99) and in the Future_10, P. Jansweijer, 29 October 1999.
* [17]_Pilot Project Modelling Page_11
Footnote 11: [http://www.nikhef.nl/pub/experiments/atlas/daq/ROBCompler-3-11-99/atlantic-robcomplex.pdf](http://www.nikhef.nl/pub/experiments/atlas/daq/ROBCompler-3-11-99/atlantic-robcomplex.pdf)
* A Hybrid Approach Combining the Power of FPGA and RISC Processors Based on Compact PCI_, K. Kormesser et al., International Symposium on Field Programmable Gate Arrays, Monterey, California, February 1999.
* [19]_Measurements Document, ask Holger for exact Name..._ A. Kugel et al., future DAQ note, 1999.
* [20]_Pattern Recognition in the TRT for the ATLAS B-Physics Trigger_, C. Hinkelbein et al., ATL-DAQ-99-012, 21 September 1999.
* [21]_GIF-Board, ask Lorne for reference paper...._ L. Levinson et al., 1999.
* [22]_Paper modelling of the ATLAS LVL2 trigger system_, J. Vermeulen, 19 November 1999.