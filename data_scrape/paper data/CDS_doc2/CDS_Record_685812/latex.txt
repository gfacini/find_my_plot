**ATLAS Internal Note**

**DAQ-NO-088**

**6 May 1998**

**EXPERIENCE WITH FIBRE CHANNEL**

**IN THE ENVIRONMENT OF THE ATLAS DAQ PROTOTYPE "-1" PROJECT**

G. Ambrosini\({}^{1}\), H.P. Beck\({}^{2}\), D. Francis\({}^{1}\), M. Joos\({}^{1}\), G. Lehmann\({}^{1,2}\), A. Mailov\({}^{3}\),

L. Mapelli\({}^{1,4}\), G. Mornacchi\({}^{1}\), M. Niculescu\({}^{1,5}\), K. Nurdan\({}^{3}\), J. Petersen\({}^{1}\), D. Prigent\({}^{1}\),

J. Rochez\({}^{1}\), M. Romano\({}^{6}\), R. Spiwoks\({}^{1}\), L. Tremblet\({}^{1}\), G. Unel\({}^{3}\), E. van der Bij\({}^{1}\), T. Wildish\({}^{1}\)

**Abstract**

Fibre Channel equipment has been evaluated in the environment of the ATLAS DAQ prototype "-1". Fibre Channel PCI and PMC cards have been tested on PowerPC-based VME processor boards running LynxOS and on Pentium-based personal computers running Windows NT. The performance in terms of overhead and bandwidth has been measured in point-to-point, arbitrated loop and fabric configuration with a Fibre Channel switch. The possible use of the equipment for event building in the ATLAS DAQ prototype "-1" has been studied.

_(Submitted to Nuclear Instruments and Methods in Physics Research)_

\({}^{1}\) CERN, Geneva, Switzerland

\({}^{2}\) Laboratory for High Energy Physics, University of Bern, Switzerland

\({}^{3}\) Bogazici University, Istanbul, Turkey

\({}^{4}\) Project leader

\({}^{5}\) Institute of Atomic Physics, Bucharest, Romania

\({}^{6}\) Politecnico di Milano, ItalyIntroduction

The data acquisition (DAQ) system of the ATLAS experiment [1] at the LHC is expected to contain an event building (EB) system with a total bandwidth of several GByte/s. This system shall be able to assemble event fragments from a few hundred to thousand sources at an input frequency of a few kHz. A list of the parameters is shown in table 1. The Fibre Channel (FC) standard is a high-speed data transfer standard which offers both the high bandwidth and the high degree of connectivity, that is required for this system.

The final design for the ATLAS DAQ system is not scheduled to take place before 1999 and further investigations of detector requirements, hardware and software technologies as well as integration issues are still needed. The ATLAS DAQ group is approaching these pre-design studies with a structured prototype [2] which will be based on the current understanding of the ATLAS DAQ architecture [1]. The prototype (called "-1") will contain a fully functional small-scale EB prototype with around ten sources and ten destinations. This prototype shall assemble event fragments, of 1 to 10 kByte, from all sources in one destination, and several events may be assembled concurrently in several destinations. Performance is not the primary issue for this prototype but shall be watched in comparison to the requirements for the final system.

FC equipment has been investigated in order to learn about this technology and to understand the feasibility of FC for the EB prototype. Several evaluation studies of FC cards in point-to-point, arbitrated loop and fabric configuration have been carried out. The results will be presented in this paper.

## 2 System Overview

### Fibre Channel Standard

The Fibre Channel standard [3] defines a high-speed data transfer mechanism that can be used for networking, storage and data transfer. Fibre Channel is organized into the following levels:

* FC-0 defines the physical media, like optical and copper media, for a variety of data rates and transfer distances.
* FC-1 defines the transmission protocol, the 8-bit/10-bit coding and the receiver and transmitter state descriptions.
* FC-2 defines the signalling protocol, including control primitives, the frame structure and different classes of service.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline event size & 1..15 kByte & \# sources & 100..1500 \\ per source & & & \\ \hline input & 1..5 kHz & \# destinations & 100..200 \\ frequency & & & \\ \hline data rate & 1..75 MByte/s & & \\ per source & & & \\ \hline total data rate & 1..20 GByte/s & \# nodes & 200..1700 \\ \hline \end{tabular}
\end{table}
Table 1: Parameters for the ATLAS Event Building System* FC-3 defines a set of services which are common among multiple ports of a node.
* FC-4 defines the mapping between the lower levels of FC and upper level protocols, e.g. the Small Computer System Interface (SCSI) and the Internet Protocol (IP).
* FC supports several topologies: point-to-point, arbitrated loop and fabric. In a point-to-point topology two nodes are directly connected. In an arbitrated loop several nodes are connected on a shared medium. In a fabric topology ports are connected to a node in the fabric (i.e. a switch or a network of switches) which routes data traffic from one port to another.

Data are organized in sequences which are made from frames with a length of up to 2112 Byte. Frames can be sent one at a time (single frame sequence, SFS) or in groups sharing the same header (multi-frame sequence, MFS).

Several classes of service are defined: class 1 is a service for transfers with dedicated connections. Class 2 is a service for connection-less transfers where frames are acknowledged. Class 3 is a service for datagram transfers.

Flow control is defined for class 2 and class 3 on a credit-based buffer-to-buffer (BB) flow control between every two directly connected ports: the receiver sends R_RDY primitives whenever it is ready to receive more frames. Class 1 and class 2 use a credit-based end-to-end (EE) flow control between two end-ports: the receiver sends ACK frames to acknowledge the reception of frames.

End-ports have to exchange service parameters, including BB and EE credit, before transferring data. This can be done implicitly by assuming parameters or by an initialisation protocol using an PLOGI frame to an end-port or an FLOGI frame to a port on the fabric.

### Network Interface Cards

Two types of FC/PCI cards, from Systran Corp. and Interphase Corp., have been tested (see figure 1).

The FibreXpress FX cards from Systran Corp. [4] have a local memory into which data have to be moved before they are picked up by the FC protocol chip, Tachyon. The V3 controller is a bridge between PCI and the local memory bus. The cards are available in the PCI and the PMC form factor. The 5526 PCI FC adapters from Interphase Corp. [5] have a bridge (TPI) between PCI and the FC protocol chip, Tachyon. Both FC cards use the Tachyon chip from Hewlett Packard Comp. [6] as the FC protocol chip and use multi-mode optical link modules

Figure 1: FC Network Interface Cards

(GLM) running at 1.0625 Gbit/s for the physical medium1. Whenever necessary PCI/PMC or PMC/PCI adapters from Technobox Corp. [7] have been used to adapt the cards from one form factor to the other.

Footnote 1: Some tests presented in this paper have been repeated with twin-ax copper link modules running at 1.0625 Gbit/s without any significant difference in the results.

The Tachyon chip handles several queues for input and output. The outbound command queue contains headers and data blocks for outgoing sequences. When the data are sent a message about the completion is put into the incoming message queue. The SFS and MFS queues contain data blocks for incoming data. When a complete sequence has been received a message about the reception is put into the incoming message queue.

### Host Computers

Two different host systems have been used for the evaluation of the FC equipment: RIO2 and PC. The RIO2 8060 or 8061 is a VME-based I/O board from CES [8] with a PowerPC running at 64 MHz or 96 MHz, respectively. The PowerPC and its memory are connected to the PCI bus via the 27-82660 (Lanai-Kauai) PCI bridge from IBM [9]. The RIO2 has two PMC slots2. The operating system used on the RIO2 was LynxOS 2.4 [10], a real-time flavour of UNIX. The PC is based on a Pentium processor running at 166 MHz. The operating system used on the PC was Windows NT 4.0.

Footnote 2: Some tests have been carried out on RTPC 8067 [8] which has the same architecture as the RIO2 8060 but only one PMC slot.

### System Software

For LynxOS a low-level library [11] was developed. This library maps the resources of the FC cards directly into the user space. The library allows to initialize the FC card, to send and receive sequences. On the Systran cards a send or receive consists of two transfers: the application moves data between the host memory and the card's memory using DMA, and the Tachyon chip moves the data between the card's memory and the FC link. On the Interphase cards the transfer is done in one step between the host memory and the FC link (see figure 1). The frame size can be programmed on the Tachyon chip and was chosen to be 2 kByte for all tests. All messages are sent in one sequence (SFS or MFS, depending on the message size) and for every class 1 transfer a dedicated class 1 connection is setup before and removed after the sequence. The sending of several messages is pipelined using the Tachyon chip's outbound queue. For the incoming message queue, polling was used. An interrupt-driven scheme was implemented and tested but not further investigated because it will not be used in the final application.

For Windows NT the FX light-weight protocol (FXLP) driver from Systran was used. The FXLP defines a simple, connection-oriented protocol in the FC-4 layer. Connections between two applications running on two different nodes are opened and closed using FC sequences. A blocking send and receive mechanism is used. Class 3 is used for the transfer of data which are sent and received in a sequential way, i.e. one at a time, doing DMA and FC transfer separately. FXLP is implemented for a loop topology, but a switch topology is not supported yet. Multicast and broadcast are not supported either. For tests between RIO2 and PC the LynxOS low-level library was extended to follow the definition of the light-weight pro tool defined by FXLP. It implements the same API as for the Windows NT driver and represents a relatively small layer on top of the low-level library.

### Method of Measurements

The aim of the evaluation was to measure latency and throughput of transfers. Therefore, test programs consisting of a send application from one or several senders to one or several receivers were used. The sender applications run in a loop sending messages as fast as possible while the receivers run in a loop waiting for messages to arrive. The time for individual transfers of a given message size is measured in a loop over many transfers. The measurement is repeated for different sizes and a linear fit of transfer time versus message size is used to obtain _overhead_ and _bandwidth_ parameters as in equation 1:

\[\mathrm{time}\,=\,\,overhead+\frac{\mathrm{size}}{bandwidth} \tag{1}\]

The _overhead_ is a measure for the transfer overhead resulting from hardware and software. For class 1 it also includes the overhead for establishing a class 1 connection, and for class 1 and class 2 it includes the round-trip-time because the ACKs have to be received before the transfer is finished. The error of the measurement for the _overhead_ is about 1 \(\upmu\)s. The _bandwidth_ parameter is the saturation bandwidth for large messages and is mainly determined by the hardware (e.g. DMA). The error of the measurement for the _bandwidth_ is about 0.5 MByte/s.

For detailed and more precise measurements an FC analyser has been used. The analyser uses the FC pre-processor from RMKI/KFKI [12] which consists of an optical splitter and a pre-processor for the 8-bit/10-bit coding. The output of the pre-processor can be examined with a Hewlett Packard logical state analyser.

## 3 Performance in Point-to-Point Configuration

### System Cards on RIO2s

Figure 1 shows the data rate vs. the size of the messages between two Systran cards on RIO2s in point-to-point configuration [13]. The fit parameters are very similar for all classes with an _overhead_ of 25 \(\upmu\)s and a _bandwidth_ of 54 MByte/s.

The _bandwidth_ of 54 MByte/s represents 54% of the FC capacity. This value is the result of two factors: the architecture where the concurrent DMA and the Tachyon transfers compete for the local memory bus, and the limited DMA bandwidth itself. The concurrence on the local memory bus can be influenced by the Tachyon chip's read and write stream parameters. These parameters determine how long the Tachyon chip can hold the bus tenancy and thereby set the relative priority of the two transfers. The read and write stream parameters can have values of 1, 4, 16 and 64. The maximum data rate was found for both parameters having a value of 4. The small differences in the data rates for the different classes can be explained by the varying performance ratio of the two transfers for different sizes. The second limitation of the bandwidth is the DMA itself. This was measured separately to be 67 MByte/s reading from the host's memory and 58 MByte/s writing into the hosts's memory. This limitation seems to be determined by the RIO2 and has been observed before [14]. Using a special configuration of the low-level library with pre-loaded data, i.e. suppressing the DMA and only having one transfer, a _bandwidth_ of 96 MByte/s has been measured. Future versions of the Systran cards will have a dual-port memory which will overcome the first limitation and increase the throughput.

The _overhead_ values are very similar for all three classes and it can be concluded that the round-trip-time and the overhead due to a class 1 connection cannot be measured this way. The end-to-end performance for messages of 1 kByte is 23 MByte/s or 24 kHz, and for messages of 10 kByte 45 MByte/s or 4.6 kHz.

### Interphase Cards on RIO2s

Figure 2 shows the data rate vs. the size of the messages between two Interphase cards on RIO2s in point-to-point configuration [15].

The connection-less class 3 transfer results in the highest data rates, whereas the connection-oriented class 1 and class 2 transfers have a lower data rate because they have to wait for ACKs (class 1) or for R_RDYs and ACKs (class 2). The angles in the curves for class 1 and class 2 transfers that occur at the frame size (= 2 kByte) can be explained by the credit-based flow-control mechanism which allows pipelining of the R_RDYs and ACKs in multi-frame

Figure 3: Data Rate for the Interphase Cards

Figure 2: Data Rate for the Systran Cards

sequences, so that transmission of the next frame can be started before the ACK (R_RDY) of the previous frame has been received1. The degree of pipelining depends on the BB credit (for R_RDYs) and EE credit (for ACKs). A value of greater or equal 3 was found to lead to the highest data rates. Alternatively the ACK_0 mechanism could be used. In this mechanism only one ACK at the end of a sequence is required. The border between SFS and MFS is defined by the programmable frame size and it has been found that reducing the frame size can increase the data rate [13]. The final value has to be chosen depending on the kind of traffic expected.

Footnote 1: This effect has not been so pronounced with the Systran cards due to the lower rates (see figure 2).

The fits for class 1 and class 2 have been carried out for the SFS and MFS separately: for SFS the _overhead_ values are 14 \(\upmu\)s and the _bandwidth_ 31 MByte/s; for MFS the _overhead_ values are 53 \(\upmu\)s and the _bandwidth_ 63 MByte/s. For class 3 the _overhead_ value is 6 \(\upmu\)s and the bandwidth 63 MByte/s, the same as for class 1 and class 2 for MFS. The saturation bandwidth is 63% of the FC capacity and is limited by the PCI as can be seen using the analyser (see section 3.3). The _overhead_ values cannot be used to measure the round-trip-time and the class 1 connection overhead. The end-to-end performance for data messages of 1 kByte is 34 MByte/s or 35 kHz for class 3 and 21 MByte/s or 22 kHz for class 1 and class 2. For messages of 10 kByte the end-to-end performance is 60 MByte/s or 6.1 kHz for class 3 and 47 MByte/s or 4.8 kHz for class 1 and class 2.

### Analyser Results

The analyser has been used to measure some parameters which could not be measured with the statistical method. The propagation delay on the optical fibres could be determined to be \(4.9\pm 0.1\) ns/m, corresponding to a refraction index of n \(\approx 1.47\). On the Interphase cards it was seen that the _bandwidth_ for reading from PCI is about 66 MByte/s, while that for writing to PCI is about 76 MByte/s. On the Systran cards, read and write bandwidth of the Tachyon chip from and to the local memory of more than 100 MByte/s have been measured, when there was no contention of the local memory bus. Other specific parameters of the Tachyon chip could be measured: the R_RDY response time of the Tachyon chip after receiving a frame is \(560\pm 10\) ns, the time to process a R_RDY when waiting for BB credit and the data are available is \(430\pm 10\) ns. The overhead due to establishing a class 1 connection is \(3.0\pm 0.2\)\(\upmu\)s.

### Interoperability

The tests have been repeated with the two different types of the FC cards being connected in a point-to-point configuration. It turns out that the cards from the two different manufacturers interoperate without any problems running on RIO2s and using the low-level library.

## 4 Performance in Arbitrated Loop Configuration

### Topology

In an FC arbitrated loop several nodes share the same medium. Only one node can transmit data at a time. A node which wants to transmit data must arbitrate for the loop control using the ARB primitive. Once it obtains the control, it uses the OPN primitive to open a connection to the destination node on the loop. The transmission of data then follows the point-topoint transfer. When all data are sent the node uses the CLS primitive to close the connection and to release the loop control for another phase of arbitration. A typical arbitrated loop configuration with four nodes is shown in figure 4.

The loop initialization protocol is used to dynamically assign arbitrated loop physical addresses (AL_PA). A unique world-wide name (WWN) for each node is used to distinguish between the nodes. It is task of the user or system software to map between static WWNs and dynamic AL_PAs. The Tachyon chip supports only class 2 and class 3 on the arbitrated loop.

### Loop Overhead

The overhead of the transfers due to the arbitration and opening phase compared to simple point-to-point transfers has been measured on RIO2s and with the low-level library [16]. It is of the order of a few percent for most message sizes, except for message sizes of a tenth of the frame size where the degradation is of the order of 10%.

Using the analyser it was found that every node introduces a delay of \(225\pm 10\) ns to forward the ARB and OPN primitives. Since the loop's BB credit at opening was chosen to be 0, an additional time of \(510\pm 10\) ns has to be added for the generation and processing of the first R_RDY. With two nodes on the loop, the overhead is about \(2\,\upmu\)s. The total _overhead_ for a whole sequence depends on how often a connection has to be opened. If the Tachyon chip is programmed to close the loop when it has no more data or BB credit available (see section 4.4) the loop will have to be re-opened once the next frame(s) can be sent. The total _overhead_ increases with the message size, effectively reducing the _bandwidth_.

### FXLP with PCs

Tests with Systran cards on PCs have been performed [17]. DMA measurements of data transfers between the PC's memory and the memory of the FC/PMC card have been carried out: the _overhead_ is \(50\,\upmu\)s, the _bandwidth_ from the FC/PMC card to the PC's memory is 85 MByte/s, and 100 MByte/s in the other direction. Using the FXLP driver for Windows NT from Systran, the end-to-end performance of data transfers between two PCs was measured to have an _overhead_ of \(75\,\upmu\)s and a _bandwidth_ of 40 MByte/s.

In order to interoperate FC cards on PCs and RIO2s, the FXLP driver for Windows NT has been used on the PCs and the FXLP extension of the low-level library on the RIO2s. Sending messages of \(64\,\mathrm{k}\mathrm{B}\mathrm{y}\mathrm{t}\mathrm{e}\) from PC to RIO2, a sustained data rate of \(40\,\mathrm{M}\mathrm{B}\mathrm{y}\mathrm{t}\mathrm{e}\mathrm{/}\mathrm{s}\) has been observed. From RIO2 to PC the data rate was \(33\,\mathrm{M}\mathrm{B}\mathrm{y}\mathrm{t}\mathrm{e}\mathrm{/}\mathrm{s}\). Sequential send had to be used in the low-level library as it turns out that the FC BB credit mechanism to slow down the sender

Figure 4: Arbitrated Loop with Four Nodes

works correctly, but that the FXLP driver cannot handle receiving data faster than 40 MByte/s. PCs and RIO2s can interoperate using the existing FXLP driver and the low-level library in a loop configuration.

### Overlapping Transfers

On an arbitrated loop several transfers of MFS can overlap, if the Tachyon chip is programmed to close the loop when it has no more data or BB credit available. Another transfer can make use of the inter-frame gaps and thus efficiently overlap with the first transfer. When overlapping transfers and using class 3, frames can be lost when the transfers go to the same receiving node. The Tachyon chip can only handle one incoming MFS at a time and will drop all incoming class 3 frames not belonging to the current sequence. Using class 2, the Tachyon chip also drops incoming class 2 frames not belonging to the current sequence, but in addition, it will send back a frame (P_BSY) to indicate that it is busy. The sending node can then retry to send the frame. The Tachyon chip can be programmed to wait for the ACK to the first frame and to retry up to 16 times if an P_BSY is received. When 16 retries are not sufficient, the user level of the software has to take action. Of course, the penalty of this method is a bigger overhead for the transfers due to the blocking on the first frame, but the advantage is that no frames can be lost.

If the Tachyon chip was not programmed to close the loop when it has no frames to send, then transfers could not overlap and an EB-like application on an arbitrated loop would be limited to the bandwidth of a single transfer. Allowing transfers to overlap and using class 2 with blocking on the first frame and retry, an EB-like application on the arbitrated loop can increase the performance and not lose any frames. This method has been tested with two nodes acting as EB sources and two nodes acting as EB destinations. The data rate vs. message size is shown in figure 5. The aggregate data rate does not scale with the number of destinations since the maximum bandwidth of this configuration is limited to 100 MByte/s and the nodes spend additional time for the arbitration. However, an increase of about 50% between the 1\(\times\)1 EB-like system to the 2\(\times\)2 EB-like system can be observed and a total bandwidth value of 80 MByte/s has been fitted in the latter case, corresponding to 80% of the maximum bandwidth. This value is, however, not reached for message sizes of 64 kByte due to an increased

Figure 5: Event Building using Arbitrated Loop (Class 2)

overhead_ of 40 \(\upmu\)s and will only be reached at message sizes of about a few hundred kByte.

### Broadcast

Broadcast on an arbitrated loop is defined for class 3 and for the well known AL_PA = 0xFF. A loop credit has to be given for as many frames which have to be broadcast since no R_RDY is sent back from the nodes. Broadcast of SFS and MFS of up to 4 kByte have been tested. The data were correctly received by all the nodes on the loop, including the sender. The performance of broadcast is the same as for unicast class 3 transfers. Multicast is not defined on a loop.

## 5 Performance in Fabric Configuration

### Brocade Switch

The FC cards have been tested successfully with the _SilkWorm_ FC switch from Brocade Communication Systems, Inc. [18],[19]. This switch supports only class 2 and class 3 and allows full connectivity between any of its ports. A typical configuration with four nodes is shown in figure 6.

The switch requires an explicit login of all ports at initialization using an FLOGI frame. A physical address is assigned by the switch corresponding to the port the node is connected to.

### Switch Overhead

For class 2 comparisons of point-to-point transfers and single transfers going through the switch have shown the delay of the sequences through the switch to be of \(2\pm 1\)\(\upmu\)s, confirming the manufacturer's specification. Using the analyser, the same time, measured as the time between a class 2 or class 3 start-of-frame going into the switch and coming out of it was found to be \(1740\pm 10\) ns. No limit on the throughput was observed even when using the special configuration of the low-level library for the Systran cards (see section 3.1) which transfers data at 96 MByte/s.

The switch responds faster to incoming frames than the Tachyon chip. The R_RDY is generated \(430\pm 10\) ns after the end-of-frame. When there is a frame in the switch which cannot be sent due to missing BB credit the time to process an R_RDY takes between 500 and 1200 ns due to time-slicing in the switch [18].

### Concurrent Transfers

In order to test the switch's behaviour under load, comparisons of single transfers and

Figure 6: Switch with Four Nodes

concurrent, but mutually exclusive transfers, using class 2 and class 3 have been carried out. No degradation of the rates has been observed. Up to three concurrent transfers have been shown to have scaling behaviour: the data rates of the transfers add up and no internal bottleneck was encountered. The switch has been loaded with about 200,000 frames/s (for messages of 4 Byte) and with about 180 MByte/s (for messages of 64 kByte). This is only a small portion of the load the switch can accept, the manufacturer states that the switch is capable of accepting a total of 8 million frames/s and 3.2 GByte/s aggregate bandwidth.

An EB-like application can use this scaling of the rates directly if it has an external mechanism of synchronisation guaranteeing that all transfers at any given time are mutually exclusive. If such a mechanism is not available several MFS can overlap at an EB destination. Due to the limitation of the Tachyon chip, the method described in section 4.4, using class 2 transfers, blocking on the first frame and retry has to be used to guarantee that no frames are lost. This method has been tested with up to six FC cards from Systran and Interphase. The data rates are shown in figure 7, for 1\(\times\)1, 2\(\times\)2 and 3\(\times\)3 EB-like systems. EB using the switch shows very clearly scaling behaviour of the data rate with the number of destinations. In particular, the data rate for message sizes of 1 kByte scales with 20 MByte/s and for 10 kByte with 35 MByte/s for each destination. Comparing these values to the values for point-to-point configuration (see sections 3.1 and 3.2), there is no difference for messages of 1 kByte (SFS). The throughput decreases by about 30% for messages of 10 kByte (MFS) because of the bigger _overhead_ due to the blocking on the first frame.

### Multicast and Broadcast

The _SilkWorm_ switch allows multicast and broadcast. Multicast which is only defined for class 3 requires registration of nodes in a multicast group. The registration is handled by an alias server which is an integral part of the _SilkWorm_ switch. Broadcast is also only defined for class 3 but does not require registration: it uses the well-known address identifier 0xFFFF. The protocol to register multicast groups with the alias server of the switch has been implemented and multicast and broadcast of transfers have been tested successfully. The throughput, however, decreases by an order of magnitude compared to unicast class 3 transfers. This degradation is not fully understood. The manufacturer did not publish how the routing mechanism works and how the information from the alias server is obtained.

Figure 7: Event Building using an FC Switch (Class 2)

### Network Management

The switch can be configured and monitored using a dedicated ethernet port and the telnet protocol. The switch also contains an SNMP server and defines the standard MIBs as well as the experimental FC MIB and a switch specific MIB. The name server is an integral part of the Brocade switch and allows the mapping of physical addresses to world-wide names. The protocol to read the information of the name server has been implemented in the low-level library and can be used to obtain a complete map of the nodes connected to the switch. It has to be noted that the standard defining the name server (FC-GS2) is not yet stable and that FC does not define an address resolution protocol for the mapping of the logical world-wide names to the FC physical addresses. The name server is intended to overcome this shortcoming.

### Cascaded Switches and Attached Loops

Other interesting features of the switch which have not been tested in our laboratory are the cascading of switches and the attaching of arbitrated loops to the switch. Both features allow to build big networks for hundreds of nodes. Attached loops will be available soon and allow to build EB systems in which arbitrated loops are used to group several EB sources sharing the bandwidth of the loop before they are connected to a network of switches. The number of switch ports can thus be reduced and each switch port will be used much more efficiently. This will reduce the cost of the system considerably. A typical configuration using attached loops and cascaded switches is shown in figure 8.

The scaling capabilities of cascaded switches and the behaviour of the end-to-end latency will have to be investigated. The behaviour of huge networks carrying the data traffic for the ATLAS EB system will have to be simulated. Previous simulations, however, show already that if the individual switch latency is small compared to the total transfer time and if the switch is non-blocking up to the rates of the ATLAS experiment, that a cascaded switching network will be scaling [20]. These simulations will have to be continued once results of measurements with cascaded switches and attached loops are available.

## 6 Conclusion

Experience with FC technology has been obtained. Hardware and software components

Figure 8: Network using Arbitrated Loops and Cascaded Switcheshave been tested successfully with simplified network applications in the environment of the ATLAS DAQ prototype "-1". Measurements have shown sustained data rates of 50 to 60 MByte/s from end-user to end-user. Low latencies of 10 to 20 \(\upmu\)s, including the software overhead, have been observed. The equipment has been tested in several network topologies, like point-to-point, shared medium and switched topologies. FC offers potentially the high bandwidth and the high degree of connectivity required for the ATLAS EB system (see table 1). In addition, FC could also be used for other applications in the ATLAS DAQ system, e.g. local data collection and mass storage.

Using the FC equipment for EB, the following two features have to be considered: a limitation in the assembly of FC sequences and inefficiency in the interfaces between the FC card and the host system. The Tachyon chip can only assemble one MFS at a time. If event fragments are greater than one frame (of up to 2 kByte), an additional synchronization mechanism (see section 4.4) has been proposed. This mechanism provides reliable data transfer on arbitrated loops and with an FC switch, at the cost of a performance penalty of about 30%. It has to be investigated if eventually FC protocol chips will be available which are able to support multiple concurrent MFS. Alternatively, it could be investigated if class 1 transfers could be used for EB, in particular with switches which support stacked connect-requests.

The interface between the FC card and the host system has a hardware and a software aspect. The host system's I/O bus and the internal resources of the FC cards are not matched to the bandwidth of the FC link. Effectively, only 50 to 60% of the FC bandwidth is available to the end-user. Commercial drivers for the platforms in the environment of the ATLAS DAQ prototype "-1" do not make efficient use of the FC hardware. The driver for Windows NT does not support a switch topology and in order to interoperate FC cards on PCs and RIO2s the loop configuration has to be used. The only system software available for LynxOS which makes satisfactory use of the FC cards had to be developed especially for this purpose.

FC is a candidate technology for EB and will be used for an implementation of the EB in the ATLAS DAQ prototype "-1" where issues like detailed integration and scalability will be addressed. The rapidly moving industrial developments will have to be watched in order to understand how FC compares to other technologies in terms of cost and performance as well as in terms of availability of components and software off-the-shelf and the longevity of this technology.

###### Acknowledgements.

 We would like to thank the people from Systran and from Brocade for their technical support and for temporary loan of equipment used in this evaluation. We are grateful to A. Razc, D. Samyn and T. Ladzinski from the CMS collaboration at CERN for the useful discussions and for lending us some FC/PCI cards needed to complete our tests.

## References

* [1] ATLAS Collaboration, Technical Proposal for a General-purpose pp Experiment at the Large Hadron Collider at CERN, CERN/LHCC/94-43, see [http://atlasinfo.cern.ch:80/Atlas/Welcome.html](http://atlasinfo.cern.ch:80/Atlas/Welcome.html).