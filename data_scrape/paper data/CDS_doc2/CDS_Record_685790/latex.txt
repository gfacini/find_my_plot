**ATLAS Internal Note**

**DAQ-No-69**

**13 May 1997**

**Evaluation of the RCNP ATM Network and Computing Facility**

**for Emulation Purposes of Network Based Trigger/DAQ Systems**

Research Report of the JSPS Fellow

Irakli MANDJAVIDZE

_DAPNIA/SEI, CEA Saclay, 91191 Gif-Sur-Yvette, France_

## Introduction

Event Selection and Data Acquisition systems for future High Energy Physics experiments [1],[2] will be based on high performance networks to satisfy the stringent demands on data transfer bandwidth. The study of various candidate network technologies is an important activity of several R&D projects. Groups of physicists and engineers from CERN\({}^{1}\), KEK\({}^{2}\), RCNP\({}^{3}\), CEA Saclay\({}^{4}\) and other institutes unified their efforts within the CERN RD31 project [3] to investigate the capabilities of Asynchronous Transfer Mode (ATM) [4] technology to satisfy the specific needs of Trigger/DAQ applications. The experience gained from these generic investigations on ATM based event builders contributed a lot to the proposal of an original architecture for the ATLAS event selection and DAQ system [5]. Two small scale demonstrators (at CERN and Saclay) have been constructed to validate the proposed principles. However, the scalability of event builder systems remain an important issue. Simulation studies of the "final" Trigger/DAQ systems predict good scaling characteristics of ATM networks under the expected traffic patterns. The emulation of event builders on large scale commercial ATM systems could confirm these predictions.

Recently, a completely new computing facility has been installed at RCNP. It is entirely based on DEC computing and networking technologies. An ATM backbone network, formed by DEC GIGA switches, connects DEC servers and workstations. The RCNP is gradually moving from the old Ethernet and FDDI computing networks towards the new one based on ATM. Professor Masaharu Nomachi proposed to use this ATM computing facility for R&D projects while the ATM network is in the transition phase and the load on it has not reached its nominal value. The proposition is very attractive as, at present, this is probably the largest ATM network accessible to researchers for R&D studies. It can bring an important experience in designing network based DAQ systems.

A detailed proposal [6] of collaboration between RCNP and CEA Saclay on the emulation of the ATLAS Trigger/DAQ system at the RCNP computing network has been elaborated at CEA Saclay. However, at the time of preparation of this document, answers to at least two important questions were not known, that could have prevent the realisation of the project, namely:

* whether the ATM technology from DEC would allow users to access the native ATM layers (e.g. AAL5) of the network protocol stack (contrary to widespread TCP and UDP over IP layers) for data transmission and reception.
* whether the RCNP ATM network could be configured, according to some specific needs of event building research studies, without disturbing the work of normal users.

A less stringent constraint, but nevertheless important for the duration of the project, is the portability on Digital UNIX platforms of the ATLAS Trigger/DAQ system demonstrator software developed at Saclay for LynxOS and WindowsNT operating systems.

To address these issues Professor M. Nomachi applied to JSPS\({}^{5}\) for my invitation at RCNP for a short period (16 days). I visited the research centre from 13th to 26th of February, 1997. The present document summarises my activity at RCNP.

## References

* [1] CERN, European Organisation for Nuclear Research, Geneva, Switzerland
* [2] KEK, National Laboratory for High Energy Physics, Tsukuba, Japan
* [3] RCNP, Research Centre for Nuclear Physics, Osaka University, Osaka, Japan
* [4] CEA Saclay, Research Centre of the French Department Of Atomic Energy, Paris, France
* [5] JSPS, Japan Society for Promotion of Science

## 1 Research Activity

The evaluation of the RCNP computing network was carried out in several steps. At first, to familiarise myself with Digital Unix, DEC stations and servers, I have performed several benchmarks. This allowed to study code portability issues on Digital UNIX, as well as to estimate the performance of DEC computers in terms of computational power and real time response. Following this exercise, I have performed some measurements of networking capabilities of DEC stations and servers. With the help of DEC engineers, the network has been configured in a suitable way. Finally, I have installed on the RCNP cluster the event building software I have developed at Saclay. I have been able to emulate a nine node event builder system.

Before going into a detailed discussion of the benchmarks and their results, I shall briefly describe the RCNP computing facility (Figure 1). An ATM backbone network, formed by four GIGA switches, connects two Digital "8400" and four Digital "4100" servers as well as about twenty Digital "500" stations.

In the Digital "8400" server, a 1.6 GByte/s bus connects 12 ALPHA processors running at 440 MHz, with 4 GByte of shared memory and a PCI bridge. DEC "ATMWorks 350" adapter attached to PCI bus provides connection to the ATM network at 155 Mbit/s rate (622 Mbit/s in few months). The CPU subsystem is interfaced via PCI to SCSI adapter to a tape robot with as much as 7 TByre of total capacity.

The Digital "4100" server comprises four ALPHA CPU-s running at 400 MHz. They are connected to 1 GByte of shared memory and PCI bridge by a 1 GByte/s bus. The peripherals attached to the PCI bus interface the servers to the ATM network over 155 Mbit/s links, to a total of 100 GByte capacity RAID drives and to a total of 1 TByte capacity tape robot.

Two "8400" servers and four "4100" servers are interconnected via an 8-port Memory Channel Hub (not shown) with a throughput of 1Gbit/s per port.

The Digital "500" stations are single processor machines which run at 333 MHz. Each is equipped with 256 MByte of memory. About twenty such stations will be connected to the ATM network by 155 Mbit/s links.

Digital ATM fabrics are build around 13 x 13 non-blocking cross-bar switch with an aggregate bandwidth of 10.4 GBit/s. Each of 13 slots of a fabric supports 800 Mbit/s and can be equipped with either four 155 Mbit/s links or one 622 Mbit/s link. GIGA switches provide Constant Bit Rate (CBR) and Unassigned Bit Rate (UBR) services. According to DEC engineers the Available Bit Rate service will be provided soon. A proprietary credit based flow control mechanism guarantees lossless data transmission in networks based on DEC ATM equipment [7].

### Digital UNIX timer resolution

The evaluation of timer granularity obtained from _gettimeofday()_ function call have shown that the Digital UNIX updates its internal timer at 1024 Hz rate. Though _gettimeofday()_ has 1 us resolution, its precision equals about 1 ms. Therefore, accurate measurements of latency for a single iteration of benchmarks and their distributions were not possible. In what follows the latency corresponds to an average time obtained from benchmark measurements repeated several hundred thousand times. Measured in such a way the _gettimeofday()_ function call overhead itself is about 1 us.

_Observation:_ It seems possible to get 1 us precision from _gettimeofday()_ function call. For this the Digital UNIX kernel has to be rebuilt adding a MICRO_TIME option to its configuration file. This question has to be discussed with the DEC engineers maintaining the RCNP computing network.

Figure 1: The RCNP computing network facility

### Computing power benchmarks

The first series of benchmarks aimed at evaluating the processing power of the DEC stations and servers installed at RCNP. It is based on a simplified version of the calorimeter feature extraction algorithm to be used in the ATLAS level 2 trigger (Figure 2). Two matrixes are presented to the algorithm which contain data on energy deposited in the Electromagnetic (EM) and Hadronic (HAC) calorimeter cells within the e/\(\gamma\)Region of Interest (RoI).

The simplified calorimeter feature extraction algorithm

The algorithm first finds energy peak within the RoI in the electromagnetic calorimeter. Then it calculates energy sums in different areas around the peak in both subdetectors. The algorithm proceeds by applying various criteria on ratios of the sums. Fixed point arithmetic is used with the only exception of ratios. Table 1 compares algorithm execution times on several platforms.

Note that the low values of the latency are due to the fact that the algorithm operates on pre-formatted data in the Region of Interest matrixes. For this measurements the processors do not perform any I/O operation to obtain data, nor do they prepare the matrixes from raw data.

### POSIX thread synchronisation benchmarks

The next series of benchmarks aimed at evaluating Digital UNIX compliance with the POSIX standard and estimating its real time characteristics. In the tests, a process created two threads: a Client and a Server. Both threads operate in the same memory space, namely the one of the process, and therefore can easily exchange information with each other. However, synchronisation of threads is necessary to guarantee mutual access to this information. The POSIX standard provides two means of threads synchronisation: mutexes and conditional variables.

_Observation:_ Digital UNIX is compliant with POSIX 1003.1c version. Minor modification was necessary for the benchmark software, which was relying on POSIX 1003.4a version. Modifications will also be necessary for the ATLAS Trigger/DAQ demonstrator software.

The sequence of benchmarks is as follows (Figure 3). The Server thread waits for a data request from the Client. It is inactive ("blocked" on a mutex or a conditional variable). The Client posts the request, "unblocks" the Server

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{2}{*}{Vendor} & \multirow{2}{*}{System} & \multirow{2}{*}{Processor} & Clock & \multirow{2}{*}{OS} & Execution time \\  & & & & \(\mu\)s \\ \hline \hline Sun & SS 20 & Sparc & 50 & Sun 2.3 & \textasci{} \\ \hline CES & RTPC & PPC 604 & 96 & Lynx 2.3 & \textasci{}15 \\ \hline HP & 735 & HPPA & 99 & HPUX & \textasci{}25 \\ \hline DELL & PC & Pentium & 133 & NT 3.5 & \textasci{}20 \\ \hline IBM & 43 P & PPC 604 & 133 & AIX 4.3.1 & \textasci{}10+ \\ \hline SUN & Ultra 1 & Sparc & 167 & Solaris & \textasci{}10- \\ \hline DELL & PC & PentiumPro & 200 & NT 4.0 & \textasci{}10 \\ \hline \hline DEC & Station & Alpha & 333 & Digital UNIX & \textasci{}5+ \\ \hline DEC & Server & 4 - 12 Alphas & 400-440 & Digital UNIX & \textasci{}5- \\ \hline \end{tabular}
\end{table}
Table 1: Execution time of the calorimeter feature extraction algorithm

Figure 2: The simplified calorimeter feature extraction algorithmand waits for data. Now the Client becomes inactive on its turn. The Server thread "prepares" data, notifies the Client thread and blocks. Then the sequence repeats.

The result of the benchmark measurement is the average latency between a data request and the data response. When data of minimal size is requested (1 word of 8 bytes for Digital UNIX), the POSIX threads context switch time is given by half of this latency. Table 2 shows the context switching time on ALPHA / Digital UNIX platform and compares it with the results obtained on other platforms.

### UNIX Inter Process Communication benchmarks

The next series of tests aimed at evaluating the standard UNIX Inter Process Communications (IPC) performance on servers and stations. In the test, two processes exchange information via shared memory. Access to the shared memory is protected and synchronisation of the processes is performed by IPC-s. One process, a Server, waits on an IPC mean (e.g. semaphore) for data request from another process, a Client. The Client, sends the request and waits on IPC for response from Server. The Server receives the request and places some dummy data in the shared memory location. Then it notifies the Client via IPC and waits again for request. Figure 4 shows the sequence of the test.

The performance for different types of inter process communication means has been measured. Table 3 lists the results for pipes, messages and semaphores and compares them with the results obtained from measurements on other platforms. In the case of pipes and messages the minimal size requests and responses of one word have

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline \multirow{2}{*}{Vendor} & \multirow{2}{*}{System} & \multirow{2}{*}{Processor} & Clock & \multirow{2}{*}{OS} & Mutex & CV \\  & & & MHz & & \(\mu\)s & \(\mu\)s \\ \hline \hline CES & VME SBC & PowerPC 604 & 96 & Lynx 2.3 & \(\sim\)20 & \(\sim\)30 \\ \hline DELL & PC & Pentium & 133 & WinNT 3.5 & \(\sim\)35 & - \\ \hline \hline DEC & Station & Alpha & 333 & Digital UNIX & \(\sim\)10 & \(\sim\)15 \\ \hline DEC & Server & 4 - 12 Alphas & 400 - 440 & Digital UNIX & \(\sim\)50 & \(\sim\)125 \\ \hline \end{tabular}
\end{table}
Table 2: POSIX thread synchronisation benchmark results

Figure 4: Test sequence for the Unix Inter Process Communication benchmarks

Figure 3: Test sequence for the POSIX thread synchronisation benchmarks

been used. In all cases the data exchanged via shared memory was 1 word.

### Point-to-point communication benchmarks

This series of benchmarks aimed at evaluating ATM communication performance of DEC platforms. Two nodes (stations or servers) interconnected via a GIGA switch exchange request/response messages. A Client node sends to a Server node a request message (Figure 5). The Server node responds with a message of the requested size. The Client has a possibility to pipeline several request messages without waiting for the corresponding response. Both nodes can send and receive data in blocking and nonblocking modes. The benchmarks allowed to study performance at different layers of the communication protocol stack.

Initially the DEC ATM software installed at RCNP was limited to communications over TCP and UDP Internet protocols via the socket interface. At a connection set up the signalling software dynamically establishes a so called Switched Virtual Connection (SVC) between the participant nodes. The SVC-s are maintained operational until an explicit request to close the connection is issued. During my stay in RCNP, DEC engineers upgraded the software so that it became possible to operate at a native ATM level by accessing the AAL5 layer of the protocol stack. However, in the current implementation, the AAL5 communications use the so called Permanent Virtual Connections. The PVC-s between communication nodes are set up in the GIGA switches by DEC engineers. DEC Application Programmers Interface (API) for native ATM communications is based on a socket interface. As in the traditional TCP and UDP cases the nodes create sockets. Both nodes then "bind" the sockets to a corresponding PVC which assures connection between them.

_Observation:_ It is straightforward and quite easy to adapt applications, initially written with TCP/UDP sockets, to native ATM protocol of DEC platforms. For the AAL5 sockets there are some specific _ioct_ function calls, related to communication set up. Currently the sockets operate on datagrams. The streams service will be provided later. On reception an application can wait for datagrams on individual virtual connections (_recv_ or _read_ function calls). It can also wait for datagrams from several VC-s simultaneously by issuing _select_ function call. An application can send datagrams by _send_ or _write_. Multicast transmit operations are supported by DEC switching hardware.

Figure 6 shows benchmark results for point-to-point communications between workstations. The request message size fits in one ATM cell (48 byte). No pipelining of requests has been used to measure the round trip request / response latency (Figure 6.a). The Client has to wait about 100 us to receive the requested 32 bytes of data when workstations communicate over AAL5 sockets. This time is approximately twice as much for TCP and UDP sock

Figure 5: Point-to-Point communication benchmarks

ets. However, for 8 kByte response messages, the TCP sockets perform better. The explanation of this fact can be as follows. The native ATM API from DEC defines a default size for the largest AAL5 packet. It equals 8 kByte (while the maximum size as specified by the standard is 64 kByte). Bigger data buffers are transported by more than one AAL5 packet which results in higher software overhead on transmission and reception.

_Observation:_ The default size for the largest AAL5 packet is programmable in the DEC ATM application programmers interface.

Figure 6.b shows the highest sustained frequency of the request / response communications. The Client node pipelines up to 8 request messages to achieve on AAL5 sockets the maximum of about 16 kHz rate. The link between the Server and the Client is used at full bandwidth for response messages above 2 kByte and this determines the achievable rate. Measurements on transmission only, have shown that already for 1 kByte messages the _send_ function call latency corresponds to 100% user load on 155 Mbit/s links. The receiver however is not capable to follow the sender at this rate and therefore, it is difficult to judge, whether the link is really used at full speed. Sending a message which fits in one ATM cell takes about 20 us. The receiver can sustain these messages if they are sent up to a 20 kHz rate.

UDP communications are the slowest as pipelining of more than 4 requests causes rare losses of either requests or responses. TCP protocol recovers from the rare losses and allows to pipeline more requests giving slightly better performance.

The point-to-point communication benchmarks has been performed on servers as well. Figure 7 compares the performance of DEC stations and servers communicating over AAL5 sockets. As in the previous benchmarks, the stations show better real-time performance than servers.

### Event building emulation benchmarks.

The last series of benchmarks aimed at investigating whether the emulation of medium size event building systems is possible on the RCNP computer network. I have installed on Digital UNIX a simplified event builder software which I have developed in the past at Saclay for my educational purposes. Several nodes interconnected by the ATM network are emulating data sources, destination processors and a supervisor. The three types of nodes are implemented in the form of skeletons, which perform the core of the required functionality. The skeletons are linked with dedicated modules which contain function calls of specific nodes (e.g. modules which emulate behav

Figure 6: Benchmark results for Point-to-Point communications

Figure 7: Point-to-Point communications over AAL5 sockets on DEC stations and servers

iour of sources, destinations and a supervisor participating in a second level of event selection). The software is UNIX compliant and does not use any POSIX extensions. The skeletons of each nodes are implemented as multi-process programs. The processes communicate with each others via shared memory.

The functionality of the supervisor is distributed over three processes (Figure 8). One process periodically checks the availability of source and destination nodes by sending once per second an "echo" request message. This allows to detect malfunctioning nodes in the system and to perform dynamic load balancing of destinations assignment. The second process generates events and assigns them to destinations. The third process receives all incoming messages from the network and processes them.

The destination processor nodes are also implemented with three processes. The first process checks the state of sources once per second, sending "echo" request messages. The second process emulates algorithm execution on assembled events and issues a decision or status message to the supervisor. The third process receives all messages from the network and takes different actions according to the message type. It is responsible for sending requests to sources and for event building (including time-out).

The tasks of the source nodes are distributed between two processes. One process receives messages from the network and treats them. The data request messages are passed to another process which prepares event fragments and emulates data preprocessing.

The software supports both partial and full event building. Figure 9.a represents full event building with the push data flow from sources to destinations. The supervisor allocates a destination processor, and multicasts its ID to all sources. The sources push event data towards the destination, which assembles the event, processes it and issues a decision. When appropriate, the destination communicates the decision to the supervisor. Figure 9.b shows partial event building case, where a destination pulls data from sources.

Figure 8: Multiprocess organisation of the event builder nodes

Figure 9: Data flow protocols supported by the event builder software

The software has a layered structure. This allows to easily add/change functionality and facilitates the porting task. As shown in Figure 10, the network API hides from the application the underlying transport protocol layer. The event builder software can operate either on TCP, on UDP or on AAL5 layer of the communication protocol stack. Moreover, in the case of IP protocols, the software does not distinguish between Ethernet and ATM networks.

To port the software on the RCNP platform it was necessary to develop a network API for the DEC Native ATM implementation. As mentioned above, direct access to the AAL5 services requires the setting up of permanent virtual connections. To have flexibility in the choice of the event builder nodes it has been decided with the DEC engineers to set up a mesh configuration where any two nodes in the network are interconnected with dedicated PVC-s. I have proposed a logical scheme to assign identifiers to PVC-s, which uses last byte of IP addresses and a few bit code to distinguish the service classes provided. The scheme facilitates the translation between the PVC identifier and the communication node. The following scheme shown on Figure 9 has been agreed on and implemented on the RCNP computing network.

According to this scheme, several VC-s are opened between any two nodes on the network. The connections differ by their class and type. Nodes can exchange information via a PVC which provides one of the three classes of traffic: CBR (Constant Bit Rate), UBR (Unspecified Bit Rate), UBR with DEC proprietary Flow Control service class. The type determines whether the connection is point-to-point or multicast. To test the multicast operation a multicast tree has been set up between several nodes. Benchmark measurement have shown that in terms of performance there is no difference between point-to-point and multicast connections.

Any application can bind an AAL5 socket to the PVC which provides the required connection to a remote host by complementing the last byte of the remote host's IP address with the desirable connection type and class bits. The IP sub-net bit determines to which of the two sub-nets in RCNP the remote host belongs (Figure 11). The label translation of ATM cells, shown on Figure 11, is performed by the network and is transparent to the application.

It should be mentioned that the mesh configuration of the PVC-s is flexible enough and should be sufficient for much more complicated tasks of the ATLAS Trigger/DAQ system emulation, as well as for other experiments at RCNP on network based DAQ systems.

Figure 11: Convention for assignment of identifiers to PVC-s

Figure 10: Layered organisation of the event builder software

During my stay at RCNP the ATM network, being operational full time, was constantly in re-configuration phase. DEC engineers were adding new nodes to the network, performing different kinds of tests. Access to some servers and stations was restricted due to installation work. The largest configuration of an event builder emulator which I could obtain is shown on Figure 12. Three GIGA switches in three different buildings of the RCNP campus formed a network. Eight stations were emulating source and destination nodes. The supervisor task was running on a DEC "4100" server.

The poor resolution of the Digital UNIX timer prevented accurate measurements of event building or decision latencies. The event building emulation results presented in this report correspond to the following conditions. The nodes communicate over AAL5 sockets. The partial event building protocol is used (Figure 9.b). However, for each event a destination requests data from all sources. For all events the data fragments have equal length. There is no preprocessing in sources and no processing in destinations. The supervisor performs dynamic load balancing on destinations. The destinations have a fixed amount of credits. When an event is assigned to a destination, the supervisor decrements its credit. When the destination returns a decision, its credit is incremented. The supervisor loops over destinations and assigns events to the ones which can accept events. It stops the generation of events if all destinations have exhausted their credits. This simple scheme prevents the destinations, and possibly the whole system, from overflowing. It also allows to determine the highest event frequency that can be achieved on the system (Figure 13).

Figure 12: Configuration of an event builder emulator

Figure 13: Highest event frequency of the event builder emulator

Figure 13 shows the dependency of the highest sustained frequency on the event size and on the number of credentials per destinations. When destinations can accept only one event, the event building emulator operates from about 3 kHz to a little bit less than 4 kHz rate, depending on the event size. Rough measurements of the time difference between the moment when supervisor assigns an event to a destination and the moment when the destination returns a decision for this event show about 1 ms decision latency.

For small size events the maximum frequency can be increased significantly by assigning more than one event to a destination. It is not very different, however, if four or eight events are pipelined to a destination and the 11-12 kHz limit is due to the software overhead. For large events the maximum frequency is determined by the bandwidth of the links, which connects the GIGA switches. The load on the links approaches 100% for 4 kByte event fragments. Nevertheless, no data loss has been detected, probably due to the DEC proprietary Flow Control congestion avoidance mechanism.

### Summary

The benchmarking exercises on the RCNP computing facility allowed to get positive answers on the questions of feasibility of its use for the emulation of network based Trigger/DAQ systems. The DEC ATM technology provides direct access to the AAL5 protocol layer with surprisingly high performance for the current commercial systems. The network has been configured in a way which satisfies the specific needs of event building applications. The demonstrator software developed so far for the sequential solution of the ATLAS trigger selection has a well organized layered structure and there are good indications that porting to the Digital UNIX platform should not require big efforts.

The event building emulator described above is probably the only system implemented so far with a network composed of interconnected switches. This example shows that even on such a small system one can achieve high loads on the internal links of the network. Therefore, the investigation of network congestion and its avoidance mechanisms by emulation studies are possible on the RCNP network.

## 2 Visits, Meetings and Presentations

Recently, the group of physicists and engineers from CEA Saclay, to which I belong, has proposed a new strategy for the ATLAS event selection system for use downstream of the first level trigger. It aims at achieving a reduction factor sufficient to bring the final event rate down to a level suitable for on-line physics analysis and/or recording on permanent storage.The proposed triggering scheme proceeds with selection in a number of sequential steps to reject uninteresting events as soon as possible. At each step, only the event data which is necessary to make a decision is acquired and analysed. The full event reconstruction is performed only when it is required for physics analysis or calibration. This method allows to reduce bandwidth and processing power requirements. We have also proposed an implementation of this scheme based on a network with several tens of Gbit/s throughput, which links about 1000 read-out memories to a similar number of processing elements. The network carries data and protocol traffic. We are investigating the use of ATM packet switching technology to build this high performance network. Several research institutes have joined their efforts to validate the proposed principles and demonstrate viability of the architecture.

On behalf of Saclay group I made presentations to data acquisition groups in RCNP and KEK, which are composed of members of several Japanese research institutes. In the presentations I have described the requirements of the ATLAS Trigger/DAQ system and options for its realisation as proposed so far. Then I gave our motivation and goals for elaborating a new triggering scheme. I have presented the sequential event selection strategy and the architecture for the ATLAS Trigger/DAQ system based on it. Finally, I exposed ongoing activities around the architecture: modelling of the full scale system, design of small scale demonstrators and large scale emulators. The very high skill of japanese researchers in this field made the presentations very lively. Big experience of KEK and RCNP physicists and engineers in event building helped me a lot to explain our ideas on network congestion avoidance mechanisms. When visiting KEK installations I have been shown an original "Transparent Event Builder" system [8], to be used in the experiments planned on KEK B-factory.

In RCNP we had several working discussions with the DEC engineers who maintain the ATM network. I had also a very interesting meeting in the Osaka headquarters of DEC Japan with representatives of electronics design groups from DEC Japan and Hitachi Zosen Corporation. RCNP has an R&D project with DEC and Hitachi Zosen to implement the so called Read-Out Buffer (ROB) to be used in future HEP experiments. The ROB modules in ATLAS [9] receive from front-end electronics data of the events which have been accepted by the first level trigger. The ROBs provide access to the data for higher levels of event selection and keep the data until the final decision is issued for the events. I have made a presentation on the specific requirements of various components in the sequential event selection option for the ATLAS Trigger/DAQ system, such as ROB-s, data sources, destination processors and the network. Long discussions on these subjects continued in a very comfortable Japanese restarant, where I had the possibility to enrich my knowledge on advanced DEC and Hitachi technologies as well as on very fine Japanese culinary art.

I would like to specially mention that I was very impressed by the very modern networking and computing installations in RCNP and KEK. Going to Japan, I knew that I would have to work on the DEC ATM network of RCNP, but I unexpectedly discovered that KEK also owns an ATM network based on FORE Systems technology, the network being of about the same size as in RCNP, if not larger. Several switches, with aggregate bandwidth of 10 Gbit/s each, are interconnected by 155 Mbit/s and 622 Mbit/s links, forming a backbone network. Smaller switches with 2.4 Gbit/s bandwidth bring ATM to the desktop by 155 Mbit/s and 25 Mbit/s optical or electrical cables. The network, however, is not accessible for the R&D research programs.

The examples of the RCNP and KEK networking installations prove that Japanese physicists and engineers continue to work on the leading edge of modern electronics, computing and networking technology.

## 3 Acknowledgments

I would like to mention the extremely good organisation of the JSPS fellowship. A fellow gets support from JSPS already at a preparation phase of the visit. I had to apply for an entry via in Japan, which has been delivered in a very short time, as my stay in the country was sponsored by JSPS. Also the procedure to obtain travelling tickets implies minimal intervention of the visitor, thanks to the very efficient collaboration between JSPS and Kintetsu International Travelling Inc. Only the space limitation of this report refrains me from describing my luxury flight to and from Japan. The JSPS fellowship requires that the home scientists and the visitor prepare and agree on a rough schedule of visits within Japan before the fellowship period starts. This requirement helps the fellow to plan his/her academic activity in advance. I believe that this very good organisation of the fellowship permitted me, in the short period of time, to accomplish the complex task of evaluating the RCNP computer network. I would like to sincerely thank JSPS for giving me a possibility to visit Japan and acquire a very rich experience.

I would like to thank RCNP which kindly allowed to use its computing network for research studies on event building systems. I would like to specially mention my RCNP colleagues and DEC engineers, Mr. Hayakawa and Mr. Yakoshi, who were supporting me during my stay in RCNP. The working atmosphere was very friendly and joyful, helping to solve easily the problems we faced. I would like to thank colleagues from KEK, especially my friend A. Manabe, who made my stay in KEK so interesting and enriching.

I would like to thank professor Masaharu Nomachi, the inviting scientist, who applied to JSPS for my fellowship, made my visit to RCNP possible and with whom we already have had a long history of fruitful collaboration.

I would like to thank D. Calvet and J.-P. Dufey, my colleagues from the RD31 project, for reading this report and for their contribution with very valuable comments and suggestions.

## References

* [1] ATLAS collaboration, "Technical Proposal for a General-Purpose pp Experiment at the Large Hadron Collider at CERN", **CERN/LHCC/94-43**, December 1994.
* [2] CMS collaboration, "The Compact Muon Solenoid Technical Proposal", **CERN/LHCC/94-39**, December 1994.
* A high performance data-driven event building architecture based on an asynchronous self-routing packet-switching network", **CERN/LHCC/95-14.**
* the Broadband Telecommunications Solution, IEE Telecommunications Series 29, **ISBN 0 85296 815 9**, London, 1993.
* [5] J. Bystricky et al., "A Sequential Processing Strategy for the ATLAS Event Selection", in Proc. Nuclear Science Symposium, Anaheim, California, 3-9 November 1996. Also to be published in **IEEE Transactions on Nuclear Science, vol. 44, 3 June 1997**.
* [6] D. Calvet, "Proposal for a Collaboration with RCNP, University of Osaka, on the ATM Event Builder Demonstrator Program", Saclay Internal Document, December, 1996.
* [7] T. E. Anderson et al., "High Speed Switch Scheduling for Local Area Networks", Digital Research Center, 130 Lytton Avenue, Palo Alto, April 26, 1993.
* [8] Y. Nagasaka, M. Nomachi, O. Sasaki, M. Tairdate. "Performance Analysis of a Switch Type Event Builder with Global Traffic Control System", **IEEE Transactions on Nuclear Science, vol. 43, 1996**.
* [9] O. Boyle et al., "ATLAS ROB: Read-out Buffer for the ATLAS Experiment at LHC", [http://www.cern.ch/HSI/rob/URD](http://www.cern.ch/HSI/rob/URD).