[MISSING_PAGE_FAIL:1]

## 1 Overview

We describe here past, present and future work to demonstrate the feasibility of the level-1 trigger design presented in the ATLAS Technical Proposal [1], and described in more detail in a back-up document [2]. An intensive R&D programme has been pursued over the last three years with the general aims of exploring the problems of a digital implementation of a calorimeter trigger at level 1. This programme has involved continuing simulation studies of various algorithms for e.m. cluster-finding and bunch-crossing identification, and hardware implementation of the chosen algorithms in custom-designed ASICs.

Evaluation of these ASICs in a realistic environment was achieved by means of technical demonstrator systems, which were thoroughly assessed in a test beam. The first phase of this programme initially studied the operation of a first-generation cluster-processing ASIC at full LHC speeds, and later studied the performance of a single-channel BCID module. The final phase of this demonstrator programme, which is now under way, will explore the remaining areas of novel technology inherent in the full ATLAS system design, and a successful conclusion of this phase will allow a realistic prototype trigger system for ATLAS to be constructed.

A parallel R&D programme is investigating alternative architectures and technologies [3]. Techniques that are shown to lead to advantages in terms of performance and/or cost will be adopted for the final system.

## 2 Phase-I demonstrator programme

The following is only a summary of this work, full details of which may be found in reference [4].

### Trigger algorithm

The electron/photon trigger is based on a localised deposit of transverse energy in the electromagnetic calorimeter. Ideally, this cluster should fully contain the energy of an e.m. shower whilst being small in comparison with a jet core. As discussed in section 2 of reference [2], simulation work was carried out to study various possible algorithms, using a reduced detector granularity of \(\Delta\eta\times\Delta\phi=0.1\times 0.1\) with one depth sampling in each of the electromagnetic and hadronic calorimeters. The algorithm used is shown schematically in figure 1.

### Phase-I ASIC

The first phase of the demonstrator programme was designed to study the feasibility of implementing in a semi-custom ASIC the cluster-finding algorithm shown above. Its aim was to verify at full LHC speed and using affordable present-day electronics the efficiency and rejection power of the selected e.m. cluster-finding algorithm, using real-time signals from prototype LHC calorimeters in a test-beam environment.

Figure 1: Phase-I cluster algorithmThe ASIC [5] implemented the operations needed by the algorithm for one e.m. trigger channel, using 8-bit energy values from 16 trigger cells. Hadronic data were not processed in this first prototype chip, and the algorithm was slightly simpler than our present choice. Separate energy sums were formed with one horizontal and one vertical neighbour to find potential e.m. clusters, and the energy in the outer 12 e.m. cells was summed to examine isolation, as shown in figure 1. Two pairs of programmable thresholds were provided to allow for two different cluster conditions. The energy sum of all 16 input cells was also formed, as it would be needed for use in jet and missing-\(E_{\mathrm{T}}\) logic.

The ASIC was a 0.8 \(\mathrm{\SIUnitSymbolMicro m}\) CMOS gate array from Fujitsu, packaged as a 179-pin ceramic pin-grid array. The algorithm was implemented as a sequence of pipelined arithmetic stages, with the 12-bit total energy sum emerging after a latency of 6 clock cycles, and the cluster-hit flags after 7 clock cycles. Tests at frequencies up to 70 MHz were successful.

### Demonstrator system

The demonstrator system to test the ASICs (figure 2) fully processed a \(3\times 3\) area of calorimeter trigger cells, thus requiring data from a \(6\times 6\) area. Signals were digitised in 8-bit flash-ADC (FADC) modules and then passed to the cluster-finding module (CFM) containing the ASICs. High-speed inter-module communication was via a wire-wrapped backplane, with a clock module distributing correctly-phased strobe signals to the FADC modules and to the CFM. During each system clock cycle FADC data were transferred to the CFM, with trigger bits emerging from the ASICs after a latency of 7 clock cycles.

High-speed memories on the CFM stored 256 time slices of the incoming FADC information, output trigger hits, and ASIC energy sums. These memories scrolled continuously at the system clock frequency. On receipt of an event signal, a stop signal was generated (after a programmable delay) to freeze the CFM memories and allow readout of their contents via VME.

Data were recorded in CERN test beams with the prototype liquid-argon calorimeters of RD3 [6] and RD33 [7]. For the RD3 configuration trigger cells of the desired granularity were formed by adding analogue signals from the calorimeter both laterally and in two \(9X_{0}\) depth samples (figure 2). Most data were recorded at the 40 MHz LHC bunch-crossing rate, with some data also taken at the original LHC design frequency of 67 MHz. Using beam positions near the centres, edges and corners of trigger cells, data were recorded with different combinations of beam energy, particle type, cluster and isolation thresholds.

Figure 2: Test-beam calorimeter and trigger system block diagram

A full description of the results obtained from this programme may be found in references [4] and [8], so only a brief summary of the most significant conclusions is presented here.

Recording both the FADC data and the ASIC outputs enabled the energy sum calculation of the ASIC-embedded algorithm to be compared with its software counterpart. Over 80% of events showed perfect correlation, and exact agreement was still found in the remainder, but a trivial memory timing error had produced a small timing slip. Figure 3 shows the correlation, having corrected for this readout problem in the analysis.

To study the two types of thresholding operations separately, the isolation requirement was disabled in some data-taking runs. Figure 4 shows the effect of the cluster threshold set just above the pedestal sum for two channels.

The results of this demonstrator programme showed that pipelined electromagnetic cluster-finding algorithms could be implemented in semi-custom ASICs and operated efficiently at full LHC rates in realistic accelerator environments.

## 3 BCID demonstrator programme

A one-channel BCID demonstrator module has been constructed, which can be used to implement all the BCID algorithms previously evaluated off-line. The BCID module was successfully tested with the RD3 liquid argon calorimeter in a CERN test beam, as described in detail in reference [9].

Building on this work, a second demonstrator system is now being designed to perform BCID on the 36 calorimeter channels which are currently instrumented for the phase-I demonstrator programme. This system will consist of three wire-wrap modules, designed to be compatible with the phase-I crate infrastructure. Each module accepts 12 channels of data digitised at 40 MHz by new VME-based FADC modules. On each module, the BCID logic will be implemented in three Xilinx field-programmable gate arrays (FPGAs) [10], with each FPGA processing four channels of data. The FPGAs will allow a range of BCID algorithms to be tested, with the devices being re-configured to implement different algorithms as required.

Data will enter the BCID modules via front-panel sockets, and the results will be transmitted to the phase-I cluster processor module via the existing wire-wrapped backplane. Extensive diagnostic and test memory will be provided on all channels.

The full BCID demonstrator programme has four main objectives:* To commission and test the Xilinx BCID implementation.
* To commission and test the new flash ADC system (see section 4.1).
* To establish in a realistic accelerator environment that the BCID algorithms operate correctly when feeding several channels simultaneously to the cluster-finding algorithms in the phase-I ASICs.
* To record signals from other prototype calorimeters with different pulse shapes, and to attempt BCID.

This test programme will be carried out in the ATLAS test beam at CERN during 1995.

## 4 Phase-II demonstrator programme

Using relatively mature technologies coupled with some novel techniques, as described in section 3 of reference [2], we propose to design a fully-synchronous level-1 calorimeter trigger processor for ATLAS using only six crates. The viability of such a system depends crucially on several key areas, which will be studied in the final phase of the trigger demonstrator programme. We plan to test this system in the ATLAS test beam at CERN during 1996.

The first phase of the programme successfully demonstrated the operation of the pipelined cluster processing and BCID algorithms, so this final phase will concentrate on understanding the problems of high-speed optical data communication, integrated optics with embedded fibres, and data fan-out using transmission-line backplanes.

As shown in figure 5, the phase-II demonstrator system will fully process a \(3\times\quad 3\) array of electromagnetic calorimeter trigger cells (as in the phase-I programme), requiring signals

Figure 5: Phase II demonstrator system block diagram

from a 6 \(\times\) 6 trigger-cell array. The demonstrator will consist of an ADC system feeding digitised signals to a set of transmitter modules. The resultant optical data will be received by a series of trigger modules communicating with each other via a transmission-line backplane and also performing the cluster processing function. Central timing will be provided by a multi-phase clocking system, controlled in the test-beam environment by a beam trigger.

An important part of the demonstrator programme is that data monitoring facilities -- usually in the form of memories -- be provided at all possible points throughout the data-processing chain.

### ADC system

Signals from a total of 36 trigger cells, formed by analogue summation (laterally and in depth) of signals from the RD3 liquid argon electromagnetic calorimeter, will be digitised by a series of 4-channel flash-ADC modules. Constructed in VME format, these modules will perform 8-bit linear digitisation at \(\geq\) 40 MHz and output the data in real-time as balanced ECL signals.

For diagnostic purposes, each channel will be provided with a \(\geq\) 256-word read/write scrolling memory accessible from VME, which may be used to monitor the incoming data. By writing to these memories they will also act as data emulators for off-line playback of pre-recorded data.

By appropriate re-configuration of the input signals, it is planned that the ADC system will also be used for recording data from hadron calorimeter prototypes in the ATLAS test-beam, to study the efficiency of the BCID algorithms with differently-shaped signals.

### Transmitter module

Digitised data from the 36-channel ADC system will be transferred by twisted-pair cables to three 12-channel transmitter modules (figure 6) residing in the single 9U trigger crate. Each

Figure 6: Transmitter module block diagram (single channel).

module will process the incoming data and transmit high-speed optical data down six multi-mode fibres. VME interfacing will be used to access all on-board memories and registers.

The first stage of processing will be to perform pedestal subtraction and apply calibration constants by means of RAM-based look-up tables. The data will then pass to the BCID logic, which will be implemented in 4-channel Xilinx FPGAs. For each calorimeter pulse, only a single data sample (corresponding to the peak of the pulse for the appropriate bunch-crossing if operating in ATLAS) will be passed to the subsequent serialisation logic, all other samplings being set to zero.

The serialisation logic will combine pairs of channels and convert the six resultant 16-bit data fields into six serial bit-streams each running at \(\geq\) 640 Mbit/s. This conversion will initially be performed by a commercial chip-set, but R&D work is underway which may provide a low-cost custom alternative.

The final stage will convert the high-speed serial bit-streams to optical format, by means of laser diodes (or possibly LEDs), and inject the resultant signals into six multi-mode optical fibres. Carrying data from two calorimeter trigger cells, each fibre will be at least 65 m in length to represent the ATLAS data transmission environment as realistically as possible.

Following the general monitoring philosophy noted above, the transmitter modules will be provided with diagnostic memories at several points along the data processing pipelines, and it will thus be possible to compare the raw input data with the output of the calibration LUTs and the results from the BCID processors.

### Trigger module

The trigger modules, residing in the same crate as the transmitter modules, will combine the functions of optical receiver and cluster processor. A total of nine modules, each receiving and fanning-out data from a 2 \(\times\) 2 array of trigger cells, will be required to process a 3 \(\times\) array in a 6 \(\times\) 6 trigger cell window, as shown in figure 7.

In figure 8 we show how input data from the transmitter modules will arrive at each trigger module on two fibres, with two trigger cells sharing each fibre. The fibres will connect to fibre pig-tails on a single 4-channel MCM per module, where much of the signal processing will take place (see section 4.3.1). This MCM will be designed to resemble the MCM for the final system, as described in section 3 of reference [2], as closely as possible.

Serial data bit-streams will be transmitted from the MCM to the cluster-processing ASICs (via the serial/parallel ASICs) and also to the transmission-line backplane for fan-out to neighbouring trigger modules. To execute the cluster-processing algorithm for a maximum of

Figure 7: Mapping of trigger modules to calorimeter trigger cell array.

four trigger cells, each trigger module will require data from up to 25 trigger cells, four of which are fed directly from the MCM and the remaining 21 coming from the backplane.

Diagnostic monitoring memories, accessible via VME, will be liberally provided at strategic points in the data flow.

#### 4.3.1 Integrated-optics MCM

This front-end MCM, shown in figure 9, is a crucial component in the system. It consists of four identical processing channels, each consisting of a electrical-to-optical converter and two digital processor dies, and performing three main functions:

* Optical-to-electrical conversion of the four incoming serial bit-streams.
* Serial-to-parallel conversion to 16-bit words _(two trigger cells)_.
* Time-multiplexing each trigger channel byte into two serial lines, each running at 160 Mbit/s.

The first function -- optical to electrical conversion -- will be achieved by a novel type of embedded-fibre MCM, which is the subject of a current R&D project with an industrial partner. This daughter-MCM, described in section 3 of reference [2], will consist of an optical fibre pigtail, accurately located in a chemically-etched V-groove in the silicon substrate, and viewed from above by a PIN diode. Light from the fibre will then reach the diode by reflection from another etched planar silicon surface at 54.7deg, and the resulting PIN diode signal will feed a trans-impedance amplifier/discriminator die to produce a logic-level signal. A parallel R&D approach aims at handling many more fibres per MCM.

After testing, the four daughter-MCMs will be mounted on the main MCM with the four fibre pig-tails emerging via sealing collars through holes in the package.

Figure 8: Layout of trigger module

Figure 10: Dual-function demonstrator ASIC.

Figure 9: Four-channel integrated-optics MCM

The second function will utilise commercial serial-to-parallel dies (part of the chip-set to be used in the transmitter modules), one of which will be mounted on each channel of the main MCM (figure 9) to re-generate 16-bit words at 40 MHz. It should be noted that the only place in the system where high-speed (\(\geq\) 640 Mbit/s) signals will be found is in the extremely short connections between the embedded-fibre MCMs and these serial-to-parallel dies, which are all internal to the main MCM. No fast clocks will therefore appear at module level.

An important advantage of using such a commercial chip-set is that the fast clock (\(\geq\) 640 MHz) is recovered from the data stream via a phase-locked loop, independently on every channel, and therefore does not need to be distributed. This neatly eliminates the numerous complications inherent in signal-handling at these very high speeds.

As noted in section 4.2, it is hoped that current R&D work may result in a replacement low-cost custom chip-set that also consumes less power.

The final processing function -- to time-multiplex the eight data bits corresponding to each trigger cell onto two 160 Mbit/s serial lines as described in section 3 of reference [2] -- will be performed by part of a dual-function semi-custom ASIC (figure 10) mounted in die form on the main MCM.

#### 4.3.2 Dual-function demonstrator ASIC

As described in references [1] and [2], the cluster-processing ASICs for the final system will process a 4 \(\times\) 4 trigger cell array, taking in both electromagnetic and hadronic information. Most of the complexity of such a large new ASIC will be in the processing arithmetic for a 16-trigger cell algorithm, an operation which has been thoroughly studied in the phase-I demonstrator programme. To save costs, therefore, the phase-I ASICs will again be used for the cluster-processing function in the phaseII demonstrator.

These chips accept only parallel data, so it will be necessary to provide serial-to-parallel conversion for the 160 Mbit/s serial bit-streams coming from the MCMs. By combining this with the time-multiplexing function on a single chip, a dual-function ASIC can be designed, almost halving the non-recurring engineering (NRE) cost.

The upper section of the chip schematic in figure 10 shows the operation of the time multiplexing function, for which four of the ASICs will be mounted on the integrated-optics MCM in un-packaged die form. The serial-to-parallel conversion function, enabling the phase-I ASICs to be re-used in the new demonstrator system, will be performed in the lower section, but in this mode the chips will be used in a packaged format.

#### 4.3.3 Transmission-line backplane

To test the multi-layer high-density transmission-line backplane which will provide electrical data fan-out in the final system (see section 3 of reference [2]), a scaled-down version with restricted capacity will be designed for the demonstrator system crate, fanning-out 36 channels of data between nine trigger modules, but offering a similar high track density. This will enable a study of potential problems of cross-talk, signal reflections and timing synchronisation to be made in as realistic way as possible.

### Timing module

Located in the cluster-processing crate will be a VME-accessible central timing module, whose function will be to distribute the multi-phase clocks required by all parts of the system. As can be seen from figures 7.5 and 7.6 all parts of the system -- the FADC, transmitter and trigger modules -- will require a 40 MHz clock to advance the processing pipelines, and in addition the trigger modules will need a 160 MHz clock to operate the serial data transmission links between the MCMs and the cluster-processing ASICs.

A flexible system of fine-resolution programmable delays will be provided to allow precise synchronisation of data passing between different stages of the system, e.g. between the FADC and transmitter modules.

The timing module will also take in a trigger signal from the test-beam logic which, after processing, will be used to freeze the system for data readout.

A range of test and calibration facilities will also be provided in the module.

## Acknowledgements

The authors wish to acknowledge the valuable contributions made by Yuri Ermolin in work related to the phase-I BCID tests.

## References

* [1]_ATLAS Technical Proposal._ CERN/LHCC/94-43 (1994).
* [2]_The Level-1 Calorimeter Trigger System for ATLAS._ I.P. Brawn et al., ATLAS internal note DAQ-NO-30.
* [3]_An R&D Programme for Alternative Technologies for the ATLAS Level-1 Calorimeter Trigger._ G. Appelquist et al., ATLAS internal note DAQ-NO-32.
* [4]_Beam Tests of a Prototype Level-1 Calorimeter Trigger for LHC Experiments._ I.P. Brawn et al., Nuclear Instruments and Methods **A349** (1994) 356-366.
* [5]_Data on the LHC Electromagnetic Cluster-Finding ASIC._ V. Perera, Rutherford Appleton Laboratory Technical Documentation RAL 114 (1992).
* [6] RD3 proposal, CERN/DRDC/90-31 (1990).
* [7] RD33 proposal, CERN/DRDC/93-2 (1993).
* [8]_An Analysis of the RD27 Level-1 Prototype Calorimeter Trigger Processor Test Beam Data -- April/May 1993._ U. Dickow, RD27 Note 23 (1994).
* [9]_Beam Tests of a Single-Channel Bunch-Crossing Identification Module for the Level-1 Trigger -- November 1993._ I.P. Brawn et al., RD27 Note 31 (1994).
* [10]_The Programmable Logic Data Book._ Xilinx Corp., San Jose, USA, part number 0401096-01 (1993).