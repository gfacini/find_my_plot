# Athena Pile-up Requirements

Version 2.1

Fred Luehring

September 7, 2001Pile Up in the ATLAS Detector

Pile-up occurs when the readout of a particle detector includes information from more than one primary beam particle interaction - these multiple interactions are said to be "piling-up". At the LHC design luminosity of \(1.0\times 10^{34}\) cm\({}^{-2}\)s\({}^{-1}\) pile-up is a major issue for ATLAS detector because the LHC beams will produce an average of 23 interactions each time they cross and the ATLAS detector is sensitive to tracks from more than one bunch crossing (the beams cross every 25 ns). This means that in addition to the hits of the physics event that triggers the detector readout, hits caused by many other interactions are recorded in the readout. The hits from these other interactions are not related to the physics event and represent a serious background. The number of interactions that will occur when the beams cross follows a Poisson distribution with an expected mean value of 23 interactions at design luminosity. The Poisson distribution has a long tail above the most probable value, so a substantial fraction of the bunch crossings will have more than the average number of interactions. The average number of interactions created in a bunch crossing scales linearly with luminosity. For example, at half design luminosity there will be have an average of 11.5 interactions per bunch crossing and at twice design luminosity will be an average of 46. In addition, most of the ATLAS subdetectors are sensitive to tracks produced in bunch crossings both before and after the bunch crossing containing the physics event. The subdetectors vary greatly in how many additional bunch crossings they are sensitive to. In this document the bunch crossings before the crossing containing the physics event are counted as being negative (-1, -2, -3, etc.), the crossing containing the physics event is 0, and the crossings after the physics event are positive (+1, +2, +3, etc.).

## 2 Description of Pile-Up for the Physics TDR

The Physics TDR pile-up studies were done using either ATLASIM or a special method for the LAr calorimeter. ATLASIM is based on FORTRAN and ZEBRA and has a convenient interactive interface that is useful in debugging and testing the pile-up process. The ATLASIM pile-up algorithm combines the hits from a large number of background events with the hits of a physics event and then digitizes the combined set of hits to create pile-up event digitizations. For the Physics TDR studies, the LAr detector used a different method for creating pile-up events by adding pile-up energy contributions to the digitizations during reconstruction. This special method was used because the LAr subdetector is sensitive hits from a very large number of beam crossings. Note: Even though DICE (the batch version of the ATLAS FORTRAN simulation software) and ATLASIM use the same underlying routines, DICE was found to be about two times slower at producing pile-up event than ATLASIM. Thus all pile-up production has been done using ATLASIM (or the special method used in LAr). The experience gained in doing this work provides guidance for the Athena design about what does and does not work in implementing a pile-up simulation algorithm. For example: the Physics TDR work clearly showed that the pile-up algorithm must be designed very carefully to avoid using memory in a way that causes swapping to occur. Since the Physics TDR, ATLASIM has been improved to work with much larger numbers of pile-up events and remains in use today in ATLAS. Section 3 describes the pile-up features of the current ATLASIM version.

### _Pile-Up in ATLASIM during Physics TDR_

The primary ATLASIM pile-up simulation algorithm uses two data streams: the P (physics events) stream and the B (background events) stream. The background events are generated with little constraint on their underlying physics and are known as "minimum-bias events". Each stream is associated with a sequential FZ ZEBRA file containing the event truth information, the GEANT vertices (VERT bank), the GEANT tracks (KINE bank), and the GEANT hits (HITS bank). Each file has header records storing the basic simulation model parameters like GEANT volumes, tracking materials, and tracking media. To pile-up hits for a subdetector, the subdetector has to have exactly the same parameters in these header records for both the physics and background files. It is possible to combine hits from physics and background files that have different parts of the full ATLAS detector simulated as long as the hits of each subdetector being studied are present in both files and both files are generated with the same geometry for the subdetectors in use. To create a pile-up event, the complete information for all background events in all of subdetectors in use must be read into the ZEBRA common (i.e. into memory) before the pile-up process begins. The version of ATLASIM used for the Physics TDR studies required reading into memory the complete set of information for all subdetectors at the beginning of each event. This was the case even though the subdetectors were digitized one at a time and the pile-up and digitization process for a subdetector only requires access to its own hits. The current ATLASIMversion can still function exactly the same manner. However it has been modified to allow the use of an additional set of background events for certain circumstances that are not kept continuously in memory and do not include hits for all detectors - see below.

Two kumac commands are used to define how many minimum-bias hits ATLASIM adds to the hits to the physics event. The first command, GBACK, simply defines how many minimum-bias events will be read from the background event file. The number of events to read is specified in terms of a starting bunch crossing, a final bunch crossing, and the average number of events to read for each bunch crossing. The number of events in each bunch crossing is smeared randomly to conform to a Poisson distribution with a mean equal to the average number of interactions expected in each bunch crossing. For example, one could read an average of 23 events per bunch crossing starting from fourth (-4) bunch crossing before and ending with second (+2) bunch crossing after the physics event. To actually specify how many minimum-bias events are used to contribute hits to the digitization of a particular subdetector, the BACK option of the MODE control card for the subdetector is used. The BACK option allows each subdetector to select to use all of, part of, or even none of the minimum-bias events by specifying which of the available bunch crossings actually provide hits used by the subdetector's digitization process.

Each subdetector specifies separately how many bunch crossings of minimum-bias events it uses to allow for variations in the length of time that each subdetector is sensitive to hits. For the Physics TDR studies, pile-up hits were selected without checking that the hit occurred during a readout time window defined by the level-1 trigger. With the exception of the TRT running in a little-used optional mode, no subdetector had a digitization routine that implements a time cut to select in-time hits out of a time history of hits generated with the LHC bunch crossing time structure and particle flight times. Instead all hits from every event in every requested bunch crossing were used. Each subdetector used the hits from a number of bunch crossings that would generate a density of minimum-bias hits equivalent to what would be generated by selecting in-time hits from a full time history of the hits in all bunch crossings affecting the readout. In writing the physics TDR section on pile-up, Stephen Haywood coined the term "standard pile-up" for the method of using all hits in fewer events with no time cuts and the term "complete pile-up" for the method of using a full time history and then applying a time cut.

Not using a full time history and a time cut was a reasonable choice given the software and hardware available when the Physics TDR simulations were run. It simply was not possible to run ATLASIM storing a full representation of the hits over a reasonable time history (say -4 to +2) and to then apply a time cut to select the hits from the tracks that were in-time with the readout. Even if recent versions of ATLASIM that remove the software limitations on doing complete pile-up had been available for the Physics TDR, the computer hardware available then would not have been powerful enough. Selecting in-time hits from a long time history is wasteful of computer resources because in most cases only a small fraction (-10%) of the read hits are actually used. All pile-up for large Monte Carlo data production for the Physics TDR was done using the standard pile-up method. This is a reasonable approach in detectors that measure signal levels and not the times when the leading or trailing edges of the electronic signals occur.

A special Physics TDR study simulating the TRT in a mode using a full time history of pile-up hits provides guidance in designing Athena and about the sort of limitations that can make simulating pile-up difficult. To understand how efficiently the TRT records leading edge (i.e. drift time) measurements, a complete pile-up study using 7 bunch crossings of TRT hits (-4 to +2 bunch crossings) was undertaken. It was eventually found that accurate results could be obtained doing complete pile-up for -2 to +1 bunch crossings. In making this study of how many bunch crossings needed to be simulated, the limiting factor on increasing the luminosity eventually was found to be the limit of 64000 banks within a single ZEBRA store (ATLSIM uses ZEBRA as its memory manager). Since each primary track is assigned a unique track number, it creates a new bank. For Physics TDR studies using 7 bunch crossings of minimum-bias events caused ATLASIM to reach a limit of storing 64,000 unique primary track identification numbers in ZEBRA at just over design luminosity, even if no track numbers and banks were assigned to secondary tracks. The current ATLASIM overcomes the 64000-track limit but of course the computer used in such a case must be able to cope with a very large memory requirement. For these special studies it required using ~100 MB ZEBRA bank to hold the pile-up event hits and other GEANT information even though only the inner detector was active and only the TRT was running complete pile-up (-2 to +1 bunch crossings). Even then, it required using special computers with much larger than usual amount of physical memory installed to prevent swapping from making the job time impracticably long. Examples of kumac files used to run the studies on the effect of pile-up on track finding and fake track rates are shown in the next section.

### Example of Physics TDR Standard Pile-Up

As an example of how pile-up was done in ATLSIM, here is a kumac fragment demonstrating the commands for standard pile-up within the inner detector:

macro pileup_prod_safer_3 _  [1]=1 _  [2]=/usr2/fred/aix/muon_safer_cuts/muon_safer_cuts.dat _  [3]=/usr2/fred/aix/mb_safer_cuts/mb_safer_cuts.dat debug off trace off qdrop * * Define input and output files: GFILE P [2] GEKH <--- geo, ev def, kine, hits from physics ev GFILE B [3] KH <--- kine, hits from background ev * gclose * RNDM 7671172 50280442 * GKINE -1 0 GBACK 3 0 8 25 10. <--- Read -3 to 0 i.e. 4 bunch crossings * * PileUp for Inner Detector: mode back geom 1 prin 1 mode pixb prin 1 back 2 <--- Pix bar uses 3 BC of hits (24 events) mode pixe prin 1 back 2 <--- Pix e-c uses 3 BC of hits (24 events) mode sctt prin 1 back 2 <--- Si bar uses 3 BC of hits (24 events) mode zsct prin 1 back 2 <--- Si e-c uses 3 BC of hits (24 events) mode xtrt prin 1 back 3 <--- TRT uses 4 BC of hits (32 events). (for this form of the mode command the number.  of bunch crossing used is one more than the number specified as the argument.)

### Complete Pile-Up

For comparison this is a kumac fragment demonstrating the commands for complete pile-up in the TRT (again within the inner detector only):

macro pileup_tof_3 _  [1]=1 _  [2]=/usr2/fred/aix/muon_99_recon_geoml/muon_99_recon_geoml.dat _  [3]=/usr2/fred/aix/mb_99_recon_geoml/mb_99_recon_geoml.dat _  [4]=/usr9/fred/pileup_tof_3.dat debug off trace off gdrop * * Define input and output files: GFILE P [2] GEKH <--- geo, ev def, kine, hits from physics ev GFILE B [3] KH <--- kine, hits from background ev GFILE O [4] GEKD <--- geo, ev def, kine, digi to output * gclose * RNDM 7671172 50280442GKINE -1 0 GBACK 2 1 23 25 10. <--- Read -2 to +1 i.e. 4 bunch crossings * 23 events per crossing mode back geom 1 prin l mode pixb prin l back 010 <--- pix bar uses l BC of hits (23 events) mode pixe prin l back 010 <--- pix e-c uses l BC of hits (23 events) mode sctt prin l back 010 <--- Si bar uses l BC of hits (23 events) mode zsct prin l back 010 <--- Si e-c uses l BC of hits (23 events) mode xtrt prin l back 211 <--- TRT uses 4 BC (-2 to +1) of hits (92 ev) (hit some limit and couldn't use -4...).

### _Special LAr Pile-Up Method for Physics TDR_

For the Physics TDR studies, it was not practical or even possible to run ATLASIM using the number of minimum-bias events that are required to fully simulate pile-up over the time that LAr is sensitive (approximately -600 ns to +50 ns). The LAr pile-up was done by modifying the LAr digitizations during reconstruction with minimum-bias event energy depositions read from one of several special direct access files containing information for up to 10000 events. The algorithm was conservative and combined information for 32 bunch crossings (-26 to +5 BC) which covers a wider time (-650 to +150 ns) than the minimum requirement. The files contained events stored in a form that could be efficiently added to the LAr digitizations and that also minimized the size of the data being held in memory. The data for each hit LAr cell was two 32-bit integers: one word was an identification number for the cell and the second word was the raw energy deposited by the minimum-bias event in the cell. Cells with no significant energy deposition were not included in the file to reduce the data size. In adding the pile-up for single simulated readout, a total of 32 bunch crossings of data were randomly selected from 6275 minimum-bias stored in about 168 MB of memory. For the in-time bunch crossing, it was possible to use the hits from the normal B data stream when creating the LAr digitizations and to later decrease the number of minimum-bias events read from the special file correspondingly (to avoid double-counting). Using hits from B data stream allowed retaining some of the time correlations between the pile-up in the rest of the ATLAS detector and the LAr calorimeter. The reader is directed to Stefan Simion's write-up of what was done to create LAr pile-up for the physics TDR: [http://cern.ch/simions/pileup/pileup.ps](http://cern.ch/simions/pileup/pileup.ps).

Current Version of ATLASIM:

This section describes the modifications made to ATLASIM to overcome the limitations of the version used for Physics TDR studies. Two main modifications have occurred: 1) removing the 64,000 truth track limitation and 2) providing a new data stream called the "Huge Bank" to provide hits in the very early and/or late bunch crossings for the subdetectors that are sensitive to large numbers of bunch crossings. This version of ATLASIM will be used through the transition period to the new GEANT4-based version of the ATLAS full detector simulation including for work on DC0 and DC1. These design changes should also provide guidance to for implementing pile-up in Athena.

### _Modifications to ATLASIM since the Physics TDR_

Since the Physics TDR, ATLASIM has been modified to remove the limitation of being only able to store 64,000 unique truth track identifying numbers by allowing use of multiple ZEBRA banks to store track identification numbers. With this limitation removed, it becomes possible to do complete pile-up with a very large number of background events (~1000). This number is more than enough for doing complete pile-up for even at least twice the design luminosity for all parts of the ATLAS detector except the LAr calorimeter and muon system, which have slower time responses and require pile-up hits from many more bunch crossings than the rest of the ATLAS detector.

An additional background event store called "The Huge Bank" was introduced to perform a function similar to the special file used by the Physics TDR LAr study and to read hits from its own input file. The Huge Bank provides hits to the slower detectors for bunch crossings outside of those needed by the rest of the ATLAS Detector. This bank dynamically allocates in the constant division the space needed to store 25-60 bunch crossings (typically, 1.2 MB per bunch crossing is required for complete ATLAS setup). The usage of this bank is optimized to minimize system swapping. Until events are needed from it, this bank really resides in the system swap space and only its parts are brought into real memory upon request. After simulation of one pile-up event is finished, only a small fraction (~10%) of the background events in the Huge Bank are replaced with new events before starting the simulation of the next pile-up minimizing the amount of time spent doing disk I/O. It is possible to replace only a small part of the background events because the background events are selected from the bank and assigned randomly to different bunch crossings for each simulated pile-up event. A word is provided in the bank structure for each hit to hold a bunch crossing time assignment and no truth track (KINE) information is stored for these events to keep the bank size manageable. Moreover, the Huge Bank pile-up processing is done separately for the digitization of each subdetector which reduces the total memory required to run ATLASIM pile-up by roughly a factor of two.

With the software limitations on the size of the pile-up event removed and addition of the Huge Bank, it became possible to run pile-up in ATLASIM limited only by system swapping file size. A 2 GB swapping space corresponds to up to several hundreds of bunch crossings at design luminosity. As soon as the real memory is big enough to hold the few (2-4) fully simulated closest bunch crossings, the swapping overhead from using the events in the Huge Bank is negligible and does not degrade the program performance.

### _Example Kumac for Pile-Up in Current ATLASIM Program_

This kumac fragment demonstrates the ATLASIM commands to run pile-up for the whole ATLAS detector using the current ATLASIM version:

gfile BS ZEBRA.B GH  note BS mode - S stands for Special  this is the maximum pile-up for this job  switch off pile-up for all non-mentioned system  mode cops back 25125  mode cryo back 25125  mode accb back 25125  mode ende back 25125  mode fwdc back 25125  mode cbeac back -1  mode crac back -1  mode ceea back -1  mode crec back -1  mode crec back -1  trig  take the full pile-up just once (took 82 seconds)gfile P ZEBRA.P GEKH | now open the standard physics stream gfile BS ZEBRA.B KH | and background with tracks (not really needed) gback 1 0 23 25 1 | require full pile-up in 2 bunch crossings mode all back 110 | now default is -1,0 BC mode pixb back 010 | reset to trigger only where needed... mode cbea back -1 | and switched off where needed... trig [n] | mixing takes now about 6 sec/event on lxplus045.

Pile-Up Simulation Requirements:

This section begins with a discussion of the basis for the Athena pile-up design requirements (section 4.1) and then shows the list of requirements (section 4.2).

### _Derivation of Requirements:_

The pile-up requirements are derived from LHC beam structure, from the design of each subdetector and its trigger, and from considerations related to computer hardware limitations.

#### 4.1.1 LHC Beam Structure

The LHC beam duty factor of ~75%, the 25 ns separation between bunches, and the ~72 mb pp cross-section at 14 TeV imply a mean of 23 interactions per bunch crossing (BC) at design luminosity. Since a bunch crossing has an integral and non-negative number of interactions this suggests that the number of interactions should be a Poisson distribution with a mean of 23. As mentioned previously, the number of interactions scales linearly with luminosity (e.g. twice design luminosity implies a mean of 46 interactions/BC while half design luminosity will have 11.5 interactions/BC).

#### 4.1.2 Inner Tracker

Within the inner tracker, it is desirable to use complete pile-up for the TRT because the TRT measures track positions using the arrival time of signal edges. Consequently multiple hits within a single TRT straw interfere ("shadow") the recording of each-other's leading edges. For the pixel and silicon strip detectors it is probably sufficient to use the standard pile-up method, so the TRT drives the pile-up requirements caused by the inner detector. The complete pile-up method requires combining the hits of an average of 7deg46=322 minimum-bias events in the worst-case scenario of twice design luminosity done from -4 to +2. There are large tails on the number of interactions in each beam crossing and it is prudent to allow for twice that average number of events (~600-700) to account for the effect of these Poisson tails. For large-scale production, it will probably be possible to use a shorter time window (probably -2 to +1) and simulate only up to design luminosity. If the TRT signal width or trailing edge is of interest, an additional later bunch crossing will be required (-2 to +2). These conditions imply combining the hits of no more than ~200 events, even with an allowance that occasionally Poisson tails produce an event with double the average number of interactions. By comparison the standard pile-up method would require using all of the hits of about 32 minimum-bias hits to simulate an equivalent hit density. Of course this method also has Poisson tails, so one should plan for a maximum of about 60 events when the standard method is in use.

#### 4.1.3 LAr

The LAr simulation requires creating pile-up events that include the effect of a much larger number of minimum-bias events than most of the other subdetectors. The maximum signal drift time of 570 ns requires adding the effect of minimum-bias events for at least that long before the physics event. It is also necessary to add the effect of minimum-bias events for at least three bunch crossings after the physics event because of the 50 ns electronics shaping time. The analog trigger used by LAr consists of five samples each 100 ns long and that also requires using at least 500 ns of minimum-bias events.

The requirement for simulating such a large number of bunch crossings of pile-up can be met in two obvious ways:

1. By structuring Athena to be able to process the complete set of fully simulated minimum-bias events over the entire range of bunch crossings that the LAr is sensitive to (the "brute-force" approach).
2. By adding an additional data stream to Athena that allows the digitizations to be modified during reconstruction or digitization (the "Physics TDR" approach).

The brute force approach requires Athena to be able to handle an extremely large memory requirement and to have a highly efficient mechanism for reading in the large amount of data required. For the brute force approach to be feasible, the input/output overhead processing the background events cannot dominate the overall digitizing time for the event. While the Physics TDR approach has been already demonstrated to work, it does have the disadvantage that the LAr background events are read from a separate external file and are not correlated with the minimum-bias events used in the rest of the ATLAS detector like they would be in the real detector. The need to maintain this correlation gives rise to an intermediate possibility: generate the pile-up for the whole ATLAS detector (including LAr) using fully simulated minimum-bias events over a reasonable range for all other detectors (probably -2 to +3 bunch crossings). Then to these fully piled-up events add the effect of the additional, earlier bunch crossings required by LAr via an external compressed Physics TDR style file (essentially what the current version of ATLASIM does with its Huge Bank). Daniel Froidevaux suggested this two-stage process as a way to maintain the correlation between the minimum-bias event hits in the different subdetectors without having to process all 25-30 bunch crossings using fully simulated minimum-bias events.

The level-1 trigger group has additional requirements for the LAr pile-up simulation caused by the need to accurately model the analog part of the trigger. The trigger simulation needs all calorimeter cells included in the pile-up and not only the cells with non-zero energy depositions. This is a change from the LAr Physics TDR algorithm that required a non-zero amount of energy to be deposited in a cell for its energy measurement to be added to the pile-up. The electronic noise associated with the dropped channels is left out of the pile-up simulation, underestimating the effect of noise in increasing the trigger rate. The trigger group would like noise to be included for all detector cells in all used bunch crossings so that the effect of noise on the trigger is fully modeled. An additional requirement is to permit the modeling of the time-dependent baseline shift caused by bipolar electronics used to shape the pulse. The tail of the bipolar shaping function can be less or more than the zero energy level when the charge associated with additional later hits in the channel arrives at the electronics. As a result there are both positive and negative pile-up fluctuations. In particular, small energy deposits can be significantly increased or decreased because of the tail. The degree to which this baseline shift is significant is strongly luminosity dependent because it requires a signal channel to have energy deposits in closely spaced bunch crossings (i.e. exactly what happens during pile-up). In addition the baseline shift tends to cause adjacent channels to be affected similarly. Since these effects are so dependent on the conditions used, the level-1 trigger group would like to have a fast, convenient way to reanalyze the data with different shaping functions and/or assumptions about the beam.

#### 4.1.4 Muon System

The muon system is also in a difficult situation for the same reason as the LAr calorimeter: the detectors are sensitive to a very wide time window. For the MDT tubes the maximum drift time is 720 ns, which implies following the background from about \(-30\) to \(+30\) bunch crossings (even longer than the LAr). There is a strong correlation between a hit's drift time and the number of bunch crossings of background hits to which it is susceptible, and this correlation must be introduced adding the effect of the background hits. Furthermore, there are two types of backgrounds in the muon system: hits that are caused by tracks correlated with the beam events, and hits that are totally uncorrelated with the beam.

The main source of correlated hits is the in-flight decays of pions and kaons into muons. These pions and kaons can be either directly produced in the LHC beam interactions or as secondary products from interactions in the pixels and silicon. This background is fully correlated with the background in the rest of the detector and has the largest effect on muon chambers at the higher rapidities (\(|\eta|>2.0\)). Just as for the LAr system, the muon system would like to consider adding hits from an additional background event stream that they control with the hits properly correlated for the bunch crossings that affect the readout of the inner tracker. Also as in the LAr system it should be possible to use fully simulated events for the bunch crossings affecting the inner detector and read from the special correlated background event stream for the other bunch crossings. If one wants to fully treat the correlations between the LAr and muon systems it will be necessary for the special correlated background files for each system to contain information about the same events and to coordinate somehow the reading of the two files. To use the brute force method to generate the entire correlated background in the muon system would require an amount of memory that may not be possible.

The muon system also needs a second background stream for treating uncorrelated hits that come from a number of sources. Uncorrelated secondaries with momenta of \(\sim\)100 MeV to 400 MeV in the muon system are caused by \(\mathrm{K}^{0}_{\mathrm{L}}\) decays into pions and muons and by proton production via neutron spallation. Soft photons (\(\sim\)1 MeV) that Compton scatter electrons in the drift gas and tube walls is another source of uncorrelated background. Finally very low energy thermal neutrons will interact directly in drift gas and also produce uncorrelated background hits. To get reasonable background hits the muon-system experts will need to generate files based on detailed studies of the sources of these low energy particles and the transport of the generated low, energy particles to the muon chambers.

#### 4.1.5 Event Size and Hardware-Related Requirements

The other main parameter dictating pile-up requirements is the size of minimum-bias events. For the physics TDR, the minimum-bias events generated by Stefan Simion were \(\sim\)2 MB big when no pseudo-rapidity cut on the generated primary tracks was used. The GEANT tracking cuts were 100 keVfor all particle types and the GEANT delta-ray and bremsstrahlung generation cuts were set to 1 MeV. The largest contribution to the 2 MB size was from the FCAL simulation hits that were space-points. Events run by Stefan with a pseudo-rapidity cut of \(|\eta|<3.2\) had a size of 0.7 MB (three-fold reduction). Dario Barberis generated minimum-bias events for the b-tagging studies that were still smaller (0.3 MB) by reducing the pseudo-rapidity cut to \(|\eta|<2.7\) which effectively eliminated FCAL from the event. Guessing an increase of 2 in the minimum-bias event size, one gets 4 MB in the worst case. An increase will occur because undoubtedly more information will be included in the new hits and GEANT4 secondary event cut is effectively much lower than the GEANT3 one in most cases. It should be pointed out that the TDR production was done with a relatively old version of Pythia (5.7) that generates about seven charged primary tracks per unit of pseudo-rapidity. We need to speak with experts in the physics group to find out how things have changed since then, because the minimum-bias event size is very sensitive to the number of primary charged tracks.

Allowing 1 GB for the combined size of the background event hits is likely generous for design luminosity but the actual memory requirement is driven by the cuts used and the design of the hits. Raising the memory allowance to 2 GB would allow for special tests at twice design luminosity or with very low secondary production cut. A combined real and virtual memory set of this size (2-3 GB) on a single machine would be required if all subdetectors should be read into and retained in memory (as was done previously). If a database is used it becomes possible to process the hits for each part of the detector separately without having everything in memory at once. Returning to the previous standard pile-up method, where flight time was not used, would reduce the event size by about half an order of magnitude.

### List of Requirements:

* pile-up event hits will not be generated as giant single events directly in GEANT. For the LAr and muon systems it should be possible to add the effect of pile-up during reconstruction.
* There should be at least five data streams: a physics event stream, a fully simulated background event stream, a LAr pile-up stream, a correlated (with the beam events) background stream for the muon system and an uncorrelated low energy background for the muon system. The muon system would accept having one combined data stream for the uncorrelated and correlated backgrounds. The possibility to have additional data streams is desirable.
* The minimum-bias events should be randomly selected from a database containing at least 5 times more minimum-bias events than are needed by the largest pile-up event. Physics requirements may force the final size requirement for the database to be larger.
* It should not be required to read or store in memory the hits for subdetectors that are not being studied, and it should even be possible to pile-up and digitize hits for only a single subdetector.
* It should be possible to reuse the minimum-bias events repeatedly by randomly recombining the minimum-bias events into different pile-up events.
* The random numbers used should be centrally managed to prevent the same physics event from being combined with exactly the same set of pile-up events in two different runs. It should be possible to recover the random number seeds from the start of an arbitrary event within a job in order to resimulate the pile-up event without having to first recreate all of the previous events in the run.
* The minimum-bias event hits should be grouped into bunch crossings and the time offset to the bunch crossing containing each hit should be easily obtainable.
* The number of minimum-bias events in each simulated bunch crossing should be randomly set using a Poisson distribution with a user-specified mean. For testing, it should be possible to force each bunch crossing to contain an exact number of events. It is desirable to be able to vary the number of added minimum-bias on an event by event basis for testing the effect of variations in LHC luminosity and for code testing.
* It should be possible to pass different numbers of minimum-bias events to each subdetector.
* The number of bunch crossings used should be specifiable. It should be possible to use at least 5 prior and 3 following bunch crossings for testing (-2 to +1 would most likely be sufficient for most productions). The brute-force approach would require -25 to +3 bunch crossings in the LAr case or -30 to +30 for in the muon system case.
* A pile-up event should be able to use hits from at least 700 minimum-bias events (more would be better). For large Monte Carlo production at design luminosity, a maximum of 200 minimum-biasevents is a reasonable limit. If the brute-force approach is adopted for the LAr and Muon correlated backgrounds, then the possibility of using at least 2000 events must be foreseen.
* It should be possible to select hits based on the pseudo-rapidity of the primary track. It should also be possible to run without any pseudo-rapidity cut.
* The pile-up should allow a minimum-bias event size of 4 MB (larger would be better). The 4 MB event size may have to be revised once the design of the detector hits is better known.
* It should be possible to use both the standard and complete pile-up methods.
* The pile-up event should be written out including sufficient information that it is possible to determine what truth tracks and hits contributed to each digitization in the event.
* To reduce the amount of memory needed, it should be possible to process the hits for parts of the detector without having all hits for all subdetectors simultaneously in memory. A reasonable starting point would be to be able to process the hits for each subdetector separately.
* The effect of calorimeter channels with low energy deposits or only noise must be included in the pile-up to model the trigger properly. The effect of the noise should be included over the full time range of bunch crossings.
* It should be possible to redo conveniently the pile-up for one or more subdetectors when reading a pile-up dataset.

## 5 Acknowledgements:

Thanks go to Stefan Simion for his careful reading of the document, numerous helpful comments, and for supplying the information describing the LAr pile-up algorithm. Thanks to Aleandro Nisati for supplying the information on the muon-system pile-up requirements, Pavel Nevski for supplying information on the current version of ATLASIM, and Alan Watson and Monika Wielers for supplying information on the level-1 calorimeter trigger.