[MISSING_PAGE_FAIL:1]

## 1 Introduction

The ATLAS detector [1] is a multi-purpose particle physics detector at the CERN Large Hadron Collider. As the initial momentum of two colliding particles in the LHC is near zero, missing transverse momentum (MET) triggers are useful in selecting events in which particles escape the detector. As the measurement of transverse momenta is mainly performed with calorimeters, the MET is also called "missing transverse-energy". The MET is a negative vectorial sum of the calorimeter energy deposits, with a magnitude of \(E_{\mathrm{T}}^{\mathrm{miss}}\). Triggers with high \(E_{\mathrm{T}}^{\mathrm{miss}}\) thresholds are used to collect samples rich in events with large momentum imbalance. These are used, for example, in searches for supersymmetry or monojets (two of many ATLAS examples are [2] and [3]), where momentum is carried off by new weakly interacting particles, or in Higgs production accompanied by Z decaying to neutrinos [4], where the neutrinos carry off momentum. Though they select many more background events, lower threshold \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers are also used, alone or in combination with other criteria, for example in selecting low-\(p_{T}\) W decays or tau leptons. Low threshold MET triggers also play an important role in efficiency studies of electron triggers, and higher \(E_{\mathrm{T}}^{\mathrm{miss}}\) threshold triggers.

The ATLAS \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger rate is completely dominated by strong-interaction processes of Quantum Chromodynamics (QCD), in which the missing transverse momentum originating from weakly interacting particles is negligible, especially when the minimum threshold on \(E_{\mathrm{T}}^{\mathrm{miss}}\) is set to values up to a few tens of GeV. The \(E_{\mathrm{T}}^{\mathrm{miss}}\) in events collected with such triggers arises mainly from measurement resolution effects: since \(E_{\mathrm{T}}^{\mathrm{miss}}\) is a positive definite quantity, any fluctuation in \(x\) or \(y\) components gives a non-zero positive contribution to \(E_{\mathrm{T}}^{\mathrm{miss}}\).

As LHC instantaneous luminosity increases, the distributions of the \(E_{\mathrm{T}}^{\mathrm{miss}}\)and the scalar sum of transverse momentum (\(\sum E_{\mathrm{T}}\)) change significantly. In order to keep rates within allowable predefined ranges while maintaining a high signal efficiency, the trigger configuration needs frequent adjustment to cope with luminosity increase. During Run I, these luminosity-related changes also made trigger rate predictions very difficult. This motivated the study of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger rates as functions of the LHC conditions documented in this note, with the aim of providing an understanding of the observed rates as a function of LHC luminosity and a mechanism of predicting \(E_{\mathrm{T}}^{\mathrm{miss}}\) distributions at higher luminosities, in order to allow planning the evolution of the configuration of this trigger to match the continuously improving LHC performance.

Depending on the algorithm being used, \(E_{\mathrm{T}}^{\mathrm{miss}}\) can be measured using different ATLAS subdetectors. The innermost part of the ATLAS detector is a precision tracking system covering the pseudorapidity range \(|\eta|<2.5\). It consists of silicon pixels, silicon strips, and straw-tube chambers operating in a \(2\,\mathrm{T}\) axial magnetic field supplied by a superconducting solenoid. Outside the solenoid are highly segmented electromagnetic and hadronic calorimeters covering \(|\eta|<4.9\). The outermost subsystem is a large muon spectrometer covering \(|\eta|<2.7\), which reconstructs muon tracks and measures their momenta using the azimuthal magnetic field produced by three sets of air-core superconducting toroids. The ATLAS trigger system rejects events at multiple levels (three in LHC Run I [5]), relying on coarse measurements of the object to be triggered on at lower levels, to take a fast decision, and finer granularity and more complex object reconstruction at higher levels, to achieve better resolution.

In a calorimeter cell-based algorithm, the missing transverse-momentum vector \(\not{E}_{\mathrm{T}}\) is defined as the negative of the vector sum of all transverse-energy vectors \(\not{E}_{\mathrm{T}i}=E_{i}\sin\theta_{i}\,\hat{\boldsymbol{i}}_{i}\) where \(E_{i}\) is the energy measured by the \(i\)-th calorimeter cell, \(\theta_{i}\) is the angle between the beam axis and the direction defined by the origin of the ATLAS coordinate system and the centre of the cell, and \(\hat{\boldsymbol{i}}_{i}\) is the unit vector correspondingto the projection of this direction onto the plane transverse to the LHC beam axis. The latter is the \((x,y)\) plane in ATLAS coordinates, with \(y\) pointing up, \(x\) pointing toward the center of the LHC ring, and \(z\) defined so that the system is right handed. The ATLAS trigger can select events based on the missing transverse energy \(E_{\mathrm{T}}^{\mathrm{miss}}\), which is the magnitude of the \(\boldsymbol{E}_{\mathrm{T}}\) vector, and on the scalar sum \(\sum E_{\mathrm{T}}\) of all transverse energies \(E_{\mathrm{T}i}=E_{i}\sin\theta_{i}\) measured by the calorimeters.

The first (L1) level trigger has coarse granularity and is implemented in custom electronics with limited spatial and energy precision and reduced dynamic range. Prior to 2012, the second level (L2) algorithm did not apply any further rejection with respect to L1. Thanks to a hardware upgrade, a new algorithm was employed in 2012, using summary information coming from read-out electronics to achieve further rejection power. However, its precision and granularity is still significantly worse than that at the highest level, called Event Filter (EF), where measurements by all calorimeter cells are available. The simplest EF algorithm computes \(E_{\mathrm{T}}^{\mathrm{miss}}\) summing over all calorimeter cells, and has been active since 2010, when LHC started producing the first collisions [6]. Its result, as well as L1 and L2 results, is computed for all events collected by ATLAS, independently of any other trigger requirement. Since 2012, thanks to the additional rejection provided by L2, a slower but more precise algorithm is running at EF [7], based on cell clustering similar to the full offline reconstruction [8]. The cluster-based algorithm has better performance, although its result is still different from the offline algorithm, as the latter applies further refinements, including corrections for energy losses in dead material and momentum carried away by muons, and more accurate energy calibrations for jets, leptons and photons, which are not taken into account by the trigger.

During LHC Run I, from 2010 to 2012, the number of proton-proton collisions per bunch crossing (BC) changed significantly, with the Poisson parameter \(\mu\), which is the expected number of collisions in each BC, ranging from peak values smaller than one up to a few in 2010, and reaching up to about 17 and 31 interactions per BC in 2011 and 2012, respectively. This improvement was obtained in a series of successive steps, in which LHC settings have been tuned to increase the luminosity per bunch crossing. Detector conditions had to be adjusted correspondingly. Thus, events belong to several "run periods" characterized by different calibration parameters, noise thresholds, etc.

The luminosity peaks at the beginning of a fill and then decreases exponentially with typical life times of 10-15 hours. This means that \(\mu\) typically changed by no more than a factor of 2 during the same LHC fill. The LHC design [9] specifies a 25 ns separation between consecutive BCs within a "train" 1 and 14 TeV centre-of-momentum energy, but most Run I data have been taken with 50 ns separation, at 7 TeV in 2011 and at 8 TeV 2012, with a small fraction taken with 75 ns spacing and 7 TeV in 2010.

Footnote 1: a train is a group of consecutive bunches; trains are separated from each other by larger gaps.

The calorimeters are not able to separate the contributions of each individual collision, hence measurement of the multiple events occurring in the same bunch crossing "pile up" and can only be distinguished (to a limited degree) during offline analysis, with the help of the tracking system. In addition, the energy deposited in previous bunch crossings in the same "train" also affects the calorimeter measurement. This "out-of-time pile-up" arises because the integration time of the calorimeter signals (typically around 0.5 \(\upmu\)s) is much longer than the BC separation (50 ns for the data considered here). While the calorimeter pulse shaping leads to a cancellation, on average, of the out-of-time pileup for most bunches, this cancellation does not apply to the first few filled bunches following the gap between bunch trains.

As a result of pile-up, \(E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\sum E_{\mathrm{T}}\) change significantly when luminosity changes, and the rates of \(E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\sum E_{\mathrm{T}}\) triggers with fixed threshold increase much faster than linearly as a function of luminosity. In this note we formulate an analytical model which provides a good description of the ATLAS trigger probability distribution as a function of the instantaneous luminosity parametrized by \(\mu\). In the low to mid MET range (up to a few tens of GeV in 2011 and 2012), we show that the \(E_{\rm T}^{\rm miss}\) distribution is dominated by stochastic effects connected with the finite energy resolution of calorimeters. In this range MET trigger rates for a fixed threshold increase faster than linearly with increasing \(\mu\), due mainly to the dependence of \(\sum E_{\rm T}\) on \(\mu\): the \(E_{\rm T}^{\rm miss}\) distribution depends on calorimeter resolution, which in turns depends directly on \(\sum E_{\rm T}\). On the other hand, at high energies the MET distribution is driven by the largest error in the determination of single jet transverse momentum. The characteristic signature in this case is a high-\(\boldsymbol{p_{\rm T}}\) jet whose transverse-momentum is parallel or antiparallel to \(\boldsymbol{E_{\rm T}}\), and the trigger rates scale roughly linearly with \(\mu\).

In this note we are interested in the \(E_{\rm T}^{\rm miss}\) and \(\sum E_{\rm T}\) distributions seen on input to the trigger system. For this reason, unless otherwise stated, the events used in this analysis are selected by special unbiased requirements. This differs considerably from event selections used in the analysis of any other physics process and in the characterization of the offline performance (like [8] or [10]). In particular, we do not require any high-\(\boldsymbol{p_{\rm T}}\) objects, nor that some relevant process (e.g. a heavy boson decay) is identified offline, nor is a veto set on the samples affected by hardware problems irrelevant for \(E_{\rm T}^{\rm miss}\) and \(\sum E_{\rm T}\) (e.g. issues in the muon spectrometer or in tracking detectors). In addition, we focus on the results computed at the EF, which has much better resolution than the previous levels. Because we focus on the trigger rate dependence on the instantaneous LHC luminosity, we consider data from run periods spanning a wide range of pile-up, taken during the whole Run I (2010-2012).

As detector conditions changed during the LHC evolution and the EF result depends on the calorimeter settings, comparing events selected in different run periods is very difficult. In addition, out-of-time pileup makes the first few BCs in each train different from the following ones, the most significant effect being sizably larger \(E_{\rm T}^{\rm miss}\) and \(\sum E_{\rm T}\) trigger rates for these bunches. Hence there are different classes of events even within the same run.2 As explained in section 2, the low-energy part of the \(E_{\rm T}^{\rm miss}\) distribution arises from detector resolution effects and depends on \(\sum E_{\rm T}\). Hence in principle it is sensitive to the differences in \(\sum E_{\rm T}\) for different BCs. Fortunately, on average these differences do not strongly impact the \(E_{\rm T}^{\rm miss}\) distribution.

Footnote 2: A special trigger configuration in 2012 allowed more stable behavior by omitting the first few BCs in each train for the \(E_{\rm T}^{\rm miss}\) triggers, but this is not considered here.

The model of the low-energy part of the \(E_{\rm T}^{\rm miss}\) distribution in section 2 forms the basis of the "\(E_{\rm T}^{\rm miss}\) significance" trigger, which was implemented at the end of 2010 and is discussed in section 3. The purpose of this new trigger is to achieve better stability with respect to the change in instantaneous LHC luminosity, while providing higher efficiency for events with relatively small \(E_{\rm T}^{\rm miss}\) due to undetectable particles.

The high-energy region is addressed in section 4, where the MET distribution is dominated by single-jet mismeasurements. Thus, two independent sources of "fake" \(E_{\rm T}^{\rm miss}\) contribute to the overall \(E_{\rm T}^{\rm miss}\) distribution, one driven by calorimeter resolution effects, the other connected to a single high-\(\boldsymbol{p_{\rm T}}\) jet. As they act at the same time, for each event the \(E_{\rm T}^{\rm miss}\) probability distribution can be simulated by performing a vector sum of the two contributions, separated by an angle \(\phi\) which is uniformly distributed between 0 and \(\pi\). This is explained in section 5.

## 2 Modelling the background \(E_{\rm T}^{\rm miss}\) distribution at low energies

In this section the \(E_{\rm T}^{\rm miss}\) distribution due to calorimeter resolution effects is described analytically, as a function of the mean number \(\mu\) of collisions per bunch crossing. First it is shown that, for a fixed value of \(\sum E_{\rm T}\), the \(E_{\rm T}^{\rm miss}\) follows a Rayleigh distribution which only depends on the resolution (section 2.1). As the \(E_{\rm T}^{\rm miss}\) resolution is a function of \(\sum E_{\rm T}\) and \(\mu\), a necessary input is the \(\sum E_{\rm T}\) probability distribution as a function of \(\mu\). The electronics noise contribution to \(\sum E_{\rm T}\) is fitted with a log-normal distribution, and the contribution from a single collision is fitted with a falling exponential. For a fixed number \(n\) of collisions per bunch crossing, an Erlang distribution is found to reproduce the observed ATLAS \(\sum E_{\rm T}\) spectrum well enough to provide \(E_{\rm T}^{\rm miss}\) trigger rate predictions at the accuracy needed for trigger operation (section 2.2). Finally, the number of collisions \(n\) in a bunch crossing follows a Poisson distribution with mean \(\mu\). Combining all of these provides the joint probability density function of \(E_{\rm T}^{\rm miss}\) and \(\sum E_{\rm T}\). The \(E_{\rm T}^{\rm miss}\) distribution as a function of \(\mu\) is found by integrating the joint probability density function over the \(\sum E_{\rm T}\) probability distribution as a function of \(\mu\) (section 2.3).

Note that the \(E_{\rm T}^{\rm miss}\) resolution is different for different run periods because of different LHC and calorimeter settings. The actual \(E_{\rm T}^{\rm miss}\) distribution changes when the colliding bunch structure changes. Thus calibration and noise settings for all calorimeters need to be updated too, which induces a further change in the measured distribution. This makes it difficult to compare the same model against data collected over a wide luminosity range.

Furthermore, within the same run the \(E_{\rm T}^{\rm miss}\) resolution is different for different algorithms. The cell-based algorithm (available during the entire Run I) adopted different noise-suppression strategies. In 2010 and 2011, \(E_{\rm T}^{\rm miss}\) and \(\sum E_{\rm T}\) were calculated at EF level by summing over all calorimeter cells that passed a "1-sided cut" on the cell energy [6]: the energy \(E_{i}\) of cell \(i\) was included in the sums only if \(E_{i}>3\sigma_{i}\), where \(\sigma_{i}\) is the noise RMS for cell \(i\). This positive-definite cut was used to prevent the possibility of large negative noise fluctuations causing triggers.3 In order to better match the offline \(E_{\rm T}^{\rm miss}\) value, the 2012 cell-based EF algorithm used instead a "2-sided cut" on cells, for which the energy \(E_{i}\) of cell \(i\) was included in the sums only if \(|E_{i}|>2\sigma_{i}\). Protection against large negative fluctuations was added by including the additional cut \(E_{i}>-5\sigma_{i}\).

Footnote 3: They are mostly caused by hardware problems during data acquisition.

As a result, the \(E_{\rm T}^{\rm miss}\) resolution has not only different values but a different functional dependence on \(\mu\) for the two approaches. In 2010 and 2011, the \(\mu\) dependence was indirect through \(\sum E_{\rm T}\) only, whereas in 2012 a linear dependence on \(\mu\) for fixed \(\sum E_{\rm T}\) appeared as the result of moving from 1-sided to 2-sided noise cuts. A similar dependence is also present in cluster-based algorithms, since 2-sided cuts are used for the cells grouped into clusters. Thus, the different noise suppression schemes complicate the comparison between lower and higher \(\mu\) ranges.

In order to simplify the notation, we will write \(x=\sum E_{\rm T}\) for the scalar sum, and \(y=E_{\rm T}^{\rm miss}\) for the vector sum of transverse-energy values. Both quantities are expressed in GeV.

### Theoretical model for the \(E_{\rm T}^{\rm miss}\) due to calorimeter resolution

For the QCD processes which dominate the LHC interaction rate, the true \(E_{\rm T}^{\rm miss}\) from momentum escaping the detector is negligible. For these events, measured \(E_{\rm T}^{\rm miss}\) will arise from random fluctuations in themeasurement of energy deposited in different directions. Measurement of the two \(E_{\rm T}^{\rm miss}\) components \(E_{x}^{\rm miss}\) and \(E_{y}^{\rm miss}\) in events selected from random crossing bunches provides an estimate of the size of these fluctuations [8], as the true missing transverse-momentum is null for these events. Both \(E_{x}^{\rm miss}\) and \(E_{y}^{\rm miss}\) are Gaussian distributed in this case, with the same standard deviation \(\sigma\), over the entire range of \(\sum E_{\rm T}\). Examples from early 2011 data and 2012 runs are shown in Figure 1. We operationally define the \(E_{\rm T}^{\rm miss}\) resolution as the standard deviation \(\sigma\) of the Gaussian fit of the \(E_{x}^{\rm miss}\) distribution for randomly triggered events.

As calorimeter measurement fluctuations roughly scale as the square root of deposited energy, it is reasonable to assume that the \(E_{\rm T}^{\rm miss}\) resolution will depend on the square root of \(\sum E_{\rm T}\). This is indeed the case. Furthermore, in 2010-2011 data (1-sided cut) there is no \(\mu\) dependence within the same run period [11], while in 2012 data (2-sided cut) it is required to account also for \(\mu\) value. This is clearly shown in Figure 2.

The 2-dimensional dependence of the \(E_{\rm T}^{\rm miss}\) resolution on \(x=\sum E_{\rm T}\) and \(\mu\) can be well reproduced with a simple fitting function:

\[\sigma(x,\mu)=a+b\sqrt{x}+c\,\mu \tag{1}\]

As \(c\) is consistent with zero for 2010 and 2011 data and about a tenth of \(b\) for 2012, the square-root

Figure 1: \(E_{x}^{\rm miss}\) distribution for randomly triggered events in 2011 (top plots) and 2012 (bottom plots). \(\sum E_{\rm T}\) is selected in the following ranges: 9.5–10.5 GeV (top-left; \(\mu\approx 2\)), 49.5–50.5 GeV (top-right; \(\mu\approx 2\)), 9.4–10.6 GeV (bottom-left; \(\mu\approx 18\)), 49.3–50.6 GeV (bottom-right; \(\mu\approx 18\)). Both \(E_{x}^{\rm miss}\) and \(E_{y}^{\rm miss}\) are Gaussian distributed with the same standard deviation \(\sigma\), over the entire range of \(\sum E_{\rm T}\). The plots are examples to show this.

dependence on \(\sum E_{\rm T}\) is the main characteristic of the \(E_{\rm T}^{\rm miss}\) resolution.

For fixed \(\sum E_{\rm T}\) and \(\mu\), \(E_{x}^{\rm miss}\) and \(E_{y}^{\rm miss}\) are Gaussian distributed with zero mean and the same standard deviation \(\sigma\). Hence \(y=\sqrt{(E_{x}^{\rm miss})^{2}+(E_{y}^{\rm miss})^{2}}\) follows a Rayleigh distribution with the same parameter \(\sigma\):

\[{\cal R}(y|\sigma)=\frac{y}{\sigma^{2}}\,\exp\left(-\frac{y^{2}}{2\sigma^{2}}\right) \tag{2}\]

where the value of the single parameter \(\sigma(x,\mu)\) is the \(E_{\rm T}^{\rm miss}\) resolution for the chosen \(\sum E_{\rm T}\) and \(\mu\) values. Note that the Rayleigh distribution is the functional form for the \(E_{\rm T}^{\rm miss}\) values reconstructed by all trigger levels and by the full offline resolution. They differ mainly in the value of the resolution \(\sigma\), with second-order deviations due to the refined corrections applied offline.

### Probability model for \(\sum E_{\rm T}\)

In this section we determine the p.d.f. of \(\sum E_{\rm T}\) as a function of \(\mu\). As discussed above, in 2010 and 2011, the ATLAS triggers omitted all cells measuring negative energy values from \(E_{\rm T}^{\rm miss}\) and \(\sum E_{\rm T}\) sums. This induces an offset in the measured \(\sum E_{\rm T}\) values [10], which is small for trigger purposes but is clearly visible when looking at events without collisions. The left plot in Figure 3 shows the distributions of \(\sum E_{\rm T}\) measured at EF level with non-collision triggers (which collect data while beam is present but in the part of the LHC cycle when there are no colliding bunches) for three parts of the same 2011 LHC fill with \(\sim 4\), 5 and 7 average interactions per bunch crossing. They are indistinguishable, as expected.

Because the calorimeter noise which contributes to these distributions changes by orders of magnitude across the different calorimeter sampling depths [1], one expects a distribution which is the sum of many independent fluctuations whose order of magnitude is unknown. This leads to the choice of a log-normal

Figure 2: EF \(E_{x}^{\rm miss}\) resolution for different amounts of pile-up. 2010 data with \(\mu\) in the intervals 1.6–2.0, 2.0–2.5, and 2.5–3.3 are shown in the left plot as a function of \(\sum E_{\rm T}\). The fits with the function \(f(x)=a+b\sqrt{x}\) give the same values within uncertainties. 2012 data for six different values of \(\langle\mu\rangle\) are shown in the right plot as a function of \(\sqrt{\sum E_{\rm T}}\). Linear fits reproduce the observations well. As calorimeter measurement fluctuations roughly scale as the square root of deposited energy, it is reasonable to assume that the \(E_{\rm T}^{\rm miss}\) resolution will depend on the square root of \(\sum E_{\rm T}\). Furthermore, in 2010–2011 data (1-sided cut) there is no \(\mu\) dependence within the same run period [11], while 2012 data (2-sided cut) require to account also for \(\mu\) value. This is clearly shown in these plots.

function to fit the "pedestal" of the \(\sum E_{\rm T}\) distribution. The left plot in Figure 3 shows indeed that the function

\[{\cal N}_{0}(x|x_{0},s)={\rm LN}(x|x_{0},s)=\frac{1}{s\sqrt{2\pi}}\,\frac{1}{x} \,\exp\left[-\frac{(\ln x-\ln x_{0})^{2}}{2s^{2}}\right],\quad x>0\, \tag{3}\]

where \(x=\sum E_{\rm T}/\)GeV, provides an excellent fit to the measured \(\sum E_{\rm T}\) distribution when there are no collisions and negative-energy cells are removed.

The right plot in Figure 3 shows the \(\sum E_{\rm T}\) distribution for non-collision triggers in 2012. The cell noise cuts were two-sided, so that on average positive and negative cell energies cancel and, unlike the case for 2011, the distribution peaks at zero. However, large negative energy values will occasionally be measured because of out-of-time pileup. The electronic shaping of the calorimeter signals results in a decrease of event rates in a negative energy tail for several collisions and negative-energy cells are removed.

The right plot in Figure 3 shows the \(\sum E_{\rm T}\) distribution for non-collision triggers in 2012. The cell noise cuts were two-sided, so that on average positive and negative cell energies cancel and, unlike the case for 2011, the distribution peaks at zero. However, large negative energy values will occasionally be measured because of out-of-time pileup. The electronic shaping of the calorimeter signals results in a decrease of event rates in a negative energy tail for several collisions and negative-energy cells are removed.

The electronic shaping of the calorimeter signals results in a negative energy tail for several bunches after the one in which there was energy deposited. If a large energy signal precedes an empty bunch, it can give rise to a total measured energy which is negative. The higher the pileup, the more energy in preceding bunches and the likelihood of negative energies is higher. This is seen as negative energy tails in the figure, which increase with \(\langle\mu\rangle\) as expected. 4

Footnote 4: The pile-up compensation is tuned for colliding bunches, but the non-collision triggers start 150 ns after the filled bunches.

In collision events only the peak \(x_{0}\) of the \(\sum E_{\rm T}\) pedestal will be relevant, as the spread is negligible compared to the range and resolution of the measurement. If \(\xi\equiv x-x_{0}\) is the pedestal-subtracted \(\sum E_{\rm T}\) value, this means that in the vast majority of collision events \(\xi\simeq x\). Our goal is to find the probability distribution for \(\xi\) as a function of the average number \(\mu\) of simultaneous collisions. This can be computed from the p.d.f. \({\cal S}_{1}(\xi)\) of the pedestal-subtracted \(\sum E_{\rm T}\) for single collisions, as the distribution of the sum

Figure 3: \(\sum E_{\rm T}\) distribution obtained at EF level for events selected by non-collision triggers for 2011 (left) and 2012 (right) data. Because the calorimeter noise which contributes to these distributions changes by orders of magnitude across the different calorimeter sampling depths [1], one expects a distribution which is the sum of many independent fluctuations whose order of magnitude is unknown. This leads to the choice of a log-normal function to fit the “pedestal” of the \(\sum E_{\rm T}\) distribution. The left plot shows indeed that the function \({\cal N}_{0}(x|x_{0},s)={\rm LN}(x|x_{0},s)=\frac{1}{s\sqrt{2\pi}}\,\frac{1} {x}\,\exp\left[-\frac{(\ln x-\ln x_{0})^{2}}{2s^{2}}\right],\quad x>0\), where \(x=\sum E_{\rm T}/\)GeV, provides an excellent fit to the measured \(\sum E_{\rm T}\) distribution when there are no collisions and negative-energy cells are removed. In 2012 the cell noise cuts were two-sided, so that on average positive and negative cell energies cancel and, unlike the case for 2011, the distribution peaks at zero. However, large negative energy values will occasionally be measured because of out-of-time pileup. That is, the electronic shaping of the calorimeter signals results in a negative energy tail for several bunches after the one in which there was energy deposited. If a large energy signal precedes an empty bunch, it can give rise to a total measured energy which is negative. The higher the pileup, the more energy in preceding bunches and the higher the likelihood of measured negative energy. This is seen as negative energy tails in the figure, which increases with \(\langle\mu\rangle\) as expected.

of \(n\) variables, each independently distributed accordingly to \(\mathcal{S}_{1}(\xi)\). The resulting p.d.f. \(\mathcal{S}_{n}(\xi)\) will be then convolved with a Poisson distribution \(\text{Poi}(n|\mu)=e^{-\mu}\mu^{n}/n!\) describing the probability to have \(n\) collisions when expecting an average of \(\mu\).

We assume here that the calorimeter signals of each collision are independent of those from other collisions in the same bunch crossing, ignoring negligible correlations due to noise suppression. This is a good approximation in LHC Run I, given the relatively low occupancy of the calorimeters, compared to the total number of channels. We also neglect the pedestal width, but this is an excellent approximation, which improves for increasing \(\mu\).

Very early 2010 events contain mostly single collisions. For these, the \(\sum E_{\text{T}}\) distribution above the pedestal peak falls almost exponentially, with deviations in the tail region above a few hundred GeV. Hence, to first approximation a single exponential

\[\mathcal{S}_{1}(\xi|\gamma)=\gamma\,e^{-\gamma\xi}, \tag{4}\]

captures most of the qualitative features of the \(\sum E_{\text{T}}\) distribution for single collisions.

In this case, the distribution \(\mathcal{S}_{n}\), which describes the sum of \(n\geq 1\) independent random variables, each exponentially distributed with the same parameter \(\gamma\), is given by the Erlang distribution:

\[\mathcal{S}_{n}(\xi|\gamma)=\frac{\gamma^{n}}{(n-1)!}\,\xi^{n-1}\,e^{-\gamma \xi}\,\quad\xi\geq 0. \tag{5}\]

which is a special case of the Gamma density

\[\text{Ga}(\xi|\alpha,\beta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\,\xi^{\alpha -1}\,e^{-\beta\xi}\]

with integer shape parameter \(\alpha=n\) and real rate parameter \(\beta=\gamma\). The mean is \(E[\xi]=n/\gamma\), the variance is \(V[\xi]=n/\gamma^{2}\) and mode is \(m[\xi]=(n-1)/\gamma\)[12].

The last step is the convolution of \(\mathcal{S}_{n}(\xi|\gamma)\) with the Poisson distribution with parameter \(\mu\). Approximating the pedestal as a delta-function, we get

\[\mathcal{P}_{\mu}(\xi|\gamma)=\text{Poi}(0|\mu)\delta(\xi)+\sum_{n=1}^{\infty} \text{Poi}(n|\mu)\,\mathcal{S}_{n}(\xi|\gamma), \tag{6}\]

which is valid for \(\xi\geq 0\).

Unless \(\mu\) is much smaller than one, which only happened in the very first LHC data-taking in 2010, requiring any collision trigger drops the first term in the right hand side of (6). Hence, practically, for the entire Run I dataset, the \(\sum E_{\text{T}}\) p.d.f. is given by the sum of Poisson-weighted Gamma densities:

\[\mathcal{P}_{\mu}(\xi|\gamma) =\sum_{n=1}^{\infty}\text{Poi}(n|\mu)\,\mathcal{S}_{n}(\xi|\gamma),\] \[=\sum_{n=1}^{\infty}e^{-\gamma\xi}e^{-\mu}\frac{(\mu\gamma)^{n} \,\xi^{n-1}}{n!(n-1)!} \tag{7}\] \[=\mu\gamma e^{-\gamma\xi-\mu}\sum_{n=1}^{\infty}\frac{(\mu\gamma \xi)^{n-1}}{n!(n-1)!}\]Actually, this infinite sum is the Taylor expansion of a modified Bessel function of the first kind, so an alternative representation is

\[\mathcal{P}_{\mu}(\xi|\gamma)=Ne^{-\gamma\xi-\mu}\sqrt{\frac{\mu\gamma}{\xi}}\;I_{ 1}(2\sqrt{\mu\gamma\xi}) \tag{8}\]

where \(N\) is the normalization constant and

\[I_{\alpha}(z)=\left(\frac{z}{2}\right)^{\alpha}\sum_{k=0}^{\infty}\frac{(z^{2}/ 4)^{k}}{k!\,\Gamma(\alpha+k+1)}\]

is the modified Bessel function of the first kind of order \(\alpha\)[12].

The only free parameter in (8) is the slope \(\gamma\) of the exponential distribution which was assumed to describe \(\sum E_{\mathrm{T}}\) for single-collision events. It is sufficient to fit with \(\mathcal{P}_{\mu}(\xi|\gamma)\) the pedestal-subtracted \(\sum E_{\mathrm{T}}\) distribution5 for collision events with known \(\mu\) to fix \(\gamma\). The same value of \(\gamma\) can then be used for any other \(\mu\) value. Of course, for large differences in \(\mu\) the model uncertainty becomes bigger, so that in practice rate predictions for higher LHC luminosities are performed by fixing the slope \(\gamma\) with the available data having the largest value of \(\mu\).

Footnote 5: Pedestal subtraction is only necessary for 2011 data.

### The joint distribution of \(E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\sum E_{\mathrm{T}}\)

All the ingredients necessary to describe the \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution for events with a given Poisson parameter \(\mu\) are now available. The joint distribution \(\mathcal{J}(\xi,y|\mu)\) of \(y=E_{\mathrm{T}}^{\mathrm{miss}}\) and \(\xi\) (the pedestal-subtracted \(\sum E_{\mathrm{T}}\)) given \(\mu\) is the product of the p.d.f. (2) of \(y\) with the p.d.f. (8) of \(\xi\):

\[\begin{cases}\mathcal{J}(\xi,y|\mu)=\mathcal{R}(y|\sigma(\xi,\mu))\;\mathcal{P }_{\mu}(\xi|\gamma)\\ \sigma(\xi,\mu)=a+b\sqrt{\xi}+c\mu\end{cases} \tag{9}\]

The \(E_{\mathrm{T}}^{\mathrm{miss}}\) resolution parameters (\(a,b,c\)) are obtained by looking at randomly triggered collision events, while the \(\sum E_{\mathrm{T}}\) pedestal is obtained from events collected in empty bunch crossings. These parameters should be separately determined for each run period, as in general they depend on the LHC settings and on the calorimeter configuration. The remaining parameter \(\gamma\) in (9) is determined by fitting with (8) the (pedestal-subtracted) \(\sum E_{\mathrm{T}}\) distribution for events with \(\mu\) in a very narrow interval. What matters operationally is that all parameters needed in (9) can be determined at the beginning of each new run period with simple procedures in a relatively short time.

In order to determine the distribution of \(E_{\mathrm{T}}^{\mathrm{miss}}\) as a function of \(\mu\) alone, the joint density \(\mathcal{J}(\xi,y)\) from equation (9) is integrated to obtain the marginal p.d.f. of \(y=E_{\mathrm{T}}^{\mathrm{miss}}\)

\[\mathcal{R}_{\mu}(y|\gamma)=\int_{0}^{\infty}\mathcal{R}(y|\sigma(\xi,\mu))\; \mathcal{P}_{\mu}(\xi|\gamma)\,\mathrm{d}\xi \tag{10}\]

This is the marginal p.d.f. for \(E_{\mathrm{T}}^{\mathrm{miss}}\) that can be used in trigger rate studies.

The model is compared with 2011 data with relatively small values of \(\mu\) in Figure 4. Once the resolution parameters \(a\) and \(b\) (\(c=0\) in 2011) are found with randomly triggered events and the SumET offset \(x_{0}\) is taken from empty bunch crossings, the last parameter \(\gamma\) is estimated by fitting the normalized \(\sum E_{\mathrm{T}}\)distribution measured for events with \(\mu=5.0\) (red dots in the left panel of Figure 4) with the probability density function (8). With no further fitting, the model is then used to calculate the expected distribution of \(\sum E_{\rm T}\) (left panel) and \(E_{\rm T}^{\rm miss}\) (right panel) for different values of \(\mu\) (continuous lines). Good agreement is obtained for \(\sum E_{\rm T}\) with events with \(\mu=4\) and \(\mu=7\) from the same run period. The comparison is less good for events with \(\mu=1\) belonging to an earlier run period, as expected (calorimeter settings were different in the two periods). Despite this, the \(E_{\rm T}^{\rm miss}\) marginal density from equation (10) shows good agreement with the data shown in the right panel of Figure 4. This means that, once integrated over \(\sum E_{\rm T}\), the model is rather robust in describing \(E_{\rm T}^{\rm miss}\) vs. \(\mu\) and is not very sensitive to the details of the \(\sum E_{\rm T}\) distribution.

As mentioned above, the algorithms were different in 2011 and 2012, hence a fair comparison between lower (2011) and higher (2012) ranges of \(\mu\) is not possible. In addition, we shall see in section 4 that for higher pile-up another contribution to \(E_{\rm T}^{\rm miss}\) emerges, that deviates from this model above a few tens of GeV. An example of both effects is shown in Figure 5, where 2012 events with \(\mu=11\) and \(\mu=21\) are compared. The model parameters are fixed as explained above, with \(\gamma\) being fixed by the fit of \(\sum E_{\rm T}\) for \(\mu=11\). The \(E_{\rm T}^{\rm miss}\) distribution is well reproduced by the model with \(\mu=11\) but the model underestimates the tail above 20-25 GeV for \(\mu=21\). As the lowest \(E_{\rm T}^{\rm miss}\) threshold was 15 GeV, this is a problem for trigger rate predictions, as we shall see in the next section, hence it is important to include also the tail in the model.

Figure 4: _Left panel:_ the normalized \(\sum E_{\rm T}\) distribution measured in 2011 with \(\mu=5\pm 0.1\) (red dots; run 180225) is fitted with equation (8) (dashed red line) to find \(\gamma=0.0420\pm 0.0001\) (statistical uncertainty only). This is the only fit in this figure. The same value is then used to describe the \(\sum E_{\rm T}\) spectrum for \(\mu=1\), 4 and 7 (green, black and blue, respectively). _Right panel:_ normalized \(E_{\rm T}^{\rm miss}\) distribution measured in 2011 with \(\mu=1,4,5,7\) (green, black, red and blue dots), compared with the theoretical predictions (continuous lines) using \(\gamma=0.042\) (there is no fit in this plot). The good agreement means that, once integrated over \(\sum E_{\rm T}\), the model is rather robust in describing \(E_{\rm T}^{\rm miss}\) vs. \(\mu\) and is not very sensitive to the details of the \(\sum E_{\rm T}\) distribution.

### Trigger rates for low \(E_{\rm T}^{\rm miss}\) thresholds

As mentioned above, the \(E_{\rm T}^{\rm miss}\) trigger rates increase much faster than linearly with increasing instantaneous luminosity. The model developed so far is able to explain why, and gives also the functional dependence on \(\mu\) of the trigger rate for some fixed threshold \(t\) on \(E_{\rm T}^{\rm miss}\). Let's define the "background efficiency" \(\varepsilon_{\rm bkg}(t|\mu)\) for a given value of \(\mu\) when triggering on a threshold \(t\) on \(y=E_{\rm T}^{\rm miss}\) as the probability of a randomly selected event passing the \(E_{\rm T}^{\rm miss}\) trigger. It is given by the integral from \(t\) to infinity of the marginal p.d.f. of \(E_{\rm T}^{\rm miss}\) defined in (10):

\[\varepsilon_{\rm bkg}(t|\mu)=\int_{t}^{\infty}\mathcal{R}_{\mu}(y|\gamma)\, \mathrm{d}y=\int_{t}^{\infty}\int_{0}^{\infty}\mathcal{R}(y|\sigma(\xi,\mu)) \,\mathcal{P}_{\mu}(\xi|\gamma)\,\mathrm{d}\xi\,\mathrm{d}y \tag{11}\]

By changing the integration order we get

\[\varepsilon_{\rm bkg}(t|\mu) = \int_{0}^{\infty}\mathcal{P}_{\mu}(\xi|\gamma)\left(\int_{t}^{ \infty}\mathcal{R}(y|\sigma(\xi,\mu))\,\mathrm{d}y\right)\,\mathrm{d}\xi \tag{12}\] \[= \int_{0}^{\infty}\mathcal{P}_{\mu}(\xi|\gamma)\,\exp\left\{-\frac {t^{2}}{2[\sigma(\xi,\mu)]^{2}}\right\}\,\mathrm{d}\xi \tag{13}\]

By taking the ratio of \(\varepsilon_{\rm bkg}(t|\mu)\) computed with different \(t\) values, one can compare the relative rates for different thresholds at fixed \(\mu\). Alternatively, one may look at the rate for the same threshold and different pile-up conditions by taking the ratio for different \(\mu\) values.

Note that the last expression (13) is only valid for low thresholds, such that the additional contribution to \(E_{\rm T}^{\rm miss}\) described later in section 4 can be neglected. To obtain rate estimates for a wide range of thresholds

Figure 5: \(\sum E_{\rm T}\) (left) and \(E_{\rm T}^{\rm miss}\) (right) distributions for \(\mu=11\) (blue dots) and \(\mu=21\) (red dots) from 2012 data. The model parameters are fixed by the fit of the observed \(\sum E_{\rm T}\) distribution with \(\mu=11\) and used to predict the \(\sum E_{\rm T}\) and \(E_{\rm T}^{\rm miss}\) distributions at high pile-up. The \(E_{\rm T}^{\rm miss}\) distribution is well reproduced by the model with \(\mu=11\) but the model underestimates the tail above 20–25 GeV for \(\mu=21\).

it is necessary to extend the innermost integrand function in the first expression (12) such that the additional contribution due to mismeasured jets (independent of \(\sum E_{\mathrm{T}}\)) is also included. Nevertheless, as the input \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution is dominated by low-energy events, (13) represents the most important contribution to the trigger rates. This is expecially important for lower trigger levels, as they are configured with the lowest threshold compatible with the \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger bandwidth allocation, in order to profit as much as possible from the better resolution at EF. The exponential behaviour of (13) explains why it is so complicated to keep the maximum allowed rate at each trigger level, which in practice was accomplished by enabling different trigger "chains" with slightly different \(E_{\mathrm{T}}^{\mathrm{miss}}\) thresholds at all levels, depending on \(\mu\).

Figure 6 shows the predicted behaviour of the 2011 EF trigger rates6, independent of previous rejection by lower level triggers7, for different thresholds on \(E_{\mathrm{T}}^{\mathrm{miss}}\), as a function of the instantaneous luminosity (note the logarithmic scale on the vertical axis). Randomly triggered events with \(\mu=5\) were used to estimate all model parameters. The rate computed with (13) for \(t=15\) GeV is normalized to the measurement in one run. The same normalization is then used for all other \(t\) and \(\mu\) values, to compare to the measurement over a large data set consisting of several runs.

Footnote 6: As the number of BCs with collisions varied from run to run, the figure shows the rate per BC.

Footnote 7: Since \(E_{\mathrm{T}}^{\mathrm{miss}}\) is meaningful even for events which did not pass an \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger, \(E_{\mathrm{T}}^{\mathrm{miss}}\) is calculated at trigger level for every event which passes any trigger.

Rates computed with (13) reproduce the measurement well for the lowest thresholds (15 and 20 GeV) but largely underestimate higher thresholds. Hence, it is apparent that some other contribution, not purely due to calorimeter resolution of the full \(\sum E_{\mathrm{T}}\), dominates the trigger rates of larger \(E_{\mathrm{T}}^{\mathrm{miss}}\) thresholds. This is due to mismeasured jets as explained in section 4. Before addressing that subject, we illustrate in the next section the \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance trigger, which is based on (13). Such a new trigger was implemented at the end of 2010, with the goal of increasing the signal efficiency for events with low true \(E_{\mathrm{T}}^{\mathrm{miss}}\) while keeping the rate as stable as possible, compared to the rapidly increasing MET trigger rates as a function of \(\mu\) observed in the early Run I phase.

Figure 6: Rates for EF \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers with thresholds of 15, 20, 30, 40, 50, 60 GeV are compared to the prediction based on (13) (solid lines). All curves have been normalized to the measured rate of the 20 GeV threshold with \(\mu=5.0\pm 0.1\) during a single run, as shown in the left panel. The scatter on the right panel is due to the use of a large dataset consisting of several runs. Rates computed with (13) well reproduce the measurement for the lowest thresholds (15 and 20 GeV) but largely underestimates higher thresholds. Hence, it is apparent that some other contribution, not purely due to calorimeter resolution of the full \(\sum E_{\mathrm{T}}\), dominates the trigger rates of larger \(E_{\mathrm{T}}^{\mathrm{miss}}\) thresholds. This is due to mismeasured jets as explained in section 4.

## 3 The \(E_{\text{T}}^{\text{miss}}\) significance trigger

During the LHC shutdown period at the end of 2010, a new trigger based on \(E_{\text{T}}^{\text{miss}}\) and \(\sum E_{\text{T}}\) and called "\(E_{\text{T}}^{\text{miss}}\) significance" (XS) was commissioned [11]. The goal of this trigger was to provide a better signal to background separation in the low \(E_{\text{T}}^{\text{miss}}\) region, where rates are dominated by the finite calorimeter resolution and the stochastic model discussed above applies. The XS of an event is operationally defined as the ratio between the \(E_{\text{T}}^{\text{miss}}\) measured in the event and the \(E_{\text{T}}^{\text{miss}}\) resolution determined from the \(\sum E_{\text{T}}\) measured in that event. Hence XS is a dimensionless number which quantifies the excess over the expected \(E_{\text{T}}^{\text{miss}}\) due to the overall calorimeter activity and its non-perfect resolution.

The actual implementation of the XS trigger is a bit more complicated, as it performs checks on the values of \(E_{\text{T}}^{\text{miss}}\) and \(\sum E_{\text{T}}\) before computing the XS ratio. First, it checks \(E_{\text{T}}^{\text{miss}}\) alone and rejects the event if it is too low, or accepts it if \(E_{\text{T}}^{\text{miss}}\) exceeds some threshold. This is the same behaviour as the normal \(E_{\text{T}}^{\text{miss}}\) trigger and, in practice, this threshold was set to a value higher than the threshold of the lowest unprescaled \(E_{\text{T}}^{\text{miss}}\) trigger8, to avoid an unaffordable net increase of \(E_{\text{T}}^{\text{miss}}\) trigger rate. This allows XS triggers to have the same efficiency as \(E_{\text{T}}^{\text{miss}}\) triggers for high \(E_{\text{T}}^{\text{miss}}\) values. The next check is based on \(\sum E_{\text{T}}\) alone. The event is rejected if its \(\sum E_{\text{T}}\) exceeds a value so large that it is likely to arise only from channel overflow or data transmission problems. If no decision is taken after these steps, which act as a protection against undesired behaviour, the actual value of XS is compared against its threshold. The example shown in figure 8, where early data show the background and Monte Carlo events with W decays into tau neutrino pairs represent one interesting signal, illustrates the behaviour of XS triggers. They select events in the upper region of the plot, with different thresholds appearing as different slopes of the boundary. Events with \(E_{\text{T}}^{\text{miss}}\) below 10 GeV are discarded before computing XS.

Footnote 8: Events selected by a trigger with prescale \(N\) are randomly discarded, such that only \(1/N\) of them are actually saved on disk.

The XS trigger is meant to reject the events whose \(E_{\text{T}}^{\text{miss}}\) value would be likely to arise from resolution effects. It is named "significance" because it is related to the statistical significance of the \(E_{\text{T}}^{\text{miss}}\) value in terms of the equivalent number of standard deviations \(Z\) of a Gaussian distributed variable. The model described above allows calculating the probability \(P(E_{\text{T}}^{\text{miss}}>t\,|\,\Sigma E_{\text{T}},\mu)\) that a randomly triggered event

Figure 8: Black line: exact relation between statistical and trigger significance (equation 15). Blue-dashed line: asymptotic relation. Red line: linear approximation in the useful region of XS triggers. A linear approximation provides an excellent description.

with a certain \(\sum E_{\rm T}\) collected with an average number of collisions per bunch crossing \(\mu\) will have a measured \(E_{\rm T}^{\rm miss}\) greater than some value t. Integrating Equation 2 for fixed \(\sum E_{\rm T}\) and \(\mu\) gives

\[P(E_{\rm T}^{\rm miss}>t\,|\,\Sigma E_{\rm T},\mu)=\exp\left(-\frac{t^{2}}{2 \sigma^{2}}\right)=\exp\left(-\frac{Y^{2}}{2}\right) \tag{14}\]

where \(Y=t/\sigma\), is XS for an event with measured \(E_{\rm T}^{\rm miss}\) equal to \(t\).

As the \(E_{\rm T}^{\rm miss}\) distribution is dominated by the background, this probability may be interpreted as a p-value, and converted into a statistical significance \(Z\), by inverting the cumulative Gaussian distribution function, \(\Phi\),

\[Z=\Phi^{-1}(1-P(E_{\rm T}^{\rm miss}>t\,|\,\Sigma E_{\rm T},\mu))=\Phi^{-1} \left(1-e^{-Y^{2}/2}\right) \tag{15}\]

which is positive for \(P<0.5\), i.e. for \(Y\geq\sqrt{2\ln 2}\simeq 1.18\). This is always true for the XS triggers used in ATLAS (no trigger threshold is less than 1.5). The lowest thresholds ever used are in the range of 2 to 3 at L1, and the thresholds are higher at EF.

It can be analytically shown that \(Y\) asymptotically approaches \(Z\) for a given \(\sum E_{\rm T}\) value. In addition, a linear approximation provides an excellent description already at the relatively low thresholds which have been used in ATLAS triggers. The functional relation between the statistical significance \(Z\) and the \(E_{\rm T}^{\rm miss}\) significance XS is shown in Figure 8, together with the asymptotic relation \(Z=Y\) and the straight line \(Z=1.07\,Y-0.879\) which provides an excellent linear approximation in the region where the ATLAS trigger thresholds are defined.

The distribution of XS \(E_{\rm T}^{\rm miss}/\sigma\) is very stable with respect to \(\mu\), for which the \(E_{x}^{\rm miss}\) depends on \(\mu\) only through \(\sum E_{\rm T}\). This is clearly shown in Figure 9, where \(E_{\rm T}^{\rm miss}\) and XS measured over a wide instantaneous luminosity range in 2011 are compared. The \(E_{\rm T}^{\rm miss}\) and XS distributions with 2012 data are also given in Figure 10. The distribution of \(E_{\rm T}^{\rm miss}\) changed shape significantly (note the logarithmic scale), inducing very large effects on trigger rates for the same threshold. In comparison, the variation of XS is very small.

Figure 11 shows the (normalized) trigger rates for a number of different \(E_{\rm T}^{\rm miss}\) and XS triggers in 2011, during which no adjustment of XS trigger parameters was made. While higher pile-up causes an increase

Figure 9: Distributions of the EF \(E_{\rm T}^{\rm miss}\) (left panel) and \(E_{\rm T}^{\rm miss}\) significance (right panel), for different degrees of pile-up, measured with 2011 data [11]. The distribution of XS \(E_{\rm T}^{\rm miss}/\sigma\) is very stable with respect to \(\mu\), for which the \(E_{x}^{\rm miss}\) depends on \(\mu\) only through \(\sum E_{\rm T}\).

in \(E_{\rm T}^{\rm miss}\) and a corresponding (rapid) increase in \(E_{\rm T}^{\rm miss}\) trigger rates, this does not happen to XS because of the simultaneous increase in \(\sum E_{\rm T}\). If the parameterization were perfect, then one would expect the XS rates to be independent of \(\mu\). However most thresholds are in the tail in the right panel of Figure 9, where the distributions are not exactly the same. This causes the variation of XS trigger rates in Figure 11, which are much smaller than the change of \(E_{\rm T}^{\rm miss}\) triggers with \(\mu\). Changes in detector configuration cause the jumps visible in this figure (mostly visible for the last two run periods).

The purpose of XS triggers is to obtain some efficiency for events with low \(E_{\rm T}^{\rm miss}\), for which \(E_{\rm T}^{\rm miss}\) triggers are rate-limited. Figure 12 compares the 2011 trigger efficiency as a function of the offline \(E_{\rm T}^{\rm miss}\) for an event sample rich in W decays to electron-neutrino pairs, for two complete \(E_{\rm T}^{\rm miss}\) ("XE") and XS triggers with comparable rates (hence similar prescale values). The W decays produce a "true" \(E_{\rm T}^{\rm miss}\) spectrum which quickly drops above 40 GeV, hence it is very important to improve the trigger efficiency below this value. Below 40 GeV the efficiency of the XS trigger is twice as large as that of the \(E_{\rm T}^{\rm miss}\) trigger with

Figure 11: Comparison between \(E_{\rm T}^{\rm miss}\) (left panel) and \(E_{\rm T}^{\rm miss}\) significance (right panel) normalized EF trigger rates in 2011. The displayed values are the rates measured online divided by the number of collision bunch crossings and multiplied by the prescale, to allow for a direct comparison. Run periods have different colors [11]. While higher pile-up causes an increase in \(E_{\rm T}^{\rm miss}\) and a corresponding (rapid) increase in \(E_{\rm T}^{\rm miss}\) trigger rates, this does not happen to XS because of the simultaneous increase in \(\sum E_{\rm T}\).

Figure 10: Distributions of the EF \(E_{\rm T}^{\rm miss}\) (left panel) and \(E_{\rm T}^{\rm miss}\) significance (right panel), for different degrees of pile-up, measured with 2012 data. The distribution of \(E_{\rm T}^{\rm miss}\) changed shape significantly (note the logarithmic scale), inducing very large effects on trigger rates for the same threshold. In comparison, the variation of XS is very small.

similar rate. An additional advantage of the XS trigger is that its prescale could be left constant as LHC luminosity increased, while the XE trigger prescale had to be significantly modified. This means that the final samples of events triggered by these two signatures have quite different signal fractions, with XS being the best choice for W decays. XS triggers were also used in combination with other triggers, XS providing some background rejection and thereby enabling reduction of other thresholds.

## 4 Modelling the background \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution at high energies

The model described above reproduces the observed dependence of low-threshold \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers on the average number of collisions per bunch crossing. However, it fails to predict the rates of \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers with higher thresholds. From data, it is found that the increase of trigger rate with \(\mu\), which is much faster than linear at low thresholds, is linear for high \(E_{\mathrm{T}}^{\mathrm{miss}}\) thresholds. Figure 13 shows these two different behaviors (note the logarithmic vertical scale in the left plot, and the linear scale in the right plot). For lower \(E_{\mathrm{T}}^{\mathrm{miss}}\) thresholds, in or close to the region described by the stochastic model developed in section 2, trigger rates in 2011 increased by two orders of magnitude when \(\mu\) changed from a few up to about 16 collisions per bunch crossing. On the other hand, for the highest thresholds, used by unprescaled triggers, the rate increased linearly in the same \(\mu\) range. Figure 14 shows that the same trend is also observed in 2012 data: the highest thresholds have linearly increasing rates with increasing instantaneous luminosity. Hence this is not dependent on the detailed configuration of \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger algorithms.

The behavior of these higher-threshold \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers is similar to those of single-object triggers, such as those on high-\(\boldsymbol{p}_{\mathrm{T}}\) electrons or muons. This indicates that high \(E_{\mathrm{T}}^{\mathrm{miss}}\) arises primarily from events whose probability therefore scales with the number of collisions in a bunch crossing. Unlike the low-energy region, for which \(E_{\mathrm{T}}^{\mathrm{miss}}\) arises from fluctuations over the full energy deposit and is therefore very sensitive to pileup, here the measured asymmetry in the energy lost in calorimeters must arise from fluctuations in

Figure 12: Comparison between the efficiency of \(E_{\mathrm{T}}^{\mathrm{miss}}\) (XE, left panel) and \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance (XS, right panel) triggers as a function of the offline \(E_{\mathrm{T}}^{\mathrm{miss}}\) measured in 2011 with a sample rich in W decays in the electron channel [11]. Only L1 and EF thresholds are listed, since in 2011 the L2 algorithm was identical to the one at L1. The purpose of XS triggers is to obtain some efficiency for events with low \(E_{\mathrm{T}}^{\mathrm{miss}}\), for which \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers are rate-limited. The W decays produce a “true” \(E_{\mathrm{T}}^{\mathrm{miss}}\) spectrum which quickly drops above 40 GeV, hence it is very important to improve the trigger efficiency below this value. Below 40 GeV the efficiency of the XS trigger is twice as large as that of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger with similar rate.

the measurement of large amounts of localized energy deposition. Indeed, as shown below, the high \(E_{\rm T}^{\rm miss}\) tail arises primarily from mismeasurement of jet energy in hard QCD events, when a jet passes through a poorly instrumented calorimeter region or when there is a large fluctuation in measuring the energy of a high-\(\mathbf{p}_{\rm T}\) jet. Even at these high \(E_{\rm T}^{\rm miss}\) values, the contribution from events with a significant amount of energy carried off by undetectable particles (such as neutrinos in decays of produced W bosons) is at the level of only a few percent: the distribution is still dominated by sources of fake \(E_{\rm T}^{\rm miss}\).

Thus, the full \(E_{\rm T}^{\rm miss}\) distribution can be thought of as consisting of three regions: a low-\(E_{\rm T}^{\rm miss}\) region described by stochastic fluctuations in the measurement of the full energy deposit in the calorimeter (which explains the dependence on \(\sum E_{\rm T}\), described above), a high-\(E_{\rm T}^{\rm miss}\) region dominated by the largest measurement error of a local energy deposit, and an overlap region where both contributions are important.

Figure 14: Normalized EF \(E_{\rm T}^{\rm miss}\) trigger rates in 2012. The right plot shows only the rates of the 3 highest thresholds in linear scale. Plots show the same trend as Figure 13. The highest thresholds have linearly increasing rates with increasing instantaneous luminosity. Hence this is not dependent on the detailed configuration of \(E_{\rm T}^{\rm miss}\) trigger algorithms.

Figure 13: Normalized EF \(E_{\rm T}^{\rm miss}\) trigger rates in 2011 for low-mid thresholds (left panel) and high thresholds (right panel) [11]. Note the logarithmic vertical scale in the left plot, compared to the linear scale in the right plot. For lower \(E_{\rm T}^{\rm miss}\) thresholds, in or close to the region described by the stochastic model developed in section 2, trigger rates in 2011 increased by two orders of magnitude when \(\mu\) changed from a few up to about 16 collisions per bunch crossing. On the other hand, for the highest thresholds, used by unprescaled triggers, the rate increased linearly in the same \(\mu\) range.

As \(\mu\) increases, the first region extends to higher \(E_{\rm T}^{\rm miss}\) values, and the intermediate region therefore also shifts to higher \(E_{\rm T}^{\rm miss}\).

Random triggers were used above to study low \(E_{\rm T}^{\rm miss}\) events. However, these triggers are rate-limited and therefore highly prescaled, so they do not collect a sufficient number of the rarer high-\(E_{\rm T}^{\rm miss}\) events. The only way of studying the tail at large \(E_{\rm T}^{\rm miss}\) values is to rely on \(E_{\rm T}^{\rm miss}\) triggers with various thresholds, chosen such that they overlap with the softer selection. When combining events, care is taken to avoid double counting of events collected with different triggers, and to weight individual events appropriately, to account for prescales and for trigger efficiencies, both contributing to the total uncertainty.

### Energy resolution and other sources of \(E_{\rm T}^{\rm miss}\)

Figure 15 compares the measured \(\sum E_{\rm T}\) and \(E_{\rm T}^{\rm miss}\) distributions in 2011 for data having \(\mu=5\) with the stochastic model developed in section 2. The left panel shows the measured \(\sum E_{\rm T}\) distribution and the model with the value of the \(\gamma\) parameter that best fits the data. This value of \(\gamma\) is then used in the model to calculate the \(E_{\rm T}^{\rm miss}\) distribution expected from calorimeter resolution effects, which is superimposed on the measured distribution in the right panel. The \(E_{\rm T}^{\rm miss}\) distribution is constructed from a weighted mixture of events passing a random trigger, the loosest unprescaled \(E_{\rm T}^{\rm miss}\) trigger (with EF threshold at 60 GeV in that run period), and prescaled \(E_{\rm T}^{\rm miss}\) triggers with thresholds ranging from 20 to 40 GeV.

The right panel of Figure 15 also shows simulated samples, normalized to the integrated luminosity of the data, of soft-QCD events (which match the model prediction at low \(E_{\rm T}^{\rm miss}\)) and QCD \(2\to 2\) processes, open heavy flavour (including top) production, prompt photon production, and electroweak \(W\), \(Z\) and \(\gamma^{*}\) processes, with the requirement of at least one jet of \(p_{\rm T}>35\) GeV. These samples are scaled proportionally to their relative leading-order cross-sections. The high-\(E_{\rm T}^{\rm miss}\) part of the distribution clearly contains more events than expected from true missing transverse-momentum or calorimeter mismeasurement of 2-jet events.

In order to better understand this difference, a few requirements have been imposed on the events. First, events are rejected if they are taken during phases in which a part of the detector is not working properly,

Figure 15: \(\sum E_{\rm T}\) (left) and \(E_{\rm T}^{\rm miss}\) (right) distributions for \(\mu=5\pm 0.1\), together with the prediction of the stochastic model. The \(\gamma\) parameter is determined by fitting the \(\sum E_{\rm T}\) distribution and is then used to model the \(E_{\rm T}^{\rm miss}\) distribution (no fit is performed in the right panel). The Monte Carlo samples shown in the right plot are explained in the text, together with the details of the event selection.

as assessed in offline reprocessing, or if they contain a bad-quality jet (as defined by the standard ATLAS "jet cleaning" cuts [13]) or any object depositing energy in a poorly-instrumented ("crack") region of the calorimeter (open triangles in Figure 15). These events are characterized by asymmetry in the measured transverse-energy deposition, hence are expected to create \(E_{\mathrm{T}}^{\mathrm{miss}}\) that could pass the trigger selection, even if the transverse-momentum carried away by non-interacting particles is negligible.

This selection strongly reduces the high \(E_{\mathrm{T}}^{\mathrm{miss}}\) tail of the distribution, indicating that a large fraction of these events is due to bad detector measurements.9 However, it should be emphasized that not all of the event cleaning can be applied online because of the part that relies on data quality assessment which is done during the first pass of offline data processing.

Footnote 9: Non-collision background, such as beam-gas or cosmic ray events, are also included in such contributions to the tail, but their effect is smaller.

When the \(E_{\mathrm{T}}^{\mathrm{miss}}\) is mainly caused by the mismeasurement of a single local object (most probably a jet in LHC collisions), it is expected that the direction of the object will be parallel (if its energy is underestimated) or antiparallel (for overestimated energy) to that of the \(\not{E}_{\mathrm{T}}\). In order to quantify this contribution, a further selection is imposed: events are discarded if any jet with \(p_{\mathrm{T}}>E_{\mathrm{T}}^{\mathrm{miss}}\) is parallel or anti-parallel to \(\not{E}_{\mathrm{T}}\) in the transverse plane (specially if the angular separation is smaller than 0.6 or larger than 2.5 radians). This eliminates QCD multi-jets events for which bad mismeasurement of the energy of one of the jets is responsible for the large observed \(E_{\mathrm{T}}^{\mathrm{miss}}\).

The resulting \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution (filled squares in Figure 15) is in good agreement with the inclusive p-p simulations at low values and with the simulated spectrum of events with real missing momentum at high energies. Data-driven techniques have been used in other analyses to estimate the residual QCD multi-jets and non-collision backgrounds in these events after the analysis selections, and it has been found to be below 3% [2, 3]. This confirms that the bulk of the rate in the tail region is due to detector and resolution effects leading to QCD jet mismeasurement or fake jets, on top of events containing undetectable particles.

In summary, the high energy tail of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution is produced by asymmetries in energy measurement, related to particles entering a calorimeter "crack" region and to single high-\(\not{p}_{\mathrm{T}}\) jets whose momentum is badly measured. Neglecting other contributions to \(E_{\mathrm{T}}^{\mathrm{miss}}\) in the same event, large \(E_{\mathrm{T}}^{\mathrm{miss}}\) can be modelled as arising from the energy measurement with largest error in an event. This is the basis for the statistical treatment of the next section, where an extreme-value distribution is employed to reproduce the tail.

Note that the requirements on the angular separation have been applied in this data analysis in order to understand the dominant source of \(E_{\mathrm{T}}^{\mathrm{miss}}\) in the high-energy region. Such cuts may also be applied online, although they are only useful for studies in which the risk of rejecting events with real invisible particles collinear with jets has no impact.

### Fake \(E_{\mathrm{T}}^{\mathrm{miss}}\) arising from jet mismeasurement

In order to model the high-\(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution in events as arising from the largest mismeasurement of energy in the event, a fit to the high \(E_{\mathrm{T}}^{\mathrm{miss}}\) data is done using a Frechet distribution, a special case of the generalized extreme-value distribution, with p.d.f.

\[\mathcal{F}(x|m,\alpha,s)=\frac{\alpha}{s}\ \left(\frac{x-m}{s}\right)^{-1- \alpha}\ \exp\left[-\left(\frac{x-m}{s}\right)^{-\alpha}\right] \tag{16}\]characterized by the offset \(m\), shape parameter \(\alpha>0\) and scale parameter \(s>0\). Figure 16 shows the same data as the right panel of Figure 15, but over a wider energy range. In addition to the stochastic model (red curve), a fit with a Frechet distribution is also shown (blue curve). The fit reproduces very well the \(E_{\rm T}^{\rm miss}\) distribution from 25 to 500 GeV, indicating that (16) is a good choice: there is no need to rely on a more general p.d.f. with more free parameters.

Furthermore, the best fit parameters are practically independent of \(\mu\): only the relative normalization of the Frechet tail changes. Figure 17 shows \(E_{\rm T}^{\rm miss}\) distributions for \(\mu=10,14,18,22,26\), and 30. These were obtained from 2012 data by combining events collected with various \(E_{\rm T}^{\rm miss}\) triggers while accounting for prescales and avoiding double counting, as mentioned above. Unlike Figures 15 and 16, no random triggers are used here, hence the low \(E_{\rm T}^{\rm miss}\) distribution is not visible.

The shape of the tail is fixed in Figure 17 by the parameters obtained from the fit to the Frechet distribution of the \(E_{\rm T}^{\rm miss}\) data distribution with \(\mu=10\) and \(E_{\rm T}^{\rm miss}>60\) GeV (top-left plot). The only free parameter in every other fit (performed over \(E_{\rm T}^{\rm miss}>100\) GeV, to avoid the transition region which moves to higher \(E_{\rm T}^{\rm miss}\) values as \(\mu\) increases) is the normalization. The Frechet density reproduces well the tail at any luminosity: the quality of the fit is similar for all values of \(\mu\), with fluctuations not exceeding 2-sigma significance over the entire high-energy range. The shape of the tail is consistent with being independent of \(\mu\), which means that only the overall intensity of the source of tail events is a function of the instantaneous luminosity. In particular, in all cases the high-\(E_{\rm T}^{\rm miss}\) data are consistent with a model of extreme-event behaviour dominating above some energy.

The functional dependence on \(\mu\) of the tail normalization can be inferred from the right plots of figures 13 and 14, showing a linear correlation between the rate of a given \(E_{\rm T}^{\rm miss}\) threshold and \(\mu\). This means that the intensity of the source of tail events scales linearly with \(\mu\), consistent with our interpretation of these events as arising from the mismeasurement of high-\(\boldsymbol{p_{\rm T}}\) jets (whose trigger rate also scales linearly with \(\mu\)).

Figure 16: \(E_{\rm T}^{\rm miss}\) distributions for 2011 events with \(\mu=5\pm 0.1\), together with the prediction of the stochastic model (red curve) and the best fitting Fréchet distribution. The fit reproduces very well the \(E_{\rm T}^{\rm miss}\) distribution from 25 to 500 GeV, indicating that Fréchet distribution is a good choice.

In conclusion, we have successfully modelled the linear rate dependence on \(\mu\) of high-threshold \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger rates, after having recognized that they are dominated by single mismeasured jets. The shape of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) tail is described by an extreme-event distribution, as expected if the biggest mismeasurement in the event dominates the measured \(E_{\mathrm{T}}^{\mathrm{miss}}\), with shape parameters independent of \(\mu\). The next (and final) step is to look at the transition region between this high-energy regime and the low energy range, in which stochastic effects dominate.

Figure 17: Top-to-bottom, left-to-right: \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution for 2012 events satisfying a XE trigger with threshold of 20 GeV or higher, with \(\mu=10,14,18,22,26,30\). The Fréchet fit with \(\mu=10\) and \(E_{\mathrm{T}}^{\mathrm{miss}}>60\) GeV fixes offset, shape and scale parameters: only the normalization is left free to vary in all other fits (performed over \(E_{\mathrm{T}}^{\mathrm{miss}}>100\) GeV). The agreement between fit and measurement, quantified by plotting the binwise significance as described in [14], is good over the entire high-energy range. This means that the shape of the tail of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution is independent of \(\mu\).

## 5 The full model for the background \(E_{\rm T}^{\rm miss}\) distribution

We identify the center of the transition region as the \(E_{\rm T}^{\rm miss}\) value for which the extreme-event tail contributes to half of the measured number of events. This can be found by fitting the ratio between observed counts and Frechet tail, for each case considered in Figure 17, with an exponential decay ending with a flat plateau at one (not shown here).

The width of the transition region can be characterized by the distance between the points at which the tail and the rest of the \(E_{\rm T}^{\rm miss}\) distribution are in some fixed proportion. For illustration purposes, we can consider the distance between the values 0.2 and 2.0 of the fitted ratio \(F/B\), where \(F\) is the Frechet contribution and \(B\) is the bulk, or between 0.1 and 10.0 (i.e. from events dominated by stochastic effects to those dominated by the Frechet distribution). Figure 18 shows the dependence on \(\mu\) of the transition region. The transition region is 20-30 GeV wide and moves to the right for increasing \(\mu\), as expected from the stochastic model developed in section 2, which predicts harder \(E_{\rm T}^{\rm miss}\) distribution for higher instantaneous luminosity.

The low- and high-energy parts of the \(E_{\rm T}^{\rm miss}\) distribution have been modelled as arising from two independent sources of mismeasurement. Low \(E_{\rm T}^{\rm miss}\) threshold trigger rates are dominated by calorimeter resolution effects over all channels, hence depend on \(\sum E_{\rm T}\). They increase much faster than linearly with \(\mu\). The high \(E_{\rm T}^{\rm miss}\) events arise mainly from mismeasurement of large local energy deposition in calorimeters, mostly of high-transverse-momentum jets. Hence their rate scales linearly with \(\mu\). In the transition region both sources contribute substantially, and need to be considered simultaneously.

Figure 18: Transition region with \(\mu=10,14,18,22,26,30\). The ratio \(R=(F+B)/F\) between the measured \(E_{\rm T}^{\rm miss}\) distribution \(F+B\), where \(B\) is the bulk of the events due to stochastic effects and \(F\) is the value of the Fréchet tail, and the tail \(F\) is first fitted with an exponential decay ending with a flat plateau (not shown here). Then the following values of \(R\) are chosen to characterize the transition region: \(R=2\) defines its center (at which \(F=B\)); \(R=11\) and \(R=3\) define the end of the bulk region (\(B=10F\)) and the point at which \(B=2F\), respectively; \(R=1.5\) and \(R=1.1\) define the point at which \(F=2B\) and the beginning of the tail region (\(F=10B\)).

As the two contributions to \(E_{\rm T}^{\rm miss}\) are statistically independent, in the transition region the missing transverse-energy vector \(\not{E}_{\rm T}\) can be simulated as the vector sum of two randomly oriented vectors. When averaged over a uniform distribution in their azimuthal separation \(\Delta\phi\), the magnitude \(M\) of the vector sum of two vectors \(A\) and \(B\) is given by [15]

\[\begin{split}\langle M\rangle&=\int_{0}^{\pi}\sqrt{ A^{2}+B^{2}+2AB\cos\phi}\ \mathrm{d}\phi\\ &=2(A+B)\,E(\pi/2,r)\end{split} \tag{17}\]

where \(E\) is the elliptical function, \(A=\|\mathbf{A}\|\), \(B=\|\mathbf{B}\|\), and \(r=\sqrt{4AB/(A+B)^{2}}\). \(\langle M\rangle\) changes from \(A\) (when \(A\gg B\)) to \(B\) (when \(B\gg A\)) as shown in Figure 19. This random sum can be accounted for in the final theoretical model, by replacing the innermost integrand function in (12) with the magnitude of the random sum of Rayleigh and Frechet contributions.

This allows the two rate regimes to be smoothly joined, giving a linear increase with \(\mu\) at high \(E_{\rm T}^{\rm miss}\) thresholds and faster than linear increase at low thresholds, in the transition region. The shift of the transition region with increasing \(\mu\) also explains the change in trend of the scatter plots for intermediate thresholds shown in Figures 13 and 14. In particular, the shift causes a transition in the functional dependence of the trigger rate on \(\mu\).

For example, a threshold of 50 GeV is in the extreme-event region at low \(\mu\) (say below \(\mu=10\)), with a rate larger than that computed with the stochastic model. However, for increasing luminosity the transition region moves toward right, which means that the same threshold enters the bulk region with non linear dependence on \(\mu\). The 50 GeV threshold is described by the stochastic model with \(\mu=18\) or higher. Thus, in the rate vs. \(\mu\) plot one should see a change (more precisely an increase) of slope for this threshold in the region \(\mu=10\)-18, which can be seen as a small slope change only in the EF_XE50 line in Figure 14.

Figure 19: Random vector sum of two vectors \(A\) and \(B\) with uniform azimuthal separation, for \(0.03\leq B/A\leq 30\). The average magnitude \(\langle M\rangle\simeq A\) when \(B\ll A\) and changes to \(\langle M\rangle\simeq B\) when \(B\gg A\). When the values of \(A\) and \(B\) are similar, the magnitude of the random sum \(\langle M\rangle<A+B\) is smaller than the sum of the two magnitudes.

## 6 Summary

Using ATLAS data from the LHC Run I, we have seen that the \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger rates are dominated by background events. The low- and high-energy parts of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution can be modelled as arising from two independent sources of mismeasurement, one connected to the energy deposited in the whole calorimeter, the other with poor measurements in localized regions with high energy density. Apart from a 20-30 GeV wide transition region, where both sources contribute to the final distribution, there are two regimes with markedly different characteristics.

The overall \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution shows a bulk component at low energy, containing the vast majority of events, and a hard tail extending to hundreds of GeV (see for example Figure 16). Triggers with low threshold values (in the bulk region) have rates that increase much faster than linearly with \(\mu\), the expected number of collisions per bunch crossing. On the other hand, the rates of \(E_{\mathrm{T}}^{\mathrm{miss}}\) triggers with thresholds in the tail region increase linearly with \(\mu\). For thresholds in the transition region, the trigger rate changes behaviour smoothly from a linear dependence on \(\mu\) at low instantaneous luminosity (when the threshold is close to or inside the tail region) to a much quicker increase at high \(\mu\) values (when the threshold is close to or inside the bulk region).

As explained in section 2, the low-energy part of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution arises from detector resolution effects and depends on \(\sum E_{\mathrm{T}}\), the total transverse-energy deposited in the whole calorimeter. For fixed \(\sum E_{\mathrm{T}}\) and \(\mu\) values, \(E_{\mathrm{T}}^{\mathrm{miss}}\) follows the Rayleigh distribution (2), whose single parameter depends linearly on both \(\sqrt{\sum E_{\mathrm{T}}}\) and \(\mu\). In addition, \(\sum E_{\mathrm{T}}\) itself depends on the luminosity as a Poisson-weighted combination of Erlang distributions, resulting in the probability distribution (8), expressed in terms of a modified Bessel function of the first kind.

The \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance triggers rely on the dependence of \(E_{\mathrm{T}}^{\mathrm{miss}}\) on \(\sum E_{\mathrm{T}}\) to provide higher efficiency for low \(E_{\mathrm{T}}^{\mathrm{miss}}\) values and better stability with \(\mu\). Triggering on events with low \(E_{\mathrm{T}}^{\mathrm{miss}}\) is important for studying tau and low-\(p_{\mathrm{T}}\) W physics and for studying the electron identification trigger and offline efficiencies. The model facilitates the design of appropriate menus exploiting such triggers.

On the other hand, high-\(E_{\mathrm{T}}^{\mathrm{miss}}\) threshold triggers are important in searches for new physics. Their rate is dominated by the single biggest mismeasurement of jet \(\boldsymbol{p}_{\mathrm{T}}\) in the event. Hence, the tail of the \(E_{\mathrm{T}}^{\mathrm{miss}}\) distribution is modelled by an extreme-event distribution. In particular, it is shown here that a single Frechet density models the tail well at different luminosities, the normalization being the only \(\mu\)-dependent parameter. This explains the linear dependence of the trigger rates with \(\mu\).

Trigger rates for thresholds in the intermediate region have a functional dependence on \(\mu\), which is more complex. At low luminosity they scale linearly with \(\mu\) and are larger than what is predicted using the stochastic model alone. However, at high luminosity the region described by the latter extends to higher energies, and the rate of the same threshold increases faster than linearly, being larger than what the extreme-distribution alone would predict. When this happens, using \(E_{\mathrm{T}}^{\mathrm{miss}}\) significance triggers will improve the trigger efficiency, compared to a normal \(E_{\mathrm{T}}^{\mathrm{miss}}\) trigger with the same output rate.

The complete model can thus be used to predict rates at higher pile-up, providing useful information for the design of trigger menus for future data-taking periods at higher luminosities.

## References

* [1] ATLAS Collaboration, _The ATLAS experiment at the CERN Large Hadron Collider_, JINST **3** (2008) S08003, doi: 10.1088/1748-0221/3/08/S08003.
* [2] ATLAS Collaboration, _Search for squarks and gluinos using final states with jets and missing transverse momentum with the ATLAS detector in \(\sqrt{s}=7\) TeV proton-proton collisions_, Phys. Lett. B **710** (2012) 67, doi: 10.1016/j.physletb.2012.02.051.
* [3] ATLAS Collaboration, _Search for new phenomena with the monojet and missing transverse momentum signature using the ATLAS detector in \(\sqrt{s}=7\) TeV proton-proton collisions_, Phys. Lett. B **705** (2011) 294, doi: 10.1016/j.physletb.2011.10.006.
* [4] ATLAS Collaboration, _Search for a CP-odd Higgs boson decaying to Zh in pp collisions at \(\sqrt{s}=8\) TeV with the ATLAS detector_, Phys. Lett. B **744** (2015) 163.
* [5] ATLAS Collaboration, _Performance of the ATLAS trigger in 2010 running at the LHC_, Eur. Phys. J. C **72** (2012) 1849, doi: 10.1140/epjc/s10052-011-1849-1.
* [6] D. Casadei et al., 'The implementation of the ATLAS missing \(E_{\mathrm{T}}\) triggers for the initial LHC operation', ATL-DAQ-PUB-2011-001, 2011.
* [7] D. Casadei, 'Performance of the ATLAS trigger system', _Proc. of CHEP_, ATL-DAQ-PROC-2012-004, 2012.
* [8] ATLAS Collaboration, _Performance of missing transverse momentum reconstruction in proton-proton collisions at \(\sqrt{s}=7\) TeV with ATLAS_, Eur. Phys. J. C **72** (2012) 1844, doi: 10.1140/epjc/s10052-011-1844-6.
* [9] L. Evans and P. B. (eds.), _LHC machine_, JINST **3** (2008) S08001, doi: 10.1088/1748-0221/3/08/S08001.
* [10] ATLAS Collaboration, 'Performance of the ATLAS transverse energy triggers with initial LHC runs at \(\sqrt{s}=7\) TeV', ATLAS-CONF-2011-072, 2011.
* [11] ATLAS Collaboration, 'The ATLAS transverse-momentum trigger performance at the LHC in 2011', ATLAS-CONF-2014-002, 2014.
* [12] M. Abramowitz and I.A. Stegun, eds., _Handbook of Mathematical Functions With Formulas, Graphs, and Mathematical Tables_, Dover Publications, 1964.
* [13] ATLAS Collaboration, 'Data-quality requirements and event cleaning for jets and missing transverse energy reconstruction with the ATLAS detector in proton-proton collisions at a center-of-mass energy of \(\sqrt{s}=7\) TeV', ATLAS-CONF-2010-038, 2010.
* [14] G. Choudalakis and D. Casadei, _Plotting the Differences Between Data and Expectation_, Eur. Phys. J. Plus **127** (2012) 25, doi: 10.1140/epjp/i2012-12025-y.
* [15] D0 Collaboration, _Pileup Effects in the D0 Calorimeter_, Proceedings of the Third International Conference on Calorimetery in High Energy Physics, Corpus Christi, Texas (1992) 762.