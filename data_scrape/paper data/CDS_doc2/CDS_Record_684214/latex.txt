# PESA high-level trigger selection software requirements

PESA Software Group (editor Simon George).

Last updated on November 6, 2021

###### Abstract

The full revision history is in appendix C.

This document is also available in PDF1, PS2 and 2-up P S3 formats.

Footnote 1: [http://www.hep.ph.rhul.ac.uk/atlas/news/reviewments/10.1/pesa-requirements.pdf](http://www.hep.ph.rhul.ac.uk/atlas/news/reviewments/10.1/pesa-requirements.pdf)

Footnote 2: [http://www.hep.ph.rhul.ac.uk/atlas/news/reviewments/10.1/pesa-requirements.ps](http://www.hep.ph.rhul.ac.uk/atlas/news/reviewments/10.1/pesa-requirements.ps)

Footnote 3: [http://www.hep.ph.rhul.ac.uk/atlas/news/reviewments/10.1/pesa-requirements.2up.ps](http://www.hep.ph.rhul.ac.uk/atlas/news/reviewments/10.1/pesa-requirements.2up.ps)

## 1 Introduction

The aim of this document is to describe the requirements for the ATLAS High Level Trigger Selection Software4. In order to do this, the scope and context of the software project are first described, and an overview of the software is given. The software is logically divided into areas for which requirements, constraints and use cases are listed. They are also been given for the interfaces shared with external software. Requirements are prioritised as either **high**, **medium** or **low**. The priorities indicate how soon these requirements should be met by the software; it is expected that eventually all requirements will be met. The meaning of the priorities is shown in the table 1 below.

Footnote 4: [http://www.hep.ph.rhul.ac.uk/atlas/news/](http://www.hep.ph.rhul.ac.uk/atlas/news/)

The low priority time scale ties in with the TDA Q Phase 2B test bed, in which it is intended to run the HLTSSW. It will be configured to provide representative vertical slices and run on simulated data to provide rejection and test data structures, data collection and execution time. The Phase 2 test beds5 are an important part of the work leading to the DAQ, DCS & HLT TDR.

Footnote 5: [http://documents.cern.ch/cgi-bin/setlink?ba=e=agenda&ca](http://documents.cern.ch/cgi-bin/setlink?ba=e=agenda&ca) teg=a00466&id=a00466s3011/transparencies

\begin{table}
\begin{tabular}{|l l|l|} \hline Priority & Time scale \\ \hline high & needed for development work before September 2001 \\ medium & intended for the September 2001 prototype \\ low & intended for test-bed studies, beginning of January 2002 and a fully functional \\  & release for PESA studies for the DAQ, DCS \& HLT TDR, April 2002. \\ \hline \end{tabular}
\end{table}
Table 1: Definition of priorities used in this document.

The need of each requirement is defined by use of the words shall, should, may or can6 in the text. Use cases are provided to clarify the requirements and provide examples.

Footnote 6: [http://attddoc.cern.ch/Atlas/DaqSoft/sde/inspect/shall.html](http://attddoc.cern.ch/Atlas/DaqSoft/sde/inspect/shall.html)

This document has been the subject of a formal inspection7. The authors would like to thank the inspection team (Andreas Bogaerts, Markus Elsing, Alina Radu, Daniel Wicke, Monika Wielers) for the considerable time and effort they spent improving the document.

Footnote 7: [http://www.hep.ph.rhul.ac.uk/atlas/newsw/requirements-inspection/](http://www.hep.ph.rhul.ac.uk/atlas/newsw/requirements-inspection/)

The document will continue to evolve as part of the documentation of the software. For version control, it will be kept in the software repository.

Whilst reading this document, it may help to refer to the glossary (section A).

Overview and scope of the project

### Context of the ATLAS High Level Trigger Selection Software

The purpose of this project is the development of the ATLAS High Level Trigger Selection Software (HLTSSW). It is part of the TDAQ software project and falls within the remit of the PESA group. There is naturally a very close relationship between this software and the TDAQ Dataflow HLT subsystem which provides the software and hardware infrastructure of the LVL2 and EF processing units. Because of the important relationship between the PESA, the HLT and the offline computing project, the HLTSSW is also referenced in the offline computing plan8.

Footnote 8: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/planning/](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/planning/)

The High Level Trigger (HLT) is part of the ATLAS TDAQ system. It operates on events which pass the first level trigger (LVL1). It must reduce the rate by a factor of \(\sim 10^{3}\) whilst retaining with high efficiency the events needed for offline analysis. This will be done in two steps: the second level trigger (LVL2) and the Event Filter (EF). They are fully described elsewhere, for example in the DAQ, DCS & HLT Technical Proposal9.

Footnote 9: [http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/SG/TP/tp_doc.html](http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/SG/TP/tp_doc.html)

If LVL1 accepts an event, the detector data will be read out and buffered in the Read Out Subsystem (ROS), after which the LVL2 decision is made. The ROS contains ROB-ins (Read Out Buffer inputs) which hold the data received from the Read Out Drivers (RODs) of the detector. If LVL2 accepts an event, the event data fragments are transfered from the ROS to the Event Builder (EB), from which complete events are transfered to the EF farm.

LVL2 processors will have access to any part of the event data which is held in the ROS. It will not usually be possible to build complete events in the LVL2 processors due to the huge network bandwidth and processing resources this would require. Instead, LVL1 triggers will be used to guide LVL2 to Regions of Interest (RoIs) within the event. Processors can request data within small regions over the network. There is a possible exception to this: after an initial stage of refining the LVL1 6 GeV muon trigger, at a comparatively low rate the B-physics trigger might access the whole event in some sub-detectors, as LVL1 cannot provide guidance to the low momentum physics signatures required. The balance between B-physics rate reduction in LVL2 and in EF is to be studied as part of the overall optimisation of the HLT.

The Event Builder (EB) puts together fragments into a complete event record, and transfers them to the EF farm. Here more precise HLT reconstruction algorithms will be used to provide further event selection, rate reduction, and possibly also classification. For example, event classification could be used for database tags or assignment of events to different recording streams online. Even though the full event will be available, HLT reconstruction algorithms may be seeded by the LVL2 results for speed. It is also intended that some quick calibrations be done in the EF for immediate availability. Due to the availability of the full event, the EF is able to monitor the performance of lower trigger levels and the detector performance. The EF could also perform some data preparation steps in advance of mass storage of accepted events, e.g. zero suppression of LAr data in inactive regions.

Athena, the offline control framework, is the baseline solution for the EF. This choice has to be evaluated, which includes comparision with the requirements in this document.

The boundary between LVL2 and EF is not precisely defined; indeed it should remain flexible in order to profit from the complementary features of LVL2 and of EF. Optimisation of the roles of LVL2 and EF need input from many studies, including some which will use the HLTSSW. A rough guide summarising the differences is given in table 2.

### Scope of the HLTSSW

The key roles of the HLTSSW are "event selection" and "event classification". Abstract objects such as electrons, jets, muons and \(J/\psi\to e^{+}e^{-}\), are reconstructed from event data by a particular set of HLT algorithms and parameters. An event is selected if the reconstructed objects satisfy at least one physics signature in the trigger menu. At both LVL2 and EF, events are rejected if they do not pass these selections, which are designed to meet the signal efficiency and rate reduction targets (table 2) of the trigger.

The event classification would include the physics signatures from the menu which were satisfied, and could also include all the reconstructed objects. The classification at one level of the trigger provides seeds for the next level. The final HLT classification can be used to assign tags to the events or even assign them to particular output streams. Tags which can be used when reading back event data offline to efficiently select interesting events for reconstruction and analysis.

The HLTSSW requirements cover the following areas. They are explained later in subsections of this document, and map closely to the project domains defined above.

* HLT selection control
* HLT selection internal data model
* History
* HLT result and event classification
* Interface to HLT algorithms
* Toolkit for analysis of HLT selection performance.
* Monitoring HLT performance in both online and offline environments.
* Interface to LVL1 data
* Access to raw event data
* Interface to offline analysis tools
* Software process.

The HLTSSW is dependent on several external projects. These are listed below. This document will concentrate on the requirements of the interfaces to this external software. Requirements and constraints placed on the external software by the HLTSSW will be also touched upon in these sections.

* HLT Algorithms:
* HLT algorithms will be (and have been) developed by PESA physics working groups. EF-specific algorithms will be developed in collaboration with the offline reconstruction group. This work is shown in the task list of the PESA group10. The software specified in this document will place requirements on these HLT algorithms and be subject to requirements arising from them and the needs of their developers. Footnote 10: [http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/SG/TP/tp_doc.html](http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/SG/TP/tp_doc.html)
* HLT Steering Strategies:
* will be developed by the whole PESA group.

\begin{table}
\begin{tabular}{|l c c|} \hline  & LVL2 & EF \\ \hline latency & \(\sim\)10 ms & \(\sim\)1 s \\ input rate & \(75-100\) kHz & 1 - 2 kHz \\ rejection & \(40-100\) & \(\sim\)10 \\ data access & event fragments & full event \\ trigger characteristic & fast rejection & precise selection \\ \hline \end{tabular}
\end{table}
Table 2: Summary of the differences between LVL2 and EF.

* LVL1 Trigger simulation: this is a closely related project to provide a simulation of the LVL1 trigger electronics. It will be run in the offline development environment along side the HLTSSW. There is an aim to use the same simulation code in online monitoring applications.
* Online software framework: developed by TDAQ systems, especially by the data collection and the HLT sub-systems of data flow. Also the 'backend-DAQ' software which provides distributed services to support this.
* Offline software framework: the control framework of the offline software, Athena, is the baseline choice for the EF, and also the preferred offline environment for HLT algorithm development. Both online and offline software frameworks will be responsible for providing the services required by HLTSSW. Every effort should therefore be made to provide the same or a similar interface to these services in both frameworks. Where this is not possible, the HLTSSW will have to provide additional layers of wrapping so that the main part of the HLTSSW is independent of the external frameworks.

## 3 General

The HLTSSW provides the tools to make the final trigger decision for ATLAS starting from the LVL1 result. It's general functionality, relationship with offline and online software and test-bed commitments are described in this section.

### General constraints for the HLTSSW

1. The TDAQ system must be built to cost. This effectively limits the affordable computing power which will be available to run the HLTSSW and HLT algorithms. Unfortunately this constraint cannot be quantified at the time of writing because the available budget and cost of computing power in the future cannot be accurately predicted. As a guideline, the total execution time of the software including HLT algorithms, and including for LVL2 the additional time for data collection, should not exceed the latency (table 2). _Related requirement UR005, section 3.2._ _Related requirement UR006, section 3.2._
2. The HLTSSW has to comply with the recording capacity and data volume limits set by the offline system. At the time of writing, the targets for ATLAS are 275 Hz at \(10^{33}\)cm\({}^{-2}\)s\({}^{-1}\) and 400 Hz at \(10^{34}\)cm\({}^{-2}\)s\({}^{-1}\) with an event size of about 200 MB. They are given in recent PESA meetings and in interaction with the LHCC referees11. Footnote 11: [http://documents.cern.ch/archive/electronic/other/agenda/a01280/a01280s113/transparencies/Tapproge.pdf](http://documents.cern.ch/archive/electronic/other/agenda/a01280/a01280s113/transparencies/Tapproge.pdf)
3. Part of the HLTSSW runs in the online TDAQ Software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 12: [http://atddoc.cern.ch/Atlas/DaqSoft/document/draft_1.html](http://atddoc.cern.ch/Atlas/DaqSoft/document/draft_1.html)
4. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 12: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
5. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 12: [http://atddoc.cern.ch/Atlas/DaqSoft/document/draft_1.html](http://atddoc.cern.ch/Atlas/DaqSoft/document/draft_1.html)
6. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
7. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
8. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
9. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
14. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
15. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
15. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
16. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
17. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
18. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
19. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
20. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment.
21. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
22. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment.
23. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment. The requirements of the back-end DAQ software12 are available but documentation of other components of the online system are not available at the time of writing; they will be added later. Footnote 13: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/FrameworkRequirements.html)
24. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment.
25. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment.
26. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment.
27. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment.
28. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment.
29. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment.
30. Part of the HLTSSW runs in the offline software environment and needs to comply with the requirements of that environment.
31. Part of the HLTSSW runs in the offline software environment and needs to comply with the * The HLTSSW will need access to "Meta Data", e.g. the magnetic field map, detector geometry, alignment and calibration data, lookup tables.
* The baseline software technology choice of ATLAS is OO design implemented in C++.

### General requirements for the HLTSSW

* The HLTSSW shall run in the general ATLAS offline environment for the following purposes:
* development, testing and verification of HLTSSW components;
* determination and tuning of the performance in terms of selection efficiency, execution time and event rates based on simulation;
* validation of the results and performance of the online system based on real or simulated data;
* use in assessing ATLAS physics performance and, later, studying trigger efficiency, acceptance and biases once real data are available.
* study of correlations between LVL1, LVL2, EF and offline in a single framework. _Priority: these are general, long term goals for the functionality of the HLTSSW. Detailed prioritise is given in the breakdown which follows._
* Parts of the HLTSSW shall run in the ATLAS online environment. These parts are the selection control and event classification framework, internal data model, HLT algorithm interface, monitoring, and interfaces to LVL1 data, online data collection and services provided by the online software. _Priority: this is a general, long term goal for the functionality of the HLTSSW. Detailed prioritise is given in the breakdown which follows._
* The "selection control" and "event classification" software should be suitable for use by both LVL2 and EF to steer the HLT algorithms and decision process. _Priority: medium_.
* The parts of the HLTSSW which run in the offline environment shall be integrated with interactive, debugging, graphics persistency and analysis features of the offline software. _See section 4.6 on the analysis toolkit_. _Priority: medium_.
* The HLTSSW shall be designed to use computing resources efficiently. _Priority: low_. _Related constraint CO001, section 3.1._
* The HLTSSW shall be designed with some flexibility in its processing time in order to help accomodate restrictions in available computing power which are not quantifiable at the time of writing. _Priority: low_. _Related constraint CO001, section 3.1._
* It shall be possible to reconfigure the HLTSSW to start a new run in a time which is insignificant (\(<1\%\)) compared to the length of a normal run. _Related requirement UR301, section 4.5._
* The HLTSSW shall not give rise to fatal errors. All types of error shall be handled in such a way that the next layer up can catch the error and determine the course of action. Different types of error shall be distinguished. _Priority: medium_. _Example: a HLT algorithm receives incomplete data, cannot proceed, so returns prematurely and indicates the reason for the failure._

### General use cases for the HLTSSW

* **HLT algorithm development environment** A developer will plug a HLT algorithm into the offline software development environment and use tools such as interactive graphics, histograms, ntuples, scripting and debuggers to develop the HLT algorithm and optimise its performance. _Related requirement UR004, section 3.2._
* **Full trigger chain studies*
* A user will perform studies using the complete trigger chain, through LVL1, LVL2, EF, offline reconstruction. Examples of such studies include the following:
* optimise overall efficiency and rejection;
* check acceptance and rejection correlations between the different trigger levels;
* study the division between LVL2 and EF;
* investigate use of LVL2 results to guide EF reconstruction;
* check trigger acceptance and bias for an interesting physics signal.
* PESA studies are fully described in Chapter 8 of the DAQ, DCS & HLT TP14 and references therein.

Footnote 14: [http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/SG/TP/tp_doc.html](http://atlasinfo.cern.ch/Atlas/GROUPS/DAQTRIG/SG/TP/tp_doc.html)

* **New trigger rate checks** To estimate the event rate for a new trigger, it is run on real minimum bias data.
* **Online performance comparison/validation** The performance of an interesting set of HLT algorithms, parameters, menus and steering is measured and monitoring output is produced in the offline environment. This is used to validate the correct operation of the same code and parameters in the online framework. _See section 4.7 on monitoring_.
* **Test beds** The HLTSSW will be run in HLT testbeds, for example to exercise the data collection system, measure latency and throughput, and investigate scaling from small to large systems.
* **Classification** The HLTSSW event classification can be used as a starting point for physics analysis, for database tags, or to assign events to different recording streams online.

## 4 Break down of HLTSSW

### HLT selection control

This is the back bone of the HLTSSW, providing the means of "steering" the selection process. This entails running trigger HLT algorithms in a particular order to produce reconstructed objects, and comparing these against a menu of physics signatures to classify the event. Reconstruction is usually started from seeds From LVL1, these seeds will be Regions of Interest (RoIs), while LVL2 provides both LVL1 RoIs and its own seeds to EF in the form of reconstructed objects and the event classification. Examples of abstract reconstructed objects are electrons, jets, muons and \(J/\psi\to e^{+}e^{-}\)

A design example for this part of the software, taken from previous work, is given in appendix B.1.

In order for different models of event selection to be studied, the most important aspect of all the requirements is flexibility.

**Constraints**

* **Seeding from LVL1** LVL1 provides Regions of Interest (RoIs) to LVL2.
* **Secondary RoIs** The LVL1 result will also provide secondary RoIs which did not pass the trigger thresholds, but do pass lower thresholds. These can also be used for seeding HLT processing and/or classifying the event.
* **B-physics selection** Beyond confirmation and refinement of the LVL1 low-\(p_{\text{T}}\) muon trigger, B-physics cannot be seeded by LVL1 because the \(p_{\text{T}}\) of the signatures is too low. B-physics events could be selected at LVL2 or EF. A prerequisite for B-physics selection is a 6 GeV muon trigger element.

**Requirements**

* **Fast rejection** The HLT selection procedure shall reject events as soon as possible. This is to avoid unnecessary data collection and processing and to minimise the average decision latency. _Priority: medium._ _The design example in appendix_B.1.3 _shows one way by which this may be achieved._
* **Seeded reconstruction** The HLTSSW shall support seeding of HLT algorithms. _Priority: high._
* **Unseeded reconstruction** The HLTSSW shall support unseeded reconstruction. _Priority: high._
* **Objects reconstructed from the same seed** Reconstructed objects derived from the same seed should be treated as mutually exclusive when testing a physics signature.
* **Reconstructed objects share position or data** Reconstructed objects derived from seeds with the same geometric position or which have in common some data from which they were reconstructed, should be treated as mutually exclusive when testing a physics signature. _Example: the "same position" could be defined in the calorimeter by a separation in eta and phi which is less than the resolution of the HLT clustering algorithms. Some reconstructed objects can be separated regardless of proximity in (eta, phi) if they are primarily identified in different detectors, e.g. muon and jet. Reconstructed objects can share some common data, e.g. a common track segment used in two reconstructed objects, shared hits on two tracks, two calorimeter clusters sharing cells._
* **Resolving reconstructed objects from the same seed** In the case of multiple objects reconstructed from the same seed, it shall be possible to propagate all elements through the entire processing chain. It shall be possibie for the menu decision to identify that these elements are from the same seed. (i.e. there shall not be an arbitrary choice early on between the seeds and only one propagated). _Related requirement UR103, section 4.1. Priority: medium._
* **Disable rejection** It could be possible to disable the early rejection of an event so that it will be reconstructed regardless of whether it passes the trigger or not. _Priority: medium._Related use case UC107, section 4.1. Related use case UC108, section 4.1.
* **Input and output data** There shall not be any ambiguity in the input and output data of a HLT algorithm. _Priority: high_.
* **Regional algorithms** It should possible to configure the selection to use different HLT algorithms in different parts of a detector. _Priority: medium_.
* **Single HLT algorithm development** The framework should not exclude the development and testing of a single HLT algorithm in isolation. _Priority: high_.
* **HLT algorithm comparisons** It should be possible to use several HLT algorithms (or the same HLT algorithms with several parameter sets) to process the same input data in a single run, for the purposes of comparing their output. _Priority: medium_.
* **Run-time modification of selection strategy** It shall be possible to change the selection strategy without rebuilding the software, e.g. substituting one HLT algorithm for another or changing the order of HLT algorithm processing through the runtime configuration (like Athena's jobOptions). _Priority: high_.
* **Reconstruction design** The HLT selection control shall support the design of the reconstruction software which is hierachical and modular. _Priority: medium_. _Related use case UC105, section 4.1._
* **HLT algorithm errors** The selection control shall handle errors that are returned by HLT algorithms. _Priority: low_.
* **Secondary RoIs** The selection control shall be able to distinguish between objects reconstructed from "trigger" and "secondary" LVL1 RoIs. _Priority: low_.
* **Seeding the EF** LVL2 should provide seeds for the EF reconstruction. The LVL2 seeds could also include LVL1 RoIs. _Priority: medium_.
* **Configuration via graphical user interface** It should be possible to configure the selection control via a GUI. _Priority: low_.
* **Use cases**
* At LVL2, one would try to reconstruct both photon and electron from an electromagnetic LVL1 RoI, since they cannot be distinguished at LVL1. If both a photon and electron are succesfully reconstructed, they cannot both be used to satisfy the same physics signature; they should be mutually exclusive. _Related requirement UR103, section 4.1_.

Related requirement UR105, section 4.1. Related requirement UR202, section 4.3. Related requirement UR154, section 4.2.
* LVL1 finds calorimeter clusters consistent with both a tau and an electromagnetic object. Their locations in the calorimeter are found to be very close together and they share some calorimeter cells. LVL2 reconstruction produces in e and tau objects from these two RoIs. Because they are overlapping (they share cells), they cannot be considered separate for the purposes of fulfilling a physics signature. _Related requirement UR104, section 4.1. Related requirement UR105, section 4.1._
* A HLT algorithm which performs inner detector track reconstruction or extrapolation, may use a calorimeter cluster, a muon system track, or an existing inner detector track as a seed. _Related requirement UR101, section 4.1._
* Unseeded reconstruction The initial reconstruction of B-physics events cannot be seeded by LVL1 RoIs because the LVL1 muon trigger is not correlated with direction of the opposite-side \(B\)-decay and \(p_{T}\) thresholds of the tracks required are too low for LVL1 calorimeter trigger to seed. Reconstruction of such events therefore begins with an unseeded search. _Related requirement UR102, section 4.1._
* Different HLT tracking algorithms are used to reconstruct tracks in the barrel, endcap and barrel/endcap overlap regions. _Example: This can be optimal when simple, fast HLT algorithms are being used rather than complex offline reconstruction algorithms. Related requirement UR108, section 4.1._
* A HLT tracking algorithm for a specific purpose is built from components of a track reconstruction toolkit. The details of how the toolkit components are being used is hidden inside the top level HLT tracking algorithm. Only the top level HLT algorithm is referred to in the sequences. _Related requirement UR112, section 4.1._
* A user wishes to study a selection strategy which uses secondary RoIs from LVL1 in an additional step of the LVL2 or EF selection and classification. _Related requirement UR114, section 4.1._
* **Disable rejection Trigger rejection is disabled in order to allow development of a HLT algorithm which would otherwise be run too infrequently. _Related requirement UR106, section 4.1._
* A user wishes to run LVL2 e/gamma selection without any rejection, to maximise statistics for studying the EF e/gamma selection and fully examine the correlations. Another user wishes to study the advantages of sequential rejection by being able to turn it off and compare performance. _Related requirement UR106, section 4.1._

### HLT selection internal data model

This section specifies the requirements for transient storage of intermediate data within the HLT. The interface to event data is discussed in a later section (access to raw event data).

Most HLT algorithms will require a seed of some sort as input data, will act on seeds and/or detector data, and produce some output data. In sequences of HLT algorithms, there may be intermediate data as for example a track is refined by several stages of pattern recognition, extrapolation and fitting.

Note that LVL2 and EF may have the same requirements but will be run separately.

**Constraints**

* The reconstruction model, which has yet to be developed by the HLT and offline reconstruction groups, will impose external constraints on the internal data model of the HLT.

**Requirements**

* **Speed** The total amount of time spent recording and retrieving data from the transient store should be less than 1% of the allowed latency (at LVL2, at EF). _Priority: low.The design example in appendix B.2 shows one way in which this could be achieved_.
* **Data preparation in the HLT processor** The HLTSSW shall support the use of HLT data preparation (preprocessing) algorithms run in the HLT processor as part of the overall HLT processing sequences. See also access to raw event data (section 4.9). _Priority: high_.
* **Common data classes** Common data classes shall be provided for use by HLT algorithms, i.e. all classes needed to represent raw data, classes for reconstructed objects: e.g. track, cluster,... (list to be completed); classes for trigger elements: e.g. electron, photon, jet,... (list to be completed) _Priority: low_.
* **Relationships between data** It shall be possible to represent relationships between data in the store. _Priority: high_. _See section 4.3 on history_.
* **Types of relationships between data** It should be possible to represent different types of relationship between objects, e.g. parent/child, mutual exclusion, related data. _Priority: medium_. _See section 4.3 on history_. _Related requirement UR202, section 4.3_.
* **HLT algorithms should have read-only access to their specified input data. _Priority: medium_.
* **HLT algorithms shall have the write access necessary to store their specified output data. _Priority: high_.

### History

This section concerns the relationships between data in the HLT reconstruction. This has two purposes: general book-keeping which will be used mainly offline e.g. development purposes, debugging; support for the data model, in which inter-object links are a critical part of the reconstruction.

It also covers the important topic of simulation truth information and relationships between this and reconstructed data.

**Requirements**

* **History of a reconstructed object** For any reconstructed object, it shall be possible to determine the data from which it was formed, the HLT algorithms that produced it and any intermediate data and the parameters of these HLTalgorithms.

_Priority: high_.

UR201: **Trigger Menu**: The relationships between the trigger menu and the reconstructed objects which are used to satisfy physics signatures shall be represented.

_Priority: medium_.

UR202: **Exclusivity**: Mutually exclusive relationships between objects should be supported.

_Priority: low_.

_Related use case UC100, section 4.1_.

UR203: **History monitoring**: For debugging purposes, it should be possible to write out all history information for a subset of events whilst running in the online system.

_Priority: low_.

UR204: **History in LVL2/EF result**: Some history information could be included in the LVL2 and/or EF result which is passed down the trigger chain.

_Priority: low_.

UR205: **Persistency**: Relationships between objects made at LVL2 must still be valid if they are passed on to the EF or stored and subsequently retrieved in the offline environment.

_Priority: low.Question: does this really belong in the data model.?_

UR206: **Simulation truth**: History information shall be available to to relate reconstructed objects to simulation truth objects when simulated data is used in the offline environment.

_Priority: high_.

UR207: **Online performance**: History information and the possibility of links to simulation truth shall not compromise online performance.

_Priority: medium_.

_Related requirement UR317, section 4.5_.

Use cases

UC200: **When using reconstructed objects to check against the trigger menu, their relationship to LVL1 RoIs is followed to find out which are derived from secondary RoIs.**: _Related requirement UR200, section 4.3_.

UC201: **A user wants to find out from which space points a track was reconstructed, and which MC truth track these points relate to.**: _Related requirement UR206, section 4.3_.

UC202: **For jet reconstruction/calibration you need to compare a jet with the mc simulation truth jet, which implies as well reconstruction on the MC level. One would need as well a trace-back to such an object, not just a simple particle.**

### HLT result and event classification

This section covers the use of trigger menus to classify the event at LVL2 and EF, and the resulting information which will be passed on (from LVL2 to EF or from EF to offline).

**Constraints**

* The "trigger menu" specifies the list of acceptable physics signatures. At least one of these must be satisfied for the event to be selected.

**Requirements**

* **Event classification** The result from each stage of the HLT (LVL2, EF) should specify all the physics signatures in the trigger menu which were consistent with the event (i.e. which the event passed). _Priority: high._ _Note: this requirement is based on the premise that the majority of events are rejected so it doesn't add much to the average time if it takes a bit longer for the accepted events. The requirement is a'should' rather than'shall' because it is conceivable that this approach could be too expensive in terms of resources (CPU time and data collection) in some cases where the function of events accepted is not insignificant, e. g. B-physics._
* **LVL2 result** The LVL2 result which is passed to the EF shall contain the event classification (as defined in UR250), and in addition the position, type and energy of all LVL1 RoIs and LVL2 trigger elements. _Priority: medium._ _Related requirement UR250, section 4.4._
* **Trigger menu specification** It should be possible to define a physics signature in terms of any combination of required and vetoed trigger elements, i.e. AND, OR and NOT logic is required. Bracketing may be required in order to construct complex expressions. _Priority: medium._ _Note: when evaluating a logical expression some terms in the physics signature may not be needed (e.g. if the first object in an AND fails, the second is not considered). It should be considered in the design whether this contradicts UR250 and how to avoid it. UR250 is the overriding requirement._
* **Topological triggers** It should be possible to make topological requirements in a physics signature in the trigger menu. _Priority: medium._ _For example, there could be a physics interest in a two object trigger with a mass requirement or a jet veto in the forward region._
* **Prescaling** It shall be possible to specify a prescale factor for physics signature. _Priority: high._
* **Forced accepts** For any physics signature, it should be possible to configure the trigger to accept a small fraction of events which otherwise wouldn't pass for monitoring purposes. _Priority: low._
* **Access to menus from preceeding trigger levels** At each level of the HLT, the trigger menus from the preceeding levels should be available.
* **Configuration via graphical user interface** It should be possible to configure the trigger menu using a GUI. _Priority: low._

**Use cases**

* With an appropriate prescale factor, LVL2 and EF accept certain LVL1 triggers without any further processing. _Related requirement UR254, section 4.4._
* The EF uses the LVL2 result to seed reconstruction. _Related requirement UR251, section 4.4._

### HLT algorithms

This section deals with the HLT algorithms. According the the glossary, a HLT algorithm is any "data processing component of the trigger software". Data processing covers many different functions, e.g. data preparation, reconstruction, which are defined in the glossary. The section addresses HLT algorithms for both LVL2 and EF. HLT algorithms are assumed to interact with the control framework which can ask them to perform certain well-defined actions such as initialisation, execution and finalisation (see below for definitions).

These are the requirements on the HLT algorithms. The requirements on the HLTSSW from the HLT algorithms are found in other sections.

It is expected that design choices will place more stringent constraints on HLT algorithms.

**Constraints**

* Algorithms may not be able to use all services available in the offline software, as some may not be available in the online environment. For example, root/hbook ntuples, particle properties, and graphics would not be available to algorithms running in the online processing farms of LVL2 or EF.
* Algorithm execution time is constrained by the total latencies for LVL2 and EF. _Related constraint CO001, section 3.1._

**Requirements**

* The libraries used within a HLT algorithm shall be confined to those available and agreed for use in each framework in which the HLT algorithm will run. This list has yet to be agreed within TDAQ, but libraries to consider are e.g. STL, CERNlib, CLHEP, NAG. The list may differ for LVL2 and EF algorithms. Libraries used by a HLT algorithm will of course be subject to the same constraints as requirements as the main part of the HLT algorithm, e.g. time constraints, thread safety. _Related constraint CO301, section 4.5._ _Related requirement UR308, section 4.5._
* It shall be possible to reconfigure/reinitialise HLT algorithms to start a new run in a time which is insignificant (\(<0.1\%\)) compared to the length of a normal run. _Priority: low._ _Related requirement UR007, section 3.2._
* HLT algorithms should not perform any unusually time-consuming tasks during execution which could be done in the initialisation step, for example configuration, file access, building of lookup tables. _Priority: low._
* HLT algorithms' parameters shall be configurable using a standard method. This should be the standard method of the respective online and offline frameworks or a distinct method for HLT algorithms which is available in both frameworks. _Priority: medium._* [UR304] HLT algorithms shall access metadata (e.g. calibration, detector geometry, predefined lookup tables) exclusively using a standard method. This should be the standard method of the respective online and offline frameworks or a distinct method for HLT algorithms which is available in both frameworks. _Priority: medium_.
* [UR305] The interface between the HLT selection control and HLT algorithms shall be independent of the framework (online or offline). _Priority: medium_.
* [UR306] The same source code shall work in offline and online frameworks. _Priority: high_.
* [UR307]**Reproducibility** HLT algorithms shall give identical results when given the same input data, regardless of when they are run. _Related requirement UR308, section 4.5_. _For example, the same results should be obtained independently of which and how many events were processed before a given event. This would be an issue when multiple copies of a HLT algorithm could exist in a multi-threaded environment, then the result clearly cannot depend on the order they are run in the separate threads. It means internal state is not a suitable place for features such as histogramming_.
* [UR308] HLT algorithms shall be thread-safe for processing events in parallel threads. _Priority: low to upgrade existing HLT algorithms, but high for new designs._
* [UR309] HLT algorithms shall meet requests to (re)initialise and finalise themselves. _Priority: medium_. _Initialisation will be requested at the start of a run (online), when conditions are loaded (offline), or when a new simulation data file is read (offline). (This is not an exclusive list; there may be other circumstances when initialisation is requested.) Examples of what initialisation could consist of:_
* _change of HLT algorithm parameters;_
* _change of detector geometry;_
* _calculation of look-up tables;_
* _instantiation of complex data structures;_
* _instantiation of private helper objects._ _When called upon to finalise, HLT algorithms would be expected to release resources such as memory. Repeating the cycle of initialisation, execution and finalisation should not suffer from any memory leaks._
* [UR310] The HLT algorithms should conform to the (HLT) reconstruction data model, which defines the classes used to exchange data between algorithms. _Priority: low_. _Related requirement UR152, section 4.2_. _Note: this requirement depends on the development of a reconstruction data model._
* [UR311] HLT algorithms shall not give rise to fatal errors to the software. All types of error shall be handled in such a way that the selection control can catch the error and determine the course of action. Different types of error shall be distinguished. _Priority: medium_. _Example: graceful handling of data corruption._ _Some related design issues are raised in appendix B.4._* [UR312] All output messages shall be categorised (e.g. error, warning, debug) and reported via a common mechanism. It shall be possible to control the level of diagnostic information provided by the HLT algorithm or to switch it off entirely. There shall be no uncontrolled message output (e.g. cout). _Priority: low Related requirement UR906, section 5.2._
* [UR313] HLT algorithms shall be designed with the goal of modular reconstruction in mind. _Priority: high Related requirement UR310, section 4.5._ _For example: break down a major reconstruction task into a reasonable number of components of manageable complexity; conform to the reconstruction data model for input, intermediate and output data classes; use common configurable, generic tools where possible._
* [UR314] Any parameters that an HLT algorithm has should be available for tuning by the user. _Priority: medium._ _For example: cuts used in the reconstruction should not be hard-coded._
* [UR315] HLT algorithms' parameters, interfaces, input and output data, test procedures and performance should be described in up-to-date user documentation. _Priority: medium._ _Related requirement UR313, section 4.5._ _Related requirement UR314, section 4.5._
* [UR316] HLT algorithms should record the association between their input and output using the history mechanism provided. _Priority: high._ _See section 4.3 on history._
* [UR317] HLT algorithms shall not use any simulation truth information. This should be restricted to separate analysis code. _Priority: low See section 4.3 on history._
* [UR318] HLT data preparation algorithms could also be provided with the correct interface for use in the ROS. _This will only apply to a subset of trigger algorithms._ _Priority: low._

### Toolkit for analysis of HLT selection performance.

This section covers the parts of the HLT SSW which are provided to aid user analysis, and also development and debugging.

The working model for use of offline and online environments is as follows. HLT algorithms will be developed in an offline environment, where developers have access to tools and services which are needed for this job, xref. UC001. The studies carried out by the PESA group as described UC002 will also for the most part be done in the offline software environment. On the other hand, certain system performance studies (measuring latency, interaction with data collection, farm CPU utilisation) will be carried out in an online environment in testbeds. Of course the entire HLT system will also be running in an online environment when ATLAS is live and taking data.

tools and services provided by the offline framework which will not be available online tend to be characterised by interactivity, file or database i/o outside the initialisation phase, and heavy time consumption. For example, hbook/root ntuples, other persistency for user objects, graphical event display, GUI, scripting user interface.

Some tools will be needed in both online and offline environments. These will be lightweight, not require interactive user input, nor regular access to files or databases. For example: timing, message logging, small monitoring tasks and histograms.

The requirements for monitoring aspects of the HLTSSW performance are specified in section 4.7.

**Constraints**

* Some analysis tools (listed above) are not suitable, and cannot be used, in the online environment.
* Some offline framework services (listed above) are not suitable, and cannot be provided, in the online environment.

**Requirements**

* The analysis tools and framework services refered to by CO350 and CO351 shall not be required in order to run the HLT algorithms. Their inclusion should be optional. _Priority: high_. _Example: use of an analysis tuple in a HLT algorithm could be separated from the rest of the code as a different method so that it can be optionally excluded at compile time or run time._
* There shall be a means of auditing resource usage (CPU and possibly memory) of HLT algorithms and the other parts of the HLTSSW. It shall work in the offline environment and should also work in the online environments. It should be controllable (turned on or off at run time) by the user at the level of individual HLT algorithms. There should also be an overall control at compile time ("master switch"). _Priority: high for CPU auditing, high for memory auditing_.
* It should be possible to book and fill histograms and ntuples. _Priority: high_. _Related use case UC352, section 4.6._

**Use cases**

* user wants to time a HLT algorithm in order to include this in the optimisation of the HLT algorithm. _Related requirement UR351, section 4.6._
* a user wants to time the code which checks an event against the trigger menu. _Related requirement UR351, section 4.6._
* Standard histograms and/or ntuples are filled and their data used to validate simulated data. _Related requirement UR352, section 4.6._
* Standard histograms and/or ntuples are filled and their data used to validate a new version of the HLT algorithms and/or HLT event selection. _Related requirement UR352, section 4.6._

### Monitoring HLT performance in both online and offline environments.

Online monitoring will be used to verify the performance and results of parts of the HLTSSW whilst they are running in the online system. It is expected that the software tools used to perform this monitoring will be provided by the framework that the software is running in (offline or online). The strategies for monitoring will have to be developed by PESA and reconstruction groups, in particular by the HLT algorithm authors and PESA physics groups.

Monitoring also has a role in the offline environment, where the same techniques can be used to test the software as part of the quality control procedure for a release, and to give brief feedback to users that the software is working correctly.

**Constraints**

* Use of monitoring tools and services is subject to the constraint for source code compatability with both online and offline frameworks. _Related requirement UR306, section 4.5._
* The amount of monitoring information that can be transmitted online from the HLT is limited because most of the CPU and bandwidth resources are needed for the trigger decision and event data.

**Requirements**

* The monitoring system should be lightweight, such that when running online, its consumption of memory, cpu and other resources of the HLT processing node are small (\(\thicksim 1\%\)) compared to the demands of the software being monitored. _Priority: low_
* It should be possible to disable the monitoring system or at least any parts of it which are resource-intensive (e.g. use a lot of CPU time or memory). _Priority: low_
* It shall be possible to accumulate statistics and read them out at a much lower frequency than the trigger rate. _Priority: low_
* It should be possible to verify some aspects of the HLTSSW performance by comparison to reference data. _Priority: low_
* **Sampling of intermediate reconstruction data**
* It should be possibie to sample the output data from a HLT algorithm in the online environment, in order to check its correct performance offline. _Priority: low_.

**Use cases**

* Monitor the rate of a certain trigger and check that it is within the range considered as normal.
* A calorimeter trigger is used in a test-beam. The calorimeter trigger expert wishes to get immediate feedback that the clusters which are being reconstructed are consistent with expectations. This could be done by comparing a histograms of reconstructed cluster parameters with those obtained from simulation or previous test-beam runs.

### Interface to LVL1 data

This section describes the interface between the HLTSSW and the LVL1 trigger. The HLTSSW will have to interface to both the real LVL1 trigger in the online system, and the LVL1 trigger simulation in the offline software environment. The LVL1 simulation could also be run online to monitor LVL1 hardware by comparitively low frequency sampling.

The online interface between LVL1 and LVL2 is defined in ATL-D-ES-000315. LVL1 sends all its output data for accepted events to special LVL1 ROBs so that they can be accessed via the standard HLT data collection mechanisms. In addition, a subset of this information is sent directly to the RoI builder and supervisor in the LVL2 system. This information comprises the LVL1 event number, the bunch crossing identifier, an 8-bit trigger pattern, RoI data and some global quantities (energy sums, other trigger information). The trigger pattern shows which thresholds have been passed, while the RoIs have the actual \(E_{T}\) calculated by LVL1.

Footnote 15: [http://edmsoraweb.cern.ch:8001/cedar/doc.page?document_id=107485k_version=0](http://edmsoraweb.cern.ch:8001/cedar/doc.page?document_id=107485k_version=0)

The LVL2 processor receives the LVL1 data from the supervisor via the data collection system. This information is used by the LVL2 trigger to form data requests.

#### Constraints

* The online interface between LVL1 and LVL2 is already defined (in the document referenced above).

#### Requirements
* The LVL1 results shall be available to LVL2. This includes trigger RoIs, secondary (non-trigger) RoIs and the LVL1 trigger pattern.
* _Priority: high_.
* Any LVL1 configuration which is needed to interpret the LVL1 results shall be available to LVL2.
* _Priority: high_.
* HLT and LVL1 simulation should use the same information to characterise LVL1 output data.
* Simulated LVL1 output data for simulated ATLAS events must be available.
* _Priority: high_.
* _Related use case UC454, section 4.8._
* _Related use case UC455, section 4.8._
* In the offline environment, history information should be available to relate simulated LVL1 RoIs to objects in MC truth and detector simulation.
* _Priority: high_.
* _See section 4.3 on history._

#### Use cases

* HLT uses LVL1 trigger RoIs to seed the reconstruction.
* _Related requirement UR450, section 4.8._
* HLT uses LVL1 secondary RoIs to seed extra reconstruction.
* _Related requirement UR450, section 4.8._
* _Related requirement UR114, section 4.1._
* _Related use case UC106, section 4.1._
* _Related constraint CO101, section 4.1._
* LVL2 uses the LVL1 trigger \(E_{T}\) threshold from the bit pattern to select the best HLT algorithms for further reconstruction.
* _Related requirement UR450, section 4.8._
* LVL2 uses the LVL1 trigger \(E_{T}\) to select the best HLT algorithms for further reconstruction.
* _Related requirement UR450, section 4.8._* 454 The HLTSSW is run offline on simulated data. The LVL1 trigger decision can only be provided by simulation. _Related requirement UR453, section 4.8._
* 455 The HLTSSW is run online in a testbed which has no LVL1 trigger. Simulated data for LVL1, correlated with the simulated detector data, is fed into the testbed. _Related requirement UR453, section 4.8._

### Access to raw event data

This section describes the software needed to access raw event data. In the online environment, this will be a matter of connecting the HLT algorithms to the data collection part of the TDAQ online software. In the offline environment, this method of data access will have to be simulated. Issues relating to the format of raw data are also covered here. it is inherently limited to LVL2.

Here is an overview of the online data path from detector to HLT algorithm. Each detector has readout drivers (RODs), which receive fragments of events that are accepted by LVL1. The ROD can perform several tasks, for example buffering, first calibration, monitoring, data preparation and data formatting. The extent to which these are done in RODs depends on how well the detector response is understood, limitations on readout bandwidth, the processing resources available in the RODs, and the resource demands of the tasks which might be performed. Event data fragments from RODs are transferred to the readout buffer input (ROB-in) modules. The ROB-ins are grouped into Read Out Subsystems (ROSs). The ROSs will be connected to the HLT processors by some sort of network(s). The ROSs may have some data preparation capability, for example data reduction to allow the sending of event fragments from only a requested subset of ROB-ins. This could be requested to be used by the HLT algorithms via the data collection API. (See proposals in John and Fred's talks at the Dataflow meeting from April 2001 TDAQ workshop16.)

Footnote 16: [http://documents.cern.ch/AGE/v2_0/fullAgemda.php?ida=a01105#9](http://documents.cern.ch/AGE/v2_0/fullAgemda.php?ida=a01105#9)

HLT algorithms running in LVL2 processors request data from a certain region of a subdetector. This region can be described in geometrical and/or logical terms; for example a range in \(\eta\) and \(\phi\) (geometrical), a sampling layer in the calorimeter (logical), an endcap of a tracking detector (logical).

The Data Collection software will be responsible for passing requests for data from LVL2 processors over the network to the target ROSs. If LVL2 accepts an event, the data collection will see that all event fragments are sent to the Event Builder. Complete events are transfered from the EB to the EF processor farm.

Another task of the data collection system is to make the LVL1 result available to the LVL2 selection control and take the LVL2 result from the LVL2 processor to the EF. See HLT selection control (section 4.1) and HLT result and event classification (section 4.4).

There will have to be some interface layer between HLT algorithms and data collection software, known here as the DC-HLT interface layer, which fulfils the requirements and constraints from both sides.

The current requirements for the interface layer are broader than those which are expected for the final system, because at this stage of the project a lot of flexibility is needed to evaluate various scenarios for ROS-DC-HLT responsibilities and interaction.

A design example for this part of the software, taken from previous work, is given in appendix B.5.

**Constraints from Data Collection and readout system**

* 1. ROS data will be treated generically by the data collection layer; the data collection never looks inside the ROS data.
* The ROS may have the capability to perform some data preparation tasks.

* The granularity of the readout system may be coarser than the granularity with which HLT algorithms would ideally be able to select their data. In other words, a data request by a HLT algorithm may return some additional data to that which was requested.
* The format of the raw data produced by the RODs may vary between some predefined formats between runs, events and even within an event. _Related use case UC500, section 4.9. Related use case UC501, section 4.9._
* Data requested by the HLT are described by an event number, the detector and data format, and a region, described in geometrical and/or logical terms.
* The data collection system expects requests for data to be made in terms of ROB identifiers and an event number. It returns raw data in the packed format as it comes from the ROSs.

**Requirements**

* **Data selection region*
* The "region" used by HLT algorithms to specify which detector data they want should be able to describe the following geometric or logical volumes in the detector:
* wedge: etamin, etamax, phimin, phimax, zmin, zmax
* optionally specify layers within a wedge, e.g. first sampling of ECAL in an RoI.
* simple shapes in other coordinate systems, e.g. Cartesian for FCAL.
* detector part, e.g. barrel/endcap TRT. The region may also allow selection of more complex shapes such as helical roads for low-pt tracks. The region shall be sufficiently flexible to accommodate unforeseen extensions. _Priority: high_.
* **DC-HLT interface** The DC-HLT interface layer should provide uniform style of data access regardless of the detector that data is being requested from or any preparation that will be applied to the data. _Priority: medium_. _Related constraint CO504, section 4.9._ _Related constraint CO505, section 4.9._
* **Data preparation requested by HLT algorithms** HLT algorithms should be able to indicate that certain data preparation steps are to be performed on the requested data. _Priority: high_. _Note: the request for some preparation steps could be implicit in the requested data objects, e.g. if a HLT tracking algorithm requests Si space-points rather than Si clusters_.
* **Requesting data preparation in ROS** HLT algorithms should be able to request that certain data preparation steps (from those available) are performed in the ROS. The request may include parameters for the HLT algorithms. _Priority: high_.
* **Data preparation in ROS** If any data preparation steps are done in the ROS, these steps shall be indicated in the data sent to the HLT so that it can be treated correctly. _Priority: low_.
* **Data selection granularity** The DC-HLT interface layer should, when suitable for HLT algorithm optimisation, allow the requireddata to be specified at a range of granularities, including ROB-in level and below.

_Priority: medium._

_Note: this requirement is for investigation of whether this functionality will be desirable in the final system. It does not require that the data from DC is reduced to the specified granularity._
* **HLT algorithm portability** The source of event data for HLT algorithms shall be hidden from them.

_Priority: medium._

_Note: this is to help HLT algorithms to be portable between the online and offline environments, and between EF and LVL2. Data sources could be for example the LVL2 data collection system or the offline transient event store._
* **Encapsulation of data collection** Low level details of data collection (e.g. data location, possible caching, network topologies and protocols) shall be hidden from the HLT algorithms.
* **Data access in the offline framework** The raw data access shall look the same to HLT algorithms, regardless of whether the data is coming via the online or offline frameworks. Implementations of the interface for both sources shall be provided.

_Note: the implementation for the offline framework is a task of HLT & PESA groups._

_Priority: medium._
* **Error reporting** When errors occur during data collection, the data returned should include an indication of the nature of error, so that the HLTSSW can decide what action to take.

_Priority: medium._

* **Use cases**
* **LAr RODs could send 0.1% of events (e.g. specially selected prescaled minimum bias) without compression and the rest compressed. This difference would have to be made clear to the receiving HLT data preparation algorithms, so they could recognise the different compression levels in the data and know how to unpack them and apply the correct calibration in each case.** _Related constraint CO503, section 4.9._
* **The compression algorithm in the LAr ROD is upgraded after the first month of LHC data. When the HLTSSW is run on this in the offline environment, the different calibration to be used for runs before and after the change would have to be managed automatically.** _Related constraint CO503, section 4.9._
* **A HLT calorimeter reconstruction algorithm may calculate the shower shape in the second sampling of the calorimeter only, as part of a sequential selection strategy. In order for this to be effective, it must be possible to limit the data transferred over the network to the layer concerned.** _Related requirement UR500, section 4.9._
* **The most convenient coordinate system to use in the FCAL is often Cartesian (**_x,y_**) rather than projective (**_\(\eta\)_,_ \(\phi\)_)._ _Related requirement UR500, section 4.9._
* **Some HLT SCT tracking algorithms may require 3D space points, as input, while others will take strips or clusters._ _Related requirement UR502, section 4.9._

### Interface to offline analysis tools

Constraints

* Classes to be visualised must inherit from the plottable class of the graphics domain.

Requirements
* The HLTSSW and offline reconstruction should use the same classes for offline analysis.

_Priority: low_

Use cases
* A user compares reconstructed electrons from LVL2, EF and offline to study e.g. correlations, biases.

_Related requirement UR550, section 4.10._

### Software process

Constraints
* The software process will comply as far as possible with both TDAQ and offline software processes.

Requirements
* **Software development process** The HLTSSW shall have adequate requirements capture, problem analysis, high-level and detailed design, implementation and testing. This should be accompanied by documentation and inspection-s/reviews at each stage.
* _Priority: high_.
* **Types of documentation** Every package, e.g. each HLT algorithm, the selection control, the data access, shall be documented for both the user, developer and maintainer.
* _Priority: medium_.
* **Up to date documentation** Software and documentation shall be stored in a version control system and up to date versions included in releases.
* _Priority: medium_.
* **Testing** As far as it is practical, packages should include a simple, self-contained test suite which can be run to verify that they are functioning correctly.

_Priority: medium_.

_Related use case UC600, section 4.11._

Use cases
* As part of the QA procedure for a release, the librarian runs the automated testing procedures in each package.

_Related requirement UR603, section 4.11._

## 5 External software

### TDAQ online software environment

**Requirements made by the online software on HLTSSW**

* **Thread safety** The part of the HLTSSW which runs in the online environment shall be thread safe. _Priority: high_. _Related requirement UR308, section 4.5._
* **Lightweight framework** The framework shall be lightweight, such that when running online, its consumption of memory, CPU and other resources of the HLT processing node are small compared to the demands of the HLT algorithms. _Example: small could mean less than 10% of the total CPU usage. Priority: high_.
* **Support for online platforms** The part of the HLTSSW which runs in the online environment shall be able to be compiled and linked with the compilers and operating systems (OS) used online. Note: this requires an exact match of compiler and OS version.

**Requirements made by HLTSSW on the online software**

* **Framework services** The online software shall provide an implementation of the framework services needed by the HLTSSW (specified in the next section). They should have the same interfaces so that client code can work without modification in either framework. _Priority: medium_.

### Offline software environment

Here are the most important requirements coming from the HLTSSW on the offline software environment. The offline control framework Athena is the baseline for provision of these services to the EF and to the whole HLTSSW when it runs on the offline environment. Athena has its own requirements document17.

Footnote 17: [http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/](http://atlas.web.cern.ch/Atlas/GROUPS/SOFTWARE/OO/architecture/General/Architecture/Documents/) FrameworkRequirements.html

**Requirements made by HLTSSW on offline software**

* **The offline software shall provide a service through which to access the Detector Geometry database. _Priority: high_.
* **The offline software shall provide a service through which to access the Calibration database. _Priority: low_.
* **The offline software shall provide a service through which to access the mapping between geometrical identifiers and readout identifiers for the detector. _Priority: high_.
* **The offline software shall provide service(s) to access all metadata required by HLT algorithms. _This consolates requirements UR900, UR901, UR902._** _Priority: high_.

* [UR904] The offline software shall provide a service through which raw data can be accessed. _Priority: medium_.
* [UR905] The offline software shall provide a means for instantiating HLT algorithms. _Priority: high_.
* [UR906] The offline software shall provide a message-logging service which is available to all components of the HLTSSW. _Priority: high_.
* [UR907] The offline software shall provide a means of transient event data storage. _Priority: high_.
* [UR908] The offline software should provide a mechanism to allow data to be written out at an arbitrary point in the HLT selection, and the resume program execution from this point when the offline software is run again. _Priority: low_.
* [UR909] The offline software shall provide a means of making selected transient event data persistent, and to retrieve this information. _Priority: low_.
* [UR910] The offline software shall provide a mechanism to simulate pile up at a user-defined luminosity within the possible LHC range. _Priority: high_.

Glossary

The original source of the glossary is a web page18 maintained by Saul Gonzalez.

Footnote 18: [http://www-wisconsin.cern.ch/](http://www-wisconsin.cern.ch/) atsaul/sw/defs.html

#### Glossary of terms used

* **Detector Data**:
* **Raw Data**: Data in the format provided by the ROD, which would have to be collected from a ROS.
* **Prepared Data**: The output from a HLT data preparation algorithm (archaic: pre-processed data).
* **Event Classification**: A list of the physics signatures identified in an event.
* **Forced accepts**: Accept events which have otherwise been rejected by the trigger. Useful for randomly sampling events rejected by the trigger for Cross checks and efficiency/bias studies. A forced accept factor is an (optional) attribute of the trigger menu for each physics signature.
* **History**, also known as **Trace Back**: information that relates data objects to the source data from which they were derived. It can also include information about how the data object was derived, such as which HLT algorithms, the sequence in which they awere applied, and their configuration parameters. A typical use is to relate a reconstructed object to simulation truth information. Another typical use is to relate objects in a reconstruction sequence, for example tracks to hits.
* **HLT algorithm**: A data processing component of the trigger software. The HLT algorithm can be sub-categorised as follows; there may be other valid categorisations which are used in the design stage.
* **Data preparation**: An HLT algorithm that modifies or reorganises the data into a format that optimises it for reconstruction or minimises the volume for transmission. Certain data preparation algorithms could run in the ROS rather than the HLT processor. _e.g. zero suppression of TRT data, removal of any data outside a defined region, forming clusters from SCT strips.
* **Reconstruction**: A HLT algorithm that attempts to abstract physical features from input data. _e.g. pattern recognition, a fil to points to measure the track parameters.
* **Hypothesis**: A HLT algorithm that takes reconstructed objects and tries to interpret them as physics entities which appear in the trigger menu. _e.g. forms an electron candidate from a track and calorimeter cluster_.
* **HLT algorithm parameters**: The parameters that control the behaviour of a HLT algorithm, e.g. whether to send out debug messages,the cell thresholds to use in a zero-suppression algorithm, or the bin size in a histograming algorithm.
* **Instance**: A term generally used to refer to an object of some class type, since objects are 'instantiated' by the software framework. Frequently used to indicate different objects of the same class, or similarly for data classes.
* **LVL1 trigger pattern**: the bit pattern which indicates which LVL1 triggers and thresholds were passed. LVL2 will receive this from LVL1.
* **Metadata**: auxilliary data pertaining to other data, e.g. calibration, detector geometry, predefined lookup tables.
* **Physics Signatures**: A combination of required and/or vetoed Trigger Elements meant for triggering a specific physics process.

* **Prescaling**: Reduction in trigger rate by random rejection of events. This complements the usual means of rate reduction by discrimination based on the event data. A prescale factor is an (optional) attribute of the trigger menu for each physics signature.
* **Readout identifier**: logical addressing scheme used in common with the Data Collection system to identify ROBs and/or ROSs.
* **Region**: The specification of an arbitrary volume within an ATLAS sub-detector by a geometric and logical description. It is used by the HLT to communicate the region from which data is requested. It has to be translated into readout identifiers to formulate a request to the data collection subsystem.
* **Region of Interest (RoI)**: Seeds coming from LVL1 to LVL2 via the supervisor. Also available to the EF. Defined as a solid angle slice of the ATLAS detector, specified as (\(\eta\), \(\phi\), \(\Delta\eta\), \(\Delta\phi\)). RoIs are those areas of the detector identified by the LVL1 trigger to contain interesting high-\(p_{T}\) activity.
* **Primary RoI**: An RoI that alone would cause an accept at LVL1.
* **Secondary RoI**: An RoI that alone would not trigger LVL1 but is available as additional information to the HLT.
* **Seed**: Reconstructed objects (e.g. tracks) or other data (e.g. LVL1 RoIs) that can be used as a starting point for reconstruction by HLT algorithms. Seeds are sometimes described as primary or secondary. This distinguishes those which are derived from LVL1 trigger RoIs (primary) and LVL1 secondary RoIs.
* **Sequence**: A particular ordering of HLT algorithms at execution time.
* **Space point**: the three-dimensional position representing the centre point of a detector measurement. Can be derived from more basic mesurements, e.g. by the combination of SCT clusters from the \(\phi\) and stereo layers of a silicon module. Described in the ATLAS coordinate system rather than any local coordinate system e.g. on a Si wafer.
* **Steering**: Component of the Trigger software that issues directives on HLT algorithm configuration, execution, evaluation of trigger decision, and classification of selected events. It controls the sequencing of HLT algorithms.
* **Transient Event Store**: Allows HLT algorithms to store and retrieve objects. The store is cleared at the end of every event. It may also provide an interface to raw and/or preprocessed event data. When used in this document, the term refers to the general concept or a TES, not especially the Athena implementation.
* see the design example in appendix B.1.3._
* **Trigger Menu**: A list of physics signatures that would trigger an event. In order to accept an event, at least one physics signature in the list must be satisfied. In addition to the physics signatures, each menu item may optionally include pre-scale and forced-accept factors.
* **Trigger Menu Item**: A single item in the Trigger Menu list, which when satisfied, triggers an event.
* **Vetoed trigger element**: a physics signature may explicitly require that a certain trigger element has not been reconstructed.

**Acronyms and abbreviations**

**API**: Application Programming Interface
**B-field**: magnetic field (of ATLAS)
**CERNlib**: (old HEP software libraries)
**CLHEP**: (new OO HEP software libraries)
**CPU**: central processing unit of a computer; also used as shorthand for the resource of CPU cycles or time which is consumed by running software
**DAQ**: Data Acquisition
**DC**: Data Collection (sub group of TDAQ)
**DCS**: Detector control system
**EB**: Event Builder
**EF**: Event Filter
**ECAL**: Electromagnetic Calorimeter (ATLAS detector)
**FCAL**: Forward Calorimeter (ATLAS detector)
**HCAL**: Hadronic Calorimeter (ATLAS detector)
**GUI**: Graphical User Interface
**HEP**: High Energy Physics (collider particle physics)
**HLT**: High Level Trigger (umbrella term for LVL2 & EF)
**HLTSSW**: The (PESA) High Level Trigger (event) Selection SoftWare
**LAr**: Liquid Argon (subdetector of ATLAS calorimeter)
**LVL1**: Level one trigger
**LVL2**: Level two trigger
**MC**: Monte Carlo (simulation)
**NAG**: Numerical Algorithms Group (provider of mathematical and numerical software libraries)
**OS**: Operating system (o/s).
**PESA**: Physics and Event Selection Architecture (sub group of TDAQ)
**QA**: Quality Assurance
**ROB**: Readout buffer
**ROB-in**: Part of readout buffer excluding the interface to HLT dataflow.
**ROB**: Readout driver (for detectors)
**ROI**: Region of interest
**ROS**: Readout system - a grouping of ROB-ins and interfaces to HLT dataflow.
**SCT**: SemiConductor Tracker (subdetector of ATLAS inner tracking detector)
**Si**: Silicon detectors (umbrella term for SCT & Pixel subdetectors)

**STL**: Standard Template Library (for ISO C++)
**TES**: Transient Event Store (of Athena)
**TDAQ**: Trigger DAQ project in ATLAS
**TDR**: Technical Design Report
**TP**: Technical Proposal
**TRT**: Transition Radiation Tracker (subdetector of ATLAS inner tracking detector)

**Common usage terms and conventions**

* **high-\(p_{T}\)**: Charged particle tracks or calorimeter clusters with \(p_{T}\geq 10\) GeV
* **low-\(p_{T}\)**: Charged particle tracks or calorimeter clusters with \(p_{T}\leq 10\) GeV
* **low-luminosity**: average target luminosity of \(10^{33}\) cm\({}^{-2}\)s\({}^{-1}\).
* **high-luminosity**: average target luminosity of \(10^{34}\) cm\({}^{-2}\)s\({}^{-1}\).
* **Threshold**: The loosely-used convention is that the \(E_{T}\) threshold is the \(E_{T}\) of a particle at 90% efficiency after the LVL2 trigger. Therefore a trigger element named "e25" implies that the trigger will select a 25 GeV electron with 90% efficiency.

Design examples

This appendix contains some example designs which may help to understand some of the requirements, constraints and use cases in the main document. They have been separated because it is expected that they will eventually be superceded, while the requirements should not be. The information here is offered as input to the design phase of the software project but it is up to the designers to decide how they use it.

### HLT selection control

#### b.1.1 Mini-glossary

Special definition of some terms for the purpose of this example:

* **Trigger Element**: A physics-motivated entity built from the features reconstructed by HLT algorithms, e.g., e2%, j360.
* **Local Trigger Element**: A Trigger Element derived from a LVL1 Region of Interest trigger (mu, em, tau, jet), typically of high-\(p_{T}\).
* **Global Trigger Element**: A Trigger Element derived from a LVL1 global trigger (ETmiss, SumET).
* **B Trigger Element**: A Trigger Element designed to select exclusive final states used in B-physics (e.g. B-?PiPi, J/Psi-?MuMu.)
* **Trigger Element Parameters**: The set of cuts on features that validate a trigger element.

#### b.1.2 Introduction

A model for LVL2 event selection has already been developed (see LVL2 testbed design note 3019) in the PESA group. It is based on some key concepts: decision steps, seeding and hypothesis sequences. The overall selection of an event is broken down into a series of steps. A step comprises some reconstruction and a comparison against a trigger menu which results in a decision to either reject or continue processing the event. The reconstruction within each step can be seed-based, for example using LVL1 RoIs or the output of the previous step. Initial LVL2 steps will normally be confined to primary RoIs, with secondary RoIs considered in later steps. For each seed, sequence(s) of HLT algorithms are run to build up and test a hypothesis that the detector data is consistent with a certain trigger element (a reconstructed particle with certain properties such at \(E_{T}\)).

Footnote 19: [http://atlas.web.cern.ch/Atlas/project/LVL2testbed/www/](http://atlas.web.cern.ch/Atlas/project/LVL2testbed/www/)

#### b.1.3 Steering

Steps in the decision process are organised sequentially, e.g. by sub-detector or by HLT algorithm complexity, and a decision can be made whether to reject the event or continue after each step.

Each seed is processed independently in turn, in multiple steps; after each step in the selection process, a decision can be made whether to terminate the processing, or continue towards the formation of a trigger element. The pseudo-code below illustrates this.

foreachstep{  foreachsequence{  sequencerpicksseedswhichitcanuse  foreachseed{  foreachHLTalgorithminsequence{  algorithm.execute(seed)  }//algorithm  }//seed  }//sequence  if(triggermenutest==false){  flagtorejectevent  break }else{  continue  } } //step

In each step, a trigger element is build up with more precision and information from more detectors. If at some stage in the sequence the trigger element is no longer consistent with the hypothesis, this causes the sequence to terminate before completion. Thus fast, early rejection of events is achieved. Since such a small fraction of events are accepted (LVL2 accepts \(\sim 2\%\), EF \(\sim 10\%\), it is not a problem to spend longer processing accepted events.

For example: when LVL1 identifies just the 2xEM15I physics signature in an event, LVL2 will process the two EM RoIs to try to form em15i trigger elements. This processing could be ordered in sequential steps organised by detector or HLT algorithm complexity. It could alternatively be ordered by sequential steps confined within RoIs, which are processed independently. Mixtures of these approaches are also possible strategies.

* By detector: the calo data would be processed for both RoIs, then the TRT data, then the SCT data, and so on. Between each processing step, there could be a decision step to check that the required features had been found. In this example, if either one or both of the two calorimeter features was found to be below the \(E_{T}\) threshold, then there would be no point in continuing to process the event as it could no longer satisfy any item on the trigger menu.
* By complexity: calo feature extraction is split into steps based on layers of the calorimeter and HLT algorithm complexity, starting with the one which offers the most rejection.
* By RoI: one RoI would be completely processed before the other. It is still possible to stop processing the RoI before all detectors have been considered, if it fails to make a feature needed by the hypothesis. However, the event cannot be accepted or rejected until all RoIs have been processed for all relevant hypotheses.

#### b.1.4 Steering history management

In order to fulfil some of the history, traceability and reproducability requirements, a possible design is to associate a unique HLT algorithm instantiation and name with each different configuration of a given HLT algorithm. Similarly, trigger elements are given names which correspond to a unique configuration of the hypothesis cuts which validate them.

For example, a Trigger Element with a given name, such as "e25i1" (for a \(\sim\)25 GeV isolate electron) corresponds to a unique sequence of data preparation, HLT FEX and hypotheses algorithms with certain sets of parameters. The index "1" at the end anticipates that there may be several sets of parameters with softer or harder selection criteria; for example a di-electron trigger would require trigger elements with softer selection criteria than a single electron trigger which has nominally the same energy threshold.

### Internal data model

#### b.2.1 Caching

Caching can be used to save time when the same data is requested repeatedly. If more than one HLT algorithm (or instance of a HLT algorithm) requires the same input data (e.g. a feature or trigger element), this data should only be produced the first time, and on subsequent requests it should be retrieved from a cache. For example, if a tau and electron both require a track search in the same road, the search need only be done once.

Note that sometimes it may be faster to rerun a HLT algorithm than to use a cache.

#### b.2.2 HLT algorithm output

One way to make sure it is clear whether or not a HLT algorithm has run is to insist that it always produces its designated output data. If for example no features are found, this is still a result and can be represented by an empty container. Missing output indicates that the HLT algorithm has not been run, or has failed to complete due to an unrecoverable error. It is up to subsequent HLT algorithms to decide what to do with an empty input container. This makes caching more effective as the empty container will be retrieved from the cache, while no output at all would result in a repeat run of the HLT algorithm.

### HLT result

Re. UR250, UR204: if the LVL2 result includes both passed and failed physics signatures, this avoids bookkeeping of menus for each run and allows different menus for LVL2/EF.

Give example trigger menu?

### HLT algorithms

#### b.4.1 Error handling

Re. UR311 example. In the case of data corruption, it could be dealt with in one of three ways.

1. In the data themselves: the data integrity is checked as it is received and data objects automatically ensure their own validity when instantiated or modified. This may be a very time consuming overhead and not cost-effective in the trigger if expected error rates are very low.
2. In the HLT algorithms: possibly more efficieny because data is only checked when it is used, and context-sensitive checks can also be made which the data cannot check for itself internally.
3. In the framework, the 'exceptions' can be used to catch certain types of error when they occur and handle them tidily at the next layer up in the software.

#### b.4.2 Message logging service

Re. UR312: the following major message categories are proposed: error, warning, info, debug.

'Error' should be accompanied by an indication of the type of error from a predefined list. Error messages imply that the software component involved is not able to complete its task successfully.

'Warning' should also be accompanied with an indication of the type from a predefined list. The implication is that there may be some minor problems which could have compromised performance but the task was completed.

'Debbug' messages containing information that is useful to developers. Subdivided into several different frequency categories, e.g. low, medium, high or 1 to 10, which can be set for each component of the software, e.g. a HLT algorithm.

'Info' messages are used to give feedback on progress to users or monitoring systems. Their frequency must be adaptable to suit the environment that the software is running in. For example, a large processing farm cannot produce messages many times per event from every process; an online system will not have sufficient bandwidth to send info messages at the same rate as events are recieved. Messages are subdivided into several different frequency categories.

Another scheme exists in the online software. It distinguishes between two categories: (1) errors, which are classified and prioritised as fatal error, recoverable error, warning or info and (2) debug classified as debug1, 2 or 3. There exists a more elaborate scheme for classification of debug levels that we should adopt.

### Access to raw event data

Data collection itself has a layered design, so that for example network technology is hidden beneath a generic message passing library, which in turn is hidden beneath the functionality for the data collection. It has been proposed that this layering be extended into the PESA software in order to achieve the abstract interface to data which is preferred for HLT algorithms. Here follows an example of this.

* The HLT algorithm asks for a certain event's data in a geometrical region, e.g. (\(\eta_{m\,in}\,\eta_{m\,ax}\), \(\phi_{m\,in}\), \(\phi_{m\,ax}\)). It also specifies the format of the data, e.g. whether it wants SCT space points or clusters.
* A translation layer converts this into a list of identifiers for the ROBs which overlap the geometrical region.
* The D.C interface accepts requests for data by ROB identifiers and event number. It returns raw data as bytes in message payload.
* Unpacking layer: the raw data is unpacked into digit objects.
* Data preparation layer: the digit objects may undergo some transformation or formatting, calibration, zero suppression, compression, reduction to the exact shape of the geometrical region. Even certain HLT algorithms which belong more to feature extraction like SCT space point formation could be done in this layer.
* HLT algorithm receives data in the requested format from the requested region. HLT data preparation algorithms could work 'on the fly' inside the preparation layer for efficiency and speed. Alternatively, they could work within the same control framework as the other types of HLT algorithm to simplify their control and configuration.

## Appendix C Revision control sheet

This is the full revision history of this document. A summary appears on the first page.

\begin{tabular}{|l|l|l|l|} \hline Version & Date & Editor & Comments \\ \hline
1.0.1 & 09 Aug 01 & Simon George & Corrections made for ATLAS note approval. \\
1.0.0 & 22 May 01 & Simon George & Major release after inspection and renumbering. \\
0.18.0 & 11 May 01 & Simon George & All actions from inspection completed. \\
0.17.1 & 02 May 01 & Simon George & Action 4 completed. \\
0.17.0 & 02 May 01 & Simon George & Action 4 from inspection logging meeting. \\
0.16.0 & 29 Apr 01 & Simon George & First iteration released to inspection team. \\
0.15.0 & 29 Apr 01 & Simon George & Misc tidy up. \\
0.14.0 & 27 Apr 01 & Simon George & Glossary modified following inspection. \\
0.13.0 & 26 Apr 01 & Simon George & Sec. 5 modified following inspection. \\
0.12.0 & 26 Apr 01 & Simon George & Secs. 4.9 \& 4.10 modified following inspection. \\
0.11.0 & 25 Apr 01 & Simon George & Sec. 4.8 modified following inspection. \\
0.10.0 & 22 Apr 01 & Simon George & Sec. 4.7 modified following inspection. \\
0.9.0 & 20 Apr 01 & Simon George & Sec. 4.6 modified following inspection. \\
0.8.0 & 19 Apr 01 & Simon George & Sec. 4.5 modified following inspection. \\
0.7.0 & 17 Apr 01 & Simon George & Sec. 4.4 modified following inspection. \\
0.6.0 & 17 Apr 01 & Simon George & Sec. 4.3 modified following inspection. \\
0.5.0 & 17 Apr 01 & Simon George & Sec. 4.2 modified following inspection. \\
0.4.0 & 17 Apr 01 & Simon George & Sec. 4.1 modified following inspection. \\  & & & New design examples and history sections. \\
0.3.0 & 16 Apr 01 & Simon George & Sec. 3 modified following inspection. \\
0.2.0 & 16 Apr 01 & Simon George & Introduction + sec. 2 modified following inspection. \\
0.1.1 & 10 Apr 01 & Simon George & A few further comments from Stefan incorporated. \\
0.1.0 & 15 Mar 01 & -- & Frozen for duration of inspection process. \\
0.0.9 &? & Simon George & Improved selection strategy explanations. \\
0.0.8 &? & Simon George & Checked priorities, formal wording, spell check. \\
0.0.7 & 14 Mar 01 & Simon George & Revamped data collection interface to reflect recent discussions \\
0.0.6 &? & Simon George & Glossary incorporated, cross references hyper-linked. \\
0.0.5 & 09 Mar 01 & Simon George & Comments from John, Francois and Nick incorporated. \\
0.0.4 & 17 Feb 01 & Simon George & Comments from Valerio, Andre, Monika, Saul and Simon. \\
0.0.3 & 23 Feb 01 & Simon George & UR/UC/CO reference numbers are now permanent. \\
0.0.2 & 09 Feb 01 & Simon George & Comments from Stefan incorporated. \\
0.0.1 & 07 Feb 01 & Simon George & First attempt. \\ \hline \end{tabular}