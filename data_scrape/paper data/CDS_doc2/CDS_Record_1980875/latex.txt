# Unified System for Processing Real and Simulated Data in the ATLAS Experiment

Mikhail Borodin

R. B. Bland

M. Borodin

K. De

University of Texas Arlington, TX, U.S.A.

Jose Garcia Navarro

Instituto de Fisica Corpuscular, Universidad de Valencia, Spain

Dmitry Golubkov

Alexei Klimentov

T. Maashi Maeno

Brookhaven National Laboratory, NY, U.S.A.

David South

Deutsches Elektronen-Synchrotron, Hamburg, Germany

Alexandre Vaniachine

Argonne National Laboratory, IL, U.S.A.

###### Abstract

The physics goals of the next Large Hadron Collider run include high precision tests of the Standard Model and searches for new physics. These goals require detailed comparison of data with computational models simulating the expected data behavior. To highlight the role which modeling and simulation plays in future scientific discovery, we report on use cases and experience with a unified system built to process both real and simulated data of growing volume and variety.

Big Data, Grid-based Simulation and Computing, Parallel and Distributed Computing, Large Scale Scientific Instruments

## 1 Introduction

In 2015 the Large Hadron Collider will open new "Gates of Nature" by reaching instantaneous luminosities exceeding 2:10\({}^{34}\) cm\({}^{-3}\)s\({}^{-1}\) and center of mass energies of 13 TeV. The physics goals of the ATLAS experiment [1] include searches for physics beyond the Standard Model and high precisionHiggs sector studies. These goals require detailed comparison of the expected physics and detector behavior with data. A rich set of computational models is employed to provide simulated data needed for this comparison with real data.

To address the corresponding Big Data processing challenge, the LHC experiments employ the computational infrastructure of the Worldwide LHC Computing Grid (WLCG) - world's largest academic distributed computing environment. The WLCG builds on the ideas proposed by Foster and Kesselman[2]. Thanks to the outstanding LHC performance, ATLAS manages over 160 petabytes of data on more than hundred computational sites. Following Big Data processing, more than ten thousand scientists analyze LHC data in search of new phenomena. ATLAS leads the WLCG usage in the number of data processing jobs and processed data volume.

Leveraging the underlying job management system PanDA developed by T. Maeno et al[3] the production system orchestrates ATLAS data processing applications for efficient usage of more than hundred thousands of CPU-cores provided by the WLCG. In order to manage the diversity of LHC physics (exceeding 35 000 physics samples per year), the individual data processing tasks are organized into workflows. During data processing the system monitors site performance and supports dynamic sharing minimizing the workflow duration. In addition, the production system manages jobs and/or tasks failures enhancing the resilience.

In preparation for data taking, the ATLAS experiment is scaling up its Big Data capabilities by upgrading a multilevel production system that unifies processing of the real and simulated data. In this paper we describe representative data processing use cases handled by the production system highlighting the role which modeling and simulation plays in future scientific discovery.

## 2 Big Data Processing Use Cases

To process Big Data, the ATLAS experiment adopted the data transformation technique, where software applications transform the input datasets of one data type into the output datasets of another data type. In Big Data processing ATLAS deals with datasets, not individual files. Similarly a task (comprised of many jobs) has become a unit of the workflow in ATLAS Big Data processing. The successful validation of this technique was achieved through the exponential growth rate in the number of new data transformations and data types used for Big Data processing in the ATLAS experiment. Table 1 lists representative use cases described below. One of the differences in data processing requirements is that losses are not tolerated for the real data, while the simulated data samples tolerate losses, which reduce the statistics without physics bias.

### Trigger Data Processing

**Stirling[4]** shows that new physics discoveries and high precision studies of rare events require the rejection of the events with "known" processes by more than ten orders of magnitude (Figure 1). The

\begin{table}
\begin{tabular}{l r r r r r} \hline Use Case & Frequency & Workflow & Number of Tasks & Tasks Duration & Data Loss \\  & & Length & & & \\ \hline Trigger Data & Weekly & Short & Several & Day & no \\ Real Data & Yearly & Medium & Hundreds & Weeks & no \\ Simulated Data & Quarterly & Long & Thousands & Months & yes \\ \hline \end{tabular}
\end{table}
Table 1: Data processing use casesmulti-tier trigger system reduces Big Data volume to a manageable level. In 2015, the ATLAS experiment will have a two-tier trigger system:

1. The hardware-based Level 1 trigger.
2. The software-based High-Level Trigger designed by the ATLAS Collaboration [5].

The Trigger Data Processing happens one step before the raw data recording. Thus, any inefficiencies or mistakes may lead to unrecoverable loss of real data. To eliminate such losses, the dedicated raw-to-raw data processing technique is employed to validate trigger software and other critical trigger changes during data taking. This technique is the main tool for commissioning the trigger for data taking.

### Real Data Processing

The "raw" data from the ATLAS detector (Figure 2) are processed to produce the reconstructed data for physics analysis. During reconstruction ATLAS applications process raw detector data to identify and reconstruct physics objects such as leptons. Figure 3 shows the data processing flow used in reconstruction. The distributed multi-tier data processing architecture handles the petascale data flow analyzed in Ref. [6]. Since the detector data are comprised of independent events, massively parallel applications process one event at a time. Events taken during few minutes are collected in one file. Thousands of files with events that are close in time are collected in one dataset.

Figure 1: Comparison of cross-sections and rates for “known” and “rare” events in proton-(anti)proton collisions

Validating these techniques, the ATLAS collaboration completed four petascale data processing campaigns on the Grid, with up to 2 PB of real data being processed every year. Table 2 lists parameters for the ATLAS yearly data processing campaigns. (In 2013 reprocessing, 2.2 PB of input

Figure 3: Data processing workflow for the real data. Arrows are labeled with data transformation applications; boxes are labeled with various data types produced.

Figure 2: The real data for processing come from the ATLAS detector

data were used for selecting about 15% of all events for reconstruction, thus reducing CPU resources vs. the 2012 reprocessing.) campaigns on the Grid, with up to 2 PB of real data being processed every year. Table 2 lists campaigns on the Grid, with up to 2 PB of real data being processed every year. Table 2 lists parameters for the ATLAS yearly data processing campaigns. (In 2013 reprocessing, 2.2 PB of input data were used for selecting about 15% of all events for reconstruction, thus reducing CPU resources vs. the 2012 reprocessing.)

### Simulated Data Processing

The computational resources required to process the simulated data dominate the overall resource usage (Figure 4). The data processing campaigns for the simulated data correspond to the data taking

\begin{table}
\begin{tabular}{l r r} \hline Campaign & Input Data & CPU Time Used for \\ year & Volume (PB) & Reconstruction (\(10^{6}h\)) \\ \hline
2010 & 1 & 2.6 \\
2011 & 1 & 3.1 \\
2012 & 2 & 14.6 \\
2013 & 2 & 4.4 \\ \hline \end{tabular}
\end{table}
Table 2: Processing campaigns for real data

Figure 4: Processing of the simulated data (MC) dominates the consumption of computational resources

[MISSING_PAGE_FAIL:6]

configure hard-processes, hadronize signal and minimum-bias (pileup) events, simulate energy deposition in the ATLAS detector, digitize electronics response, simulate triggers, reconstruct data, transform the reconstructed data into data types for physics analysis, etc. The intermediate outputs are merged and/or filtered as necessary to optimize the chain.

An example of a more complex workflow used to simulate the ATLAS trigger using dedicated hardware for fast tracking (FTK) designed by the ATLAS Collaboration[7] is shown on Figure 7, where to keep the computational resources for the FTK simulation below practical limits, Adelman et al.[8] split every event into 256 \(\eta\)-\(\phi\) sub-regions. In the three-step workflow, each event is processed by 64 jobs; each job simulates tracks in four FTK sub-regions one after another. The sub-region merging is done in two steps: producing whole regions, then whole events in the NTUP_FTK files. The final step uses FTK tracks in trigger simulations producing the reconstructed data in DESD_FTK files or adds FTK tracks to the simulated events in the RDO_FTK files.

Validating the Big Data processing techniques, four different sub-campaigns of the mc11 campaign implemented the pileup conditions, detector conditions and geometry increasingly closer to those in real data. During the mc12 campaign, the majority of the events was simulated in the sub-campaign mc12b. Later, the mc12c sub-campaign implemented an improved detector geometry description. Figure 8 shows the variety of the simulated of event samples sizes for more than 22 000 different datasets produced during mc12 campaign. The goal of the mc14 campaign was to prepare for the 2015 data taking. The 8 TeV events were processed with improved and updated simulation, digitization and reconstruction software while using the same conditions as in the mc12 campaign.

Figure 8: The simulation of the FTK hardware splits every event into 256 sub-events

* [128] The 13 TeV campaign had the center of mass energy expected for the 2015 data taking with estimated pileup and detector conditions. The mc14 campaign used the new ATLAS Integrated Simulations Framework described by Debenedetti [9], with multicore processing becoming the default for major simulated data processing steps: simulation, digitization and reconstruction.

## 3 Multilayer Data Processing System

The LHC shutdown provided an opportunity for upgrading the production system, making implementations more scalable, whilst retaining most valued core capabilities. To assure scalability, the production system was upgraded with extra layers. Avoiding inherent fragility of the monolithic systems, we separated the core concerns: the system logic layer is separated from the presentation layer, providing a familiar but improved interface for task requests.

The upgraded data processing system generates actual workflow tasks and their jobs are executed across more than a hundred distributed computing sites via PanDA - the ATLAS workload management system. Figure 9 shows that on top, the Task Request interface encapsulates the presentation layer for users, while the lower Task Definition layer implements the core data processing

Figure 9: Big Data variety represented by event samples sizes from more than 22 000 different datasets produced during mc12 campaign. Bubble sizes are proportional to the number of the samples, labels correspond to the number of events in the sample, e.g. most of the samples were produced with 5 000 events.

logic that empowers production managers with templated workflow definitions through the Database Engine for Tasks (DEfT) designed by De et al.[10]. At the layer below, Borodin et al.[11] integrated the Job Execution and Definition Interface (JEDI) with PanDA to provide dynamic job definition tailored to the sites capabilities.

In the WLCG distributed computing environment, PanDA provides transparency of data and processing. As a result, the production system sees a unified computing facility that is used to run all data processing for the experiment, even though the sites are physically located all over the world. The production system supports a diverse range of workflows handling centrally ATLAS petascale data processing of the real and simulated data, including the mixture of both.

## 4 Conclusion

During LHC data taking, the ATLAS production system unified a diverse range of workflows and special use cases including processing of the real and the simulated data at large scales. The ATLAS production system fully satisfies the Big Data processing requirements of the ATLAS experiment through the unified approach for real data processing and simulations as well as the mixture of both. Golubkov et al.[12] reported that this technique enabled to address a much wider range of physics analyses, with a higher level of precision, surpassing the most optimistic expectations. In addition, detailed physics studies established that the simulated data are of unprecedented quality compared to previous generations of experiments, describing the detector behavior quite well in most analyses. The unified capabilities for real and simulated data processing significantly enhanced ATLAS physics output, and they motivated production of higher than foreseen simulated data volumes.

In preparations for the LHC data taking, we upgraded the production system to further improve the performance and accommodate a growing number of new requirements and use cases. Currently, the upgraded production system is undergoing commissioning with a growing number of tasks for processing simulated and real data for future scientific discoveries.

## Acknowledgements

We wish to thank all our colleagues who contributed to ATLAS Big Data processing activities. This work was funded in part by the U. S. Department of Energy, Office of Science, High Energy Physics and ASCR Contract No. DE-AC02-98CH10886 and Contract No. DE-AC02-06CH11357. NRC KI team work was funded by the Russian Ministry of Science and Education under Contract No.14 Z50 31 0024.

Figure 10: Multilayer architecture of the ATLAS production system for processing real and simulated data

## References

* [1] The ATLAS Collaboration. The ATLAS experiment at the CERN Large Hadron Collider. _J. Inst._ 2008; 3:S08003.
* [2] The Grid: Blueprint for a New Computing Infrastructure. Kesselman C and Foster I, editors. San Francisco: Morgan Kaufmann; 1999.
* [3] Maeno T et al. Evolution of the ATLAS PanDA production and distributed analysis system _J. Phys.: Conf. Ser._ 2012; 396:032071.
* [4] Stirling WJ, private communication.
* [5] The ATLAS Collaboration. _Technical Design Report for the Phase-I Upgrade of the ATLAS TDAQ System_, LHCC Report CERN-LHCC-2013-018, Geneva: CERN; 2013.
* [6] Vaniachine AV for the ATLAS Collaboration. ATLAS detector data processing on the Grid. In: _IEEE Nuclear Science Symposium and Medical Imaging Conference_; 2011, p. 104-107.
* [7] The ATLAS Collaboration. _ATLAS Fast TracKer (FTK) Technical Design Report,_ LHCC Report CERN-LHCC-2013-007, ATLAS-TDR-021, Geneva: CERN; 2013.
* [8] Adelman J et al. _ATLAS FTK challenge: simulation of a billion-fold hardware parallelism_, ATLAS Note ATL-DAQ-PROC-2014-030, Geneva: CERN; 2014.
* [9] Debenedetti C on behalf of the ATLAS Collaboration. Concepts for fast large scale Monte Carlo production for the ATLAS experiment. _J. Phys.: Conf. Ser._ 2014; 513: 022006.
* [10] De K et al. Task management in the new ATLAS production system. _J. Phys.: Conf. Ser._ 2014; 513:032078.
* [11] Borodin M et al. _Multilevel Workflow System in the ATLAS Experiment_. ATLAS Note ATL-SOFT-PROC-2014-005 Geneva: CERN; 2014.
* [12] Golubkov D et al. ATLAS Grid Data Processing: system evolution and scalability. _J. Phys.: Conf. Ser._ 2012; 396:032049.