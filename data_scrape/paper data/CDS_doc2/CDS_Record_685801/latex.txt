# A Hybrid Approach for the ATLAS Level-2 Trigger

A. Kugel, R. Lay, J. Ludvig, R. Manner, K.-H. Noffz,

S. Ruhl, M. Sessler, H. Simmler, H. Singpiel

Universitat Mannheim, Germany

J.R. Hubbard, P. Le Da, M. Smizanska

Centre D'Etudes Nucleaires De Saclay, France

R. Bock

CERN, Switzerland

###### Abstract

There are several different concepts for the ATLAS level-2 trigger under investigation, all of which have to cope with the following challenges

* The high-luminosity trigger has to cope with an event frequency of 100 kHz. One result of the level-2 demonstrator program was that such an event frequency is a serious problem for present farm solutions.
* For the low-luminosity trigger, the critical item is the processing of the complete data volume of the Transition-Radiation-Tracker (TRT). This leads to an enormous demand in terms of computing power and bandwidth.

These above problems could be resolved by a hybrid trigger, combining an FPGA processor and a processor farm. The FPGA processor layer executes reconstruction algorithms and applies loose physics selection criteria for the 100 kHz trigger. These criteria already reject \(\geq 90\%\) of the events with a high - \(p_{t}\) trigger. For the low-luminosity trigger the feature extraction in the TRT done in FPGAs reduces drastically as well the bandwidth as the computing demand. Due to the FPGA layer, the farm layer can be scaled down in comparison to a pure farm solution and the networking requirements are strongly decreased.

The FPGA processor layer and the farm together build a cost-effective hybrid solution for the level-2 trigger.

## 1 Critical Issues of the Level-2 Trigger

There are **two main tasks** for the ATLAS level-2 trigger, **first** the **reduction** of the **rate from up to 100 kHz after level-1 to about 1 kHz**. For this task full precision data from most of the detectors are available, but only regions of the detector identified by level-1 as containing interesting information are used. This concept reduces the requirements for the level-2 trigger processors because only a few percent of the whole event data stream of 150 GByte/s have to be processed.

The **second task**, the **b-physics**, occurs with a lower frequency and acts only on a subset of detectors. However, due to the special character of b-physics events, the locality information from level-1, the Regions of Interest (RoI), is insufficient for selecting the desired b-physics processes. Therefore for b-physics studies in the low and intermediate luminosity runs the whole detector volume has to be investigated.

In 1997 there was a demonstrator program where the different existing concepts to build the ATLAS level-2 trigger should demonstrate their capability to act as future trigger for the ATLAS detector. The two farm-based trigger architectures showed that existing affordable hardware is not yet suited to cope with a 100 kHz event rate. The trigger has to be designed to cope with 100 kHz event rate, and for this high event rate the networking problem is more serious than the required processing power. On the other hand, the FPGA-based trigger solution presented very good results for high event frequencies, and enables an implementation in a very compact system due to the efficient use of the hardware ressources. The disadvantage of this solution is that for the implementation of complex algorithms there is less gain.

Besides this high event frequency problem there is also the b-physics, where the level-2 trigger has to identify tracks in the complete data volume of the TRT. This and the search down to a transversal momentum of 0.5 GeV makes the full scan TRT algorithm one of the most demanding trigger problems for ATLAS. Animplementation on a farm of standard microprocessors communicating over a switch network is difficult for two reasons: the long execution time on the processors and the nearly coherent data flow into the switch by a large number of ReadOut Buffers (ROB), to a single destination node. The execution times for the current reconstruction algorithms in current state-of-the-art processors make this not feasible, whereas an FPGA processor even now fulfils the requirements. However, the full b-physics task contains not only the track reconstruction in the TRT but also complex physics selection criteria, which would slow down the FPGA processor drastically.

One recognizes that each implementation in a uniform hardware environment has to cope with different tasks where it is not perfect suited. So it is obvious that in a hybrid hardware environment the different hardware devices could fully exploit their strenghtes.

### Description of an FPGA processor

The FPGA layer is mainly based on an **FPGA processor**, so that this section explains the fundamental architecture of such a system.

An FPGA processor consists of a computing core using a set of Field Programmable Gate Arrays (FPGA) which are configurable by software to form a hardware implementation of an algorithm. Further components of the computing core are distributed Static RAM (SRAM) and programmable crossbar switches (FPID). This enables a fully programmable system which offers almost the speed of special-purpose hardware due to parallel and pipelined execution of the algorithm. This paper is based on the FPGA processor **Enable++**[10], which is additionally equipped with a configurable high bandwidth I/O system capable to support different data transmission standards. The system is scalable and adaptable to different detectors and trigger algorithms by reconfiguration.

In comparison to standard processors one (nowadays) moderate computing core board of the FPGA processor Enable\(++\) can replace more than 100 state-of-the-art RISC processors for the initial tracking in the TRT (chapter 4) and for several other algorithms [1].

## 2 A Hybrid Solution for the Level-2 Trigger

In this chapter the hardware of a hybrid 2nd level trigger is presented. It consists of two almost independant layers, the FPGA layer and the farm layer. Since the job of the FPGAs is different for the high and low \(p_{t}\), these two cases are described seperately (figure 1 and 2).

In **both cases** the FPGA layer consists of the FPGA processor and the RoI Collectors (RoIC, Coll.). The RoICs are connected to the ReadOut Buffers (ROB). Furthermore, there is a connection between the supervisors of the FPGA processor and the processor farm to allow a communication between the two layers. The FPGA processor uses only a subset of detectors, because for the steps done by the FPGA layer the TRT, Calorimeter and Muon information is sufficient. A more detailed description of the RoI Collection (RoIC) and the FPGA processor architecture can be found in [11].

For the **high - \(p_{t}\) trigger** the FPGA layer works in a _transparent mode_, so that only the supervisors of the two layers communicate. Both layers are provided with the same data, and for the high - \(p_{t}\) trigger the FPGA-supervisor tells the farm supervisor which events to take and which events are already discarded by the FPGA layer. So the FPGA layer meets a decision and reduces therefore the rate for the succeeding farm. This leads to a reduced average latency which strongly decreases the required memory for the ROBs.

The strategy for the **low - \(p_{t}\) trigger** is different. Here it is envisaged to execute in FPGAs the global pattern

Figure 1: _Hybrid trigger:_ FPGA processor and processor farm for high - \(p_{t}\) triggering. The farm is relieved from the 100 kHz event frequency and only has to cope with 10 kHz.

Figure 2: _Hybrid trigger:_ FPGA processor and processor farm for low - \(p_{t}\) triggering.

recognition using data from the whole TRT volume. The full feature extraction will also be done and only the features with some additional information are sent to the farm. This releases the farm processors from computing tasks for several hundred processors. And the network only has to handle the low bandwidth after the FEX instead of the full TRT data.

The second layer, the **farm layer**, can be a single farm due to the fact that it only copes with the reduced event frequency of 10 kHz. For the b-physics the event frequency, which is anyway low, is not reduced, but the FEX is already done in the FPGAs. In both cases the farm combines different subdetectors and applies physics selection criteria to reduce the rate to the demanded 2nd level output rate. A description of a single farm solution can be found in [1].

## 3 Algorithms of FPGA Processor Layer

Besides the hardware there is also a need of appropriate algorithms which can be implemented in FPGAs, achieve high execution speeds and a good quality of the output. This chapter explains certain algorithms and the next chapter shows the resulting speed and quality. The chosen algorithms refer to the TRT, since most recent studies concern the TRT. Calorimeter studies have already been done years ago with heavier algorithms than requested now [1]. The implementation in FPGAs even worked with nowadays obsolete FPGAs and demonstrated a speedup of almost factor 1000. Muon studies could not be done because we had no reference algorithms and data available.

PreprocessingThe preprocessing task for the TRT is the transformation of raw data obtained from the ROBs to a new data format that is easier to handle for the following steps of the level-2 trigger. This could profitably be done in an FPGA-based coprocessor [2] next to the ROBs, which then reduces the required bandwidth between the ROBs and the 2nd level trigger [1]. The execution time for the preprocessing is not negligible in comparison with the feature extraction, whereas in an FPGA architecture the preprocessing can easily be done in a pipelined fashion without introducing much latency.

Initial track findingBoth the full scan and the RoI scan of the TRT are based on a histogramming method in the Hough space. The bins in the Hough space are predefined and stored in LookUp-Tables (LUT). The histogramming on Enable++ is done in parallel and pipelined, so that within each clock cycle up to 864 histogram counter can be increased. This number of bins is not sufficient for the b-physics due to the full scan down to 0.5 GeV. Consequently, to increase the number of available bins the active straw information has to be passed several times. On the other hand, locality information from the ROBs is used to group the active straws to the corresponding Hough space. This allows a very fast operation with up to 80 000 bins.

The principle of working with LUTs allows to calculate any trajectory in the Hough space. This is useful for the barrel, where the trajectories of physical interest are only approximately straight lines, and is essential for the inhomogeneous magnetic field of the end cap.

Elimination of double tracksOne physical track can be reconstructed several times due to the discrete straw information and the pattern recognition. To facilitate the elimination of so-called double tracks, the LUT is ordered in a way that neighboring bins in the LUT represent spatially adjacent patterns instead of neighbored bins in the Hough space. This allows a more efficient and faster elimination of double tracks, because only local communication is required.

## 4 The Two Roles of the FPGA Processor Layer

The crucial point of the hybrid solution is if any physics losses appear. Therefore after discussing the hardware and the algorithms this chapter refers to physics quality and benchmark studies.

There are two main tasks for the level-2 trigger. For the low-luminosity run, the high - \(p_{t}\) and the low - \(p_{t}\) trigger (b-physics) run simultaneously, whereas for the high-luminosity run, there is only the high - \(p_{t}\) trigger task. These tasks are seperately treated in the following sections.

### FPGAs for the low - \(p_{t}\) trigger

For the low - \(p_{t}\) trigger (b-physics), there is no RoI-guidance from level-1 because of the special character of b-physics events. For the required physics selection criteria there is also a wide variety of physics channels to be investigated in the trigger menu. To allow the anticipated rejection in all of this channels almost all detector information has to be used. Therefore the strategy of the hybrid solution is to combine the advantages of the two different hardware levels. The FPGA layer, well suited for pattern recognition tasks, executes the entire feature extraction in the complete TRT volume. The processor farm, however, combines information from the different subdetectors and applies physics selection criteria.

A key component of the trigger menu for b-physics channels is an electron-pair trigger. This channel is representative for the requirements on the TRT in termsof track finding efficiency and particle identification [17]. Studies of the TRT are chosen, because there is no externally defined RoI in these events, so that the most time demanding step is the track finding in the whole volume of the detector. Since the signatures for many processes involve low - \(p_{t}\) tracks, good efficiency must be maintained down to a transverse momentum of 0.5 GeV. This increases the demand of computing power drastically in comparison to the high - \(p_{t}\) trigger. For the following studies the reconstruction capability of leptons in J/\(\psi\) decays was investigated, which represents the critical issue for the low - \(p_{t}\) trigger.

**Quality of lepton reconstruction in the TRT** These studies were made by providing a good and slow offline algorithm and the Enable\(++\) online algorithm with an identical set of data. The data set consisted of \(bB_{d}^{0}X\to J/\psi(e^{+}e^{-})K_{s}^{0}X^{\prime}\) decays without pile-up. The results from the Enable\(++\) point of view are the following [18]:

* \(\phi\)**-resolution** is **as good as offline** (xrecon)
* track **reconstruction efficiency** of **Enable\(++\)** is the **same as offline** (initial tracking + fine tuning)!
* \(p_{t}\)**-resolution** is **worse than offline**
* average of **11.3 simulated** tracks and **11.9** of **Enable\(++\) reconstructed** tracks (in barrel)

The \(p_{t}\)-resolution of Enable\(++\) is worse due to two reasons. First, the offline algorithm executes a track fitting step including TRT drift time information. Second, Enable\(++\) employs a smaller number of steps in \(p_{t}\) than the offline code. The precision of \(p_{t}\) determination is a matter of repeating the algorithm for a bigger amount of predefined \(p_{t}\) values which consequently increases the computing time linearly. Enable\(++\) uses few steps in \(p_{t}\), since the initial track finding should be followed by a track fitting for refining \(p_{t}\). The simultaneous optimization of both steps is under investigation.

After the histogramming step there is a 2- dimensional maximum finder to eliminate double tracks. This step counts to the initial track finding on Enable\(++\), because there it can be done "on the fly" during the search of the bins exceeding the threshold. In the xrecon code and in the implementation of LUTs on standard processors, the elimination of double tracks are done in a seperate step and do not produce exactly identical results.

**Benchmarking** Benchmarking of even slightly different algorithms is difficult, because computing time has to be seen in function of algorithm quality. Since there is even in modular programs perhaps no level where identical quality of results is produced (e.g. elimination of double tracks, fine tuning), one has to be careful with benchmarking results and should not only compare the times. For these studies comparable running conditions were taken where it was possible.

A benchmarking was done for average full scan TRT events without pile-up including track finding and maximum finding in the barrel (rounded) 1 :

Footnote 1: To estimate execution times for the pattern recognition in the full TRT (barrel and end cap) with an average of 1.8 pile-up events for low-luminosity, the presented times have roughly to be multiplied by a factor of 12. At the same time the fine tuning step is not covered yet, which is essential for the feature extraction.

* offline, xrecon (100 MHz HP): **250 ms**
* LUT-based (400 MHz Alpha): **15 ms**
* LUT-based (50 MHz Enable\(++\)): **140 \(\mu\)s**

The time for the Enable\(++\)-machine already includes Data-I/O, and it can be seen that **1 FPGA processor board equals \(\geq\) 100 standard processors for the track finding task.

### FPGAs for the high - \(p_{t}\) trigger

The high - \(p_{t}\) trigger is the main task of the level-2 trigger. The event frequency should be reduced from 100 kHz to 1 kHz in level-2. A first reduction by a factor of 10 is done with the FPGA layer, whereas the next reduction to 1 kHz or lower is done in the farm. The first step needs an investigation of the Calorimeter, Muon and TRT information. For the trigger requirement with the highest level-1 rate, the search for one isolated electromagnetic cluster, one can even reduce the rate with loose physics selection criteria, based on the Calorimeter and TRT information, by a factor of 18 [17]. Therefore the subsequent studies were made on samples of single electrons with pile-up, to compare the capability of the LUT-based Hough-transform with offline code. For the Calorimeter there exist already older studies.

**Quality and benchmarking of TRT track reconstruction** For the 100 kHz trigger, two studies with different data sets were made. Both compared the results of ATRIG (ATlas TRIGger) reconstruction software with LUT-based track finding.

From the table it can be seen that the reconstruction efficiency (30 GeV e\({}^{-}\)) is even higher with the LUT method. This is due to the higher average number of reconstructed tracks per RoI, which also leads to a higher rate of fake tracks. However, since the results of the reconstruction are not very different, it seems that the two different algorithms produce comparable results. The rejection of jets with the TRT information is at the moment under study, but it is mainly based on the threshold and Calorimeter information. The LUT-based \(p_{t}\) measurement is much worse than the ATRIG offline code, but since the latter code executes a track fitting step exploiting drift time information it should be better. The differences in speed of the LUT-based histogramming and the ATRIG offline (initial tracking + fine tuning) is a factor of 45, so that one can not expect the same results. One should keep in mind that the intended aim of the FPGA layer is a rate reduction factor of 10, which can be performed without a high precision \(p_{t}\)-measurement in the TRT. For the remaining rejection factor 10 from 10 kHz to 1 kHz to be done by the farm it is inevitable to work with a higher reconstruction precision and to combine several subdetectors.

## 5 Conclusions

* Enable\(++\) is well suited for the pattern recognition in the TRT, the **reconstruction efficiency** is **as good as offline**.

* \(p_{t}\) trigger, loose physics selection criteria can already be applied using a subset of detectors and the FPGA processor. This reduces the event rate by a factor of 10. There are **no physics losses expected** due to the **splitting of the reduction** in two steps **from 100 kHz to 10 kHz with FPGAs** and **from 10 kHz to 1 kHz with the farm**. The **size of the farm** can **significantly be reduced**.
* \(p_{t}\) trigger, the entire **feature extraction** of the **TRT** can be done in FPGAs, which is an economy in computing power of **several hundred processors**.
* The processor farm can fully exploit its strenght of highly complex analyses combining several subdetectors and applying sophisticated selection criteria as a result of the reduced event frequency.
* A two-layer trigger allows the combination of the advantages of the different hardware complexities, which are the high speed of the FPGAs and the high complexity of the general-purpose processors.
* The **FPGA processor** layer and the **farm together** build a **cost-effective** hybrid solution for the level-2 trigger.

## References

* [Bel95] D. Belosloudtsev et al. _Programmable active memories in real time tasks: implementing data-driven triggers for LHC experiments_, Nuclear Instruments and Methods in Physics Research, A 356 (1995) 457-467, 1995.
* Algorithm, Implementations and Benchmarks_, March 1997.
* [Bys96] J. Bystricky et al. _A Sequential Processing Strategy For The ATLAS Event Selection_, Nuclear Science Symposium and Conference in Anaheim/ California, 1996.
* [Gal97] Y. Gal et al. _A FPGA-based Coprocessor proposed for Preprocessing in ATLAS_, Proceedings Computing in High Energy Physics (CHEP), April 1997.
* A second generation FPGA processor for ATLAS_, 1994.
* A Hybrid Model of Architecture A_, June 1997.
* [Smi97] M. Smizanska. _Pattern Recognition: Enable\(++\) versus offline in TRT LVL2 global scan at low luminosity_, pres. ATLAS-week, March 1997. [http://wwwcn.cern.ch/](http://wwwcn.cern.ch/)\({}^{\sim}\)msmizans/ algorithm/ 02.html
* [TDR97]_ATLAS Inner Detector Technical Design Report_, April 1997.

Figure 3: _High - \(p_{t}\) electrons (30 GeV) with pile-up_. ATRIG track fitting exploits drift time information, which adds up 61 ms on a 100 MHz HP. Preprocessing time is not considered here. Data-I/O is only included on Enable\(++\). Enable\(++\) time for one computing array board given (scalable).